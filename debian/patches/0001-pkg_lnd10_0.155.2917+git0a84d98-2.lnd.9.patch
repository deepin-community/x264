diff --git a/Makefile b/Makefile
index d8baf659..c1a9c5b0 100644
--- a/Makefile
+++ b/Makefile
@@ -200,13 +200,29 @@ endif
 
 # LOONGARCH optimization
 ifeq ($(SYS_ARCH),LOONGARCH)
+SRCASM_X += common/loongarch/cabac-a.S
 ifneq ($(findstring HAVE_LASX 1, $(CONFIG)),)
+SRCASM_X += common/loongarch/mc-a.S \
+           common/loongarch/sad-a.S \
+           common/loongarch/pixel-a.S \
+           common/loongarch/dct-a.S \
+           common/loongarch/quant-a.S \
+           common/loongarch/predict-a.S
+
 SRCS_X += common/loongarch/pixel-c.c \
           common/loongarch/predict-c.c \
           common/loongarch/quant-c.c \
-          common/loongarch/dct-c.c \
           common/loongarch/mc-c.c \
           common/loongarch/deblock-c.c
+
+OBJASM +=
+ifneq ($(findstring HAVE_BITDEPTH8 1, $(CONFIG)),)
+OBJASM += $(SRCASM_X:%.S=%-8.o)
+endif
+ifneq ($(findstring HAVE_BITDEPTH10 1, $(CONFIG)),)
+OBJASM += $(SRCASM_X:%.S=%-10.o)
+endif
+
 endif
 endif
 
diff --git a/common/cabac.h b/common/cabac.h
index b573416e..8a6eb619 100644
--- a/common/cabac.h
+++ b/common/cabac.h
@@ -84,6 +84,10 @@ void x264_cabac_encode_flush( x264_t *h, x264_cabac_t *cb );
 #define x264_cabac_encode_decision x264_cabac_encode_decision_asm
 #define x264_cabac_encode_bypass x264_cabac_encode_bypass_asm
 #define x264_cabac_encode_terminal x264_cabac_encode_terminal_asm
+#elif defined(ARCH_LOONGARCH) && !HIGH_BIT_DEPTH
+#define x264_cabac_encode_decision x264_cabac_encode_decision_asm
+#define x264_cabac_encode_bypass x264_cabac_encode_bypass_asm
+#define x264_cabac_encode_terminal x264_cabac_encode_terminal_asm
 #else
 #define x264_cabac_encode_decision x264_cabac_encode_decision_c
 #define x264_cabac_encode_bypass x264_cabac_encode_bypass_c
diff --git a/common/dct.c b/common/dct.c
index a24a0f21..149b0c47 100644
--- a/common/dct.c
+++ b/common/dct.c
@@ -734,16 +734,19 @@ void x264_dct_init( int cpu, x264_dct_function_t *dctf )
 #if HAVE_LASX
     if( cpu&X264_CPU_LASX )
     {
-        dctf->sub4x4_dct       = x264_sub4x4_dct_lasx;
+        dctf->sub4x4_dct       = x264_sub4x4_dct_lsx;
         dctf->sub8x8_dct       = x264_sub8x8_dct_lasx;
         dctf->sub16x16_dct     = x264_sub16x16_dct_lasx;
-        dctf->add4x4_idct      = x264_add4x4_idct_lasx;
+        dctf->add4x4_idct      = x264_add4x4_idct_lsx;
         dctf->add8x8_idct      = x264_add8x8_idct_lasx;
         dctf->add8x8_idct8     = x264_add8x8_idct8_lasx;
         dctf->add16x16_idct    = x264_add16x16_idct_lasx;
-        dctf->sub8x8_dct8      = x264_sub8x8_dct8_lasx;
+        dctf->sub8x8_dct8      = x264_sub8x8_dct8_lsx;
         dctf->sub16x16_dct8    = x264_sub16x16_dct8_lasx;
         dctf->add8x8_idct_dc   = x264_add8x8_idct_dc_lasx;
+        dctf->add16x16_idct_dc = x264_add16x16_idct_dc_lasx;
+        dctf->dct4x4dc         = x264_dct4x4dc_lasx;
+        dctf->idct4x4dc        = x264_idct4x4dc_lasx;
     }
 #endif
 
@@ -1107,5 +1110,12 @@ void x264_zigzag_init( int cpu, x264_zigzag_function_t *pf_progressive, x264_zig
         pf_progressive->scan_4x4  = x264_zigzag_scan_4x4_frame_msa;
     }
 #endif
+
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf_progressive->scan_4x4  = x264_zigzag_scan_4x4_frame_lasx;
+    }
+#endif
 #endif // !HIGH_BIT_DEPTH
 }
diff --git a/common/loongarch/asm.S b/common/loongarch/asm.S
new file mode 100644
index 00000000..bb42e6b2
--- /dev/null
+++ b/common/loongarch/asm.S
@@ -0,0 +1,661 @@
+/*****************************************************************************
+ * asm.S: LoongArch utility macros
+ *****************************************************************************
+ * Copyright (C) 2015-2022 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#define GLUE(a, b) a ## b
+#define JOIN(a, b) GLUE(a, b)
+
+#ifdef BIT_DEPTH
+#define  ASM_PREF  JOIN(JOIN(x264_, BIT_DEPTH), _)
+#else
+#define  ASM_PREF  x264_
+#endif  /* BIT_DEPTH */
+
+#define DEFAULT_ALIGN    5
+#define FENC_STRIDE      16
+#define FDEC_STRIDE      32
+
+.macro   function name, export=1, align=DEFAULT_ALIGN
+    .macro   endfunc
+    jirl     $r0, $r1, 0x0
+.if \export
+    .size    ASM_PREF\name, . - ASM_PREF\name
+.else
+    .size    \name, . - \name
+.endif
+    .purgem  endfunc
+    .endm
+.text
+.align    \align
+.if \export
+.globl    ASM_PREF\name
+.type     ASM_PREF\name, @function
+ASM_PREF\name:
+.else
+.globl    \name
+.type     \name, @function
+\name:
+.endif
+.endm
+
+.macro    const name, align=DEFAULT_ALIGN
+    .macro endconst
+    .size  \name, . - \name
+    .purgem endconst
+    .endm
+.section .rodata
+.align   \align
+\name:
+.endm
+
+/**********************************************
+* LoongArch register alias
+***********************************************/
+#define a0 $a0
+#define a1 $a1
+#define a2 $a2
+#define a3 $a3
+#define a4 $a4
+#define a5 $a5
+#define a6 $a6
+#define a7 $a7
+
+#define t0 $t0
+#define t1 $t1
+#define t2 $t2
+#define t3 $t3
+#define t4 $t4
+#define t5 $t5
+#define t6 $t6
+#define t7 $t7
+#define t8 $t8
+
+#define s0 $s0
+#define s1 $s1
+#define s2 $s2
+#define s3 $s3
+#define s4 $s4
+#define s5 $s5
+#define s6 $s6
+#define s7 $s7
+#define s8 $s8
+
+#define zero $zero
+#define sp   $sp
+#define ra   $ra
+
+#define f0  $f0
+#define f1  $f1
+#define f2  $f2
+#define f3  $f3
+#define f4  $f4
+#define f5  $f5
+#define f6  $f6
+#define f7  $f7
+#define f8  $f8
+#define f9  $f9
+#define f10 $f10
+#define f11 $f11
+#define f12 $f12
+#define f13 $f13
+#define f14 $f14
+#define f15 $f15
+#define f16 $f16
+#define f17 $f17
+#define f18 $f18
+#define f19 $f19
+#define f20 $f20
+#define f21 $f21
+#define f22 $f22
+#define f23 $f23
+#define f24 $f24
+#define f25 $f25
+#define f26 $f26
+#define f27 $f27
+#define f28 $f28
+#define f29 $f29
+#define f30 $f30
+#define f31 $f31
+
+#define vr0 $vr0
+#define vr1 $vr1
+#define vr2 $vr2
+#define vr3 $vr3
+#define vr4 $vr4
+#define vr5 $vr5
+#define vr6 $vr6
+#define vr7 $vr7
+#define vr8 $vr8
+#define vr9 $vr9
+#define vr10 $vr10
+#define vr11 $vr11
+#define vr12 $vr12
+#define vr13 $vr13
+#define vr14 $vr14
+#define vr15 $vr15
+#define vr16 $vr16
+#define vr17 $vr17
+#define vr18 $vr18
+#define vr19 $vr19
+#define vr20 $vr20
+#define vr21 $vr21
+#define vr22 $vr22
+#define vr23 $vr23
+#define vr24 $vr24
+#define vr25 $vr25
+#define vr26 $vr26
+#define vr27 $vr27
+#define vr28 $vr28
+#define vr29 $vr29
+#define vr30 $vr30
+#define vr31 $vr31
+
+#define xr0 $xr0
+#define xr1 $xr1
+#define xr2 $xr2
+#define xr3 $xr3
+#define xr4 $xr4
+#define xr5 $xr5
+#define xr6 $xr6
+#define xr7 $xr7
+#define xr8 $xr8
+#define xr9 $xr9
+#define xr10 $xr10
+#define xr11 $xr11
+#define xr12 $xr12
+#define xr13 $xr13
+#define xr14 $xr14
+#define xr15 $xr15
+#define xr16 $xr16
+#define xr17 $xr17
+#define xr18 $xr18
+#define xr19 $xr19
+#define xr20 $xr20
+#define xr21 $xr21
+#define xr22 $xr22
+#define xr23 $xr23
+#define xr24 $xr24
+#define xr25 $xr25
+#define xr26 $xr26
+#define xr27 $xr27
+#define xr28 $xr28
+#define xr29 $xr29
+#define xr30 $xr30
+#define xr31 $xr31
+
+/**********************************************
+* LSX/LASX synthesize instructions
+***********************************************/
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - vj, vk
+ *               Outputs - vd
+ *               Return Type - halfword
+ * =============================================================================
+ */
+
+.macro vdp2.h.bu vd, vj, vk
+    vmulwev.h.bu      \vd,    \vj,    \vk
+    vmaddwod.h.bu     \vd,    \vj,    \vk
+.endm
+
+.macro vdp2.h.bu.b vd, vj, vk
+    vmulwev.h.bu.b    \vd,    \vj,    \vk
+    vmaddwod.h.bu.b   \vd,    \vj,    \vk
+.endm
+
+.macro vdp2.w.h vd, vj, vk
+    vmulwev.w.h       \vd,    \vj,    \vk
+    vmaddwod.w.h      \vd,    \vj,    \vk
+.endm
+
+.macro xvdp2.h.bu xd, xj, xk
+    xvmulwev.h.bu    \xd,    \xj,    \xk
+    xvmaddwod.h.bu   \xd,    \xj,    \xk
+.endm
+
+.macro xvdp2.h.bu.b xd, xj, xk
+    xvmulwev.h.bu.b    \xd,  \xj,    \xk
+    xvmaddwod.h.bu.b   \xd,  \xj,    \xk
+.endm
+
+.macro xvdp2.w.h xd, xj, xk
+    xvmulwev.w.h       \xd,  \xj,    \xk
+    xvmaddwod.w.h      \xd,  \xj,    \xk
+.endm
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of halfword vector elements
+ * Arguments   : Inputs  - vj, vk
+ *               Outputs - vd
+ *               Return Type - twice size of input
+ * =============================================================================
+ */
+
+.macro vdp2add.h.bu vd, vj, vk
+    vmaddwev.h.bu     \vd,    \vj,    \vk
+    vmaddwod.h.bu     \vd,    \vj,    \vk
+.endm
+
+.macro vdp2add.h.bu.b vd, vj, vk
+    vmaddwev.h.bu.b   \vd,    \vj,    \vk
+    vmaddwod.h.bu.b   \vd,    \vj,    \vk
+.endm
+
+.macro vdp2add.w.h vd, vj, vk
+    vmaddwev.w.h      \vd,    \vj,    \vk
+    vmaddwod.w.h      \vd,    \vj,    \vk
+.endm
+
+.macro xvdp2add.h.bu.b xd, xj, xk
+    xvmaddwev.h.bu.b   \xd,  \xj,    \xk
+    xvmaddwod.h.bu.b   \xd,  \xj,    \xk
+.endm
+
+.macro xvdp2add.w.h xd, xj, xk
+    xvmaddwev.w.h      \xd,  \xj,    \xk
+    xvmaddwod.w.h      \xd,  \xj,    \xk
+.endm
+
+/*
+ * =============================================================================
+ * Description : Range each element of vector
+ * clip: vj > vk ? vj : vk && vj < va ? vj : va
+ * clip255: vj < 255 ? vj : 255 && vj > 0 ? vj : 0
+ * =============================================================================
+ */
+
+.macro vclip.h  vd,  vj, vk, va
+    vmax.h    \vd,  \vj,   \vk
+    vmin.h    \vd,  \vd,   \va
+.endm
+
+.macro vclip255.w  vd, vj
+    vmaxi.w   \vd,   \vj,  0
+    vsat.wu   \vd,   \vd,  7
+.endm
+
+.macro xvclip.h  xd,  xj, xk, xa
+    xvmax.h    \xd,  \xj,   \xk
+    xvmin.h    \xd,  \xd,   \xa
+.endm
+
+.macro xvclip255.h  xd, xj
+    xvmaxi.h   \xd,   \xj,  0
+    xvsat.hu   \xd,   \xd,  7
+.endm
+
+.macro xvclip255.w  xd, xj
+    xvmaxi.w   \xd,   \xj,  0
+    xvsat.wu   \xd,   \xd,  7
+.endm
+
+/*
+ * =============================================================================
+ * Description : Store elements of vector
+ * vd : Data vector to be stroed
+ * rk : Address of data storage
+ * ra : Offset of address
+ * si : Index of data in vd
+ * =============================================================================
+ */
+
+.macro vstelmx.b vd, rk, ra, si
+    add.d      \rk,  \rk,  \ra
+    vstelm.b   \vd,  \rk,  0, \si
+.endm
+
+.macro vstelmx.h vd, rk, ra, si
+    add.d      \rk,  \rk,  \ra
+    vstelm.h   \vd,  \rk,  0, \si
+.endm
+
+.macro vstelmx.w vd, rk, ra, si
+    add.d      \rk,  \rk,  \ra
+    vstelm.w   \vd,  \rk,  0, \si
+.endm
+
+.macro vstelmx.d  vd, rk, ra, si
+    add.d      \rk,  \rk,  \ra
+    vstelm.d   \vd,  \rk,  0, \si
+.endm
+
+.macro xmov xd, xj
+    xvor.v  \xd,  \xj,  \xj
+.endm
+
+.macro xvstelmx.d  xd, rk, ra, si
+    add.d      \rk, \rk,  \ra
+    xvstelm.d  \xd, \rk,  0, \si
+.endm
+
+/**********************************************
+* LSX/LASX custom macros
+***********************************************/
+
+.macro LASX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    xvld    \out0,    \src,    0
+    xvldx   \out1,    \src,    \stride
+    xvldx   \out2,    \src,    \stride2
+    xvldx   \out3,    \src,    \stride3
+.endm
+
+.macro LSX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    vld     \out0,    \src,    0
+    vldx    \out1,    \src,    \stride
+    vldx    \out2,    \src,    \stride2
+    vldx    \out3,    \src,    \stride3
+.endm
+
+.macro FLDS_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    fld.s     \out0,    \src,    0
+    fldx.s    \out1,    \src,    \stride
+    fldx.s    \out2,    \src,    \stride2
+    fldx.s    \out3,    \src,    \stride3
+.endm
+
+.macro FLDD_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    fld.d     \out0,    \src,    0
+    fldx.d    \out1,    \src,    \stride
+    fldx.d    \out2,    \src,    \stride2
+    fldx.d    \out3,    \src,    \stride3
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * =============================================================================
+ */
+
+.macro LSX_TRANSPOSE4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                          tmp0, tmp1
+    vilvl.h   \tmp0,  \in1,   \in0
+    vilvl.h   \tmp1,  \in3,   \in2
+    vilvl.w   \out0,  \tmp1,  \tmp0
+    vilvh.w   \out2,  \tmp1,  \tmp0
+    vilvh.d   \out1,  \out0,  \out0
+    vilvh.d   \out3,  \out0,  \out2
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * =============================================================================
+ */
+
+.macro LSX_TRANSPOSE4x4_W in0, in1, in2, in3, out0, out1, out2, out3, \
+                          tmp0, tmp1, tmp2, tmp3
+    vilvl.w    \tmp0, \in1, \in0
+    vilvh.w    \tmp1, \in1, \in0
+    vilvl.w    \tmp2, \in3, \in2
+    vilvh.w    \tmp3, \in3, \in2
+
+    vilvl.d    \out0, \tmp2, \tmp0
+    vilvh.d    \out1, \tmp2, \tmp0
+    vilvl.d    \out2, \tmp3, \tmp1
+    vilvh.d    \out3, \tmp3, \tmp1
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * =============================================================================
+ */
+
+.macro LSX_TRANSPOSE8x8_H in0, in1, in2, in3, in4, in5, in6, in7, out0, out1,   \
+                          out2, out3, out4, out5, out6, out7, tmp0, tmp1, tmp2, \
+                          tmp3, tmp4, tmp5, tmp6, tmp7
+    vilvl.h      \tmp0,    \in6,   \in4
+    vilvl.h      \tmp1,    \in7,   \in5
+    vilvl.h      \tmp2,    \in2,   \in0
+    vilvl.h      \tmp3,    \in3,   \in1
+
+    vilvl.h      \tmp4,    \tmp1,  \tmp0
+    vilvh.h      \tmp5,    \tmp1,  \tmp0
+    vilvl.h      \tmp6,    \tmp3,  \tmp2
+    vilvh.h      \tmp7,    \tmp3,  \tmp2
+
+    vilvh.h      \tmp0,    \in6,   \in4
+    vilvh.h      \tmp1,    \in7,   \in5
+    vilvh.h      \tmp2,    \in2,   \in0
+    vilvh.h      \tmp3,    \in3,   \in1
+
+    vpickev.d    \out0,    \tmp4,  \tmp6
+    vpickod.d    \out1,    \tmp4,  \tmp6
+    vpickev.d    \out2,    \tmp5,  \tmp7
+    vpickod.d    \out3,    \tmp5,  \tmp7
+
+    vilvl.h      \tmp4,    \tmp1,  \tmp0
+    vilvh.h      \tmp5,    \tmp1,  \tmp0
+    vilvl.h      \tmp6,    \tmp3,  \tmp2
+    vilvh.h      \tmp7,    \tmp3,  \tmp2
+
+    vpickev.d    \out4,    \tmp4,  \tmp6
+    vpickod.d    \out5,    \tmp4,  \tmp6
+    vpickev.d    \out6,    \tmp5,  \tmp7
+    vpickod.d    \out7,    \tmp5,  \tmp7
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * =============================================================================
+ */
+
+.macro LASX_TRANSPOSE4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                           tmp0, tmp1
+    xvilvl.h   \tmp0,  \in1,   \in0
+    xvilvl.h   \tmp1,  \in3,   \in2
+    xvilvl.w   \out0,  \tmp1,  \tmp0
+    xvilvh.w   \out2,  \tmp1,  \tmp0
+    xvilvh.d   \out1,  \out0,  \out0
+    xvilvh.d   \out3,  \out0,  \out2
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x8 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * =============================================================================
+ */
+
+.macro LASX_TRANSPOSE4x8_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                           tmp0, tmp1, tmp2, tmp3
+    xvilvl.h      \tmp0,    \in2,   \in0
+    xvilvl.h      \tmp1,    \in3,   \in1
+    xvilvl.h      \tmp2,    \tmp1,  \tmp0
+    xvilvh.h      \tmp3,    \tmp1,  \tmp0
+
+    xvilvl.d      \out0,    \tmp2,  \tmp2
+    xvilvh.d      \out1,    \tmp2,  \tmp2
+    xvilvl.d      \out2,    \tmp3,  \tmp3
+    xvilvh.d      \out3,    \tmp3,  \tmp3
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * =============================================================================
+ */
+
+.macro LASX_TRANSPOSE8x8_H in0, in1, in2, in3, in4, in5, in6, in7,         \
+                           out0, out1, out2, out3, out4, out5, out6, out7, \
+                           tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
+    xvilvl.h     \tmp0,   \in6,     \in4
+    xvilvl.h     \tmp1,   \in7,     \in5
+    xvilvl.h     \tmp2,   \in2,     \in0
+    xvilvl.h     \tmp3,   \in3,     \in1
+
+    xvilvl.h     \tmp4,   \tmp1,    \tmp0
+    xvilvh.h     \tmp5,   \tmp1,    \tmp0
+    xvilvl.h     \tmp6,   \tmp3,    \tmp2
+    xvilvh.h     \tmp7,   \tmp3,    \tmp2
+
+    xvilvh.h     \tmp0,   \in6,     \in4
+    xvilvh.h     \tmp1,   \in7,     \in5
+    xvilvh.h     \tmp2,   \in2,     \in0
+    xvilvh.h     \tmp3,   \in3,     \in1
+
+    xvpickev.d   \out0,   \tmp4,    \tmp6
+    xvpickod.d   \out1,   \tmp4,    \tmp6
+    xvpickev.d   \out2,   \tmp5,    \tmp7
+    xvpickod.d   \out3,   \tmp5,    \tmp7
+
+    xvilvl.h     \tmp4,   \tmp1,    \tmp0
+    xvilvh.h     \tmp5,   \tmp1,    \tmp0
+    xvilvl.h     \tmp6,   \tmp3,    \tmp2
+    xvilvh.h     \tmp7,   \tmp3,    \tmp2
+
+    xvpickev.d   \out4,   \tmp4,    \tmp6
+    xvpickod.d   \out5,   \tmp4,    \tmp6
+    xvpickev.d   \out6,   \tmp5,    \tmp7
+    xvpickod.d   \out7,   \tmp5,    \tmp7
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * =============================================================================
+ */
+
+.macro LASX_TRANSPOSE8x8_W in0, in1, in2, in3, in4, in5, in6, in7,         \
+                           out0, out1, out2, out3, out4, out5, out6, out7, \
+                           tmp0, tmp1, tmp2, tmp3
+    xvilvl.w    \tmp0,    \in2,     \in0
+    xvilvl.w    \tmp1,    \in3,     \in1
+    xvilvh.w    \tmp2,    \in2,     \in0
+    xvilvh.w    \tmp3,    \in3,     \in1
+    xvilvl.w    \out0,    \tmp1,    \tmp0
+    xvilvh.w    \out1,    \tmp1,    \tmp0
+    xvilvl.w    \out2,    \tmp3,    \tmp2
+    xvilvh.w    \out3,    \tmp3,    \tmp2
+
+    xvilvl.w    \tmp0,    \in6,     \in4
+    xvilvl.w    \tmp1,    \in7,     \in5
+    xvilvh.w    \tmp2,    \in6,     \in4
+    xvilvh.w    \tmp3,    \in7,     \in5
+    xvilvl.w    \out4,    \tmp1,    \tmp0
+    xvilvh.w    \out5,    \tmp1,    \tmp0
+    xvilvl.w    \out6,    \tmp3,    \tmp2
+    xvilvh.w    \out7,    \tmp3,    \tmp2
+
+    xmov        \tmp0,    \out0
+    xmov        \tmp1,    \out1
+    xmov        \tmp2,    \out2
+    xmov        \tmp3,    \out3
+    xvpermi.q   \out0,    \out4,    0x02
+    xvpermi.q   \out1,    \out5,    0x02
+    xvpermi.q   \out2,    \out6,    0x02
+    xvpermi.q   \out3,    \out7,    0x02
+    xvpermi.q   \out4,    \tmp0,    0x31
+    xvpermi.q   \out5,    \tmp1,    0x31
+    xvpermi.q   \out6,    \tmp2,    0x31
+    xvpermi.q   \out7,    \tmp3,    0x31
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 16x8 block with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * =============================================================================
+ */
+
+.macro LASX_TRANSPOSE16X8_B in0, in1, in2, in3, in4, in5, in6, in7,        \
+                            in8, in9, in10, in11, in12, in13, in14, in15,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7,\
+                            tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
+    xvilvl.b   \tmp0,    \in2,     \in0
+    xvilvl.b   \tmp1,    \in3,     \in1
+    xvilvl.b   \tmp2,    \in6,     \in4
+    xvilvl.b   \tmp3,    \in7,     \in5
+    xvilvl.b   \tmp4,    \in10,    \in8
+    xvilvl.b   \tmp5,    \in11,    \in9
+    xvilvl.b   \tmp6,    \in14,    \in12
+    xvilvl.b   \tmp7,    \in15,    \in13
+    xvilvl.b   \out0,    \tmp1,    \tmp0
+    xvilvh.b   \out1,    \tmp1,    \tmp0
+    xvilvl.b   \out2,    \tmp3,    \tmp2
+    xvilvh.b   \out3,    \tmp3,    \tmp2
+    xvilvl.b   \out4,    \tmp5,    \tmp4
+    xvilvh.b   \out5,    \tmp5,    \tmp4
+    xvilvl.b   \out6,    \tmp7,    \tmp6
+    xvilvh.b   \out7,    \tmp7,    \tmp6
+    xvilvl.w   \tmp0,    \out2,    \out0
+    xvilvh.w   \tmp2,    \out2,    \out0
+    xvilvl.w   \tmp4,    \out3,    \out1
+    xvilvh.w   \tmp6,    \out3,    \out1
+    xvilvl.w   \tmp1,    \out6,    \out4
+    xvilvh.w   \tmp3,    \out6,    \out4
+    xvilvl.w   \tmp5,    \out7,    \out5
+    xvilvh.w   \tmp7,    \out7,    \out5
+    xvilvl.d   \out0,    \tmp1,    \tmp0
+    xvilvh.d   \out1,    \tmp1,    \tmp0
+    xvilvl.d   \out2,    \tmp3,    \tmp2
+    xvilvh.d   \out3,    \tmp3,    \tmp2
+    xvilvl.d   \out4,    \tmp5,    \tmp4
+    xvilvh.d   \out5,    \tmp5,    \tmp4
+    xvilvl.d   \out6,    \tmp7,    \tmp6
+    xvilvh.d   \out7,    \tmp7,    \tmp6
+.endm
+
+/*
+ * =============================================================================
+ * Description : Transpose 2x4x4 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * =============================================================================
+ */
+
+.macro LASX_TRANSPOSE2x4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                             tmp0, tmp1, tmp2, tmp3, tmp4
+    xvilvh.h   \tmp4,    \in0,     \in1
+    xvilvl.h   \tmp1,    \in0,     \in1
+    xvilvh.h   \tmp0,    \in2,     \in3
+    xvilvl.h   \tmp3,    \in2,     \in3
+    xvilvh.w   \tmp2,    \tmp3,    \tmp1
+    xvilvl.w   \tmp1,    \tmp3,    \tmp1
+    xvilvh.w   \tmp3,    \tmp0,    \tmp4
+    xvilvl.w   \tmp4,    \tmp0,    \tmp4
+    xvilvh.d   \out0,    \tmp4,    \tmp1
+    xvilvl.d   \out2,    \tmp4,    \tmp1
+    xvilvh.d   \out1,    \tmp3,    \tmp2
+    xvilvl.d   \out3,    \tmp3,    \tmp2
+.endm
diff --git a/common/loongarch/cabac-a.S b/common/loongarch/cabac-a.S
new file mode 100644
index 00000000..622ac399
--- /dev/null
+++ b/common/loongarch/cabac-a.S
@@ -0,0 +1,222 @@
+/*****************************************************************************
+ * cabac-a.S: LoongArch arithmetic coder
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *          ytn <yuantingning@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+
+#if !HIGH_BIT_DEPTH
+
+#define LOW_OFFSET 0
+#define RANGE_OFFSET 4
+#define QUEUE_OFFSET 8
+#define OUTSTANDING_OFFSET 12
+#define P_OFFSET 24
+#define STATE_OFFSET 68
+
+/* void cabac_encode_terminal( x264_cabac_t *cb )
+ */
+function cabac_encode_terminal_asm
+    ld.w        t1, a0, RANGE_OFFSET         /* t1: cb->i_range */
+    ld.w        t2, a0, LOW_OFFSET
+    ld.w        t5, a0, QUEUE_OFFSET
+    addi.w      t1, t1, -2
+    srai.w      t4, t1, 3                    /* t4: cb->i_range>>3 */
+    la.global   t3, x264_cabac_renorm_shift
+    ldx.b       t4, t3, t4
+    sll.w       t1, t1, t4                   /* t1: cb->i_range <<= shift */
+    sll.w       t2, t2, t4                   /* t2: cb->i_low <<= shift */
+    add.w       t5, t5, t4                   /* t5: cb->i_queue += shift */
+    blt         t5, zero, .STORE_TERMINAL
+    addi.w      t3, t5, 10
+    sra.w       t4, t2, t3
+    li.w        t6, 0x400
+    sll.w       t3, t6, t5
+    addi.w      t3, t3, -1
+    and         t2, t2, t3
+    addi.w      t5, t5, -8
+
+    andi        t7, t4, 0xff
+    ld.w        t3, a0, OUTSTANDING_OFFSET   /* t3: cb->outstanding */
+    li.w        t6, 0xff
+    beq         t7, t6, .EQUAL_TERMINAL
+    srli.w      t6, t4, 8
+    ld.d        t7, a0, P_OFFSET
+    ld.bu       t8, t7, -1                   /* t8: cb->p[-1] */
+    add.w       t8, t8, t6
+    st.b        t8, t7, -1
+
+    addi.w      t6, t6, -1
+.STARTWHILE_TERMINAL:
+    bge         zero, t3, .ENDWHILE_TERMINAL  /* begin while */
+    addi.w      t3, t3, -1                    /* t3: bytes_outstanding-- */
+    st.b        t6, t7, 0
+    addi.d      t7, t7, 1
+    b          .STARTWHILE_TERMINAL
+.ENDWHILE_TERMINAL:
+    st.b        t4, t7, 0                     /* *(cb->p++)=out */
+    addi.d      t7, t7, 1
+    st.d        t7, a0, P_OFFSET
+    st.w        zero, a0, OUTSTANDING_OFFSET
+    b          .STORE_TERMINAL
+.EQUAL_TERMINAL:
+    addi.w      t3, t3, 1
+    st.w        t3, a0, OUTSTANDING_OFFSET
+.STORE_TERMINAL:
+    st.w        t1, a0, RANGE_OFFSET
+    st.w        t2, a0, LOW_OFFSET
+    st.w        t5, a0, QUEUE_OFFSET
+.ENDFUNC_TERMINAL:
+endfunc
+
+function cabac_encode_bypass_asm
+    ld.w        t1, a0, RANGE_OFFSET
+    ld.w        t2, a0, LOW_OFFSET
+    ld.w        t5, a0, QUEUE_OFFSET
+    slli.w      t2, t2, 1
+    and         t6, a1, t1
+    add.w       t2, t2, t6
+    addi.w      t5, t5, 1
+    blt         t5, zero, .STORE_BYPASS
+    addi.w      t3, t5, 10
+    srl.w       t4, t2, t3
+    li.w        t6, 0x400
+    sll.w       t3, t6, t5
+    addi.w      t5, t5, -8
+    addi.w      t3, t3, -1
+    and         t2, t2, t3
+
+    andi        t7, t4, 0xff
+    ld.w        t3, a0, OUTSTANDING_OFFSET      /* t3: bytes_outstanding */
+    li.w        t6, 0xff
+    beq         t7, t6, .EQUAL_BYPASS
+    srli.w      t6, t4, 8
+    ld.d        t7, a0, P_OFFSET
+    ld.bu       t8, t7, -1                      /* t8: cb->p[-1] */
+    add.w       t8, t8, t6                      /* t8: cb->p[-1]+=carry */
+    st.b        t8, t7, -1
+
+    addi.w      t6, t6, -1
+.STARTWHILE_BYPASS:
+    bge         zero, t3, .ENDWHILE_BYPASS      /* begin while */
+    addi.w      t3, t3, -1                      /* t3: bytes_outstanding-- */
+    st.b        t6, t7, 0
+    addi.d      t7, t7, 1
+    b          .STARTWHILE_BYPASS
+.ENDWHILE_BYPASS:
+    st.b        t4, t7, 0                       /* *(cb->p++)=out */
+    addi.d      t7, t7, 1
+    st.d        t7, a0, P_OFFSET
+    st.w        zero, a0, OUTSTANDING_OFFSET
+    b          .STORE_BYPASS
+.EQUAL_BYPASS:
+    addi.w      t3, t3, 1
+    st.w        t3, a0, OUTSTANDING_OFFSET
+.STORE_BYPASS:
+    st.w        t1, a0, RANGE_OFFSET
+    st.w        t2, a0, LOW_OFFSET
+    st.w        t5, a0, QUEUE_OFFSET
+.ENDFUNC_BYPASS:
+endfunc
+
+function cabac_encode_decision_asm
+    addi.d      a3, a0, STATE_OFFSET
+    ldx.bu      a4, a3, a1            /* i_state */
+    srli.w      a5, a4, 1             /* a5: i_state>>1 */
+    ld.w        t1, a0, RANGE_OFFSET  /* t1: cb->i_range */
+    ld.w        t2, a0, LOW_OFFSET
+    ld.w        t5, a0, QUEUE_OFFSET
+    srli.w      a6, t1, 6             /* a6: cb->i_range>>6 */
+    addi.w      a6, a6, -4            /* a6: (cb->i_range>>6)-4 */
+    la.global   t3, x264_cabac_range_lps
+    li.w        t6, 4
+    mul.w       t6, t6, a5
+    add.w       t6, t6, a6
+    ldx.bu      t4, t3, t6            /* t4: i_range_lps */
+    sub.w       t1, t1, t4
+    li.w        t6, 1
+    and         t6, t6, a4
+    beq         a2, t6, .ENDIF_DECISION
+    add.w       t2, t2, t1
+    addi.w      t1, t4, 0
+.ENDIF_DECISION:
+    la.global   t3, x264_cabac_transition
+    li.w        t6, 2
+    mul.w       t6, t6, a4
+    add.w       t6, t6, a2
+    ldx.bu      t4, t3, t6
+    stx.b       t4, a3, a1
+
+    srai.w      t6, t1, 3
+    la.global   t3, x264_cabac_renorm_shift
+    ldx.b       t4, t3, t6
+    sll.w       t1, t1, t4
+    sll.w       t2, t2, t4
+    add.w       t5, t5, t4
+
+    blt         t5, zero, .STORE_DECISION
+    addi.w      t3, t5, 10
+    sra.w       t4, t2, t3
+    li.w        t6, 0x400
+    sll.w       t3, t6, t5
+    addi.w      t5, t5, -8
+    addi.w      t3, t3, -1
+    and         t2, t2, t3
+
+    andi        t7, t4, 0xff
+    ld.w        t3, a0, OUTSTANDING_OFFSET
+    li.w        t6, 0xff
+    beq         t7, t6, .EQUAL_DECISION
+    srli.w      t6, t4, 8
+    ld.d        t7, a0, P_OFFSET
+    ld.bu       t8, t7, -1                  /* t8: cb->p[-1] */
+    add.w       t8, t8, t6                  /* t8: cb->p[-1]+=carry */
+    st.b        t8, t7, -1
+
+    addi.w      t6, t6, -1                  /* t6: carry -1 */
+.STARTWHILE_DECISION:
+    bge         zero, t3, .ENDWHILE_DECISION  /* begin while */
+    addi.w      t3, t3, -1                  /* t3: bytes_outstanding-- */
+    st.b        t6, t7, 0
+    addi.d      t7, t7, 1
+    b          .STARTWHILE_DECISION
+.ENDWHILE_DECISION:
+    st.b        t4, t7, 0                   /* *(cb->p++)=out */
+    addi.d      t7, t7, 1
+    st.d        t7, a0, P_OFFSET
+    st.w        zero, a0, OUTSTANDING_OFFSET
+    b          .STORE_DECISION
+.EQUAL_DECISION:
+    ld.w        t3, a0, OUTSTANDING_OFFSET  /* t3: cb->outstanding */
+    addi.w      t3, t3, 1
+    st.w        t3, a0, OUTSTANDING_OFFSET
+.STORE_DECISION:
+    st.w        t1, a0, RANGE_OFFSET
+    st.w        t2, a0, LOW_OFFSET
+    st.w        t5, a0, QUEUE_OFFSET
+.ENDFUNC_DECISION:
+endfunc
+#endif
+
diff --git a/common/loongarch/dct-a.S b/common/loongarch/dct-a.S
new file mode 100644
index 00000000..41d14682
--- /dev/null
+++ b/common/loongarch/dct-a.S
@@ -0,0 +1,982 @@
+/*****************************************************************************
+ * dct-a.S: LoongArch transform and zigzag
+ *****************************************************************************
+ * Copyright (C) 2015-2022 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+#include "asm.S"
+
+const hsub_mul
+.rept 16
+.byte 1, -1
+.endr
+endconst
+
+const last64_shuf
+.int 0, 4, 1, 5, 2, 6, 3, 7
+endconst
+
+const zigzag_scan4
+.short 0, 4, 1, 2, 5, 8, 12, 9, 6, 3, 7, 10, 13, 14, 11, 15
+endconst
+
+.macro LOAD_DIFF8x4_LASX s1, s2, s3, s4, s5, s6, s7, s8, s9, s10
+    fld.d           $f\s1,  a1,      FENC_STRIDE * \s7
+    fld.d           $f\s2,  a1,      FENC_STRIDE * \s8
+    fld.d           $f\s5,  a1,      FENC_STRIDE * \s9
+    fld.d           $f\s6,  a1,      FENC_STRIDE * \s10
+    xvinsve0.d      $xr\s1, $xr\s5,  2
+    xvinsve0.d      $xr\s2, $xr\s6,  2
+    fld.d           $f\s3,  a2,      FDEC_STRIDE * \s7
+    fld.d           $f\s4,  a2,      FDEC_STRIDE * \s8
+    fld.d           $f\s5,  a2,      FDEC_STRIDE * \s9
+    fld.d           $f\s6,  a2,      FDEC_STRIDE * \s10
+    xvinsve0.d      $xr\s3, $xr\s5,  2
+    xvinsve0.d      $xr\s4, $xr\s6,  2
+    xvilvl.b        $xr\s1, xr8,     $xr\s1
+    xvilvl.b        $xr\s2, xr8,     $xr\s2
+    xvilvl.b        $xr\s3, xr8,     $xr\s3
+    xvilvl.b        $xr\s4, xr8,     $xr\s4
+    xvsub.h         $xr\s1, $xr\s1,  $xr\s3
+    xvsub.h         $xr\s2, $xr\s2,  $xr\s4
+.endm
+
+.macro DCT4_1D_LASX s0, s1, s2, s3, s4
+    xvadd.h         \s4,    \s3,     \s0
+    xvsub.h         \s0,    \s0,     \s3
+    xvadd.h         \s3,    \s2,     \s1
+    xvsub.h         \s1,    \s1,     \s2
+    xvadd.h         \s2,    \s3,     \s4
+    xvsub.h         \s4,    \s4,     \s3
+    xvsub.h         \s3,    \s0,     \s1
+    xvsub.h         \s3,    \s3,     \s1
+    xvadd.h         \s0,    \s0,     \s0
+    xvadd.h         \s0,    \s0,     \s1
+.endm
+
+.macro LSX_SUMSUB_H sum, sub, a, b
+    vadd.h          \sum,   \a,      \b
+    vsub.h          \sub,   \a,      \b
+.endm
+
+.macro DCT4_1D_LSX s0, s1, s2, s3, s4, s5, s6, s7
+    LSX_SUMSUB_H \s1, \s6, \s5, \s6
+    LSX_SUMSUB_H \s3, \s7, \s4, \s7
+    vadd.h          \s0,    \s3,     \s1
+    vadd.h          \s4,    \s7,     \s7
+    vadd.h          \s5,    \s6,     \s6
+    vsub.h          \s2,    \s3,     \s1
+    vadd.h          \s1,    \s4,     \s6
+    vsub.h          \s3,    \s7,     \s5
+.endm
+
+.macro SUB8x8_DCT_CORE_LASX
+    LOAD_DIFF8x4_LASX 0, 1, 2, 3, 4, 5, 0, 1, 4, 5
+    LOAD_DIFF8x4_LASX 2, 3, 4, 5, 6, 7, 2, 3, 6, 7
+    DCT4_1D_LASX xr0, xr1, xr2, xr3, xr4
+    LASX_TRANSPOSE2x4x4_H xr0, xr2, xr3, xr4, xr0, xr1, \
+                          xr2, xr3, xr0, xr2, xr3, xr4, xr1
+
+    DCT4_1D_LASX xr2, xr0, xr3, xr1, xr4
+    xvilvh.d        xr0,    xr2,     xr3 /* 6, 2 */
+    xvilvl.d        xr3,    xr2,     xr3 /* 4, 0 */
+    xvilvh.d        xr2,    xr1,     xr4 /* 7, 3 */
+    xvilvl.d        xr4,    xr1,     xr4 /* 5, 1 */
+    xvor.v          xr1,    xr3,     xr3
+    xvpermi.q       xr3,    xr4,     0x02 /* 1, 0 */
+    xvor.v          xr5,    xr0,     xr0
+    xvpermi.q       xr0,    xr2,     0x02 /* 3, 2 */
+    xvpermi.q       xr1,    xr4,     0x13 /* 4, 5 */
+    xvpermi.q       xr5,    xr2,     0x13 /* 7, 6 */
+    xvst            xr3,    a0,      0
+    xvst            xr0,    a0,      16 * 2
+    xvst            xr1,    a0,      16 * 4
+    xvst            xr5,    a0,      16 * 6
+.endm
+
+/* void subwxh_dct( dctcoef*, pixel*, pixel* ) */
+function sub4x4_dct_lsx
+    fld.s           f0,     a1,      0
+    fld.s           f4,     a2,      0
+    fld.s           f1,     a1,      FENC_STRIDE
+    fld.s           f5,     a2,      FDEC_STRIDE
+    vext2xv.hu.bu   xr0,    xr0
+    vext2xv.hu.bu   xr1,    xr1
+    vext2xv.hu.bu   xr4,    xr4
+    vext2xv.hu.bu   xr5,    xr5
+    fld.s           f2,     a1,      FENC_STRIDE * 2
+    fld.s           f6,     a2,      FDEC_STRIDE * 2
+    fld.s           f3,     a1,      FENC_STRIDE * 3
+    fld.s           f7,     a2,      FDEC_STRIDE * 3
+    vext2xv.hu.bu   xr2,    xr2
+    vext2xv.hu.bu   xr3,    xr3
+    vext2xv.hu.bu   xr6,    xr6
+    vext2xv.hu.bu   xr7,    xr7
+    vsub.h          vr0,    vr0,     vr4
+    vsub.h          vr1,    vr1,     vr5
+    vsub.h          vr2,    vr2,     vr6
+    vsub.h          vr3,    vr3,     vr7
+
+    DCT4_1D_LSX vr4, vr5, vr6, vr7, vr0, vr1, vr2, vr3
+    LSX_TRANSPOSE4x4_H vr4, vr5, vr6, vr7, vr4, vr5, vr6, vr7, vr0, vr1
+    DCT4_1D_LSX vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vshuf4i.d       vr0,    vr1,     0x8
+    vshuf4i.d       vr2,    vr3,     0x8
+    vst             vr0,    a0,      0
+    vst             vr2,    a0,      16
+endfunc
+
+function sub8x8_dct_lasx
+    xvxor.v         xr8,    xr8,     xr8
+    SUB8x8_DCT_CORE_LASX
+endfunc
+
+function sub16x16_dct_lasx
+    xvxor.v  xr8,  xr8,  xr8
+    SUB8x8_DCT_CORE_LASX
+    addi.d         a0,      a0,    32 * 4
+    addi.d         a1,      a1,    8
+    addi.d         a2,      a2,    8
+    SUB8x8_DCT_CORE_LASX
+    addi.d         a0,      a0,    32 * 4
+    addi.d         a1,      a1,    8*FENC_STRIDE - 8
+    addi.d         a2,      a2,    8*FDEC_STRIDE - 8
+    SUB8x8_DCT_CORE_LASX
+    addi.d         a0,      a0,    32 * 4
+    addi.d         a1,      a1,    8
+    addi.d         a2,      a2,    8
+    SUB8x8_DCT_CORE_LASX
+endfunc
+
+/*
+ * void add4x4_idct( pixel *p_dst, dctcoef dct[16] )
+ */
+function add4x4_idct_lsx
+    vxor.v          vr0,    vr1,   vr1
+
+    fld.d           f1,     a1,    0
+    fld.d           f2,     a1,    8
+    fld.d           f3,     a1,    16
+    fld.d           f4,     a1,    24
+
+    vsrai.h         vr5,    vr2,   1
+    vsrai.h         vr6,    vr4,   1
+
+    vilvl.h         vr1,    vr1,   vr3
+    vilvl.h         vr15,   vr2,   vr6
+    vilvl.h         vr16,   vr5,   vr4
+
+    vhaddw.w.h      vr7,    vr1,  vr1
+    vhsubw.w.h      vr8,    vr1,  vr1
+    vhaddw.w.h      vr9,    vr15, vr15
+    vhsubw.w.h      vr10,   vr16, vr16
+
+    vadd.w          vr1,    vr7,  vr9
+    vadd.w          vr2,    vr8,  vr10
+    vsub.w          vr3,    vr8,  vr10
+    vsub.w          vr4,    vr7,  vr9
+
+    vpickev.h       vr1,    vr1,  vr1
+    vpickev.h       vr2,    vr2,  vr2
+    vpickev.h       vr3,    vr3,  vr3
+    vpickev.h       vr4,    vr4,  vr4
+
+    LSX_TRANSPOSE4x4_H vr1, vr2, vr3, vr4, vr1, vr2, vr3, vr4, vr5, vr6
+
+    vsrai.h         vr5,    vr2,   1
+    vsrai.h         vr6,    vr4,   1
+
+    vilvl.h         vr1,    vr1,   vr3
+    vilvl.h         vr15,   vr2,   vr6
+    vilvl.h         vr16,   vr5,   vr4
+
+    vhaddw.w.h      vr7,    vr1,   vr1
+    vhsubw.w.h      vr8,    vr1,   vr1
+    vhaddw.w.h      vr9,    vr15,  vr15
+    vhsubw.w.h      vr10,   vr16,  vr16
+
+    vadd.w          vr1,    vr7,   vr9
+    vadd.w          vr2,    vr8,   vr10
+    vsub.w          vr3,    vr8,   vr10
+    vsub.w          vr4,    vr7,   vr9
+
+    vssrarni.h.w    vr2,    vr1,   6
+    vssrarni.h.w    vr4,    vr3,   6
+
+    fld.s           f1,     a0,    0
+    fld.s           f5,     a0,    FDEC_STRIDE
+    fld.s           f3,     a0,    FDEC_STRIDE * 2
+    fld.s           f6,     a0,    FDEC_STRIDE * 3
+
+    vilvl.b         vr1,    vr0,   vr1
+    vilvl.b         vr5,    vr0,   vr5
+    vilvl.b         vr3,    vr0,   vr3
+    vilvl.b         vr6,    vr0,   vr6
+
+    vilvl.d         vr1,    vr5,   vr1
+    vilvl.d         vr3,    vr6,   vr3
+    vadd.h          vr7,    vr1,   vr2
+    vadd.h          vr8,    vr3,   vr4
+
+    vssrarni.bu.h   vr8,    vr7,   0
+
+    vstelm.w        vr8,   a0,     0,                  0
+    vstelm.w        vr8,   a0,     FDEC_STRIDE,        1
+    vstelm.w        vr8,   a0,     FDEC_STRIDE * 2,    2
+    vstelm.w        vr8,   a0,     FDEC_STRIDE * 3,    3
+endfunc
+
+.macro LASX_SUMSUB_W sum, diff, in0, in1
+    xvadd.w         \sum,   \in0,  \in1
+    xvsub.w         \diff,  \in0,  \in1
+.endm
+
+.macro add8x4_idct_core_lasx
+    fld.d           f1,     a1,    0
+    fld.d           f2,     a1,    8
+    fld.d           f3,     a1,    16
+    fld.d           f4,     a1,    24
+
+    fld.d           f5,     a1,    32
+    fld.d           f6,     a1,    40
+    fld.d           f7,     a1,    48
+    fld.d           f8,     a1,    56
+
+    xvinsve0.d      xr1,    xr5,   1
+    xvinsve0.d      xr2,    xr6,   1
+    xvinsve0.d      xr3,    xr7,   1
+    xvinsve0.d      xr4,    xr8,   1
+
+    xvsrai.h        xr8,    xr2,   1
+    xvsrai.h        xr9,    xr4,   1
+
+    vext2xv.w.h     xr1,    xr1
+    vext2xv.w.h     xr5,    xr2
+    vext2xv.w.h     xr6,    xr3
+    vext2xv.w.h     xr7,    xr4
+    vext2xv.w.h     xr8,    xr8
+    vext2xv.w.h     xr9,    xr9
+
+    LASX_SUMSUB_W xr10, xr11, xr1, xr6
+    xvadd.w         xr12,   xr5,   xr9
+    xvsub.w         xr13,   xr8,   xr7
+
+    LASX_SUMSUB_W xr6, xr9, xr10, xr12
+    LASX_SUMSUB_W xr7, xr8, xr11, xr13
+
+    xvpickev.h      xr10,   xr6,   xr6
+    xvpickev.h      xr11,   xr7,   xr7
+    xvpickev.h      xr12,   xr8,   xr8
+    xvpickev.h      xr13,   xr9,   xr9
+
+    LASX_TRANSPOSE4x8_H xr10, xr11, xr12, xr13, xr10, xr11, xr12, xr13, \
+                        xr4, xr5, xr6, xr7
+
+    xvsllwil.w.h    xr10,   xr10,  0
+    xvsllwil.w.h    xr11,   xr11,  0
+    xvsllwil.w.h    xr12,   xr12,  0
+    xvsllwil.w.h    xr13,   xr13,  0
+    xvsrai.w        xr14,   xr11,  1
+    xvsrai.w        xr15,   xr13,  1
+
+    LASX_SUMSUB_W xr4, xr5, xr10, xr12
+    xvadd.w         xr6,    xr11,  xr15
+    xvsub.w         xr7,    xr14,  xr13
+
+    LASX_SUMSUB_W xr10, xr13, xr4, xr6
+    LASX_SUMSUB_W xr11, xr12, xr5, xr7
+
+    xvssrarni.h.w   xr11,   xr10,  6
+    xvssrarni.h.w   xr13,   xr12,  6
+
+    fld.s           f1,     a0,    0
+    fld.s           f2,     a0,    FDEC_STRIDE
+    fld.s           f3,     a0,    FDEC_STRIDE * 2
+    fld.s           f4,     a0,    FDEC_STRIDE * 3
+
+    fld.s           f5,     a0,    4
+    fld.s           f6,     a0,    FDEC_STRIDE + 4
+    fld.s           f7,     a0,    FDEC_STRIDE * 2 + 4
+    fld.s           f8,     a0,    FDEC_STRIDE * 3 + 4
+
+    xvinsve0.w      xr1,    xr2,   1
+    xvinsve0.w      xr3,    xr4,   1
+    xvinsve0.w      xr5,    xr6,   1
+    xvinsve0.w      xr7,    xr8,   1
+
+    xvinsve0.d      xr1,    xr5,   2
+    xvinsve0.d      xr3,    xr7,   2
+
+    xvilvl.b        xr1,    xr0,   xr1
+    xvilvl.b        xr3,    xr0,   xr3
+
+    xvadd.h         xr1,    xr1,   xr11
+    xvadd.h         xr3,    xr3,   xr13
+
+    xvssrarni.bu.h  xr3,    xr1,   0
+
+    xvstelm.w       xr3,    a0,    0,                     0
+    xvstelm.w       xr3,    a0,    FDEC_STRIDE,           1
+    xvstelm.w       xr3,    a0,    FDEC_STRIDE * 2,       2
+    xvstelm.w       xr3,    a0,    FDEC_STRIDE * 3,       3
+
+    xvstelm.w       xr3,    a0,    4,                     4
+    xvstelm.w       xr3,    a0,    FDEC_STRIDE + 4,       5
+    xvstelm.w       xr3,    a0,    FDEC_STRIDE * 2 + 4,   6
+    xvstelm.w       xr3,    a0,    FDEC_STRIDE * 3 + 4,   7
+.endm
+
+/*
+ * void add8x8_idct( pixel *p_dst, dctcoef dct[4][16] )
+ *
+ */
+function add8x8_idct_lasx
+    xvxor.v         xr0,    xr1,   xr1
+    add8x4_idct_core_lasx
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    64
+    add8x4_idct_core_lasx
+endfunc
+/*
+ * void add16x16_idct( pixel *p_dst, dctcoef dct[16][16] )
+ */
+function add16x16_idct_lasx
+    move            t4,     a0
+    move            t5,     a1
+
+    xvxor.v         xr0,    xr1,   xr1
+    add8x4_idct_core_lasx
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    64
+    add8x4_idct_core_lasx
+
+    addi.d          a0,     t4,    8
+    addi.d          a1,     t5,    128
+    add8x4_idct_core_lasx
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    64
+    add8x4_idct_core_lasx
+
+    addi.d          t6,     t4,    FDEC_STRIDE * 8
+    move            a0,     t6
+    addi.d          a1,     t5,    256
+    add8x4_idct_core_lasx
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    64
+    add8x4_idct_core_lasx
+
+
+    addi.d          a0,     t6,    8
+    addi.d          a1,     t5,    384
+    add8x4_idct_core_lasx
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    64
+    add8x4_idct_core_lasx
+endfunc
+
+/*
+ * void add8x8_idct8( pixel *dst, dctcoef dct[64] )
+ */
+function add8x8_idct8_lasx
+    xvxor.v         xr20,   xr1,   xr1
+
+    // dct[0] += 32
+    ld.h            t0,     a1,    0
+    addi.w          t0,     t0,    32
+    st.h            t0,     a1,    0
+
+    vld             vr0,    a1,    0
+    vld             vr2,    a1,    32
+    vld             vr4,    a1,    64
+    vld             vr6,    a1,    96
+
+    vsrai.h         vr8,    vr2,   1
+    vsrai.h         vr10,   vr6,   1
+
+    vext2xv.w.h     xr0,    xr0
+    vext2xv.w.h     xr2,    xr2
+    vext2xv.w.h     xr4,    xr4
+    vext2xv.w.h     xr6,    xr6
+    vext2xv.w.h     xr8,    xr8
+    vext2xv.w.h     xr10,   xr10
+
+    LASX_SUMSUB_W xr11, xr12, xr0, xr4
+    xvsub.w         xr13,   xr8,   xr6
+    xvadd.w         xr14,   xr10,  xr2
+
+    LASX_SUMSUB_W xr15, xr18, xr11, xr14
+    LASX_SUMSUB_W xr16, xr17, xr12, xr13
+
+    vld             vr0,    a1,    16
+    vld             vr2,    a1,    48
+    vld             vr4,    a1,    80
+    vld             vr6,    a1,    112
+
+    vsrai.h         vr1,    vr0,   1
+    vsrai.h         vr3,    vr2,   1
+    vsrai.h         vr5,    vr4,   1
+    vsrai.h         vr7,    vr6,   1
+
+    vext2xv.w.h     xr0,    xr0
+    vext2xv.w.h     xr2,    xr2
+    vext2xv.w.h     xr4,    xr4
+    vext2xv.w.h     xr6,    xr6
+    vext2xv.w.h     xr1,    xr1
+    vext2xv.w.h     xr3,    xr3
+    vext2xv.w.h     xr5,    xr5
+    vext2xv.w.h     xr7,    xr7
+
+    LASX_SUMSUB_W xr9, xr10, xr4, xr2
+    LASX_SUMSUB_W xr11, xr12, xr6, xr0
+
+    xvsub.w         xr10,   xr10,  xr6
+    xvsub.w         xr10,   xr10,  xr7
+    xvsub.w         xr11,   xr11,  xr2
+    xvsub.w         xr11,   xr11,  xr3
+    xvadd.w         xr12,   xr12,  xr4
+    xvadd.w         xr12,   xr12,  xr5
+    xvadd.w         xr9,    xr9,   xr0
+    xvadd.w         xr9,    xr9,   xr1
+
+    xvsrai.w        xr1,    xr10,  2
+    xvsrai.w        xr2,    xr11,  2
+    xvsrai.w        xr3,    xr12,  2
+    xvsrai.w        xr4,    xr9,   2
+
+    xvadd.w         xr5,    xr4,   xr10
+    xvadd.w         xr6,    xr3,   xr11
+    xvsub.w         xr7,    xr2,   xr12
+    xvsub.w         xr8,    xr9,   xr1
+
+    LASX_SUMSUB_W xr1, xr14, xr15, xr8
+    LASX_SUMSUB_W xr2, xr13, xr16, xr7
+    LASX_SUMSUB_W xr3, xr12, xr17, xr6
+    LASX_SUMSUB_W xr4, xr11, xr18, xr5
+
+    LASX_TRANSPOSE8x8_W xr1, xr2, xr3, xr4, xr11, xr12, xr13, xr14, \
+                        xr5, xr6, xr7, xr8, xr15, xr16, xr17, xr18, \
+                        xr9, xr10, xr21, xr22
+
+    xvsrai.h        xr9,    xr7,   1
+    xvsrai.h        xr10,   xr17,  1
+
+    xvaddwev.w.h    xr1,    xr5,   xr15
+    xvsubwev.w.h    xr2,    xr5,   xr15
+    xvsubwev.w.h    xr3,    xr9,   xr17
+    xvaddwev.w.h    xr4,    xr10,  xr7
+
+    LASX_SUMSUB_W xr11, xr14, xr1, xr4
+    LASX_SUMSUB_W xr12, xr13, xr2, xr3
+
+    xvsrai.h        xr1,    xr6,   1
+    xvsrai.h        xr2,    xr8,   1
+    xvsrai.h        xr3,    xr16,  1
+    xvsrai.h        xr4,    xr18,  1
+
+    xvaddwev.w.h    xr5,    xr16,  xr8
+    xvsubwev.w.h    xr10,   xr16,  xr8
+    xvaddwev.w.h    xr7,    xr18,  xr6
+    xvsubwev.w.h    xr9,    xr18,  xr6
+
+    xvaddwev.w.h    xr4,    xr18,  xr4
+    xvsub.w         xr10,   xr10,  xr4
+    xvaddwev.w.h    xr2,    xr8,   xr2
+    xvsub.w         xr7,    xr7,   xr2
+    xvaddwev.w.h    xr3,    xr16,  xr3
+    xvadd.w         xr9,    xr9,   xr3
+    xvaddwev.w.h    xr1,    xr6,   xr1
+    xvadd.w         xr5,    xr5,   xr1
+
+    xvsrai.w        xr1,    xr10,  2
+    xvsrai.w        xr2,    xr7,   2
+    xvsrai.w        xr3,    xr9,   2
+    xvsrai.w        xr4,    xr5,   2
+
+    xvadd.w         xr15,   xr4,   xr10
+    xvadd.w         xr16,   xr7,   xr3
+    xvsub.w         xr17,   xr2,   xr9
+    xvsub.w         xr18,   xr5,   xr1
+
+    LASX_SUMSUB_W xr1, xr8, xr11, xr18
+    LASX_SUMSUB_W xr2, xr7, xr12, xr17
+    LASX_SUMSUB_W xr3, xr6, xr13, xr16
+    LASX_SUMSUB_W xr4, xr5, xr14, xr15
+
+    xvsrai.w        xr11,   xr1,   6
+    xvsrai.w        xr12,   xr2,   6
+    xvsrai.w        xr13,   xr3,   6
+    xvsrai.w        xr14,   xr4,   6
+    xvsrai.w        xr15,   xr5,   6
+    xvsrai.w        xr16,   xr6,   6
+    xvsrai.w        xr17,   xr7,   6
+    xvsrai.w        xr18,   xr8,   6
+
+    fld.d           f1,     a0,    0
+    fld.d           f2,     a0,    FDEC_STRIDE
+    fld.d           f3,     a0,    FDEC_STRIDE * 2
+    fld.d           f4,     a0,    FDEC_STRIDE * 3
+
+    fld.d           f5,     a0,    FDEC_STRIDE * 4
+    fld.d           f6,     a0,    FDEC_STRIDE * 5
+    fld.d           f7,     a0,    FDEC_STRIDE * 6
+    fld.d           f8,     a0,    FDEC_STRIDE * 7
+
+    vext2xv.wu.bu   xr1,    xr1
+    vext2xv.wu.bu   xr2,    xr2
+    vext2xv.wu.bu   xr3,    xr3
+    vext2xv.wu.bu   xr4,    xr4
+    vext2xv.wu.bu   xr5,    xr5
+    vext2xv.wu.bu   xr6,    xr6
+    vext2xv.wu.bu   xr7,    xr7
+    vext2xv.wu.bu   xr8,    xr8
+
+    xvadd.w         xr1,    xr1,   xr11
+    xvadd.w         xr2,    xr2,   xr12
+    xvadd.w         xr3,    xr3,   xr13
+    xvadd.w         xr4,    xr4,   xr14
+    xvadd.w         xr5,    xr5,   xr15
+    xvadd.w         xr6,    xr6,   xr16
+    xvadd.w         xr7,    xr7,   xr17
+    xvadd.w         xr8,    xr8,   xr18
+
+    xvssrarni.hu.w  xr2,    xr1,   0
+    xvssrarni.hu.w  xr4,    xr3,   0
+    xvssrarni.hu.w  xr6,    xr5,   0
+    xvssrarni.hu.w  xr8,    xr7,   0
+
+    xvpermi.d       xr12,   xr2,   0xd8
+    xvpermi.d       xr14,   xr4,   0xd8
+    xvpermi.d       xr16,   xr6,   0xd8
+    xvpermi.d       xr18,   xr8,   0xd8
+
+    xvssrlni.bu.h   xr14,   xr12,  0
+    xvssrlni.bu.h   xr18,   xr16,  0
+
+    xvstelm.d       xr14,   a0,    0,                  0
+    xvstelm.d       xr14,   a0,    FDEC_STRIDE,        2
+    xvstelm.d       xr14,   a0,    FDEC_STRIDE * 2,    1
+    xvstelm.d       xr14,   a0,    FDEC_STRIDE * 3,    3
+
+    xvstelm.d       xr18,   a0,    FDEC_STRIDE * 4,    0
+    xvstelm.d       xr18,   a0,    FDEC_STRIDE * 5,    2
+    xvstelm.d       xr18,   a0,    FDEC_STRIDE * 6,    1
+    xvstelm.d       xr18,   a0,    FDEC_STRIDE * 7,    3
+endfunc
+
+.macro add8x4_idct_dc_lasx
+    xvldrepl.h      xr11,   a1,    0
+    xvldrepl.h      xr12,   a1,    2
+    xvilvl.d        xr12,   xr12,  xr11
+    xvsrari.h       xr12,   xr12,  6
+
+    fld.d           f0,     a0,    0
+    fld.d           f1,     a0,    FDEC_STRIDE
+    fld.d           f2,     a0,    FDEC_STRIDE * 2
+    fld.d           f3,     a0,    FDEC_STRIDE * 3
+
+    xvinsve0.d      xr0,    xr1,   1
+    xvinsve0.d      xr2,    xr3,   1
+
+    vext2xv.hu.bu   xr0,    xr0
+    vext2xv.hu.bu   xr2,    xr2
+
+    xvadd.h         xr0,    xr0,   xr12
+    xvadd.h         xr2,    xr2,   xr12
+    xvssrarni.bu.h  xr2,    xr0,   0
+
+    xvstelm.d       xr2,    a0,    0,                  0
+    xvstelm.d       xr2,    a0,    FDEC_STRIDE,        2
+    xvstelm.d       xr2,    a0,    FDEC_STRIDE * 2,    1
+    xvstelm.d       xr2,    a0,    FDEC_STRIDE * 3,    3
+.endm
+
+/*
+ * void add8x8_idct_dc( pixel *p_dst, dctcoef dct[4] )
+ */
+function add8x8_idct_dc_lasx
+    add8x4_idct_dc_lasx
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    4
+    add8x4_idct_dc_lasx
+endfunc
+
+.macro add_16x16_idct_dc_core_lasx a0, a1
+    vldrepl.h       vr11,   \a1,   0
+    vldrepl.h       vr12,   \a1,   2
+    vldrepl.h       vr13,   \a1,   4
+    vldrepl.h       vr14,   \a1,   6
+
+    xvinsve0.d      xr11,   xr12,  1
+    xvinsve0.d      xr11,   xr13,  2
+    xvinsve0.d      xr11,   xr14,  3
+
+    xvsrari.h       xr11,   xr11,  6
+
+    vld             vr0,    \a0,   0
+    vld             vr1,    \a0,   FDEC_STRIDE
+    vld             vr2,    \a0,   FDEC_STRIDE * 2
+    vld             vr3,    \a0,   FDEC_STRIDE * 3
+    vext2xv.hu.bu   xr0,    xr0
+    vext2xv.hu.bu   xr1,    xr1
+    vext2xv.hu.bu   xr2,    xr2
+    vext2xv.hu.bu   xr3,    xr3
+    xvadd.h         xr0,    xr0,   xr11
+    xvadd.h         xr1,    xr1,   xr11
+    xvadd.h         xr2,    xr2,   xr11
+    xvadd.h         xr3,    xr3,   xr11
+    xvssrarni.bu.h  xr1,    xr0,   0
+    xvssrarni.bu.h  xr3,    xr2,   0
+    xvpermi.d       xr4,    xr1,   0xD8
+    xvpermi.d       xr5,    xr1,   0x8D
+    xvpermi.d       xr6,    xr3,   0xD8
+    xvpermi.d       xr7,    xr3,   0x8D
+    vst             vr4,    \a0,   0
+    vst             vr5,    \a0,   FDEC_STRIDE
+    vst             vr6,    \a0,   FDEC_STRIDE * 2
+    vst             vr7,    \a0,   FDEC_STRIDE * 3
+.endm
+
+/*
+ * void add16x16_idct_dc( pixel *p_dst, dctcoef dct[16] )
+ */
+function add16x16_idct_dc_lasx
+    add_16x16_idct_dc_core_lasx a0, a1
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    8
+    add_16x16_idct_dc_core_lasx a0, a1
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    8
+    add_16x16_idct_dc_core_lasx a0, a1
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    8
+    add_16x16_idct_dc_core_lasx a0, a1
+endfunc
+
+/*
+ * void idct4x4dc( dctcoef d[16] )
+ */
+function idct4x4dc_lasx
+    la.local        t0,     last64_shuf
+    xvld            xr0,    a0,    0
+    xvld            xr20,   t0,    0
+    xvshuf4i.b      xr1,    xr0,   0x4E
+    xvhaddw.w.h     xr2,    xr0,   xr0
+    xvhsubw.w.h     xr3,    xr1,   xr1
+    xvshuf4i.h      xr2,    xr2,   0x4E
+    xvshuf4i.h      xr3,    xr3,   0x4E
+    xvhaddw.d.w     xr4,    xr2,   xr2
+    xvhsubw.d.w     xr5,    xr2,   xr2
+    xvhsubw.d.w     xr6,    xr3,   xr3
+    xvhaddw.d.w     xr7,    xr3,   xr3
+    xvpickev.w      xr8,    xr5,   xr4
+    xvpickev.w      xr9,    xr7,   xr6
+    xvpickev.h      xr10,   xr9,   xr8
+    xvperm.w        xr10,   xr10,  xr20
+    xvshuf4i.b      xr11,   xr10,  0x4E
+    xvhaddw.w.h     xr12,   xr10,  xr10
+    xvhsubw.w.h     xr13,   xr11,  xr11
+    xvshuf4i.h      xr12,   xr12,  0x4E
+    xvshuf4i.h      xr13,   xr13,  0x4E
+    xvhaddw.d.w     xr14,   xr12,  xr12
+    xvhsubw.d.w     xr15,   xr12,  xr12
+    xvhsubw.d.w     xr16,   xr13,  xr13
+    xvhaddw.d.w     xr17,   xr13,  xr13
+    xvpackev.w      xr18,   xr15,  xr14
+    xvpackev.w      xr19,   xr17,  xr16
+    xvilvl.d        xr0,    xr19,  xr18
+    xvilvh.d        xr1,    xr19,  xr18
+    xvpickev.h      xr2,    xr1,   xr0
+    xvst            xr2,    a0,    0
+endfunc
+
+/*
+ * void dct4x4dc( dctcoef d[16] )
+ */
+function dct4x4dc_lasx
+    la.local        t0,     last64_shuf
+    xvld            xr0,    a0,    0
+    xvld            xr20,   t0,    0
+    xvshuf4i.b      xr1,    xr0,   0x4E
+    xvhaddw.w.h     xr2,    xr0,   xr0
+    xvhsubw.w.h     xr3,    xr1,   xr1
+    xvshuf4i.h      xr2,    xr2,   0x4E
+    xvshuf4i.h      xr3,    xr3,   0x4E
+    xvhaddw.d.w     xr4,    xr2,   xr2
+    xvhsubw.d.w     xr5,    xr2,   xr2
+    xvhsubw.d.w     xr6,    xr3,   xr3
+    xvhaddw.d.w     xr7,    xr3,   xr3
+    xvpickev.w      xr8,    xr5,   xr4
+    xvpickev.w      xr9,    xr7,   xr6
+    xvpickev.h      xr10,   xr9,   xr8
+    xvperm.w        xr10,   xr10,  xr20
+    xvshuf4i.b      xr11,   xr10,  0x4E
+    xvhaddw.w.h     xr12,   xr10,  xr10
+    xvhsubw.w.h     xr13,   xr11,  xr11
+    xvshuf4i.h      xr12,   xr12,  0x4E
+    xvshuf4i.h      xr13,   xr13,  0x4E
+    xvhaddw.d.w     xr14,   xr12,  xr12
+    xvhsubw.d.w     xr15,   xr12,  xr12
+    xvhsubw.d.w     xr16,   xr13,  xr13
+    xvhaddw.d.w     xr17,   xr13,  xr13
+    xvpackev.w      xr18,   xr15,  xr14
+    xvpackev.w      xr19,   xr17,  xr16
+    xvsrari.w       xr18,   xr18,  1
+    xvsrari.w       xr19,   xr19,  1
+    xvilvl.d        xr0,    xr19,  xr18
+    xvilvh.d        xr1,    xr19,  xr18
+    xvpickev.h      xr2,    xr1,   xr0
+    xvst            xr2,    a0,    0
+endfunc
+
+.macro LSX_LOAD_PIX_2 data1, data2
+    vld             vr0,    a1,    0
+    vld             vr1,    a1,    FENC_STRIDE
+    vld             vr2,    a2,    0
+    vld             vr3,    a2,    FDEC_STRIDE
+
+    vilvl.b         vr0,    vr8,   vr0
+    vilvl.b         vr1,    vr8,   vr1
+    vilvl.b         vr2,    vr8,   vr2
+    vilvl.b         vr3,    vr8,   vr3
+
+    vsub.h          \data1, vr0,   vr2
+    vsub.h          \data2, vr1,   vr3
+    addi.d          a1,     a1,    FENC_STRIDE * 2
+    addi.d          a2,     a2,    FDEC_STRIDE * 2
+.endm
+
+.macro LSX_DCT8_1D
+    LSX_SUMSUB_H vr0, vr8, vr12, vr19
+    LSX_SUMSUB_H vr1, vr9, vr13, vr18
+    LSX_SUMSUB_H vr2, vr10, vr14, vr17
+    LSX_SUMSUB_H vr3, vr11, vr15, vr16
+
+    LSX_SUMSUB_H vr4, vr6, vr0, vr3
+    LSX_SUMSUB_H vr5, vr7, vr1, vr2
+
+    vsrai.h         vr20,   vr8,   1
+    vadd.h          vr20,   vr20,  vr9
+    vadd.h          vr20,   vr20,  vr10
+    vadd.h          vr0,    vr20,  vr8
+
+    vsrai.h         vr20,   vr10,  1
+    vsub.h          vr21,   vr8,   vr11
+    vsub.h          vr21,   vr21,  vr10
+    vsub.h          vr1,    vr21,  vr20
+
+    vsrai.h         vr20,   vr9,   1
+    vadd.h          vr21,   vr8,   vr11
+    vsub.h          vr21,   vr21,  vr9
+    vsub.h          vr2,    vr21,  vr20
+
+    vsrai.h         vr20,   vr11,  1
+    vsub.h          vr21,   vr9,   vr10
+    vadd.h          vr21,   vr21,  vr11
+    vadd.h          vr3,    vr21,  vr20
+
+    vadd.h          vr12,   vr4,   vr5
+    vsrai.h         vr20,   vr3,   2
+    vadd.h          vr13,   vr0,   vr20
+    vsrai.h         vr20,   vr7,   1
+    vadd.h          vr14,   vr6,   vr20
+    vsrai.h         vr20,   vr2,   2
+    vadd.h          vr15,   vr1,   vr20
+
+    vsub.h          vr16,   vr4,   vr5
+    vsrai.h         vr20,   vr1,   2
+    vsub.h          vr17,   vr2,   vr20
+    vsrai.h         vr20,   vr6,   1
+    vsub.h          vr18,   vr20,  vr7
+    vsrai.h         vr20,   vr0,   2
+    vsub.h          vr19,   vr20,  vr3
+.endm
+
+/*
+ * void sub8x8_dct8( dctcoef dct[64], pixel *pix1, pixel *pix2 )
+ */
+function sub8x8_dct8_lsx
+    vor.v           vr8,    vr0,   vr0
+
+    // vr12 ... vr19
+    LSX_LOAD_PIX_2  vr12, vr13
+    LSX_LOAD_PIX_2  vr14, vr15
+    LSX_LOAD_PIX_2  vr16, vr17
+    LSX_LOAD_PIX_2  vr18, vr19
+
+    LSX_DCT8_1D
+    LSX_TRANSPOSE8x8_H vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr19, \
+                       vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr19, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    LSX_DCT8_1D
+
+    vst             vr12,   a0,    0
+    vst             vr13,   a0,    16
+    vst             vr14,   a0,    32
+    vst             vr15,   a0,    48
+    vst             vr16,   a0,    64
+    vst             vr17,   a0,    80
+    vst             vr18,   a0,    96
+    vst             vr19,   a0,    112
+endfunc
+
+.macro LASX_LOAD_PIX_2 data1, data2
+    xvld            xr0,    a1,    0
+    xvld            xr1,    a1,    FENC_STRIDE
+    xvld            xr2,    a2,    0
+    xvld            xr3,    a2,    FDEC_STRIDE
+
+    xvpermi.d       xr0,    xr0,   0x50
+    xvpermi.d       xr1,    xr1,   0x50
+    xvpermi.d       xr2,    xr2,   0x50
+    xvpermi.d       xr3,    xr3,   0x50
+
+    xvxor.v         xr4,    xr0,   xr0
+    xvilvl.b        xr0,    xr4,   xr0
+    xvilvl.b        xr1,    xr4,   xr1
+    xvilvl.b        xr2,    xr4,   xr2
+    xvilvl.b        xr3,    xr4,   xr3
+
+    xvsub.h         \data1, xr0,   xr2
+    xvsub.h         \data2, xr1,   xr3
+    addi.d           a1,     a1,   FENC_STRIDE * 2
+    addi.d           a2,     a2,   FDEC_STRIDE * 2
+.endm
+
+.macro LASX_SUMSUB_H sum, diff, a, b
+    xvadd.h         \sum,   \a,    \b
+    xvsub.h         \diff,  \a,    \b
+.endm
+
+.macro LASX_DCT8_1D
+    LASX_SUMSUB_H xr0, xr8, xr12, xr19
+    LASX_SUMSUB_H xr1, xr9, xr13, xr18
+    LASX_SUMSUB_H xr2, xr10, xr14, xr17
+    LASX_SUMSUB_H xr3, xr11, xr15, xr16
+
+    LASX_SUMSUB_H xr4, xr6, xr0, xr3
+    LASX_SUMSUB_H xr5, xr7, xr1, xr2
+
+    xvsrai.h        xr20,   xr8,   1
+    xvadd.h         xr20,   xr20,  xr9
+    xvadd.h         xr20,   xr20,  xr10
+    xvadd.h         xr0,    xr20,  xr8
+
+    xvsrai.h        xr20,   xr10,  1
+    xvsub.h         xr21,   xr8,   xr11
+    xvsub.h         xr21,   xr21,  xr10
+    xvsub.h         xr1,    xr21,  xr20
+
+    xvsrai.h        xr20,   xr9,   1
+    xvadd.h         xr21,   xr8,   xr11
+    xvsub.h         xr21,   xr21,  xr9
+    xvsub.h         xr2,    xr21,  xr20
+
+    xvsrai.h        xr20,   xr11,  1
+    xvsub.h         xr21,   xr9,   xr10
+    xvadd.h         xr21,   xr21,  xr11
+    xvadd.h         xr3,    xr21,  xr20
+
+    xvadd.h         xr12,   xr4,   xr5
+    xvsrai.h        xr20,   xr3,   2
+    xvadd.h         xr13,   xr0,   xr20
+    xvsrai.h        xr20,   xr7,   1
+    xvadd.h         xr14,   xr6,   xr20
+    xvsrai.h        xr20,   xr2,   2
+    xvadd.h         xr15,   xr1,   xr20
+
+    xvsub.h         xr16,   xr4,   xr5
+    xvsrai.h        xr20,   xr1,   2
+    xvsub.h         xr17,   xr2,   xr20
+    xvsrai.h        xr20,   xr6,   1
+    xvsub.h         xr18,   xr20,  xr7
+    xvsrai.h        xr20,   xr0,   2
+    xvsub.h         xr19,   xr20,  xr3
+.endm
+
+.macro SUB16x8_DCT8_LASX
+    LASX_LOAD_PIX_2 xr12, xr13
+    LASX_LOAD_PIX_2 xr14, xr15
+    LASX_LOAD_PIX_2 xr16, xr17
+    LASX_LOAD_PIX_2 xr18, xr19
+
+    LASX_DCT8_1D
+    LASX_TRANSPOSE8x8_H xr12, xr13, xr14, xr15, xr16, xr17, xr18, xr19, \
+                        xr12, xr13, xr14, xr15, xr16, xr17, xr18, xr19, \
+                        xr0, xr1, xr2, xr3, xr4, xr5, xr6, xr7
+    LASX_DCT8_1D
+
+    xmov            xr0,    xr13
+    xvpermi.q       xr13,   xr12,  0x20
+    xvst            xr13,   a0,    0
+    xmov            xr1,    xr15
+    xvpermi.q       xr15,   xr14,  0x20
+    xvst            xr15,   a0,    32
+    xmov            xr2,    xr17
+    xvpermi.q       xr17,   xr16,  0x20
+    xvst            xr17,   a0,    64
+    xmov            xr3,    xr19
+    xvpermi.q       xr19,   xr18,  0x20
+    xvst            xr19,   a0,    96
+
+    xvpermi.q       xr12,   xr0,   0x13
+    xvpermi.q       xr14,   xr1,   0x13
+    xvpermi.q       xr16,   xr2,   0x13
+    xvpermi.q       xr18,   xr3,   0x13
+
+    xvst            xr12,   a0,    128
+    xvst            xr14,   a0,    160
+    xvst            xr16,   a0,    192
+    xvst            xr18,   a0,    224
+.endm
+
+/*
+ * void sub16x16_dct8( dctcoef dct[4][64], pixel *pix1, pixel *pix2 )
+ */
+function sub16x16_dct8_lasx
+    move            t1,     a1
+    move            t3,     a2
+    SUB16x8_DCT8_LASX
+
+    addi.d          a0,     a0,    256
+    addi.d          a1,     t1,    FENC_STRIDE * 8
+    addi.d          a2,     t3,    FDEC_STRIDE * 8
+    SUB16x8_DCT8_LASX
+endfunc
+
+/*
+ * void zigzag_scan_4x4_frame( dctcoef level[16], dctcoef dct[16] )
+ */
+function zigzag_scan_4x4_frame_lasx
+    xvld            xr1,    a1,    0
+    xvor.v          xr2,    xr1,   xr1
+    xvpermi.q       xr2,    xr2,   0x13
+    xvpermi.q       xr1,    xr1,   0x02
+    la.local        t0,     zigzag_scan4
+    xvld            xr3,    t0,    0
+    xvshuf.h        xr3,    xr2,   xr1
+    xvst            xr3,    a0,    0
+endfunc
diff --git a/common/loongarch/dct-c.c b/common/loongarch/dct-c.c
deleted file mode 100644
index 38631969..00000000
--- a/common/loongarch/dct-c.c
+++ /dev/null
@@ -1,724 +0,0 @@
-/*****************************************************************************
- * dct-c.c: loongarch transform and zigzag
- *****************************************************************************
- * Copyright (C) 2015-2018 x264 project
- * Copyright (C) 2020 Loongson Technology Corporation Limited
- *
- * Authors: zhou peng <zhoupeng@loongson.cn>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
- *
- * This program is also available under a commercial proprietary license.
- * For more information, contact us at licensing@x264.com.
- *****************************************************************************/
-#include "common/common.h"
-#include "loongson_intrinsics.h"
-#include "dct.h"
-
-#if !HIGH_BIT_DEPTH
-
-#define LASX_LD4x4( p_src, out0, out1, out2, out3 )     \
-{                                                       \
-    out0 = __lasx_xvld( p_src, 0 );                     \
-    out1 = __lasx_xvpermi_d( out0, 0x55 );              \
-    out2 = __lasx_xvpermi_d( out0, 0xAA );              \
-    out3 = __lasx_xvpermi_d( out0, 0xFF );              \
-}
-
-#define LASX_ITRANS_H( in0, in1, in2, in3, out0, out1, out2, out3 )         \
-{                                                                           \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
-                                                                            \
-    tmp0_m = __lasx_xvadd_h( in0, in2 );                                    \
-    tmp1_m = __lasx_xvsub_h( in0, in2 );                                    \
-    tmp2_m = __lasx_xvsrai_h( in1, 1 );                                     \
-    tmp2_m = __lasx_xvsub_h( tmp2_m, in3 );                                 \
-    tmp3_m = __lasx_xvsrai_h( in3, 1 );                                     \
-    tmp3_m = __lasx_xvadd_h( in1, tmp3_m );                                 \
-                                                                            \
-    LASX_BUTTERFLY_4_H( tmp0_m, tmp1_m, tmp2_m, tmp3_m,                     \
-                        out0, out1, out2, out3 );                           \
-}
-
-#define LASX_ADDBLK_ST4x4_128SV( in0, in1, in2, in3, p_dst, stride )        \
-{                                                                           \
-    uint32_t src0_m, src1_m, src2_m, src3_m;                                \
-    uint8_t *p_dst0;                                                        \
-    __m256i inp0_m, inp1_m, res0_m, res1_m;                                 \
-    __m256i dst0_m = __lasx_xvldi( 0 );                                     \
-    __m256i dst1_m = __lasx_xvldi( 0 );                                     \
-    __m256i zero_m = __lasx_xvldi( 0 );                                     \
-                                                                            \
-    DUP2_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, inp0_m, inp1_m );       \
-    src0_m = *( uint32_t* )( p_dst );                                       \
-    p_dst0 = p_dst + stride;                                                \
-    src1_m = *( uint32_t* )( p_dst0 );                                      \
-    p_dst0 += stride;                                                       \
-    src2_m = *( uint32_t* )( p_dst0 );                                      \
-    p_dst0 += stride;                                                       \
-    src3_m = *( uint32_t* )( p_dst0 );                                      \
-    dst0_m = __lasx_xvinsgr2vr_w( dst0_m, src0_m, 0 );                      \
-    dst0_m = __lasx_xvinsgr2vr_w( dst0_m, src1_m, 1 );                      \
-    dst1_m = __lasx_xvinsgr2vr_w( dst1_m, src2_m, 0 );                      \
-    dst1_m = __lasx_xvinsgr2vr_w( dst1_m, src3_m, 1 );                      \
-    DUP2_ARG2( __lasx_xvilvl_b, zero_m, dst0_m, zero_m, dst1_m, res0_m,     \
-               res1_m );                                                    \
-    res0_m = __lasx_xvadd_h( res0_m, inp0_m );                              \
-    res1_m = __lasx_xvadd_h( res1_m, inp1_m );                              \
-    DUP2_ARG1( __lasx_xvclip255_h, res0_m, res1_m, res0_m, res1_m );        \
-    DUP2_ARG2( __lasx_xvpickev_b, res0_m, res0_m, res1_m, res1_m, dst0_m,   \
-               dst1_m );                                                    \
-                                                                            \
-    __lasx_xvstelm_w( dst0_m, p_dst, 0, 0 );                                \
-    p_dst0 = p_dst + stride;                                                \
-    __lasx_xvstelm_w( dst0_m, p_dst0, 0, 1 );                               \
-    p_dst0 += stride;                                                       \
-    __lasx_xvstelm_w( dst1_m, p_dst0, 0, 0 );                               \
-    p_dst0 += stride;                                                       \
-    __lasx_xvstelm_w( dst1_m, p_dst0, 0, 1 );                               \
-}
-
-static void avc_sub4x4_dct_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                 uint8_t *p_ref, int32_t i_dst_stride,
-                                 int16_t *p_dst )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i inp0, inp1, tmp;
-    __m256i diff0, diff1, diff2, diff3;
-    __m256i temp0, temp1, temp2, temp3;
-
-    src0 = __lasx_xvldrepl_w( p_src, 0 );
-    p_src += i_src_stride;
-    src1 = __lasx_xvldrepl_w( p_src, 0 );
-    p_src += i_src_stride;
-    src2 = __lasx_xvldrepl_w( p_src, 0 );
-    p_src += i_src_stride;
-    src3 = __lasx_xvldrepl_w( p_src, 0 );
-    src0 = __lasx_xvpackev_w( src1, src0 );
-    src1 = __lasx_xvpackev_w( src3, src2 );
-    src0 = __lasx_xvpackev_d( src1, src0 );
-
-    ref0 = __lasx_xvldrepl_w( p_ref, 0 );
-    p_ref += i_dst_stride;
-    ref1 = __lasx_xvldrepl_w( p_ref, 0 );
-    p_ref += i_dst_stride;
-    ref2 = __lasx_xvldrepl_w( p_ref, 0 );
-    p_ref += i_dst_stride;
-    ref3 = __lasx_xvldrepl_w( p_ref, 0 );
-    ref0 = __lasx_xvpackev_w( ref1, ref0 );
-    ref1 = __lasx_xvpackev_w( ref3, ref2 );
-    ref0 = __lasx_xvpackev_d( ref1, ref0 );
-
-    inp0 = __lasx_xvilvl_b( src0, ref0 );
-    inp1 = __lasx_xvilvh_b( src0, ref0 );
-    DUP2_ARG2( __lasx_xvhsubw_hu_bu, inp0, inp0, inp1, inp1, diff0, diff2 );
-    DUP2_ARG2( __lasx_xvilvh_d, diff0, diff0, diff2, diff2, diff1, diff3 );
-
-    LASX_BUTTERFLY_4_H( diff0, diff1, diff2, diff3, temp0, temp1, temp2, temp3 );
-
-    diff0 = __lasx_xvadd_h( temp0, temp1);
-    tmp = __lasx_xvslli_h( temp3, 1);
-    diff1 = __lasx_xvadd_h( tmp, temp2);
-    diff2 = __lasx_xvsub_h( temp0, temp1 );
-    tmp = __lasx_xvslli_h( temp2, 1);
-    diff3 = __lasx_xvsub_h( temp3, tmp );
-
-    LASX_TRANSPOSE4x4_H( diff0, diff1, diff2, diff3, temp0, temp1, temp2, temp3 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp2, temp3, diff0, diff1, diff2, diff3 );
-
-    temp0 = __lasx_xvadd_h( diff0, diff1);
-    tmp = __lasx_xvslli_h( diff3, 1);
-    temp1 = __lasx_xvadd_h( tmp, diff2);
-    temp2 = __lasx_xvsub_h( diff0, diff1 );
-    tmp = __lasx_xvslli_h( diff2, 1);
-    temp3 = __lasx_xvsub_h( diff3, tmp );
-
-    DUP2_ARG2( __lasx_xvilvl_d, temp1, temp0, temp3, temp2, inp0, inp1 );
-    inp0 = __lasx_xvpermi_q(inp1, inp0, 0x20);
-    __lasx_xvst( inp0, p_dst, 0 );
-}
-
-void x264_sub4x4_dct_lasx( int16_t p_dst[16], uint8_t *p_src,
-                           uint8_t *p_ref )
-{
-    avc_sub4x4_dct_lasx( p_src, FENC_STRIDE, p_ref, FDEC_STRIDE, p_dst );
-}
-
-void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src,
-                           uint8_t *p_ref )
-{
-    avc_sub4x4_dct_lasx( &p_src[0], FENC_STRIDE,
-                         &p_ref[0], FDEC_STRIDE, p_dst[0] );
-    avc_sub4x4_dct_lasx( &p_src[4], FENC_STRIDE, &p_ref[4],
-                         FDEC_STRIDE, p_dst[1] );
-    avc_sub4x4_dct_lasx( &p_src[4 * FENC_STRIDE + 0],
-                         FENC_STRIDE, &p_ref[4 * FDEC_STRIDE + 0],
-                         FDEC_STRIDE, p_dst[2] );
-    avc_sub4x4_dct_lasx( &p_src[4 * FENC_STRIDE + 4],
-                         FENC_STRIDE, &p_ref[4 * FDEC_STRIDE + 4],
-                         FDEC_STRIDE, p_dst[3] );
-}
-
-void x264_sub16x16_dct_lasx( int16_t p_dst[16][16],
-                             uint8_t *p_src,
-                             uint8_t *p_ref )
-{
-    x264_sub8x8_dct_lasx( &p_dst[ 0], &p_src[0], &p_ref[0] );
-    x264_sub8x8_dct_lasx( &p_dst[ 4], &p_src[8], &p_ref[8] );
-    x264_sub8x8_dct_lasx( &p_dst[ 8], &p_src[8 * FENC_STRIDE + 0],
-                          &p_ref[8*FDEC_STRIDE+0] );
-    x264_sub8x8_dct_lasx( &p_dst[12], &p_src[8 * FENC_STRIDE + 8],
-                          &p_ref[8*FDEC_STRIDE+8] );
-}
-
-static void avc_idct4x4_addblk_lasx( uint8_t *p_dst, int16_t *p_src,
-                                     int32_t i_dst_stride )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i hres0, hres1, hres2, hres3;
-    __m256i vres0, vres1, vres2, vres3;
-
-    LASX_LD4x4( p_src, src0, src1, src2, src3 );
-    LASX_ITRANS_H( src0, src1, src2, src3, hres0, hres1, hres2, hres3 );
-    LASX_TRANSPOSE4x4_H( hres0, hres1, hres2, hres3, hres0, hres1, hres2, hres3 );
-    LASX_ITRANS_H( hres0, hres1, hres2, hres3, vres0, vres1, vres2, vres3 );
-
-    DUP4_ARG2( __lasx_xvsrari_h, vres0, 6, vres1, 6, vres2, 6, vres3, 6,
-               vres0, vres1, vres2, vres3 );
-    LASX_ADDBLK_ST4x4_128SV( vres0, vres1, vres2, vres3, p_dst, i_dst_stride );
-}
-
-void x264_add4x4_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16] )
-{
-    avc_idct4x4_addblk_lasx( p_dst, pi_dct, FDEC_STRIDE );
-}
-
-void x264_add8x8_idct8_lasx( uint8_t *dst, int16_t dct[64] )
-{
-    int32_t stride2, stride3, stride4;
-    uint8_t* dst_tmp;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
-    __m256i reg0, reg1, reg2, reg3, reg4, reg5, reg6, reg7;
-    __m256i shift = {0x0000000400000000, 0x0000000500000001,
-                     0x0000000600000002, 0x0000000700000003};
-
-    dct[0] += 32;
-    stride2 = FDEC_STRIDE << 1;
-    stride3 = FDEC_STRIDE + stride2;
-    stride4 = stride2 << 1;
-    dst_tmp = dst + stride4;
-
-    src0 = __lasx_xvld(dct, 0);
-    src2 = __lasx_xvld(dct, 32);
-    src4 = __lasx_xvld(dct, 64);
-    src6 = __lasx_xvld(dct, 96);
-    src1 = __lasx_xvpermi_d(src0, 0x4E);
-    src3 = __lasx_xvpermi_d(src2, 0x4E);
-    src5 = __lasx_xvpermi_d(src4, 0x4E);
-    src7 = __lasx_xvpermi_d(src6, 0x4E);
-
-    src0 = __lasx_vext2xv_w_h(src0);
-    src1 = __lasx_vext2xv_w_h(src1);
-    src2 = __lasx_vext2xv_w_h(src2);
-    src3 = __lasx_vext2xv_w_h(src3);
-    src4 = __lasx_vext2xv_w_h(src4);
-    src5 = __lasx_vext2xv_w_h(src5);
-    src6 = __lasx_vext2xv_w_h(src6);
-    src7 = __lasx_vext2xv_w_h(src7);
-
-    tmp0 = __lasx_xvadd_w(src0, src4);
-    tmp2 = __lasx_xvsub_w(src0, src4);
-    tmp4 = __lasx_xvsrai_w(src2, 1);
-    tmp4 = __lasx_xvsub_w(tmp4, src6);
-    tmp6 = __lasx_xvsrai_w(src6, 1);
-    tmp6 = __lasx_xvadd_w(tmp6, src2);
-    reg7 = __lasx_xvsrai_w(src7, 1);
-    reg3 = __lasx_xvsrai_w(src3, 1);
-    reg5 = __lasx_xvsrai_w(src5, 1);
-    reg1 = __lasx_xvsrai_w(src1, 1);
-    tmp1 = __lasx_xvsub_w(src5, src3);
-    tmp3 = __lasx_xvadd_w(src1, src7);
-    tmp5 = __lasx_xvsub_w(src7, src1);
-    tmp7 = __lasx_xvadd_w(src3, src5);
-    reg7 = __lasx_xvadd_w(src7, reg7);
-    reg3 = __lasx_xvadd_w(src3, reg3);
-    reg5 = __lasx_xvadd_w(reg5, src5);
-    reg1 = __lasx_xvadd_w(reg1, src1);
-    tmp1 = __lasx_xvsub_w(tmp1, reg7);
-    tmp3 = __lasx_xvsub_w(tmp3, reg3);
-    tmp5 = __lasx_xvadd_w(reg5, tmp5);
-    tmp7 = __lasx_xvadd_w(tmp7, reg1);
-    reg0 = __lasx_xvadd_w(tmp0, tmp6);
-    reg2 = __lasx_xvadd_w(tmp2, tmp4);
-    reg4 = __lasx_xvsub_w(tmp2, tmp4);
-    reg6 = __lasx_xvsub_w(tmp0, tmp6);
-    reg1 = __lasx_xvsrai_w(tmp7, 2);
-    reg3 = __lasx_xvsrai_w(tmp5, 2);
-    reg5 = __lasx_xvsrai_w(tmp3, 2);
-    reg7 = __lasx_xvsrai_w(tmp1, 2);
-    reg1 = __lasx_xvadd_w(tmp1, reg1);
-    reg3 = __lasx_xvadd_w(tmp3, reg3);
-    reg5 = __lasx_xvsub_w(reg5, tmp5);
-    reg7 = __lasx_xvsub_w(tmp7, reg7);
-
-    src0 = __lasx_xvadd_w(reg0, reg7);
-    src1 = __lasx_xvadd_w(reg2, reg5);
-    src2 = __lasx_xvadd_w(reg4, reg3);
-    src3 = __lasx_xvadd_w(reg6, reg1);
-    src4 = __lasx_xvsub_w(reg6, reg1);
-    src5 = __lasx_xvsub_w(reg4, reg3);
-    src6 = __lasx_xvsub_w(reg2, reg5);
-    src7 = __lasx_xvsub_w(reg0, reg7);
-
-    LASX_TRANSPOSE8x8_W(src0, src1, src2, src3, src4, src5, src6, src7,
-                        src0, src1, src2, src3, src4, src5, src6, src7);
-
-    tmp0 = __lasx_xvaddwev_w_h(src0, src4);
-    tmp2 = __lasx_xvsubwev_w_h(src0, src4);
-    tmp4 = __lasx_xvsrai_h(src2, 1);
-    tmp4 = __lasx_xvsubwev_w_h(tmp4, src6);
-    tmp6 = __lasx_xvsrai_h(src6, 1);
-    tmp6 = __lasx_xvaddwev_w_h(tmp6, src2);
-    reg7 = __lasx_xvsrai_h(src7, 1);
-    reg3 = __lasx_xvsrai_h(src3, 1);
-    reg5 = __lasx_xvsrai_h(src5, 1);
-    reg1 = __lasx_xvsrai_h(src1, 1);
-    tmp1 = __lasx_xvsubwev_w_h(src5, src3);
-    tmp3 = __lasx_xvaddwev_w_h(src1, src7);
-    tmp5 = __lasx_xvsubwev_w_h(src7, src1);
-    tmp7 = __lasx_xvaddwev_w_h(src3, src5);
-    reg7 = __lasx_xvaddwev_w_h(src7, reg7);
-    reg3 = __lasx_xvaddwev_w_h(src3, reg3);
-    reg5 = __lasx_xvaddwev_w_h(reg5, src5);
-    reg1 = __lasx_xvaddwev_w_h(reg1, src1);
-
-    tmp1 = __lasx_xvsub_w(tmp1, reg7);
-    tmp3 = __lasx_xvsub_w(tmp3, reg3);
-    tmp5 = __lasx_xvadd_w(reg5, tmp5);
-    tmp7 = __lasx_xvadd_w(tmp7, reg1);
-    reg0 = __lasx_xvadd_w(tmp0, tmp6);
-    reg2 = __lasx_xvadd_w(tmp2, tmp4);
-    reg4 = __lasx_xvsub_w(tmp2, tmp4);
-    reg6 = __lasx_xvsub_w(tmp0, tmp6);
-    reg1 = __lasx_xvsrai_w(tmp7, 2);
-    reg3 = __lasx_xvsrai_w(tmp5, 2);
-    reg5 = __lasx_xvsrai_w(tmp3, 2);
-    reg7 = __lasx_xvsrai_w(tmp1, 2);
-    reg1 = __lasx_xvadd_w(tmp1, reg1);
-    reg3 = __lasx_xvadd_w(tmp3, reg3);
-    reg5 = __lasx_xvsub_w(reg5, tmp5);
-    reg7 = __lasx_xvsub_w(tmp7, reg7);
-    src0 = __lasx_xvadd_w(reg0, reg7);
-    src1 = __lasx_xvadd_w(reg2, reg5);
-    src2 = __lasx_xvadd_w(reg4, reg3);
-    src3 = __lasx_xvadd_w(reg6, reg1);
-    src4 = __lasx_xvsub_w(reg6, reg1);
-    src5 = __lasx_xvsub_w(reg4, reg3);
-    src6 = __lasx_xvsub_w(reg2, reg5);
-    src7 = __lasx_xvsub_w(reg0, reg7);
-
-    src0 = __lasx_xvsrai_w(src0, 6);
-    src1 = __lasx_xvsrai_w(src1, 6);
-    src2 = __lasx_xvsrai_w(src2, 6);
-    src3 = __lasx_xvsrai_w(src3, 6);
-    src4 = __lasx_xvsrai_w(src4, 6);
-    src5 = __lasx_xvsrai_w(src5, 6);
-    src6 = __lasx_xvsrai_w(src6, 6);
-    src7 = __lasx_xvsrai_w(src7, 6);
-
-    reg0 = __lasx_xvld(dst, 0);
-    reg1 = __lasx_xvld(dst, FDEC_STRIDE);
-    reg2 = __lasx_xvldx(dst, stride2);
-    reg3 = __lasx_xvldx(dst, stride3);
-    reg4 = __lasx_xvld(dst_tmp, 0);
-    reg5 = __lasx_xvld(dst_tmp, FDEC_STRIDE);
-    reg6 = __lasx_xvldx(dst_tmp, stride2);
-    reg7 = __lasx_xvldx(dst_tmp, stride3);
-
-    reg0 = __lasx_vext2xv_wu_bu(reg0);
-    reg1 = __lasx_vext2xv_wu_bu(reg1);
-    reg2 = __lasx_vext2xv_wu_bu(reg2);
-    reg3 = __lasx_vext2xv_wu_bu(reg3);
-    reg4 = __lasx_vext2xv_wu_bu(reg4);
-    reg5 = __lasx_vext2xv_wu_bu(reg5);
-    reg6 = __lasx_vext2xv_wu_bu(reg6);
-    reg7 = __lasx_vext2xv_wu_bu(reg7);
-    reg0 = __lasx_xvadd_w(reg0, src0);
-    reg1 = __lasx_xvadd_w(reg1, src1);
-    reg2 = __lasx_xvadd_w(reg2, src2);
-    reg3 = __lasx_xvadd_w(reg3, src3);
-    reg4 = __lasx_xvadd_w(reg4, src4);
-    reg5 = __lasx_xvadd_w(reg5, src5);
-    reg6 = __lasx_xvadd_w(reg6, src6);
-    reg7 = __lasx_xvadd_w(reg7, src7);
-
-    reg0 = __lasx_xvmaxi_w(reg0, 0);
-    reg1 = __lasx_xvmaxi_w(reg1, 0);
-    reg2 = __lasx_xvmaxi_w(reg2, 0);
-    reg3 = __lasx_xvmaxi_w(reg3, 0);
-    reg4 = __lasx_xvmaxi_w(reg4, 0);
-    reg5 = __lasx_xvmaxi_w(reg5, 0);
-    reg6 = __lasx_xvmaxi_w(reg6, 0);
-    reg7 = __lasx_xvmaxi_w(reg7, 0);
-    src0 = __lasx_xvssrlni_hu_w(reg1, reg0, 0);
-    src1 = __lasx_xvssrlni_hu_w(reg3, reg2, 0);
-    src2 = __lasx_xvssrlni_hu_w(reg5, reg4, 0);
-    src3 = __lasx_xvssrlni_hu_w(reg7, reg6, 0);
-    src0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
-    src1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
-    src0 = __lasx_xvperm_w(src0, shift);
-    src1 = __lasx_xvperm_w(src1, shift);
-    __lasx_xvstelm_d(src0, dst, 0, 0);
-    dst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src0, dst, 0, 1);
-    dst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src0, dst, 0, 2);
-    dst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src0, dst, 0, 3);
-    dst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src1, dst, 0, 0);
-    dst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src1, dst, 0, 1);
-    dst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src1, dst, 0, 2);
-    dst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src1, dst, 0, 3);
-}
-
-void x264_add8x8_idct_lasx( uint8_t *p_dst, int16_t pi_dct[4][16] )
-{
-    avc_idct4x4_addblk_lasx( &p_dst[0], &pi_dct[0][0], FDEC_STRIDE );
-    avc_idct4x4_addblk_lasx( &p_dst[4], &pi_dct[1][0], FDEC_STRIDE );
-    avc_idct4x4_addblk_lasx( &p_dst[4 * FDEC_STRIDE + 0],
-                            &pi_dct[2][0], FDEC_STRIDE );
-    avc_idct4x4_addblk_lasx( &p_dst[4 * FDEC_STRIDE + 4],
-                            &pi_dct[3][0], FDEC_STRIDE );
-}
-
-void x264_add16x16_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16][16] )
-{
-    x264_add8x8_idct_lasx( &p_dst[0], &pi_dct[0] );
-    x264_add8x8_idct_lasx( &p_dst[8], &pi_dct[4] );
-    x264_add8x8_idct_lasx( &p_dst[8 * FDEC_STRIDE + 0], &pi_dct[8] );
-    x264_add8x8_idct_lasx( &p_dst[8 * FDEC_STRIDE + 8], &pi_dct[12] );
-}
-
-void x264_add8x8_idct_dc_lasx( uint8_t *pdst, int16_t dct[4] )
-{
-    int32_t stride2 = FDEC_STRIDE << 1;
-    int32_t stride3 = FDEC_STRIDE + stride2;
-    int32_t stride4 = stride2 << 1;
-    uint8_t *pdst_tmp = pdst + stride4;
-    __m256i vec_dct, vec_dct0, vec_dct1;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i const_32 = __lasx_xvldi(0x420);
-
-    vec_dct = __lasx_xvldrepl_d(dct, 0);
-    vec_dct = __lasx_xvadd_h(vec_dct, const_32);
-    vec_dct = __lasx_xvsrai_h(vec_dct, 6);
-    vec_dct = __lasx_xvilvl_h(vec_dct, vec_dct);
-    vec_dct0 = __lasx_xvilvl_w(vec_dct, vec_dct);
-    vec_dct1 = __lasx_xvilvh_w(vec_dct, vec_dct);
-
-    src0 = __lasx_xvld(pdst, 0);
-    src1 = __lasx_xvld(pdst, FDEC_STRIDE);
-    src2 = __lasx_xvldx(pdst, stride2);
-    src3 = __lasx_xvldx(pdst, stride3);
-    src4 = __lasx_xvld(pdst_tmp, 0);
-    src5 = __lasx_xvld(pdst_tmp, FDEC_STRIDE);
-    src6 = __lasx_xvldx(pdst_tmp, stride2);
-    src7 = __lasx_xvldx(pdst_tmp, stride3);
-
-    src0 = __lasx_xvilvl_d(src1, src0);
-    src1 = __lasx_xvilvl_d(src3, src2);
-    src2 = __lasx_xvilvl_d(src5, src4);
-    src3 = __lasx_xvilvl_d(src7, src6);
-
-    src0 = __lasx_vext2xv_hu_bu(src0);
-    src1 = __lasx_vext2xv_hu_bu(src1);
-    src2 = __lasx_vext2xv_hu_bu(src2);
-    src3 = __lasx_vext2xv_hu_bu(src3);
-
-    src0 = __lasx_xvadd_h(src0, vec_dct0);
-    src1 = __lasx_xvadd_h(src1, vec_dct0);
-    src2 = __lasx_xvadd_h(src2, vec_dct1);
-    src3 = __lasx_xvadd_h(src3, vec_dct1);
-
-    src0 = __lasx_xvmaxi_h(src0, 0);
-    src1 = __lasx_xvmaxi_h(src1, 0);
-    src2 = __lasx_xvmaxi_h(src2, 0);
-    src3 = __lasx_xvmaxi_h(src3, 0);
-    src0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
-    src1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
-    __lasx_xvstelm_d(src0, pdst, 0, 0);
-    pdst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src0, pdst, 0, 2);
-    pdst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src0, pdst, 0, 1);
-    pdst += FDEC_STRIDE;
-    __lasx_xvstelm_d(src0, pdst, 0, 3);
-    __lasx_xvstelm_d(src1, pdst_tmp, 0, 0);
-    pdst_tmp += FDEC_STRIDE;
-    __lasx_xvstelm_d(src1, pdst_tmp, 0, 2);
-    pdst_tmp += FDEC_STRIDE;
-    __lasx_xvstelm_d(src1, pdst_tmp, 0, 1);
-    pdst_tmp += FDEC_STRIDE;
-    __lasx_xvstelm_d(src1, pdst_tmp, 0, 3);
-}
-
-/****************************************************************************
- * 8x8 transform:
- ****************************************************************************/
-
-void x264_sub8x8_dct8_lasx( int16_t pi_dct[64], uint8_t *p_pix1,
-                            uint8_t *p_pix2 )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
-    __m256i temp, temp1;
-    __m256i zero = {0};
-    __m256i s07, s16, s25, s34, d07, d16, d25, d34;
-    __m256i a0, a1, a2, a3, a4, a5, a6, a7;
-
-#define LOAD_PIX_DATA_2(data1, data2)                                     \
-    DUP2_ARG2( __lasx_xvld, p_pix1, 0, p_pix1, FENC_STRIDE, src0, src1 ); \
-    DUP2_ARG2( __lasx_xvld, p_pix2, 0, p_pix2, FDEC_STRIDE, src2, src3 ); \
-    DUP4_ARG2( __lasx_xvilvl_b, zero, src0, zero, src1, zero, src2, zero, \
-               src3, src0, src1, src2, src3 );                            \
-    data1 = __lasx_xvsub_h( src0, src2 );                                 \
-    data2 = __lasx_xvsub_h( src1, src3 );                                 \
-    p_pix1 += ( FENC_STRIDE << 1 );                                       \
-    p_pix2 += ( FDEC_STRIDE << 1 );
-
-    LOAD_PIX_DATA_2(tmp0, tmp1);
-    LOAD_PIX_DATA_2(tmp2, tmp3);
-    LOAD_PIX_DATA_2(tmp4, tmp5);
-    LOAD_PIX_DATA_2(tmp6, tmp7);
-
-#undef LOAD_PIX_DATA_2
-
-#define LASX_DCT8_1D                      \
-    s07 = __lasx_xvadd_h( tmp0, tmp7 );   \
-    s16 = __lasx_xvadd_h( tmp1, tmp6 );   \
-    s25 = __lasx_xvadd_h( tmp2, tmp5 );   \
-    s34 = __lasx_xvadd_h( tmp3, tmp4 );   \
-    a0 = __lasx_xvadd_h( s07, s34 );      \
-    a1 = __lasx_xvadd_h( s16, s25 );      \
-    a2 = __lasx_xvsub_h( s07, s34 );      \
-    a3 = __lasx_xvsub_h( s16, s25 );      \
-                                          \
-    d07 = __lasx_xvsub_h( tmp0, tmp7 );   \
-    d16 = __lasx_xvsub_h( tmp1, tmp6 );   \
-    d25 = __lasx_xvsub_h( tmp2, tmp5 );   \
-    d34 = __lasx_xvsub_h( tmp3, tmp4 );   \
-                                          \
-    temp = __lasx_xvsrai_h( d07, 1 );     \
-    temp = __lasx_xvadd_h( temp, d16 );   \
-    temp = __lasx_xvadd_h( temp, d25 );   \
-    a4 = __lasx_xvadd_h( temp, d07 );     \
-                                          \
-    temp = __lasx_xvsrai_h( d25, 1 );     \
-    temp1 = __lasx_xvsub_h( d07, d34 );   \
-    temp1 = __lasx_xvsub_h( temp1, d25 ); \
-    a5 = __lasx_xvsub_h( temp1, temp );   \
-                                          \
-    temp = __lasx_xvsrai_h( d16, 1 );     \
-    temp1 = __lasx_xvadd_h( d07, d34 );   \
-    temp1 = __lasx_xvsub_h( temp1, d16 ); \
-    a6 = __lasx_xvsub_h( temp1, temp );   \
-                                          \
-    temp = __lasx_xvsrai_h( d34, 1 );     \
-    temp1 = __lasx_xvsub_h( d16, d25 );   \
-    temp1 = __lasx_xvadd_h( temp1, d34 ); \
-    a7 = __lasx_xvadd_h( temp1, temp );   \
-                                          \
-    tmp0 = __lasx_xvadd_h( a0, a1 );      \
-    temp = __lasx_xvsrai_h( a7, 2 );      \
-    tmp1 = __lasx_xvadd_h( a4, temp );    \
-    temp = __lasx_xvsrai_h( a3, 1 );      \
-    tmp2 = __lasx_xvadd_h( a2, temp );    \
-    temp = __lasx_xvsrai_h( a6, 2 );      \
-    tmp3 = __lasx_xvadd_h( a5, temp );    \
-    tmp4 = __lasx_xvsub_h( a0, a1 );      \
-    temp = __lasx_xvsrai_h( a5, 2 );      \
-    tmp5 = __lasx_xvsub_h( a6, temp );    \
-    temp = __lasx_xvsrai_h( a2, 1 );      \
-    tmp6 = __lasx_xvsub_h( temp, a3 );    \
-    temp = __lasx_xvsrai_h( a4, 2 );      \
-    tmp7 = __lasx_xvsub_h( temp, a7 );
-
-    LASX_DCT8_1D;
-    LASX_TRANSPOSE8x8_H( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
-                         tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-    LASX_DCT8_1D;
-
-#undef LASX_DCT8_1D
-
-    __lasx_xvstelm_d( tmp0, &pi_dct[0], 0, 0 );
-    __lasx_xvstelm_d( tmp0, &pi_dct[4], 0, 1 );
-    __lasx_xvstelm_d( tmp1, &pi_dct[8], 0, 0 );
-    __lasx_xvstelm_d( tmp1, &pi_dct[12], 0, 1 );
-    __lasx_xvstelm_d( tmp2, &pi_dct[16], 0, 0 );
-    __lasx_xvstelm_d( tmp2, &pi_dct[20], 0, 1 );
-    __lasx_xvstelm_d( tmp3, &pi_dct[24], 0, 0 );
-    __lasx_xvstelm_d( tmp3, &pi_dct[28], 0, 1 );
-    __lasx_xvstelm_d( tmp4, &pi_dct[32], 0, 0 );
-    __lasx_xvstelm_d( tmp4, &pi_dct[36], 0, 1 );
-    __lasx_xvstelm_d( tmp5, &pi_dct[40], 0, 0 );
-    __lasx_xvstelm_d( tmp5, &pi_dct[44], 0, 1 );
-    __lasx_xvstelm_d( tmp6, &pi_dct[48], 0, 0 );
-    __lasx_xvstelm_d( tmp6, &pi_dct[52], 0, 1 );
-    __lasx_xvstelm_d( tmp7, &pi_dct[56], 0, 0 );
-    __lasx_xvstelm_d( tmp7, &pi_dct[60], 0, 1 );
-}
-
-static void x264_sub8x8_dct8_ext_lasx( int16_t pi_dct1[64],
-                                       uint8_t *p_pix1, uint8_t *p_pix2,
-                                       int16_t pi_dct2[64] )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
-    __m256i temp, temp1;
-    __m256i zero = {0};
-    __m256i s07, s16, s25, s34, d07, d16, d25, d34;
-    __m256i a0, a1, a2, a3, a4, a5, a6, a7;
-
-#define LOAD_PIX_DATA_2_EXT(data1, data2)                                 \
-    DUP2_ARG2( __lasx_xvld, p_pix1, 0, p_pix1, FENC_STRIDE, src0, src1 ); \
-    DUP2_ARG2( __lasx_xvld, p_pix2, 0, p_pix2, FDEC_STRIDE, src2, src3 ); \
-    src0 = __lasx_xvpermi_d( src0, 0x50 );                                \
-    src1 = __lasx_xvpermi_d( src1, 0x50 );                                \
-    src2 = __lasx_xvpermi_d( src2, 0x50 );                                \
-    src3 = __lasx_xvpermi_d( src3, 0x50 );                                \
-                                                                          \
-    DUP4_ARG2( __lasx_xvilvl_b, zero, src0, zero, src1, zero, src2, zero, \
-               src3, src0, src1, src2, src3 );                            \
-    data1 = __lasx_xvsub_h( src0, src2 );                                 \
-    data2 = __lasx_xvsub_h( src1, src3 );                                 \
-    p_pix1 += ( FENC_STRIDE << 1 );                                       \
-    p_pix2 += ( FDEC_STRIDE << 1 );
-
-    LOAD_PIX_DATA_2_EXT(tmp0, tmp1);
-    LOAD_PIX_DATA_2_EXT(tmp2, tmp3);
-    LOAD_PIX_DATA_2_EXT(tmp4, tmp5);
-    LOAD_PIX_DATA_2_EXT(tmp6, tmp7);
-
-#undef LOAD_PIX_DATA_2_EXT
-
-#define LASX_DCT8_1D_EXT                  \
-    s07 = __lasx_xvadd_h( tmp0, tmp7 );   \
-    s16 = __lasx_xvadd_h( tmp1, tmp6 );   \
-    s25 = __lasx_xvadd_h( tmp2, tmp5 );   \
-    s34 = __lasx_xvadd_h( tmp3, tmp4 );   \
-    a0 = __lasx_xvadd_h( s07, s34 );      \
-    a1 = __lasx_xvadd_h( s16, s25 );      \
-    a2 = __lasx_xvsub_h( s07, s34 );      \
-    a3 = __lasx_xvsub_h( s16, s25 );      \
-                                          \
-    d07 = __lasx_xvsub_h( tmp0, tmp7 );   \
-    d16 = __lasx_xvsub_h( tmp1, tmp6 );   \
-    d25 = __lasx_xvsub_h( tmp2, tmp5 );   \
-    d34 = __lasx_xvsub_h( tmp3, tmp4 );   \
-                                          \
-    temp = __lasx_xvsrai_h( d07, 1 );     \
-    temp = __lasx_xvadd_h( temp, d16 );   \
-    temp = __lasx_xvadd_h( temp, d25 );   \
-    a4 = __lasx_xvadd_h( temp, d07 );     \
-                                          \
-    temp = __lasx_xvsrai_h( d25, 1 );     \
-    temp1 = __lasx_xvsub_h( d07, d34 );   \
-    temp1 = __lasx_xvsub_h( temp1, d25 ); \
-    a5 = __lasx_xvsub_h( temp1, temp );   \
-                                          \
-    temp = __lasx_xvsrai_h( d16, 1 );     \
-    temp1 = __lasx_xvadd_h( d07, d34 );   \
-    temp1 = __lasx_xvsub_h( temp1, d16 ); \
-    a6 = __lasx_xvsub_h( temp1, temp );   \
-                                          \
-    temp = __lasx_xvsrai_h( d34, 1 );     \
-    temp1 = __lasx_xvsub_h( d16, d25 );   \
-    temp1 = __lasx_xvadd_h( temp1, d34 ); \
-    a7 = __lasx_xvadd_h( temp1, temp );   \
-                                          \
-    tmp0 = __lasx_xvadd_h( a0, a1 );      \
-    temp = __lasx_xvsrai_h( a7, 2 );      \
-    tmp1 = __lasx_xvadd_h( a4, temp );    \
-    temp = __lasx_xvsrai_h( a3, 1 );      \
-    tmp2 = __lasx_xvadd_h( a2, temp );    \
-    temp = __lasx_xvsrai_h( a6, 2 );      \
-    tmp3 = __lasx_xvadd_h( a5, temp );    \
-    tmp4 = __lasx_xvsub_h( a0, a1 );      \
-    temp = __lasx_xvsrai_h( a5, 2 );      \
-    tmp5 = __lasx_xvsub_h( a6, temp );    \
-    temp = __lasx_xvsrai_h( a2, 1 );      \
-    tmp6 = __lasx_xvsub_h( temp, a3 );    \
-    temp = __lasx_xvsrai_h( a4, 2 );      \
-    tmp7 = __lasx_xvsub_h( temp, a7 );
-
-    LASX_DCT8_1D_EXT;
-    LASX_TRANSPOSE8x8_H( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
-                         tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
-    LASX_DCT8_1D_EXT;
-
-#undef LASX_DCT8_1D_EXT
-
-    __lasx_xvstelm_d( tmp0, &pi_dct1[0], 0, 0 );
-    __lasx_xvstelm_d( tmp0, &pi_dct1[4], 0, 1 );
-    __lasx_xvstelm_d( tmp1, &pi_dct1[8], 0, 0 );
-    __lasx_xvstelm_d( tmp1, &pi_dct1[12], 0, 1 );
-    __lasx_xvstelm_d( tmp2, &pi_dct1[16], 0, 0 );
-    __lasx_xvstelm_d( tmp2, &pi_dct1[20], 0, 1 );
-    __lasx_xvstelm_d( tmp3, &pi_dct1[24], 0, 0 );
-    __lasx_xvstelm_d( tmp3, &pi_dct1[28], 0, 1 );
-    __lasx_xvstelm_d( tmp4, &pi_dct1[32], 0, 0 );
-    __lasx_xvstelm_d( tmp4, &pi_dct1[36], 0, 1 );
-    __lasx_xvstelm_d( tmp5, &pi_dct1[40], 0, 0 );
-    __lasx_xvstelm_d( tmp5, &pi_dct1[44], 0, 1 );
-    __lasx_xvstelm_d( tmp6, &pi_dct1[48], 0, 0 );
-    __lasx_xvstelm_d( tmp6, &pi_dct1[52], 0, 1 );
-    __lasx_xvstelm_d( tmp7, &pi_dct1[56], 0, 0 );
-    __lasx_xvstelm_d( tmp7, &pi_dct1[60], 0, 1 );
-
-    __lasx_xvstelm_d( tmp0, &pi_dct2[0], 0, 2 );
-    __lasx_xvstelm_d( tmp0, &pi_dct2[4], 0, 3 );
-    __lasx_xvstelm_d( tmp1, &pi_dct2[8], 0, 2 );
-    __lasx_xvstelm_d( tmp1, &pi_dct2[12], 0, 3 );
-    __lasx_xvstelm_d( tmp2, &pi_dct2[16], 0, 2 );
-    __lasx_xvstelm_d( tmp2, &pi_dct2[20], 0, 3 );
-    __lasx_xvstelm_d( tmp3, &pi_dct2[24], 0, 2 );
-    __lasx_xvstelm_d( tmp3, &pi_dct2[28], 0, 3 );
-    __lasx_xvstelm_d( tmp4, &pi_dct2[32], 0, 2 );
-    __lasx_xvstelm_d( tmp4, &pi_dct2[36], 0, 3 );
-    __lasx_xvstelm_d( tmp5, &pi_dct2[40], 0, 2 );
-    __lasx_xvstelm_d( tmp5, &pi_dct2[44], 0, 3 );
-    __lasx_xvstelm_d( tmp6, &pi_dct2[48], 0, 2 );
-    __lasx_xvstelm_d( tmp6, &pi_dct2[52], 0, 3 );
-    __lasx_xvstelm_d( tmp7, &pi_dct2[56], 0, 2 );
-    __lasx_xvstelm_d( tmp7, &pi_dct2[60], 0, 3 );
-
-}
-
-void x264_sub16x16_dct8_lasx( int16_t pi_dct[4][64], uint8_t *p_pix1,
-                              uint8_t *p_pix2 )
-{
-    x264_sub8x8_dct8_ext_lasx( pi_dct[0], &p_pix1[0], &p_pix2[0],
-                               pi_dct[1] );
-    x264_sub8x8_dct8_ext_lasx( pi_dct[2], &p_pix1[8 * FENC_STRIDE + 0],
-                               &p_pix2[8*FDEC_STRIDE+0], pi_dct[3] );
-}
-
-#endif
diff --git a/common/loongarch/dct.h b/common/loongarch/dct.h
index 8a0991e4..b91cfeaa 100644
--- a/common/loongarch/dct.h
+++ b/common/loongarch/dct.h
@@ -27,8 +27,8 @@
 #ifndef X264_LOONGARCH_DCT_H
 #define X264_LOONGARCH_DCT_H
 
-#define x264_sub4x4_dct_lasx x264_template(sub4x4_dct_lasx)
-void x264_sub4x4_dct_lasx( int16_t p_dst[16], uint8_t *p_src, uint8_t *p_ref );
+#define x264_sub4x4_dct_lsx x264_template(sub4x4_dct_lsx)
+void x264_sub4x4_dct_lsx( int16_t p_dst[16], uint8_t *p_src, uint8_t *p_ref );
 #define x264_sub8x8_dct_lasx x264_template(sub8x8_dct_lasx)
 void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src,
                            uint8_t *p_ref );
@@ -36,15 +36,15 @@ void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src,
 void x264_sub16x16_dct_lasx( int16_t p_dst[16][16], uint8_t *p_src,
                              uint8_t *p_ref );
 
-#define x264_sub8x8_dct8_lasx x264_template(sub8x8_dct8_lasx)
-void x264_sub8x8_dct8_lasx( int16_t pi_dct[64], uint8_t *p_pix1,
+#define x264_sub8x8_dct8_lsx x264_template(sub8x8_dct8_lsx)
+void x264_sub8x8_dct8_lsx( int16_t pi_dct[64], uint8_t *p_pix1,
                             uint8_t *p_pix2 );
 #define x264_sub16x16_dct8_lasx x264_template(sub16x16_dct8_lasx)
 void x264_sub16x16_dct8_lasx( int16_t pi_dct[4][64], uint8_t *p_pix1,
                               uint8_t *p_pix2 );
 
-#define x264_add4x4_idct_lasx x264_template(add4x4_idct_lasx)
-void x264_add4x4_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16] );
+#define x264_add4x4_idct_lsx x264_template(add4x4_idct_lsx)
+void x264_add4x4_idct_lsx( uint8_t *p_dst, int16_t pi_dct[16] );
 #define x264_add8x8_idct_lasx x264_template(add8x8_idct_lasx)
 void x264_add8x8_idct_lasx( uint8_t *p_dst, int16_t pi_dct[4][16] );
 #define x264_add16x16_idct_lasx x264_template(add16x16_idct_lasx)
@@ -53,5 +53,15 @@ void x264_add16x16_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16][16] );
 void x264_add8x8_idct8_lasx( uint8_t *p_dst, int16_t pi_dct[64] );
 #define x264_add8x8_idct_dc_lasx x264_template(add8x8_idct_dc_lasx)
 void x264_add8x8_idct_dc_lasx( uint8_t *p_dst, int16_t dct[4] );
+#define x264_add16x16_idct_dc_lasx x264_template(add16x16_idct_dc_lasx)
+void x264_add16x16_idct_dc_lasx( uint8_t *p_dst, int16_t dct[16] );
+
+#define x264_idct4x4dc_lasx x264_template(idct4x4dc_lasx)
+void x264_idct4x4dc_lasx( int16_t d[16] );
+#define x264_dct4x4dc_lasx x264_template(dct4x4dc_lasx)
+void x264_dct4x4dc_lasx( int16_t d[16] );
+
+#define x264_zigzag_scan_4x4_frame_lasx x264_template(zigzag_scan_4x4_frame_lasx)
+void x264_zigzag_scan_4x4_frame_lasx( int16_t level[16], int16_t dct[16] );
 
 #endif
diff --git a/common/loongarch/mc-a.S b/common/loongarch/mc-a.S
new file mode 100644
index 00000000..4a5ef444
--- /dev/null
+++ b/common/loongarch/mc-a.S
@@ -0,0 +1,530 @@
+/*****************************************************************************
+ * mc-a.S: LoongArch motion compensation
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+
+const ch_shuf
+.byte 0, 2, 2, 4, 4, 6, 6, 8, 1, 3, 3, 5, 5, 7, 7, 9
+.byte 0, 2, 2, 4, 4, 6, 6, 8, 1, 3, 3, 5, 5, 7, 7, 9
+endconst
+
+const pw_1024
+.rept 16
+.short 1024
+.endr
+endconst
+
+const filt_mul20
+.rept 32
+.byte 20
+.endr
+endconst
+
+const filt_mul15
+.rept 16
+.byte 1, -5
+.endr
+endconst
+
+const filt_mul51
+.rept 16
+.byte -5, 1
+.endr
+endconst
+
+const hpel_shuf
+.rept 2
+.byte 0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15
+.endr
+endconst
+
+const shuf_12
+.rept 2
+.byte 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27
+.endr
+endconst
+
+const shuf_14
+.rept 2
+.byte 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29
+.endr
+endconst
+
+const shuf_15
+.rept 2
+.byte 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30
+.endr
+endconst
+
+const shuf_1
+.rept 2
+.byte 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
+.endr
+endconst
+
+const shuf_2
+.rept 2
+.byte 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17
+.endr
+endconst
+
+const shuf_3
+.rept 2
+.byte 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18
+.endr
+endconst
+
+const shuf_4
+.rept 2
+.byte 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19
+.endr
+endconst
+
+const shuf_6
+.rept 2
+.byte 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21
+.endr
+endconst
+
+#if !HIGH_BIT_DEPTH
+
+.macro  MC_CHROMA_START
+    srai.d  t0,  a5,  3
+    srai.d  t1,  a6,  3
+    slli.d  t0,  t0,  1
+    mul.d   t1,  t1,  a4
+    add.d   t1,  t1,  t0
+    add.d   a3,  a3,  t1 /* src += (m_vy >> 3) * i_src_stride + (m_vx >> 3) * 2 */
+.endm
+
+/* void mc_chroma( uint8_t *p_dst_u, uint8_t *p_dst_v,
+ *                 intptr_t i_dst_stride,
+ *                 uint8_t *p_src, intptr_t i_src_stride,
+ *                 int32_t m_vx, int32_t m_vy,
+ *                 int32_t i_width, int32_t i_height )
+ */
+function mc_chroma_lasx
+    MC_CHROMA_START
+    andi    a5,    a5,    0x07    /* m_vx & 0x07 */
+    andi    a6,    a6,    0x07    /* m_vy & 0x07 */
+    move    t0,    a5
+    slli.d  t0,    t0,    8
+    sub.d   t0,    t0,    a5
+    li.d    a5,    8
+    addi.d  t0,    t0,    8
+    sub.d   a5,    a5,    a6
+    mul.d   a6,    a6,    t0      /* (x * 255 + 8) * y */
+    mul.d   a5,    a5,    t0      /* (x * 255 + 8) * (8 - y) */
+    xvreplgr2vr.h  xr6,   a6      /* cD cC ... cD cC */
+    xvreplgr2vr.h  xr7,   a5      /* cB cA ... cB cA */
+    la.local t0,   ch_shuf
+    xvld    xr5,   t0,    0
+    addi.d  t0,    a7,    -4
+    ldptr.w a7,    sp,    0       /* a7 = i_height */
+    slli.d  t1,    a4,    1
+    blt     zero,  t0,    .L_WIDTH8
+.L_LOOP4:
+    vld       vr0,    a3,   0
+    vldx      vr1,    a3,   a4
+    vldx      vr2,    a3,   t1
+    xvpermi.q xr0,   xr1,  0x02
+    xvpermi.q xr1,   xr2,  0x02
+    xvshuf.b  xr0,   xr0,  xr0,   xr5
+    xvshuf.b  xr1,   xr1,  xr1,   xr5
+    xvdp2.h.bu xr2,  xr0,  xr7
+    xvdp2.h.bu xr3,  xr1,  xr6
+    xvadd.h   xr0,   xr2,  xr3
+    xvssrlrni.bu.h   xr0,  xr0,   6
+    xvstelm.w xr0,   a0,   0,     0
+    xvstelm.w xr0,   a1,   0,     1
+    add.d     a0,    a0,   a2
+    add.d     a1,    a1,   a2
+    xvstelm.w xr0,   a0,   0,     4
+    xvstelm.w xr0,   a1,   0,     5
+    add.d     a0,    a0,   a2
+    add.d     a1,    a1,   a2
+    add.d     a3,    a3,   t1
+    addi.d    a7,    a7,   -2
+    blt       zero,  a7,   .L_LOOP4
+    b         .ENDFUNC
+.L_WIDTH8:
+    xvld      xr0,   a3,    0
+    xvpermi.d xr0,   xr0,   0x94
+    xvshuf.b  xr0,   xr0,   xr0,   xr5
+.L_LOOP8:
+    xvldx     xr3,   a3,    a4
+    xvpermi.d xr3,   xr3,   0x94
+    xvshuf.b  xr3,   xr3,   xr3,   xr5
+    xvdp2.h.bu xr1,  xr0,   xr7
+    xvdp2.h.bu xr2,  xr3,   xr6
+    xvdp2.h.bu xr8,  xr3,   xr7
+
+    xvldx     xr0,   a3,    t1
+    xvpermi.d xr0,   xr0,   0x94
+    xvshuf.b  xr0,   xr0,   xr0,   xr5
+    xvdp2.h.bu xr4,  xr0,   xr6
+    xvadd.h   xr1,   xr1,   xr2
+    xvadd.h   xr3,   xr8,   xr4
+
+    xvssrlrni.bu.h   xr3,   xr1,    6
+
+    xvpermi.q   xr4,   xr3,    0x01
+    xvpackev.w  xr8,   xr4,    xr3
+    xvpackod.w  xr9,   xr4,    xr3
+    vstelm.d    vr8,   a0,     0,    0
+    vstelm.d    vr9,   a1,     0,    0
+    add.d       a0,    a0,     a2
+    add.d       a1,    a1,     a2
+    vstelm.d    vr8,   a0,     0,    1
+    vstelm.d    vr9,   a1,     0,    1
+
+    addi.d      a7,    a7,     -2
+    add.d       a0,    a0,     a2
+    add.d       a1,    a1,     a2
+    add.d       a3,    a3,     t1
+    blt         zero,  a7,     .L_LOOP8
+.ENDFUNC:
+    endfunc
+
+.macro PIXEL_AVG_START
+    slli.d  t0,  a3,  1
+    add.w   t1,  t0,  a3
+    slli.d  t2,  a3,  2
+    slli.d  t3,  a5,  1
+    add.w   t4,  t3,  a5
+    slli.d  t5,  a5,  2
+    slli.d  t6,  a1,  1
+    add.w   t7,  t6,  a1
+    slli.d  t8,  a1,  2
+.endm
+
+.macro BIWEIGHT_AVG_START
+    addi.d          t0,    zero,  64
+    sub.d           t0,    t0,    a6
+    xvreplgr2vr.b   xr0,   a6
+    xvreplgr2vr.b   xr1,   t0
+    xvpackev.b      xr8,   xr1,   xr0
+    xvxor.v         xr9,   xr9,   xr9
+    xvaddi.hu       xr9,   xr9,   6
+.endm
+
+.macro BIWEIGHT_AVG_CORE a, b
+    xvpermi.d \a,   \a,  0x50
+    xvpermi.d \b,   \b,  0x50
+    xvilvl.b  \a,   \b,  \a
+    xvmulwev.h.bu.b  \b,  \a,  xr8
+    xvmaddwod.h.bu.b \b,  \a,  xr8
+    xvmaxi.h  \b,   \b,  0
+    xvssrlrn.bu.h    \b,  \b,  xr9
+    xvpermi.d        \b,  \b,  0x08
+.endm
+
+function pixel_avg_weight_w16_lasx export=0
+    BIWEIGHT_AVG_START
+    PIXEL_AVG_START
+.L_HEIGHT_LOOP_T:
+    LSX_LOADX_4  a2,  a3, t0, t1, vr0, vr1, vr2, vr3
+    LSX_LOADX_4  a4,  a5, t3, t4, vr4, vr5, vr6, vr7
+    BIWEIGHT_AVG_CORE xr0, xr4
+    BIWEIGHT_AVG_CORE xr1, xr5
+    vst              vr4,  a0,   0
+    vstx             vr5,  a0,   a1
+    BIWEIGHT_AVG_CORE xr2, xr6
+    BIWEIGHT_AVG_CORE xr3, xr7
+    vstx             vr6,  a0,   t6
+    vstx             vr7,  a0,   t7
+    add.d     a2,    a2,   t2
+    add.d     a4,    a4,   t5
+    add.d     a0,    a0,   t8
+    addi.d    a7,    a7,   -4
+    bnez      a7,    .L_HEIGHT_LOOP_T
+endfunc
+
+function pixel_avg_w16_lasx export=0
+    PIXEL_AVG_START
+.L_HEIGHT_LOOP:
+    vld       vr0,   a2,   0
+    vldx      vr1,   a2,   a3
+    vldx      vr2,   a2,   t0
+    vldx      vr3,   a2,   t1
+    vld       vr4,   a4,   0
+    vldx      vr5,   a4,   a5
+    vldx      vr6,   a4,   t3
+    vldx      vr7,   a4,   t4
+    vavgr.bu  vr0,   vr0,  vr4
+    vavgr.bu  vr1,   vr1,  vr5
+    vavgr.bu  vr2,   vr2,  vr6
+    vavgr.bu  vr3,   vr3,  vr7
+    vst       vr0,   a0,   0
+    vstx      vr1,   a0,   a1
+    vstx      vr2,   a0,   t6
+    vstx      vr3,   a0,   t7
+    add.d     a0,    a0,   t8
+    add.d     a2,    a2,   t2
+    add.d     a4,    a4,   t5
+
+    vld       vr0,   a2,   0
+    vldx      vr1,   a2,   a3
+    vldx      vr2,   a2,   t0
+    vldx      vr3,   a2,   t1
+    vld       vr4,   a4,   0
+    vldx      vr5,   a4,   a5
+    vldx      vr6,   a4,   t3
+    vldx      vr7,   a4,   t4
+    vavgr.bu  vr0,   vr0,  vr4
+    vavgr.bu  vr1,   vr1,  vr5
+    vavgr.bu  vr2,   vr2,  vr6
+    vavgr.bu  vr3,   vr3,  vr7
+    vst       vr0,   a0,   0
+    vstx      vr1,   a0,   a1
+    vstx      vr2,   a0,   t6
+    vstx      vr3,   a0,   t7
+    add.d     a2,    a2,   t2
+    add.d     a4,    a4,   t5
+    add.d     a0,    a0,   t8
+    addi.d    a7,    a7,   -8
+    bnez      a7,    .L_HEIGHT_LOOP
+endfunc
+
+.macro FILT_PACK_LASX s1, s2, s3
+    xvmulwev.w.h    xr16,  \s1,  \s3
+    xvmulwev.w.h    xr17,  \s2,  \s3
+    xvsrarni.h.w   xr17,  xr16, 15
+    xvmaxi.h        xr17,  xr17, 0
+    xvsat.hu        xr17,  xr17, 7
+    xvmulwod.w.h    xr18,  \s1,  \s3
+    xvmulwod.w.h    xr19,  \s2,  \s3
+    xvsrarni.h.w   xr19,  xr18,  15
+    xvmaxi.h        xr19,  xr19,  0
+    xvsat.hu        xr19,  xr19,  7
+    xvpackev.b      \s1,   xr19,  xr17
+.endm
+
+/* s3: temp, s4: UNUSED, s5: imm */
+.macro DO_FILT_V_LASX s1, s2, s3, s4, s5
+    alsl.d    t1,  a2,  a1,  1  /* t1 = a1 + 2 * a2 */
+    alsl.d    t2,  a2,  a3,  1  /* t2 = a3 + 2 * a2 */
+    //preld     0,   t1,  32
+    xvld      xr1, a3,  0
+    xvldx     xr2, a3,  a2
+    xvld      \s3, t2,  0
+    xvld      xr3, a1,  0
+    xvldx     \s1, a1,  a2
+    xvld      \s2, t1,  0
+    xvilvh.b  xr16, xr2, xr1
+    xvilvl.b  xr17, xr2, xr1
+    xvilvh.b  xr18, \s2, \s1
+    xvilvl.b  xr19, \s2, \s1
+    xvilvh.b  xr20, \s3, xr3
+    xvilvl.b  xr21, \s3, xr3
+    xvdp2.h.bu.b    xr1,  xr17,  xr12
+    xvdp2.h.bu.b    xr4,  xr16,  xr12
+    xvdp2.h.bu.b    \s1,  xr19,  xr0
+    xvdp2.h.bu.b    xr2,  xr18,  xr0
+    xvdp2.h.bu.b    xr3,  xr21,  xr14
+    xvdp2.h.bu.b    \s2,  xr20,  xr14
+    xvadd.h   xr1,  xr1,  \s1
+    xvadd.h   xr4,  xr4,  xr2
+    xvadd.h   xr1,  xr1,  xr3
+    xvadd.h   xr4,  xr4,  \s2
+    xmov      \s1,  xr1
+    xmov      \s2,  xr1
+    addi.d    a3,   a3,   32
+    addi.d    a1,   a1,   32
+    xvpermi.q \s1,  xr4,  0x2
+    xvpermi.q \s2,  xr4,  0x13
+    FILT_PACK_LASX  xr1,  xr4,  xr15
+    addi.d    t1,   a4,   \s5
+    xvstx     xr1,  t0,   t1
+.endm
+
+.macro FILT_H s1, s2, s3
+    xvsub.h    \s1,  \s1,  \s2
+    xvsrai.h   \s1,  \s1,  2
+    xvsub.h    \s1,  \s1,  \s2
+    xvadd.h    \s1,  \s1,  \s3
+    xvsrai.h   \s1,  \s1,  2
+    xvadd.h    \s1,  \s1,  \s3
+.endm
+
+.macro FILT_C s1, s2, s3
+    xmov    xr3,  \s1
+    xvpermi.q   xr3,  \s2,  0x03
+    xvshuf.b    xr1,  \s2,  xr3,  xr23
+    xvshuf.b    xr2,  \s2,  xr3,  xr24
+    xmov    \s1,  \s2
+    xvpermi.q   \s1,  \s3,  0x03
+    xvshuf.b   xr3,  \s1,  \s2,  xr29
+    xvshuf.b   xr4,  \s1,  \s2,  xr27
+    xvadd.h    xr3,  xr2,  xr3
+    xmov       xr2,  \s1
+    xmov       \s1,  \s3
+    xvshuf.b   \s3,  xr2,  \s2,  xr30
+    xvadd.h    xr4,  xr4,  \s2
+    xvadd.h    \s3,  \s3,  xr1
+    FILT_H     \s3,  xr3,  xr4
+.endm
+
+.macro DO_FILT_C_LASX s1, s2, s3, s4
+    FILT_C \s1, \s2, \s3
+    FILT_C \s2, \s1, \s4
+    FILT_PACK_LASX \s3, \s4, xr15
+    xvpermi.d   \s3, \s3, 0xd8
+    xvstx  \s3, a5,  a4
+.endm
+
+.macro DO_FILT_H_LASX s1, s2, s3
+    xmov    xr3,  \s1
+    xvpermi.q     xr3,  \s2,  0x03
+    xvshuf.b      xr1,  \s2,  xr3,  xr24
+    xvshuf.b      xr2,  \s2,  xr3,  xr25
+    xmov    xr3,  \s2
+    xvpermi.q     xr3,  \s3,  0x03
+    xvshuf.b      xr4,  xr3,  \s2,  xr26
+    xvshuf.b      xr5,  xr3,  \s2,  xr27
+    xvshuf.b      xr6,  xr3,  \s2,  xr28
+    xmov    \s1,  \s2
+    xvdp2.h.bu.b  xr16, xr1,  xr12
+    xvdp2.h.bu.b  xr17, xr2,  xr12
+    xvdp2.h.bu.b  xr18, \s2,  xr14
+    xvdp2.h.bu.b  xr19, xr4,  xr14
+    xvdp2.h.bu.b  xr20, xr5,  xr0
+    xvdp2.h.bu.b  xr21, xr6,  xr0
+    xvadd.h       xr1,  xr16, xr18
+    xvadd.h       xr2,  xr17, xr19
+    xvadd.h       xr1,  xr1,  xr20
+    xvadd.h       xr2,  xr2,  xr21
+    FILT_PACK_LASX  xr1, xr2, xr15
+    xvshuf.b        xr1, xr1, xr1,  xr22
+    xvstx    xr1,  a0,  a4
+    xmov     \s2,  \s3
+.endm
+
+//-----------------------------------------------------------------------------
+// void hpel_filter( uint8_t *dsth, uint8_t *dstv, uint8_t *dstc,
+//                   uint8_t *src, intptr_t stride, int width, int height )
+//-----------------------------------------------------------------------------
+function hpel_filter_lasx
+    addi.d sp,  sp, -56
+    fst.d  f24, sp, 0
+    fst.d  f25, sp, 8
+    fst.d  f26, sp, 16
+    fst.d  f27, sp, 24
+    fst.d  f28, sp, 32
+    fst.d  f29, sp, 40
+    fst.d  f30, sp, 48
+
+    move   a7,  a3
+    addi.d a5,  a5, -32
+    move   t0,  a1
+    andi   a7,  a7,  31
+    sub.d  a3,  a3,  a7
+    add.d  a0,  a0,  a5
+    add.d  t0,  t0,  a5
+    add.d  a7,  a7,  a5
+    add.d  a5,  a5,  a2
+    move   a2,  a4
+    sub.d  a7,  zero,  a7
+    add.d  a1,  a3,  a2
+    sub.d  a3,  a3,  a2
+    sub.d  a3,  a3,  a2
+    move   a4,  a7
+    la.local    t1,  filt_mul51
+    xvld  xr0,  t1,  0
+    la.local    t2,  filt_mul15
+    xvld  xr12, t2,  0
+    la.local    t3,  filt_mul20
+    xvld  xr14, t3,  0
+    la.local    t4,  pw_1024
+    xvld  xr15, t4,  0
+    la.local    t1,  hpel_shuf
+    xvld  xr22, t1,  0
+    la.local    t2,  shuf_12
+    xvld  xr23, t2,  0
+    la.local    t3,  shuf_1
+    xvld  xr26, t3,  0
+    xvaddi.bu   xr24,  xr23,  2 /* shuf_14  */
+    xvaddi.bu   xr25,  xr23,  3 /* shuf_15  */
+    xvaddi.bu   xr27,  xr26,  1 /* shuf_2   */
+    xvaddi.bu   xr28,  xr26,  2 /* shuf_3   */
+    xvaddi.bu   xr29,  xr26,  3 /* shuf_4   */
+    xvaddi.bu   xr30,  xr26,  5 /* shuf_6   */
+    xvxor.v     xr9,   xr9,   xr9
+    xvxor.v     xr10,  xr10,  xr10
+.LOOPY:
+    DO_FILT_V_LASX xr8, xr7, xr13, xr12, 0
+.LOOPX:
+    DO_FILT_V_LASX xr6, xr5, xr11, xr12, 32
+.LASTX:
+    xvsrli.h  xr15, xr15, 1
+    DO_FILT_C_LASX  xr9,  xr8,  xr7,  xr6
+    xvadd.h   xr15, xr15, xr15
+    xmov      xr7,  xr5
+    DO_FILT_H_LASX  xr10, xr13, xr11
+    addi.d    a4,   a4,   32
+    blt       a4,   zero, .LOOPX
+    addi.d    t1,   a4,   -32
+    blt       t1,   zero, .LASTX
+    //setup regs for next y
+    sub.d     a4,   a4,   a7
+    sub.d     a4,   a4,   a2
+    sub.d     a1,   a1,   a4
+    sub.d     a3,   a3,   a4
+    add.d     a0,   a0,   a2
+    add.d     t0,   t0,   a2
+    add.d     a5,   a5,   a2
+    move      a4,   a7
+    addi.d    a6,   a6,   -1
+    blt       zero, a6,   .LOOPY
+    fld.d  f24, sp, 0
+    fld.d  f25, sp, 8
+    fld.d  f26, sp, 16
+    fld.d  f27, sp, 24
+    fld.d  f28, sp, 32
+    fld.d  f29, sp, 40
+    fld.d  f30, sp, 48
+    addi.d sp,  sp, 56
+endfunc
+
+//-----------------------------------------------------------------------------
+// void pixel_avg_wxh( pixel *dst, intptr_t dst_stride, pixel *src1, intptr_t src1_stride,
+//                     pixel *src2, intptr_t src2_stride, int weight );
+//-----------------------------------------------------------------------------
+.macro PIXEL_AVG w, h
+function pixel_avg_\w\()x\h\()_lasx
+    addi.d  t0,  a6,   -32
+    addi.d  a7,  zero, \h
+    bne     t0,  zero,  pixel_avg_weight_w16_lasx
+    b       pixel_avg_w16_lasx
+endfunc
+.endm
+
+PIXEL_AVG 16, 16
+PIXEL_AVG 16,  8
+
+.end    mc-a.S
+
+#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/mc-c.c b/common/loongarch/mc-c.c
index d442efcd..8be3f1d1 100644
--- a/common/loongarch/mc-c.c
+++ b/common/loongarch/mc-c.c
@@ -30,28 +30,6 @@
 
 #if !HIGH_BIT_DEPTH
 
-static const uint8_t pu_luma_mask_arr[16 * 6] =
-{
-    0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12,
-    0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12,
-    1, 4, 2, 5, 3, 6, 4, 7, 5, 8, 6, 9, 7, 10, 8, 11,
-    1, 4, 2, 5, 3, 6, 4, 7, 5, 8, 6, 9, 7, 10, 8, 11,
-    2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10,
-    2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10
-};
-
-static const uint8_t pu_chroma_mask_arr[16 * 2] =
-{
-    0, 2, 2, 4, 4, 6, 6, 8, 16, 18, 18, 20, 20, 22, 22, 24,
-    0, 2, 2, 4, 4, 6, 6, 8, 16, 18, 18, 20, 20, 22, 22, 24
-};
-
-static const uint8_t pu_chroma_mask_arr1[16 * 2] =
-{
-    0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16,
-    0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16
-};
-
 static const uint8_t pu_core_mask_arr[16 * 2] =
 {
     1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
@@ -773,9 +751,10 @@ static void avc_biwgt_opscale_4x2_nw_lasx( uint8_t *p_src1,
 
     src0 = __lasx_xvilvl_b( src0, src2 );
 
-    src0 = __lasx_xvdp2_h_bu( src0, wgt );
+    src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);
+    src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);
     src0 = __lasx_xvmaxi_h( src0, 0 );
-    src0 = __lasx_xvssrln_bu_h(src0, denom);
+    src0 = __lasx_xvssrlrn_bu_h(src0, denom);
 
     __lasx_xvstelm_w(src0, p_dst, 0, 0);
     __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
@@ -834,9 +813,10 @@ static void avc_biwgt_opscale_4x4multiple_nw_lasx( uint8_t *p_src1,
 
         src0 = __lasx_xvilvl_b( src0, tmp0 );
 
-        src0 = __lasx_xvdp2_h_bu( src0, wgt );
+        src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);
+        src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);
         src0 = __lasx_xvmaxi_h( src0, 0 );
-        src0 = __lasx_xvssrln_bu_h(src0, denom);
+        src0 = __lasx_xvssrlrn_bu_h(src0, denom);
 
         __lasx_xvstelm_w(src0, p_dst, 0, 0);
         __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
@@ -914,9 +894,10 @@ static void avc_biwgt_opscale_8width_nw_lasx( uint8_t *p_src1,
     src1 = __lasx_xvpermi_q( src2, src3, 0x02 );         \
     src0 = __lasx_xvilvl_b( src1, src0 );                \
                                                          \
-    src0 = __lasx_xvdp2_h_bu( src0, wgt );               \
+    src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);            \
+    src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);     \
     src0 = __lasx_xvmaxi_h( src0, 0 );                   \
-    src0 = __lasx_xvssrln_bu_h(src0, denom);             \
+    src0 = __lasx_xvssrlrn_bu_h(src0, denom);            \
                                                          \
     __lasx_xvstelm_d(src0, p_dst, 0, 0);                 \
     __lasx_xvstelm_d(src0, p_dst + i_dst_stride, 0, 2);  \
@@ -930,66 +911,6 @@ static void avc_biwgt_opscale_8width_nw_lasx( uint8_t *p_src1,
 
 #undef BIWGT_OPSCALE_8W_NW
 
-}
-static void avc_biwgt_opscale_16width_nw_lasx( uint8_t *p_src1,
-                                               int32_t i_src1_stride,
-                                               uint8_t *p_src2,
-                                               int32_t i_src2_stride,
-                                               uint8_t *p_dst,
-                                               int32_t i_dst_stride,
-                                               int32_t i_height,
-                                               int32_t i_log2_denom,
-                                               int32_t i_src1_weight,
-                                               int32_t i_src2_weight )
-{
-    uint8_t u_cnt;
-    __m256i src1_wgt, src2_wgt, wgt;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i denom;
-    int32_t i_src1_stride_x2 = i_src1_stride << 1;
-    int32_t i_src1_stride_x4 = i_src1_stride << 2;
-    int32_t i_src2_stride_x2 = i_src2_stride << 1;
-    int32_t i_src2_stride_x4 = i_src2_stride << 2;
-    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
-    int32_t i_src2_stride_x3 = i_src2_stride_x2 + i_src2_stride;
-
-    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
-    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
-
-    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
-
-#define BIWGT_OPSCALE_16W_NW( srcA, srcB )       \
-    srcA = __lasx_xvpermi_d( srcA, 0x50 );       \
-    srcB = __lasx_xvpermi_d( srcB, 0x50 );       \
-    srcA = __lasx_xvilvl_b( srcB, srcA );        \
-                                                 \
-    srcA = __lasx_xvdp2_h_b( srcA, wgt );        \
-    srcA = __lasx_xvmaxi_h( srcA, 0 );           \
-    srcA = __lasx_xvssrln_bu_h(srcA, denom);     \
-                                                 \
-    __lasx_xvstelm_d(srcA, p_dst, 0, 0);         \
-    __lasx_xvstelm_d(srcA, p_dst + 8, 0, 2);     \
-    p_dst += i_dst_stride;
-
-    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
-    {
-        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
-                   i_src1_stride_x2, p_src1, i_src1_stride_x3, src0, src1, src2, src3 );
-        p_src1 += i_src1_stride_x4;
-
-        DUP4_ARG2( __lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, p_src2,
-                   i_src2_stride_x2, p_src2, i_src2_stride_x3, src4, src5, src6, src7 );
-        p_src2 += i_src2_stride_x4;
-
-        BIWGT_OPSCALE_16W_NW( src0, src4 );
-        BIWGT_OPSCALE_16W_NW( src1, src5 );
-        BIWGT_OPSCALE_16W_NW( src2, src6 );
-        BIWGT_OPSCALE_16W_NW( src3, src7 );
-    }
-
-#undef BIWGT_OPSCALE_16W_NW
-
 }
 
 static void avc_biwgt_opscale_4x2_lasx( uint8_t *p_src1,
@@ -1198,72 +1119,6 @@ static void avc_biwgt_opscale_8width_lasx( uint8_t *p_src1,
 
 }
 
-static void avc_biwgt_opscale_16width_lasx( uint8_t *p_src1,
-                                            int32_t i_src1_stride,
-                                            uint8_t *p_src2,
-                                            int32_t i_src2_stride,
-                                            uint8_t *p_dst,
-                                            int32_t i_dst_stride,
-                                            int32_t i_height,
-                                            int32_t i_log2_denom,
-                                            int32_t i_src1_weight,
-                                            int32_t i_src2_weight,
-                                            int32_t i_offset_in )
-{
-    uint8_t u_cnt;
-    __m256i src1_wgt, src2_wgt, wgt;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i denom, offset;
-    int32_t i_src1_stride_x2 = i_src1_stride << 1;
-    int32_t i_src1_stride_x4 = i_src1_stride << 2;
-    int32_t i_src2_stride_x2 = i_src2_stride << 1;
-    int32_t i_src2_stride_x4 = i_src2_stride << 2;
-    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
-    int32_t i_src2_stride_x3 = i_src2_stride_x2 + i_src2_stride;
-
-    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
-
-    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
-    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
-    offset = __lasx_xvreplgr2vr_h( i_offset_in );
-
-    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
-
-#define BIWGT_OPSCALE_16W( srcA, srcB )          \
-    srcA = __lasx_xvpermi_d( srcA, 0x50 );       \
-    srcB = __lasx_xvpermi_d( srcB, 0x50 );       \
-    srcA = __lasx_xvilvl_b( srcB, srcA );        \
-                                                 \
-    srcA = __lasx_xvdp2_h_bu( srcA, wgt );       \
-    srcA = __lasx_xvsadd_h( srcA, offset );      \
-    srcA = __lasx_xvmaxi_h( srcA, 0 );           \
-    srcA = __lasx_xvssrln_bu_h(srcA, denom);     \
-                                                 \
-    __lasx_xvstelm_d(srcA, p_dst, 0, 0);         \
-    __lasx_xvstelm_d(srcA, p_dst + 8, 0, 2);     \
-    p_dst += i_dst_stride;
-
-    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
-    {
-        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
-                   i_src1_stride_x2, p_src1, i_src1_stride_x3, src0, src1, src2, src3 );
-        p_src1 += i_src1_stride_x4;
-
-        DUP4_ARG2( __lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, p_src2,
-                   i_src2_stride_x2, p_src2, i_src2_stride_x3, src4, src5, src6, src7 );
-        p_src2 += i_src2_stride_x4;
-
-        BIWGT_OPSCALE_16W( src0, src4 );
-        BIWGT_OPSCALE_16W( src1, src5 );
-        BIWGT_OPSCALE_16W( src2, src6 );
-        BIWGT_OPSCALE_16W( src3, src7 );
-    }
-
-#undef BIWGT_OPSCALE_16W
-
-}
-
 static void avg_src_width4_lasx( uint8_t *p_src1, int32_t i_src1_stride,
                                  uint8_t *p_src2, int32_t i_src2_stride,
                                  uint8_t *p_dst, int32_t i_dst_stride,
@@ -1349,90 +1204,6 @@ static void avg_src_width8_lasx( uint8_t *p_src1, int32_t i_src1_stride,
     );
 }
 
-static void avg_src_width16_lasx( uint8_t *p_src1, int32_t i_src1_stride,
-                                  uint8_t *p_src2, int32_t i_src2_stride,
-                                  uint8_t *p_dst, int32_t i_dst_stride,
-                                  int32_t i_height )
-{
-    int32_t i_cnt = i_height >> 3;
-    int32_t i_src1_stride_x2, i_src1_stride_x3, i_src1_stride_x4;
-    int32_t i_src2_stride_x2, i_src2_stride_x3, i_src2_stride_x4;
-    int32_t i_dst_stride_x2, i_dst_stride_x3, i_dst_stride_x4;
-
-    __asm__ volatile(
-    "slli.w    %[src1_stride2],  %[src1_stride1],  1                    \n\t"
-    "add.w     %[src1_stride3],  %[src1_stride2],  %[src1_stride1]      \n\t"
-    "slli.w    %[src1_stride4],  %[src1_stride1],  2                    \n\t"
-    "slli.w    %[src2_stride2],  %[src2_stride1],  1                    \n\t"
-    "add.w     %[src2_stride3],  %[src2_stride2],  %[src2_stride1]      \n\t"
-    "slli.w    %[src2_stride4],  %[src2_stride1],  2                    \n\t"
-    "slli.w    %[dst_stride2],   %[dst_stride1],   1                    \n\t"
-    "add.w     %[dst_stride3],   %[dst_stride2],   %[dst_stride1]       \n\t"
-    "slli.w    %[dst_stride4],   %[dst_stride1],   2                    \n\t"
-    "beqz      %[cnt],           2f                                     \n\t"
-    "1:                                                                 \n\t"
-    "addi.w    %[cnt],           %[cnt],           -1                   \n\t"
-    "vld       $vr0,             %[src1],          0                    \n\t"
-    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
-    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
-    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
-    "vld       $vr4,             %[src2],          0                    \n\t"
-    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
-    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
-    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
-    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
-    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
-    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
-    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
-    "vst       $vr0,             %[dst],           0                    \n\t"
-    "vstx      $vr1,             %[dst],           %[dst_stride1]       \n\t"
-    "vstx      $vr2,             %[dst],           %[dst_stride2]       \n\t"
-    "vstx      $vr3,             %[dst],           %[dst_stride3]       \n\t"
-    "add.d     %[dst],           %[dst],           %[dst_stride4]       \n\t"
-    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
-    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
-
-    "vld       $vr0,             %[src1],          0                    \n\t"
-    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
-    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
-    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
-    "vld       $vr4,             %[src2],          0                    \n\t"
-    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
-    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
-    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
-    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
-    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
-    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
-    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
-    "vst       $vr0,             %[dst],           0                    \n\t"
-    "vstx      $vr1,             %[dst],           %[dst_stride1]       \n\t"
-    "vstx      $vr2,             %[dst],           %[dst_stride2]       \n\t"
-    "vstx      $vr3,             %[dst],           %[dst_stride3]       \n\t"
-    "add.d     %[dst],           %[dst],           %[dst_stride4]       \n\t"
-    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
-    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
-
-    "bnez      %[cnt],           1b                                     \n\t"
-    "2:                                                                 \n\t"
-     : [src1]"+&r"(p_src1),
-       [src2]"+&r"(p_src2),
-       [src1_stride2]"=&r"(i_src1_stride_x2),
-       [src1_stride3]"=&r"(i_src1_stride_x3),
-       [src1_stride4]"=&r"(i_src1_stride_x4),
-       [src2_stride2]"=&r"(i_src2_stride_x2),
-       [src2_stride3]"=&r"(i_src2_stride_x3),
-       [src2_stride4]"=&r"(i_src2_stride_x4),
-       [dst_stride2]"=&r"(i_dst_stride_x2),
-       [dst_stride3]"=&r"(i_dst_stride_x3),
-       [dst_stride4]"=&r"(i_dst_stride_x4),
-       [dst]"+&r"(p_dst), [cnt]"+&r"(i_cnt)
-     : [src1_stride1]"r"(i_src1_stride),
-       [src2_stride1]"r"(i_src2_stride),
-       [dst_stride1]"r"(i_dst_stride)
-     : "memory"
-    );
-}
-
 static void *x264_memcpy_aligned_lasx(void *dst, const void *src, size_t n)
 {
     int64_t zero = 0, d;
@@ -1473,62 +1244,6 @@ static void *x264_memcpy_aligned_lasx(void *dst, const void *src, size_t n)
     return NULL;
 }
 
-static void pixel_avg_16x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                  uint8_t *p_pix2, intptr_t pix2_stride,
-                                  uint8_t *p_pix3, intptr_t pix3_stride,
-                                  int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width16_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                              p_pix1, pix1_stride, 16 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_16width_nw_lasx( p_pix2, pix2_stride,
-                                           p_pix3, pix3_stride,
-                                           p_pix1, pix1_stride,
-                                           16, 5, i_weight,
-                                           ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_16width_lasx( p_pix2, pix2_stride,
-                                        p_pix3, pix3_stride,
-                                        p_pix1, pix1_stride,
-                                        16, 5, i_weight,
-                                        ( 64 - i_weight ), 0 );
-    }
-}
-
-static void pixel_avg_16x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                 uint8_t *p_pix2, intptr_t pix2_stride,
-                                 uint8_t *p_pix3, intptr_t pix3_stride,
-                                 int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width16_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                              p_pix1, pix1_stride, 8 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_16width_nw_lasx( p_pix2, pix2_stride,
-                                           p_pix3, pix3_stride,
-                                           p_pix1, pix1_stride,
-                                           8, 5, i_weight,
-                                           ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_16width_lasx( p_pix2, pix2_stride,
-                                        p_pix3, pix3_stride,
-                                        p_pix1, pix1_stride,
-                                        8, 5, i_weight,
-                                        ( 64 - i_weight ), 0 );
-    }
-}
-
 static void pixel_avg_8x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
                                  uint8_t *p_pix2, intptr_t pix2_stride,
                                  uint8_t *p_pix3, intptr_t pix3_stride,
@@ -2288,417 +2003,6 @@ static uint8_t *get_ref_lasx( uint8_t *p_dst, intptr_t *p_dst_stride,
     }
 }
 
-static void avc_interleaved_chroma_hv_2x2_lasx( uint8_t *p_src,
-                                                int32_t i_src_stride,
-                                                uint8_t *p_dst_u,
-                                                uint8_t *p_dst_v,
-                                                int32_t i_dst_stride,
-                                                uint32_t u_coef_hor0,
-                                                uint32_t u_coef_hor1,
-                                                uint32_t u_coef_ver0,
-                                                uint32_t u_coef_ver1 )
-{
-    __m256i src0, src1, src2, src3, src4;
-    __m256i mask, mask1;
-
-    __m256i coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
-    __m256i coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
-    __m256i coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
-    __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
-    __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
-
-    mask = __lasx_xvld(pu_chroma_mask_arr, 0);
-    mask1 = __lasx_xvaddi_bu(mask, 1);
-
-    src0 = __lasx_xvld( p_src, 0);
-    src1 = __lasx_xvldx( p_src, i_src_stride);
-    src2 = __lasx_xvldx( p_src, (i_src_stride << 1));
-
-    DUP2_ARG3( __lasx_xvshuf_b, src1, src0, mask1, src2, src1, mask1, src3, src4 );
-    DUP2_ARG3( __lasx_xvshuf_b, src1, src0, mask, src2, src1, mask, src0, src1 );
-
-    src0 = __lasx_xvpermi_q( src0, src3, 0x02 );
-    src1 = __lasx_xvpermi_q( src1, src4, 0x02 );
-    DUP2_ARG2( __lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
-    src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-    src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
-    src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-    __lasx_xvstelm_h( src0, p_dst_u, 0, 0 );
-    __lasx_xvstelm_h( src0, p_dst_u + i_dst_stride, 0, 2 );
-    __lasx_xvstelm_h( src0, p_dst_v, 0, 8 );
-    __lasx_xvstelm_h( src0, p_dst_v + i_dst_stride, 0, 10 );
-}
-
-static void avc_interleaved_chroma_hv_2x4_lasx( uint8_t *p_src,
-                                                int32_t i_src_stride,
-                                                uint8_t *p_dst_u,
-                                                uint8_t *p_dst_v,
-                                                int32_t i_dst_stride,
-                                                uint32_t u_coef_hor0,
-                                                uint32_t u_coef_hor1,
-                                                uint32_t u_coef_ver0,
-                                                uint32_t u_coef_ver1 )
-{
-    int32_t src_stride2 = i_src_stride << 1;
-    int32_t src_stride3 = i_src_stride + src_stride2;
-    int32_t src_stride4 = src_stride2 << 1;
-    int32_t dst_stride2 = i_dst_stride << 1;
-    int32_t dst_stride3 = i_dst_stride + dst_stride2;
-    int32_t dst_stride4 = dst_stride2 << 1;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
-    __m256i mask, mask1;
-
-    __m256i coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
-    __m256i coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
-    __m256i coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
-    __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
-    __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
-
-    mask = __lasx_xvld( pu_chroma_mask_arr, 0);
-    mask1 = __lasx_xvaddi_bu(mask, 1);
-
-    src0 = __lasx_xvld( p_src, 0 );
-    src1 = __lasx_xvldx( p_src, i_src_stride);
-    src2 = __lasx_xvldx( p_src, src_stride2);
-    src3 = __lasx_xvldx( p_src, src_stride3);
-    src4 = __lasx_xvldx( p_src, src_stride4);
-
-    DUP4_ARG3( __lasx_xvshuf_b, src1, src0, mask1, src2, src1, mask1, src3, src2,
-               mask1, src4, src3, mask1, src5, src6, src7, src8 );
-    DUP4_ARG3( __lasx_xvshuf_b, src1, src0, mask, src2, src1, mask, src3, src2, mask,
-               src4, src3, mask, src0, src1, src2, src3 );
-
-    src0 = __lasx_xvpermi_q( src0, src2, 0x02 );
-    src1 = __lasx_xvpermi_q( src1, src3, 0x02 );
-    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
-    src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-    src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
-    src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-    __lasx_xvstelm_h(src0, p_dst_u, 0, 0);
-    __lasx_xvstelm_h(src0, p_dst_u + i_dst_stride, 0, 2);
-    __lasx_xvstelm_h(src0, p_dst_u + dst_stride2, 0, 8);
-    __lasx_xvstelm_h(src0, p_dst_u + dst_stride3, 0, 10);
-    p_dst_u += dst_stride4;
-
-    src0 = __lasx_xvpermi_q( src5, src7, 0x02 );
-    src1 = __lasx_xvpermi_q( src6, src8, 0x02 );
-    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
-    src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-    src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
-    src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-    __lasx_xvstelm_h(src0, p_dst_v, 0, 0);
-    __lasx_xvstelm_h(src0, p_dst_v + i_dst_stride, 0, 2);
-    __lasx_xvstelm_h(src0, p_dst_v + dst_stride2, 0, 8);
-    __lasx_xvstelm_h(src0, p_dst_v + dst_stride3, 0, 10);
-    p_dst_v += dst_stride4;
-}
-
-static void avc_interleaved_chroma_hv_2w_lasx( uint8_t *p_src,
-                                               int32_t i_src_stride,
-                                               uint8_t *p_dst_u,
-                                               uint8_t *p_dst_v,
-                                               int32_t i_dst_stride,
-                                               uint32_t u_coef_hor0,
-                                               uint32_t u_coef_hor1,
-                                               uint32_t u_coef_ver0,
-                                               uint32_t u_coef_ver1,
-                                               int32_t i_height )
-{
-    if( 2 == i_height )
-    {
-        avc_interleaved_chroma_hv_2x2_lasx( p_src, i_src_stride,
-                                            p_dst_u, p_dst_v, i_dst_stride,
-                                            u_coef_hor0, u_coef_hor1,
-                                            u_coef_ver0, u_coef_ver1 );
-    }
-    else if( 4 == i_height )
-    {
-        avc_interleaved_chroma_hv_2x4_lasx( p_src, i_src_stride,
-                                            p_dst_u, p_dst_v, i_dst_stride,
-                                            u_coef_hor0, u_coef_hor1,
-                                            u_coef_ver0, u_coef_ver1 );
-    }
-}
-
-static void avc_interleaved_chroma_hv_4x2_lasx( uint8_t *p_src,
-                                                int32_t i_src_stride,
-                                                uint8_t *p_dst_u,
-                                                uint8_t *p_dst_v,
-                                                int32_t i_dst_stride,
-                                                uint32_t u_coef_hor0,
-                                                uint32_t u_coef_hor1,
-                                                uint32_t u_coef_ver0,
-                                                uint32_t u_coef_ver1 )
-{
-    __m256i src0, src1, src2, src3, src4;
-    __m256i mask, mask1;
-
-    __m256i coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
-    __m256i coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
-    __m256i coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
-    __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
-    __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
-
-    mask = __lasx_xvld( pu_chroma_mask_arr, 0 );
-    mask1 = __lasx_xvaddi_bu(mask, 1);
-
-    src0 = __lasx_xvld( p_src, 0 );
-    src1 = __lasx_xvldx( p_src, i_src_stride);
-    src2 = __lasx_xvldx( p_src, (i_src_stride << 1));
-
-    DUP2_ARG3( __lasx_xvshuf_b, src1, src0, mask1, src2, src1, mask1, src3, src4 );
-    DUP2_ARG3( __lasx_xvshuf_b, src1, src0, mask, src2, src1, mask, src0, src1 );
-
-    src0 = __lasx_xvpermi_q( src0, src3, 0x02 );
-    src1 = __lasx_xvpermi_q( src1, src4, 0x02 );
-    DUP2_ARG2( __lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
-    src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-    src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
-    src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-    __lasx_xvstelm_w( src0, p_dst_u, 0, 0 );
-    __lasx_xvstelm_w( src0, p_dst_u + i_dst_stride, 0, 1 );
-    __lasx_xvstelm_w( src0, p_dst_v, 0, 4 );
-    __lasx_xvstelm_w( src0, p_dst_v + i_dst_stride, 0,  5 );
-}
-
-static void avc_interleaved_chroma_hv_4x4mul_lasx( uint8_t *p_src,
-                                                   int32_t i_src_stride,
-                                                   uint8_t *p_dst_u,
-                                                   uint8_t *p_dst_v,
-                                                   int32_t i_dst_stride,
-                                                   uint32_t u_coef_hor0,
-                                                   uint32_t u_coef_hor1,
-                                                   uint32_t u_coef_ver0,
-                                                   uint32_t u_coef_ver1,
-                                                   int32_t i_height )
-{
-    uint32_t u_row;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
-    __m256i mask, mask1;
-    int32_t src_stride2 = i_src_stride << 1;
-    int32_t dst_stride2 = i_dst_stride << 1;
-    int32_t src_stride3 = i_src_stride + src_stride2;
-    int32_t dst_stride3 = i_dst_stride + dst_stride2;
-    int32_t src_stride4 = src_stride2 << 1;
-    int32_t dst_stride4 = dst_stride2 << 1;
-    __m256i coeff_hz_vec0, coeff_hz_vec1;
-    __m256i coeff_hz_vec;
-    __m256i coeff_vt_vec0, coeff_vt_vec1;
-
-    coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
-    coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
-    coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
-    coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
-    coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
-
-    mask = __lasx_xvld( pu_chroma_mask_arr, 0 );
-    mask1 = __lasx_xvaddi_bu(mask, 1);
-
-    src0 = __lasx_xvld( p_src, 0 );
-
-    for( u_row = ( i_height >> 2 ); u_row--; )
-    {
-        src1 = __lasx_xvldx(p_src, i_src_stride);
-        src2 = __lasx_xvldx(p_src, src_stride2);
-        src3 = __lasx_xvldx(p_src, src_stride3);
-        src4 = __lasx_xvldx(p_src, src_stride4);
-        p_src += src_stride4;
-
-        DUP4_ARG3( __lasx_xvshuf_b, src1, src0, mask1, src2, src1, mask1, src3, src2,
-                   mask1, src4, src3, mask1, src5, src6, src7, src8 );
-        DUP4_ARG3( __lasx_xvshuf_b, src1, src0, mask, src2, src1, mask, src3, src2,
-                   mask, src4, src3, mask, src0, src1, src2, src3 );
-
-        src0 = __lasx_xvpermi_q( src0, src2, 0x02 );
-        src1 = __lasx_xvpermi_q( src1, src3, 0x02 );
-        DUP2_ARG2( __lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
-        src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-        src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
-        src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-        __lasx_xvstelm_w(src0, p_dst_u, 0, 0);
-        __lasx_xvstelm_w(src0, p_dst_u + i_dst_stride, 0, 1);
-        __lasx_xvstelm_w(src0, p_dst_u + dst_stride2, 0, 4);
-        __lasx_xvstelm_w(src0, p_dst_u + dst_stride3, 0, 5);
-        p_dst_u += dst_stride4;
-
-        src0 = __lasx_xvpermi_q( src5, src7, 0x02 );
-        src1 = __lasx_xvpermi_q( src6, src8, 0x02 );
-        DUP2_ARG2( __lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
-        src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-        src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
-        src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-        __lasx_xvstelm_w(src0, p_dst_v, 0, 0);
-        __lasx_xvstelm_w(src0, p_dst_v + i_dst_stride, 0, 1);
-        __lasx_xvstelm_w(src0, p_dst_v + dst_stride2, 0, 4);
-        __lasx_xvstelm_w(src0, p_dst_v + dst_stride3, 0, 5);
-        p_dst_v += dst_stride4;
-
-        src0 = src4;
-    }
-}
-
-static void avc_interleaved_chroma_hv_4w_lasx( uint8_t *p_src,
-                                               int32_t i_src_stride,
-                                               uint8_t *p_dst_u,
-                                               uint8_t *p_dst_v,
-                                               int32_t i_dst_stride,
-                                               uint32_t u_coef_hor0,
-                                               uint32_t u_coef_hor1,
-                                               uint32_t u_coef_ver0,
-                                               uint32_t u_coef_ver1,
-                                               int32_t i_height )
-{
-    if( 2 == i_height )
-    {
-        avc_interleaved_chroma_hv_4x2_lasx( p_src, i_src_stride,
-                                            p_dst_u, p_dst_v, i_dst_stride,
-                                            u_coef_hor0, u_coef_hor1,
-                                            u_coef_ver0, u_coef_ver1 );
-    }
-    else
-    {
-        avc_interleaved_chroma_hv_4x4mul_lasx( p_src, i_src_stride,
-                                               p_dst_u, p_dst_v, i_dst_stride,
-                                               u_coef_hor0, u_coef_hor1,
-                                               u_coef_ver0, u_coef_ver1,
-                                               i_height );
-    }
-}
-
-static void avc_interleaved_chroma_hv_8w_lasx( uint8_t *p_src,
-                                               int32_t i_src_stride,
-                                               uint8_t *p_dst_u,
-                                               uint8_t *p_dst_v,
-                                               int32_t i_dst_stride,
-                                               uint32_t u_coef_hor0,
-                                               uint32_t u_coef_hor1,
-                                               uint32_t u_coef_ver0,
-                                               uint32_t u_coef_ver1,
-                                               int32_t i_height )
-{
-    uint32_t u_row;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
-    __m256i mask, mask1;
-    __m256i coeff_hz_vec0, coeff_hz_vec1;
-    __m256i coeff_hz_vec;
-    __m256i coeff_vt_vec0, coeff_vt_vec1;
-    __m256i tmp0, tmp1, tmp2, tmp3;
-    __m256i head_u, head_v;
-
-    int32_t src_stride2 = i_src_stride << 1;
-    int32_t dst_stride2 = i_dst_stride << 1;
-    int32_t src_stride3 = i_src_stride + src_stride2;
-    int32_t dst_stride3 = i_dst_stride + dst_stride2;
-    int32_t src_stride4 = src_stride2 << 1;
-    int32_t dst_stride4 = dst_stride2 << 1;
-
-    coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
-    coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
-    coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
-    coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
-    coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
-
-    mask = __lasx_xvld( pu_chroma_mask_arr1, 0 );
-    mask1 = __lasx_xvaddi_bu(mask, 1);
-
-    src0 = __lasx_xvld( p_src, 0 );
-    tmp0 = __lasx_xvpermi_q( src0, src0, 0x11 );
-    DUP2_ARG3( __lasx_xvshuf_b, tmp0, src0, mask, tmp0, src0, mask1, head_u, head_v );
-    DUP2_ARG2( __lasx_xvdp2_h_bu, head_u, coeff_hz_vec, head_v, coeff_hz_vec,
-               head_u, head_v );
-
-    for( u_row = ( i_height >> 2 ); u_row--; )
-    {
-        src1 = __lasx_xvldx(p_src, i_src_stride);
-        src2 = __lasx_xvldx(p_src, src_stride2);
-        src3 = __lasx_xvldx(p_src, src_stride3);
-        src4 = __lasx_xvldx(p_src, src_stride4);
-        p_src += src_stride4;
-        src5 = __lasx_xvpermi_q( src1, src2, 0x02 );
-        src6 = __lasx_xvpermi_q( src1, src2, 0x13 );
-        src7 = __lasx_xvpermi_q( src3, src4, 0x02 );
-        src8 = __lasx_xvpermi_q( src3, src4, 0x13 );
-
-        DUP2_ARG3( __lasx_xvshuf_b, src6, src5, mask, src8, src7, mask, tmp0, tmp1 );
-        DUP2_ARG2( __lasx_xvdp2_h_bu, tmp0, coeff_hz_vec, tmp1, coeff_hz_vec, tmp0, tmp1);
-        tmp2 = __lasx_xvpermi_q( head_u, tmp0, 0x02 );
-        tmp3 = __lasx_xvpermi_q( tmp0, tmp1, 0x03 );
-        head_u = __lasx_xvpermi_q( tmp1, tmp1, 0x11 );
-
-        tmp0 = __lasx_xvmul_h( tmp0, coeff_vt_vec0 );
-        tmp1 = __lasx_xvmul_h( tmp1, coeff_vt_vec0 );
-        tmp0 = __lasx_xvmadd_h( tmp0, tmp2, coeff_vt_vec1 );
-        tmp1 = __lasx_xvmadd_h( tmp1, tmp3, coeff_vt_vec1 );
-
-        tmp0 = __lasx_xvssrlrni_bu_h(tmp1, tmp0, 6);
-
-        __lasx_xvstelm_d(tmp0, p_dst_u, 0, 0);
-        __lasx_xvstelm_d(tmp0, p_dst_u + i_dst_stride, 0, 2);
-        __lasx_xvstelm_d(tmp0, p_dst_u + dst_stride2, 0, 1);
-        __lasx_xvstelm_d(tmp0, p_dst_u + dst_stride3, 0, 3);
-        p_dst_u += dst_stride4;
-
-        DUP2_ARG3( __lasx_xvshuf_b, src6, src5, mask1, src8, src7, mask1, tmp0, tmp1 );
-        DUP2_ARG2( __lasx_xvdp2_h_bu, tmp0, coeff_hz_vec, tmp1, coeff_hz_vec, tmp0, tmp1);
-        tmp2 = __lasx_xvpermi_q( head_v, tmp0, 0x02 );
-        tmp3 = __lasx_xvpermi_q( tmp0, tmp1, 0x03 );
-        head_v = __lasx_xvpermi_q( tmp1, tmp1, 0x11 );
-
-        tmp0 = __lasx_xvmul_h( tmp0, coeff_vt_vec0 );
-        tmp1 = __lasx_xvmul_h( tmp1, coeff_vt_vec0 );
-        tmp0 = __lasx_xvmadd_h( tmp0, tmp2, coeff_vt_vec1 );
-        tmp1 = __lasx_xvmadd_h( tmp1, tmp3, coeff_vt_vec1 );
-
-        tmp0 = __lasx_xvssrlrni_bu_h(tmp1, tmp0, 6);
-        __lasx_xvstelm_d(tmp0, p_dst_v, 0, 0);
-        __lasx_xvstelm_d(tmp0, p_dst_v + i_dst_stride, 0, 2);
-        __lasx_xvstelm_d(tmp0, p_dst_v + dst_stride2, 0, 1);
-        __lasx_xvstelm_d(tmp0, p_dst_v + dst_stride3, 0, 3);
-        p_dst_v += dst_stride4;
-    }
-}
-
-static void mc_chroma_lasx( uint8_t *p_dst_u, uint8_t *p_dst_v,
-                            intptr_t i_dst_stride,
-                            uint8_t *p_src, intptr_t i_src_stride,
-                            int32_t m_vx, int32_t m_vy,
-                            int32_t i_width, int32_t i_height )
-{
-    int32_t i_d8x = m_vx & 0x07;
-    int32_t i_d8y = m_vy & 0x07;
-    int32_t i_coeff_horiz1 = ( 8 - i_d8x );
-    int32_t i_coeff_vert1 = ( 8 - i_d8y );
-    int32_t i_coeff_horiz0 = i_d8x;
-    int32_t i_coeff_vert0 = i_d8y;
-
-    p_src += ( m_vy >> 3 ) * i_src_stride + ( m_vx >> 3 ) * 2;
-
-    if( 4 == i_width )
-    {
-        avc_interleaved_chroma_hv_4w_lasx( p_src, i_src_stride,
-                                           p_dst_u, p_dst_v, i_dst_stride,
-                                           i_coeff_horiz0, i_coeff_horiz1,
-                                           i_coeff_vert0, i_coeff_vert1,
-                                           i_height );
-    }
-    else if( 8 == i_width )
-    {
-        avc_interleaved_chroma_hv_8w_lasx( p_src, i_src_stride,
-                                           p_dst_u, p_dst_v, i_dst_stride,
-                                           i_coeff_horiz0, i_coeff_horiz1,
-                                           i_coeff_vert0, i_coeff_vert1,
-                                           i_height );
-    }
-    else if( 2 == i_width )
-    {
-        avc_interleaved_chroma_hv_2w_lasx( p_src, i_src_stride,
-                                           p_dst_u, p_dst_v, i_dst_stride,
-                                           i_coeff_horiz0, i_coeff_horiz1,
-                                           i_coeff_vert0, i_coeff_vert1,
-                                           i_height );
-    }
-}
-
 static void copy_width4_lasx( uint8_t *p_src, int32_t i_src_stride,
                               uint8_t *p_dst, int32_t i_dst_stride,
                               int32_t i_height )
@@ -2908,145 +2212,6 @@ static void mc_luma_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
     }
 }
 
-static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                  uint8_t *p_dst, int32_t i_dst_stride,
-                                  int32_t i_height )
-{
-    uint32_t u_loop_cnt, u_h4w;
-    const int16_t i_filt_const0 = 0xfb01;
-    const int16_t i_filt_const1 = 0x1414;
-    const int16_t i_filt_const2 = 0x1fb;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
-    __m256i src10_h, src32_h, src54_h, src76_h;
-    __m256i src21_h, src43_h, src65_h, src87_h;
-    __m256i src10_l, src32_l, src54_l, src76_l;
-    __m256i src21_l, src43_l, src65_l, src87_l;
-    __m256i out10_h, out32_h, out10_l, out32_l;
-    __m256i res10_h, res32_h, res10_l, res32_l;
-    __m256i tmp10_h, tmp32_h, tmp10_l, tmp32_l;
-    __m256i filt0, filt1, filt2;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_src_stride_x4 = i_src_stride << 2;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-
-    u_h4w = i_height % 4;
-    filt0 = __lasx_xvreplgr2vr_h( i_filt_const0 );
-    filt1 = __lasx_xvreplgr2vr_h( i_filt_const1 );
-    filt2 = __lasx_xvreplgr2vr_h( i_filt_const2 );
-
-    src0 = __lasx_xvld( p_src, 0 );
-    p_src += i_src_stride;
-    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
-               p_src, i_src_stride_x3, src1, src2, src3, src4 );
-    p_src += i_src_stride_x4;
-
-    src0 = __lasx_xvxori_b( src0, 128 );
-    DUP4_ARG2( __lasx_xvxori_b, src1, 128, src2, 128, src3, 128, src4, 128,
-               src1, src2, src3, src4 );
-
-    src10_l = __lasx_xvilvl_b( src1, src0 );
-    src10_h = __lasx_xvilvh_b( src1, src0 );
-    src21_l = __lasx_xvilvl_b( src2, src1 );
-    src21_h = __lasx_xvilvh_b( src2, src1 );
-    src32_l = __lasx_xvilvl_b( src3, src2 );
-    src32_h = __lasx_xvilvh_b( src3, src2 );
-    src43_l = __lasx_xvilvl_b( src4, src3 );
-    src43_h = __lasx_xvilvh_b( src4, src3 );
-    res10_h = __lasx_xvpermi_q( src21_h, src10_h, 0x20 );
-    res32_h = __lasx_xvpermi_q( src43_h, src32_h, 0x20 );
-    res10_l = __lasx_xvpermi_q( src21_l, src10_l, 0x20 );
-    res32_l = __lasx_xvpermi_q( src43_l, src32_l, 0x20 );
-
-    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
-    {
-        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
-                   p_src, i_src_stride_x3, src5, src6, src7, src8 );
-        p_src += i_src_stride_x4;
-
-        DUP4_ARG2( __lasx_xvxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5,
-                   src6, src7, src8 );
-        src54_l = __lasx_xvilvl_b( src5, src4 );
-        src54_h = __lasx_xvilvh_b( src5, src4 );
-        src65_l = __lasx_xvilvl_b( src6, src5 );
-        src65_h = __lasx_xvilvh_b( src6, src5 );
-        src76_l = __lasx_xvilvl_b( src7, src6 );
-        src76_h = __lasx_xvilvh_b( src7, src6 );
-        src87_l = __lasx_xvilvl_b( src8, src7 );
-        src87_h = __lasx_xvilvh_b( src8, src7 );
-        tmp10_h = __lasx_xvpermi_q( src65_h, src54_h, 0x20 );
-        tmp32_h = __lasx_xvpermi_q( src87_h, src76_h, 0x20 );
-        tmp10_l = __lasx_xvpermi_q( src65_l, src54_l, 0x20 );
-        tmp32_l = __lasx_xvpermi_q( src87_l, src76_l, 0x20 );
-
-        out10_h = __lasx_xvdp2_h_b( res10_h, filt0 );
-        out10_h = __lasx_xvdp2add_h_b( out10_h, res32_h, filt1 );
-        out10_h = __lasx_xvdp2add_h_b( out10_h, tmp10_h, filt2 );
-
-
-        out32_h = __lasx_xvdp2_h_b( res32_h, filt0 );
-        out32_h = __lasx_xvdp2add_h_b( out32_h, tmp10_h, filt1 );
-        out32_h = __lasx_xvdp2add_h_b( out32_h, tmp32_h, filt2 );
-
-        out10_l = __lasx_xvdp2_h_b( res10_l, filt0 );
-        out10_l = __lasx_xvdp2add_h_b( out10_l, res32_l, filt1 );
-        out10_l = __lasx_xvdp2add_h_b( out10_l, tmp10_l, filt2 );
-
-        out32_l = __lasx_xvdp2_h_b( res32_l, filt0 );
-        out32_l = __lasx_xvdp2add_h_b( out32_l, tmp10_l, filt1 );
-        out32_l = __lasx_xvdp2add_h_b( out32_l, tmp32_l, filt2 );
-
-        out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
-        out32_l = __lasx_xvssrarni_b_h(out32_h, out32_l, 5);
-        DUP2_ARG2( __lasx_xvxori_b, out10_l, 128, out32_l, 128, out10_l, out32_l );
-
-        __lasx_xvstelm_d( out10_l, p_dst, 0, 0 );
-        __lasx_xvstelm_d( out10_l, p_dst, 8, 1 );
-        __lasx_xvstelm_d( out10_l, p_dst + i_dst_stride, 0, 2 );
-        __lasx_xvstelm_d( out10_l, p_dst + i_dst_stride, 8, 3 );
-        p_dst += i_dst_stride_x2;
-        __lasx_xvstelm_d( out32_l, p_dst, 0, 0 );
-        __lasx_xvstelm_d( out32_l, p_dst, 8, 1 );
-        __lasx_xvstelm_d( out32_l, p_dst + i_dst_stride, 0, 2 );
-        __lasx_xvstelm_d( out32_l, p_dst + i_dst_stride, 8, 3 );
-        p_dst += i_dst_stride_x2;
-
-        res10_h = tmp10_h;
-        res32_h = tmp32_h;
-        res10_l = tmp10_l;
-        res32_l = tmp32_l;
-        src4 = src8;
-    }
-
-    for( u_loop_cnt = u_h4w; u_loop_cnt--; )
-    {
-        src5 = __lasx_xvld( p_src, 0 );
-        p_src += i_src_stride;
-        src5 = __lasx_xvxori_b( src5, 128 );
-        src54_h = __lasx_xvilvl_b( src5, src4 );
-        src54_l = __lasx_xvilvh_b( src5, src4 );
-        out10_h = __lasx_xvdp2_h_b( src10_h, filt0 );
-        out10_h = __lasx_xvdp2add_h_b( out10_h, src32_h, filt1 );
-        out10_h = __lasx_xvdp2add_h_b( out10_h, src54_h, filt2 );
-
-        out10_l = __lasx_xvdp2_h_b( src10_l, filt0 );
-        out10_l = __lasx_xvdp2add_h_b( out10_l, src32_l, filt1 );
-        out10_l = __lasx_xvdp2add_h_b( out10_l, src54_l, filt2 );
-        out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
-        out10_l = __lasx_xvxori_b( out10_l, 128 );
-        __lasx_xvstelm_d( out10_l, p_dst, 0, 0 );
-        __lasx_xvstelm_d( out10_l, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-
-        src10_h = src21_h;
-        src32_h = src43_h;
-        src10_l = src21_l;
-        src32_l = src43_l;
-
-        src4 = src5;
-    }
-}
-
 #define LASX_HORZ_FILTER_SH( in, mask0, mask1, mask2 )         \
 ( {                                                            \
     __m256i out0_m;                                            \
@@ -3090,220 +2255,6 @@ static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
     out0_m;                                                                \
 } )
 
-static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                   uint8_t *p_dst, int32_t i_dst_stride,
-                                   int32_t i_height )
-{
-    uint32_t u_loop_cnt, u_h4w;
-    int32_t minus = -5, plus = 20;
-    __m256i src0, src1, src2, src3, src4;
-    __m256i src5, src6, src7, src8;
-    __m256i mask0, mask1, mask2;
-    __m256i dst0, dst1, dst2, dst3;
-    __m256i out0, out1;
-    __m256i minus5b, plus20b, minus5h, plus20h;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_src_stride_x4 = i_src_stride << 2;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-
-    minus5b = __lasx_xvreplgr2vr_b( minus );
-    plus20b = __lasx_xvreplgr2vr_b( plus );
-    minus5h = __lasx_xvreplgr2vr_h( minus );
-    plus20h = __lasx_xvreplgr2vr_h( plus );
-
-    u_h4w = i_height & 3;
-    mask0 = __lasx_xvld( pu_luma_mask_arr, 0 );
-    mask1 = __lasx_xvld( &pu_luma_mask_arr[32], 0 );
-    mask2 = __lasx_xvld( &pu_luma_mask_arr[64], 0 );
-
-    src0 = __lasx_xvld( p_src, 0 );
-    p_src += i_src_stride;
-    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
-               p_src, i_src_stride_x3, src1, src2, src3, src4 );
-    p_src += i_src_stride_x4;
-    src0 = __lasx_xvpermi_d( src0, 0x94);
-    src1 = __lasx_xvpermi_d( src1, 0x94);
-    src2 = __lasx_xvpermi_d( src2, 0x94);
-    src3 = __lasx_xvpermi_d( src3, 0x94);
-    src4 = __lasx_xvpermi_d( src4, 0x94);
-
-    src0 = __lasx_xvxori_b( src0, 128 );
-    DUP4_ARG2( __lasx_xvxori_b, src1, 128, src2, 128, src3, 128, src4, 128, src1, src2,
-               src3, src4 );
-
-    src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
-    src1 = LASX_HORZ_FILTER_SH( src1, mask0, mask1, mask2 );
-    src2 = LASX_HORZ_FILTER_SH( src2, mask0, mask1, mask2 );
-    src3 = LASX_HORZ_FILTER_SH( src3, mask0, mask1, mask2 );
-    src4 = LASX_HORZ_FILTER_SH( src4, mask0, mask1, mask2 );
-
-    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
-    {
-        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
-                   i_src_stride_x2, p_src, i_src_stride_x3, src5, src6, src7, src8 );
-        p_src += i_src_stride_x4;
-        src5 = __lasx_xvpermi_d( src5, 0x94);
-        src6 = __lasx_xvpermi_d( src6, 0x94);
-        src7 = __lasx_xvpermi_d( src7, 0x94);
-        src8 = __lasx_xvpermi_d( src8, 0x94);
-
-        DUP4_ARG2( __lasx_xvxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
-                   src5, src6, src7, src8 );
-
-        src5 = LASX_HORZ_FILTER_SH( src5, mask0, mask1, mask2 );
-        src6 = LASX_HORZ_FILTER_SH( src6, mask0, mask1, mask2 );
-        src7 = LASX_HORZ_FILTER_SH( src7, mask0, mask1, mask2 );
-        src8 = LASX_HORZ_FILTER_SH( src8, mask0, mask1, mask2 );
-        dst0 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src0, src1, src2,
-                                                 src3, src4, src5 );
-        dst1 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src1, src2, src3,
-                                                 src4, src5, src6 );
-        dst2 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src2, src3, src4,
-                                                 src5, src6, src7 );
-        dst3 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src3, src4, src5,
-                                                 src6, src7, src8 );
-        out0 = __lasx_xvpickev_b( dst1, dst0 );
-        out1 = __lasx_xvpickev_b( dst3, dst2 );
-        DUP2_ARG2( __lasx_xvxori_b, out0, 128, out1, 128, out0, out1 );
-        __lasx_xvstelm_d( out0, p_dst, 0, 0 );
-        __lasx_xvstelm_d( out0, p_dst, 8, 2 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( out0, p_dst, 0, 1 );
-        __lasx_xvstelm_d( out0, p_dst, 8, 3 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( out1, p_dst, 0, 0 );
-        __lasx_xvstelm_d( out1, p_dst, 8, 2 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( out1, p_dst, 0, 1 );
-        __lasx_xvstelm_d( out1, p_dst, 8, 3 );
-        p_dst += i_dst_stride;
-
-        src3 = src7;
-        src1 = src5;
-        src5 = src4;
-        src4 = src8;
-        src2 = src6;
-        src0 = src5;
-    }
-
-    for( u_loop_cnt = u_h4w; u_loop_cnt--; )
-    {
-        src5 = __lasx_xvld( p_src, 0 );
-        p_src += i_src_stride;
-        src5 = __lasx_xvpermi_d( src5, 0x94);
-        src5 = __lasx_xvxori_b( src5, 128 );
-
-        src5 = LASX_HORZ_FILTER_SH( src5, mask0, mask1, mask2 );
-        dst0 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src0, src1,
-                                                 src2, src3,
-                                                 src4, src5 );
-
-        out0 = __lasx_xvpickev_b( dst0, dst0 );
-        out0 = __lasx_xvxori_b( out0, 128 );
-        __lasx_xvstelm_d( out0, p_dst, 0, 0);
-        __lasx_xvstelm_d( out0, p_dst, 8, 2);
-        p_dst += i_dst_stride;
-
-        src0 = src1;
-        src1 = src2;
-        src2 = src3;
-        src3 = src4;
-        src4 = src5;
-    }
-}
-
-static void avc_luma_hz_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                  uint8_t *p_dst, int32_t i_dst_stride,
-                                  int32_t i_height )
-{
-    uint32_t u_loop_cnt, u_h4w;
-    int32_t minus = -5, plus = 20;
-    __m256i src0, src1, src2, src3;
-    __m256i mask0, mask1, mask2;
-    __m256i minus5b, plus20b;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_src_stride_x4 = i_src_stride << 2;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-
-    minus5b = __lasx_xvreplgr2vr_b( minus );
-    plus20b = __lasx_xvreplgr2vr_b( plus );
-
-    u_h4w = i_height & 3;
-    mask0 = __lasx_xvld( pu_luma_mask_arr, 0 );
-    mask1 = __lasx_xvld( &pu_luma_mask_arr[32], 0 );
-    mask2 = __lasx_xvld( &pu_luma_mask_arr[64], 0 );
-
-    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
-    {
-        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
-                   p_src, i_src_stride_x3, src0, src1, src2, src3 );
-        p_src += i_src_stride_x4;
-        src0 = __lasx_xvpermi_d( src0, 0x94);
-        src1 = __lasx_xvpermi_d( src1, 0x94);
-        src2 = __lasx_xvpermi_d( src2, 0x94);
-        src3 = __lasx_xvpermi_d( src3, 0x94);
-
-        DUP4_ARG2( __lasx_xvxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
-                   src0, src1, src2, src3 );
-
-        src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
-        src1 = LASX_HORZ_FILTER_SH( src1, mask0, mask1, mask2 );
-        src2 = LASX_HORZ_FILTER_SH( src2, mask0, mask1, mask2 );
-        src3 = LASX_HORZ_FILTER_SH( src3, mask0, mask1, mask2 );
-
-        src0 = __lasx_xvssrarni_b_h(src1, src0, 5);
-        src1 = __lasx_xvssrarni_b_h(src3, src2, 5);
-        DUP2_ARG2( __lasx_xvxori_b, src0, 128, src1, 128, src0, src1);
-        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src0, p_dst, 8, 2 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( src0, p_dst, 0, 1);
-        __lasx_xvstelm_d( src0, p_dst, 8, 3);
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( src1, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src1, p_dst, 8, 2 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( src1, p_dst, 0, 1 );
-        __lasx_xvstelm_d( src1, p_dst, 8, 3 );
-        p_dst += i_dst_stride;
-    }
-
-    for( u_loop_cnt = u_h4w; u_loop_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src, 0 );
-        p_src += i_src_stride;
-        src0 = __lasx_xvpermi_d( src0, 0x94);
-
-        src0 = __lasx_xvxori_b( src0, 128 );
-        src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
-        src0 = __lasx_xvssrarni_b_h(src0, src0, 5);
-        src0 = __lasx_xvxori_b( src0, 128 );
-        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src0, p_dst, 8, 2 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static void hpel_filter_lasx( uint8_t *p_dsth, uint8_t *p_dst_v,
-                              uint8_t *p_dstc, uint8_t *p_src,
-                              intptr_t i_stride, int32_t i_width,
-                              int32_t i_height, int16_t *p_buf )
-{
-    for( int32_t i = 0; i < ( i_width >> 4 ); i++ )
-    {
-        avc_luma_vt_16w_lasx( p_src - 2 - ( 2 * i_stride ), i_stride,
-                              p_dst_v - 2, i_stride, i_height );
-        avc_luma_mid_16w_lasx( p_src - 2 - ( 2 * i_stride ) , i_stride,
-                               p_dstc, i_stride, i_height );
-        avc_luma_hz_16w_lasx( p_src - 2, i_stride, p_dsth, i_stride, i_height );
-
-        p_src += 16;
-        p_dst_v += 16;
-        p_dsth += 16;
-        p_dstc += 16;
-    }
-}
-
 static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
                                               int32_t i_src_stride,
                                               uint8_t *p_dst0,
@@ -3382,11 +2333,13 @@ static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
             p_dst3 += 16;
         }
 
-        for( i_loop_width = i_w16_mul; i_loop_width < i_width;
-             i_loop_width += 8 )
+        if (i_w16_mul < i_width)
         {
             src3  = __lasx_xvld( p_src, 0 );
             DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src4, src5 );
+            src6 = __lasx_xvpermi_q( src3, src3, 0x11 );
+            src7 = __lasx_xvpermi_q( src4, src4, 0x11 );
+            src8 = __lasx_xvpermi_q( src5, src5, 0x11 );
             p_src += 16;
 
             pckev_vec0 = __lasx_xvpickev_b( src3, src0 );
@@ -3395,26 +2348,26 @@ static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
             pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
             pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
             pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
-            DUP4_ARG2( __lasx_xvhsubw_hu_bu, pckev_vec1, pckev_vec0, pckod_vec1,
+            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1,
                        pckod_vec0, pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
                        tmp0, tmp1, tmp2, tmp3 );
-            DUP2_ARG2( __lasx_xvhsubw_hu_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
             __lasx_xvstelm_d( tmp0, p_dst0, 0, 0 );
             __lasx_xvstelm_d( tmp1, p_dst2, 0, 0 );
 
-            DUP2_ARG3( __lasx_xvshuf_b, src3, src0, src4, src1, mask,
+            DUP2_ARG3( __lasx_xvshuf_b, src3, src0, mask, src4, src1,
                        mask, sld1_vec0, sld1_vec1 );
-            DUP2_ARG3( __lasx_xvshuf_b, src5, src2, src3, src3, mask,
+            DUP2_ARG3( __lasx_xvshuf_b, src5, src2, mask, src6, src3,
                        mask, sld1_vec2, sld1_vec3 );
-            DUP2_ARG3( __lasx_xvshuf_b, src4, src4, src5, src5, mask,
+            DUP2_ARG3( __lasx_xvshuf_b, src7, src4, mask, src8, src5,
                        mask, sld1_vec4, sld1_vec5 );
             pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
             pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
             pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
-            DUP4_ARG2( __lasx_xvhsubw_hu_bu, pckev_vec1, pckev_vec0, pckod_vec1,
-                       pckod_vec0, pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
-                       tmp0, tmp1, tmp2, tmp3 );
-            DUP2_ARG2( __lasx_xvhsubw_hu_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
+                       tmp3 );
+            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
             __lasx_xvstelm_d( tmp0, p_dst1, 0, 0 );
             __lasx_xvstelm_d( tmp1, p_dst3, 0, 0 );
             p_dst0 += 8;
@@ -3912,14 +2865,14 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
             __lasx_xvstelm_d( vec_ilv_l0, p_dst, 0, 0 );
             __lasx_xvstelm_d( vec_ilv_l0, p_dst, 8, 1 );
             p_dst_t = p_dst + i_dst_stride;
-            __lasx_xvstelm_d( vec_ilv_l1, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_ilv_l1, p_dst, 8, 1 );
+            __lasx_xvstelm_d( vec_ilv_l1, p_dst_t, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l1, p_dst_t, 8, 1 );
             p_dst_t = p_dst_t + i_dst_stride;
-            __lasx_xvstelm_d( vec_ilv_l2, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_ilv_l2, p_dst, 8, 1 );
+            __lasx_xvstelm_d( vec_ilv_l2, p_dst_t, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l2, p_dst_t, 8, 1 );
             p_dst_t = p_dst_t + i_dst_stride;
-            __lasx_xvstelm_d( vec_ilv_l3, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_ilv_l3, p_dst, 8, 1 );
+            __lasx_xvstelm_d( vec_ilv_l3, p_dst_t, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l3, p_dst_t, 8, 1 );
 
             p_src0 += 8;
             p_src1 += 8;
@@ -3965,13 +2918,13 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
         {
             src0 = __lasx_xvld( p_src0, 0 );
             src4 = __lasx_xvld( p_src1, 0 );
-            vec_ilv_h0 = __lasx_xvilvl_b( src4, src0 );
-            vec_ilv_l0 = __lasx_xvilvh_b( src4, src0 );
+            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
+            vec_ilv_h0 = __lasx_xvilvh_b( src4, src0 );
 
             src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
             src1 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x13 );
             __lasx_xvst( src0, p_dst, 0 );
-            __lasx_xvst( src0, p_dst, 32 );
+            __lasx_xvst( src1, p_dst, 32 );
 
             p_src0 += 32;
             p_src1 += 32;
@@ -3982,8 +2935,8 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
         {
             src0 = __lasx_xvld( p_src0, 0 );
             src4 = __lasx_xvld( p_src1, 0 );
-            vec_ilv_h0 = __lasx_xvilvl_b( src4, src0 );
-            vec_ilv_l0 = __lasx_xvilvh_b( src4, src0 );
+            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
+            vec_ilv_h0 = __lasx_xvilvh_b( src4, src0 );
 
             vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
             __lasx_xvst( vec_ilv_l0, p_dst, 0 );
@@ -4270,6 +3223,12 @@ static void prefetch_fenc_420_lasx( uint8_t *pix_y, intptr_t stride_y,
     );
 }
 
+#define x264_mc_chroma_lasx x264_template(mc_chroma_lasx)
+void x264_mc_chroma_lasx( uint8_t *p_dst_u, uint8_t *p_dst_v,
+                          intptr_t i_dst_stride,
+                          uint8_t *p_src, intptr_t i_src_stride,
+                          int32_t m_vx, int32_t m_vy,
+                          int32_t i_width, int32_t i_height );
 #endif // !HIGH_BIT_DEPTH
 
 void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
@@ -4278,11 +3237,11 @@ void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
     if( cpu & X264_CPU_LASX )
     {
         pf->mc_luma = mc_luma_lasx;
-        pf->mc_chroma = mc_chroma_lasx;
+        pf->mc_chroma = x264_mc_chroma_lasx;
         pf->get_ref = get_ref_lasx;
 
-        pf->avg[PIXEL_16x16]= pixel_avg_16x16_lasx;
-        pf->avg[PIXEL_16x8] = pixel_avg_16x8_lasx;
+        pf->avg[PIXEL_16x16]= x264_pixel_avg_16x16_lasx;
+        pf->avg[PIXEL_16x8] = x264_pixel_avg_16x8_lasx;
         pf->avg[PIXEL_8x16] = pixel_avg_8x16_lasx;
         pf->avg[PIXEL_8x8] = pixel_avg_8x8_lasx;
         pf->avg[PIXEL_8x4] = pixel_avg_8x4_lasx;
@@ -4309,7 +3268,7 @@ void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
         pf->plane_copy_deinterleave = plane_copy_deinterleave_lasx;
         pf->plane_copy_deinterleave_yuyv = plane_copy_deinterleave_lasx;
 
-        pf->hpel_filter = hpel_filter_lasx;
+        pf->hpel_filter = x264_hpel_filter_lasx;
         pf->memcpy_aligned = x264_memcpy_aligned_lasx;
         pf->memzero_aligned = memzero_aligned_lasx;
         pf->frame_init_lowres_core = frame_init_lowres_core_lasx;
diff --git a/common/loongarch/mc.h b/common/loongarch/mc.h
index ce1b9a78..29b35250 100644
--- a/common/loongarch/mc.h
+++ b/common/loongarch/mc.h
@@ -30,4 +30,12 @@
 #define x264_mc_init_loongarch x264_template(mc_init_loongarch)
 void x264_mc_init_loongarch( int cpu, x264_mc_functions_t *pf );
 
+#define x264_pixel_avg_16x16_lasx x264_template(pixel_avg_16x16_lasx)
+void x264_pixel_avg_16x16_lasx( pixel *, intptr_t, pixel *, intptr_t, pixel *, intptr_t, int );
+#define x264_pixel_avg_16x8_lasx x264_template(pixel_avg_16x8_lasx)
+void x264_pixel_avg_16x8_lasx( pixel *, intptr_t, pixel *, intptr_t, pixel *, intptr_t, int );
+#define x264_hpel_filter_lasx x264_template(hpel_filter_lasx)
+void x264_hpel_filter_lasx( pixel *, pixel *, pixel *, pixel *, intptr_t, int, int, int16_t * );
+
+
 #endif
diff --git a/common/loongarch/pixel-a.S b/common/loongarch/pixel-a.S
new file mode 100644
index 00000000..776bfe07
--- /dev/null
+++ b/common/loongarch/pixel-a.S
@@ -0,0 +1,1062 @@
+/*****************************************************************************
+ * pixel-a.S: LoongArch pixel metrics
+ *****************************************************************************
+ * Copyright (C) 2015-2022 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+#if !HIGH_BIT_DEPTH
+
+const hmul_8p
+.byte 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1
+.byte 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1
+endconst
+
+const mask_ac4b
+.short 0, -1, 0, -1, -1, -1, -1, -1
+.short 0, -1, 0, -1, -1, -1, -1, -1
+endconst
+
+const mask_ac8
+.short 0, -1, -1, -1, -1, -1, -1, -1
+.short 0, -1, -1, -1, -1, -1, -1, -1
+endconst
+
+
+.macro LOAD_INC_8x4W n1, n2, n3, n4, n5
+    vld    $vr\n1,  a0,  0
+    vldx   $vr\n2,  a0,  a1
+    vldx   $vr\n3,  a0,  t0
+    vldx   $vr\n4,  a0,  t1
+    xvpermi.d   xr18,  $xr\n1,  0x05
+    xvpermi.d   xr19,  $xr\n2,  0x05
+    xvpermi.d   xr20,  $xr\n3,  0x05
+    xvpermi.d   xr21,  $xr\n4,  0x05
+    add.d   a0,  a0,  t2
+    xvdp2.h.bu.b   $xr\n1,     xr18,    $xr\n5
+    xvdp2.h.bu.b   $xr\n2,     xr19,    $xr\n5
+    xvdp2.h.bu.b   $xr\n3,     xr20,    $xr\n5
+    xvdp2.h.bu.b   $xr\n4,     xr21,    $xr\n5
+.endm
+
+.macro SUMSUB_BADC a, b, c, d
+    xvadd.h  \a,  \a,  \b
+    xvadd.h  \c,  \c,  \d
+    xvadd.h  \b,  \b,  \b
+    xvadd.h  \d,  \d,  \d
+    xvsub.h  \b,  \b,  \a
+    xvsub.h  \d,  \d,  \c
+.endm
+
+.macro HADAMARD4_V a, b, c, d
+    SUMSUB_BADC  \a,  \b,  \c,  \d
+    SUMSUB_BADC  \a,  \c,  \b,  \d
+.endm
+
+.macro HADAMARD_1 a, b, tmp
+    xmov \tmp, \a
+    xvpackod.h  \a, \b, \a
+    xvpackev.h  \b, \b, \tmp
+    xvadd.h     \tmp, \a, \b
+    xvsub.h     \b, \b,  \a
+    xmov        \a, \tmp
+.endm
+
+.macro HADAMARD_2 a, b, c
+    xvpickod.w  \c,  \b,  \a
+    xvpickev.w  \a,  \b,  \a
+    xvadda.h    \a,  \a,  xr17
+    xvadda.h    \c,  \c,  xr17
+    xvmax.h     \a,  \a,  \c
+.endm
+
+.macro  HADAMARD_AC_WXH_LASX  w, h
+function pixel_hadamard_ac_\w\()x\h\()_lasx
+    add.d       t0,     a1,     a1
+    add.d       t1,     a1,     t0
+    add.d       t2,     t1,     a1
+    xvxor.v     xr17,   xr17,  xr17
+    move        t4,     ra
+    bl hadamard_ac_16x8_lasx
+.if \h == 16
+    xmov    xr11,   xr9
+    xmov    xr10,   xr8
+    bl hadamard_ac_16x8_lasx
+    xvadd.h   xr9,   xr9,   xr11
+    xvadd.h   xr8,   xr8,   xr10
+.endif
+    move        ra,     t4
+    xvhaddw.wu.hu   xr8,    xr8,    xr8
+    xvhaddw.du.wu   xr8,    xr8,    xr8
+    xvhaddw.qu.du   xr8,    xr8,    xr8
+    xvpickve2gr.wu  t0,     xr8,    0
+    xvpickve2gr.wu  t1,     xr8,    4
+    add.d     t0,   t0,     t1
+    xvhaddw.wu.hu   xr9,    xr9,    xr9
+    xvhaddw.du.wu   xr9,    xr9,    xr9
+    xvhaddw.qu.du   xr9,    xr9,    xr9
+    xvpickve2gr.wu  t1,     xr9,    0
+    xvpickve2gr.wu  t2,     xr9,    4
+    add.d     t1,   t1,     t2
+    srli.d      t0,     t0,    2
+    srli.d      t1,     t1,    1
+    slli.d      t0,     t0,    32
+    add.d       a0,     t0,    t1
+endfunc
+.endm
+
+function hadamard_ac_16x8_lasx, export=0
+/* Load intermediate variable */
+    la.local    t3,     hmul_8p
+    xvld        xr8,    t3,     0
+    LOAD_INC_8x4W  0, 1, 2, 3, 8
+    HADAMARD4_V    xr0, xr1, xr2, xr3
+    LOAD_INC_8x4W  4, 5, 6, 7, 8
+    HADAMARD4_V    xr4, xr5, xr6, xr7
+    HADAMARD_1     xr0, xr1, xr8
+    HADAMARD_1     xr2, xr3, xr8
+    xmov   xr18, xr1
+    HADAMARD_1     xr4, xr5, xr8
+    HADAMARD_1     xr6, xr7, xr8
+    xmov   xr19,   xr2
+    xmov   xr20,   xr3
+    xvadda.h  xr1,  xr0,  xr4
+    xvsub.h   xr21, xr4,  xr0
+    xvadd.h   xr0,  xr4,  xr0
+    la.local    t3,     mask_ac4b
+    xvld        xr8,    t3,     0
+    xvand.v     xr1,    xr1,    xr8
+    xvadda.h    xr1,    xr1,    xr5
+    xvadda.h    xr1,    xr1,    xr18
+    xvadda.h    xr1,    xr1,    xr19
+    xvadda.h    xr1,    xr1,    xr20
+    xvadda.h    xr1,    xr1,    xr6
+    xvadda.h    xr9,    xr1,    xr7
+
+    xvadd.h     xr3,    xr7,    xr20
+    xvsub.h     xr7,    xr7,    xr20
+    xvadd.h     xr2,    xr6,    xr19
+    xvsub.h     xr6,    xr6,    xr19
+    xvadd.h     xr1,    xr5,    xr18
+    xvsub.h     xr5,    xr5,    xr18
+
+    HADAMARD_2  xr3,    xr7,    xr18
+    HADAMARD_2  xr2,    xr6,    xr19
+    HADAMARD_2  xr1,    xr5,    xr20
+
+    xvpickod.w  xr5,    xr21,   xr0
+    xvpickev.w  xr0,    xr21,   xr0
+    xmov        xr4,    xr5
+    xvadd.h     xr5,    xr0,    xr4
+    xvsub.h     xr4,    xr4,    xr0
+
+    xvadd.h     xr2,    xr2,    xr3
+    xvadd.h     xr2,    xr2,    xr1
+    xvadd.h     xr2,    xr2,    xr2
+
+    la.local    t3,     mask_ac8
+    xvld        xr8,    t3,     0
+    xvand.v     xr0,    xr5,    xr8
+
+    xvadda.h    xr2,    xr2,    xr4
+    xvadda.h    xr8,    xr2,    xr0
+endfunc
+
+HADAMARD_AC_WXH_LASX 16, 8
+HADAMARD_AC_WXH_LASX 16, 16
+
+
+/* int x264_pixel_satd_16x16_lasx(pixel *pix1, intptr_t i_pix1,
+ *                                pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_satd_16x16_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    slli.d          t4,    a1,   2
+    slli.d          t5,    a3,   2
+    add.d           t6,    a1,   t2
+    add.d           t7,    a3,   t3
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t6,  vr0,  vr1,  vr2,  vr3
+    add.d           a0,    a0,   t4
+    LSX_LOADX_4     a0,    a1,   t2,  t6,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4     a2,    a3,   t3,  t7,  vr8,  vr9,  vr10, vr11
+    add.d           a2,    a2,   t5
+    LSX_LOADX_4     a2,    a3,   t3,  t7,  vr12, vr13, vr14, vr15
+    xvpermi.q       xr0,   xr4,  0x02
+    xvpermi.q       xr1,   xr5,  0x02
+    xvpermi.q       xr2,   xr6,  0x02
+    xvpermi.q       xr3,   xr7,  0x02
+    xvpermi.q       xr8,   xr12, 0x02
+    xvpermi.q       xr9,   xr13, 0x02
+    xvpermi.q       xr10,  xr14, 0x02
+    xvpermi.q       xr11,  xr15, 0x02
+
+    // HADAMARD4
+    xvsubwev.h.bu   xr4,   xr0,  xr8
+    xvsubwod.h.bu   xr5,   xr0,  xr8
+    xvsubwev.h.bu   xr6,   xr1,  xr9
+    xvsubwod.h.bu   xr7,   xr1,  xr9
+    xvsubwev.h.bu   xr8,   xr2,  xr10
+    xvsubwod.h.bu   xr9,   xr2,  xr10
+    xvsubwev.h.bu   xr12,  xr3,  xr11
+    xvsubwod.h.bu   xr13,  xr3,  xr11
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr8,  xr9
+    xvsub.h         xr5,   xr8,  xr9
+    xvadd.h         xr6,   xr12, xr13
+    xvsub.h         xr7,   xr12, xr13
+    xvpackev.h      xr8,   xr5,  xr4
+    xvpackod.h      xr9,   xr5,  xr4
+    xvpackev.h      xr10,  xr7,  xr6
+    xvpackod.h      xr11,  xr7,  xr6
+    xvpackev.h      xr4,   xr1,  xr0
+    xvpackod.h      xr5,   xr1,  xr0
+    xvpackev.h      xr6,   xr3,  xr2
+    xvpackod.h      xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr8,  xr9
+    xvsub.h         xr5,   xr8,  xr9
+    xvadd.h         xr6,   xr10, xr11
+    xvsub.h         xr7,   xr10, xr11
+    xvilvl.h        xr8,   xr1,  xr0
+    xvilvl.h        xr9,   xr3,  xr2
+    xvilvl.h        xr10,  xr5,  xr4
+    xvilvl.h        xr11,  xr7,  xr6
+    xvilvh.h        xr0,   xr1,  xr0
+    xvilvh.h        xr1,   xr3,  xr2
+    xvilvh.h        xr2,   xr5,  xr4
+    xvilvh.h        xr3,   xr7,  xr6
+    xvadd.h         xr4,   xr8,  xr9
+    xvadd.h         xr6,   xr10, xr11
+    xvsub.h         xr5,   xr8,  xr9
+    xvsub.h         xr7,   xr10, xr11
+    xvadd.h         xr8,   xr4,  xr6
+    xvadd.h         xr9,   xr5,  xr7
+    xvsub.h         xr10,  xr4,  xr6
+    xvsub.h         xr11,  xr5,  xr7
+    xvadd.h         xr4,   xr0,  xr1
+    xvadd.h         xr6,   xr2,  xr3
+    xvsub.h         xr5,   xr0,  xr1
+    xvsub.h         xr7,   xr2,  xr3
+    xvadd.h         xr0,   xr4,  xr6
+    xvadd.h         xr1,   xr5,  xr7
+    xvsub.h         xr2,   xr4,  xr6
+    xvsub.h         xr3,   xr5,  xr7
+    xvadda.h        xr8,   xr8,  xr9
+    xvadda.h        xr9,   xr10, xr11
+    xvadda.h        xr0,   xr0,  xr1
+    xvadda.h        xr1,   xr2,  xr3
+    xvadd.h         xr8,   xr8,  xr9
+    xvadd.h         xr0,   xr0,  xr1
+    xvadd.h         xr16,  xr0,  xr8
+
+    add.d           a0,    a0,   t4
+    add.d           a2,    a2,   t5
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t6,  vr0,  vr1,  vr2,  vr3
+    add.d           a0,    a0,   t4
+    LSX_LOADX_4     a0,    a1,   t2,  t6,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4     a2,    a3,   t3,  t7,  vr8,  vr9,  vr10, vr11
+    add.d           a2,    a2,   t5
+    LSX_LOADX_4     a2,    a3,   t3,  t7,  vr12, vr13, vr14, vr15
+    xvpermi.q       xr0,   xr4,  0x02
+    xvpermi.q       xr1,   xr5,  0x02
+    xvpermi.q       xr2,   xr6,  0x02
+    xvpermi.q       xr3,   xr7,  0x02
+    xvpermi.q       xr8,   xr12, 0x02
+    xvpermi.q       xr9,   xr13, 0x02
+    xvpermi.q       xr10,  xr14, 0x02
+    xvpermi.q       xr11,  xr15, 0x02
+
+    // HADAMARD4
+    xvsubwev.h.bu   xr4,   xr0,  xr8
+    xvsubwod.h.bu   xr5,   xr0,  xr8
+    xvsubwev.h.bu   xr6,   xr1,  xr9
+    xvsubwod.h.bu   xr7,   xr1,  xr9
+    xvsubwev.h.bu   xr8,   xr2,  xr10
+    xvsubwod.h.bu   xr9,   xr2,  xr10
+    xvsubwev.h.bu   xr12,  xr3,  xr11
+    xvsubwod.h.bu   xr13,  xr3,  xr11
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr8,  xr9
+    xvsub.h         xr5,   xr8,  xr9
+    xvadd.h         xr6,   xr12, xr13
+    xvsub.h         xr7,   xr12, xr13
+    xvpackev.h      xr8,   xr5,  xr4
+    xvpackod.h      xr9,   xr5,  xr4
+    xvpackev.h      xr10,  xr7,  xr6
+    xvpackod.h      xr11,  xr7,  xr6
+    xvpackev.h      xr4,   xr1,  xr0
+    xvpackod.h      xr5,   xr1,  xr0
+    xvpackev.h      xr6,   xr3,  xr2
+    xvpackod.h      xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr8,  xr9
+    xvsub.h         xr5,   xr8,  xr9
+    xvadd.h         xr6,   xr10, xr11
+    xvsub.h         xr7,   xr10, xr11
+    xvilvl.h        xr8,   xr1,  xr0
+    xvilvl.h        xr9,   xr3,  xr2
+    xvilvl.h        xr10,  xr5,  xr4
+    xvilvl.h        xr11,  xr7,  xr6
+    xvilvh.h        xr0,   xr1,  xr0
+    xvilvh.h        xr1,   xr3,  xr2
+    xvilvh.h        xr2,   xr5,  xr4
+    xvilvh.h        xr3,   xr7,  xr6
+    xvadd.h         xr4,   xr8,  xr9
+    xvadd.h         xr6,   xr10, xr11
+    xvsub.h         xr5,   xr8,  xr9
+    xvsub.h         xr7,   xr10, xr11
+    xvadd.h         xr8,   xr4,  xr6
+    xvadd.h         xr9,   xr5,  xr7
+    xvsub.h         xr10,  xr4,  xr6
+    xvsub.h         xr11,  xr5,  xr7
+    xvadd.h         xr4,   xr0,  xr1
+    xvadd.h         xr6,   xr2,  xr3
+    xvsub.h         xr5,   xr0,  xr1
+    xvsub.h         xr7,   xr2,  xr3
+    xvadd.h         xr0,   xr4,  xr6
+    xvadd.h         xr1,   xr5,  xr7
+    xvsub.h         xr2,   xr4,  xr6
+    xvsub.h         xr3,   xr5,  xr7
+    xvadda.h        xr8,   xr8,  xr9
+    xvadda.h        xr9,   xr10, xr11
+    xvadda.h        xr0,   xr0,  xr1
+    xvadda.h        xr1,   xr2,  xr3
+    xvadd.h         xr8,   xr8,  xr9
+    xvadd.h         xr0,   xr0,  xr1
+    xvadd.h         xr0,   xr0,  xr8
+    xvadd.h         xr0,   xr0,  xr16
+
+    xvhaddw.wu.hu   xr0,   xr0,  xr0
+    xvhaddw.du.wu   xr0,   xr0,  xr0
+    xvhaddw.qu.du   xr0,   xr0,  xr0
+    xvpickve2gr.wu  t0,    xr0,  0
+    xvpickve2gr.wu  t1,    xr0,  4
+    add.w           t0,    t0,   t1
+    srli.d          a0,    t0,   1
+endfunc
+
+/* int x264_pixel_satd_16x8_lasx(pixel *pix1, intptr_t i_pix1,
+ *                               pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_satd_16x8_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    slli.d          t4,    t2,   1
+    slli.d          t5,    t3,   1
+    add.d           t6,    a1,   t2
+    add.d           t7,    a3,   t3
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t6,  vr0,  vr1,  vr2,  vr3
+    add.d           a0,    a0,   t4
+    LSX_LOADX_4     a0,    a1,   t2,  t6,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4     a2,    a3,   t3,  t7,  vr8,  vr9,  vr10, vr11
+    add.d           a2,    a2,   t5
+    LSX_LOADX_4     a2,    a3,   t3,  t7,  vr12, vr13, vr14, vr15
+    xvpermi.q       xr0,   xr4,  0x02
+    xvpermi.q       xr1,   xr5,  0x02
+    xvpermi.q       xr2,   xr6,  0x02
+    xvpermi.q       xr3,   xr7,  0x02
+    xvpermi.q       xr8,   xr12, 0x02
+    xvpermi.q       xr9,   xr13, 0x02
+    xvpermi.q       xr10,  xr14, 0x02
+    xvpermi.q       xr11,  xr15, 0x02
+
+    // HADAMARD4
+    xvsubwev.h.bu   xr4,   xr0,  xr8
+    xvsubwod.h.bu   xr5,   xr0,  xr8
+    xvsubwev.h.bu   xr6,   xr1,  xr9
+    xvsubwod.h.bu   xr7,   xr1,  xr9
+    xvsubwev.h.bu   xr8,   xr2,  xr10
+    xvsubwod.h.bu   xr9,   xr2,  xr10
+    xvsubwev.h.bu   xr12,  xr3,  xr11
+    xvsubwod.h.bu   xr13,  xr3,  xr11
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr8,  xr9
+    xvsub.h         xr5,   xr8,  xr9
+    xvadd.h         xr6,   xr12, xr13
+    xvsub.h         xr7,   xr12, xr13
+    xvpackev.h      xr8,   xr5,  xr4
+    xvpackod.h      xr9,   xr5,  xr4
+    xvpackev.h      xr10,  xr7,  xr6
+    xvpackod.h      xr11,  xr7,  xr6
+    xvpackev.h      xr4,   xr1,  xr0
+    xvpackod.h      xr5,   xr1,  xr0
+    xvpackev.h      xr6,   xr3,  xr2
+    xvpackod.h      xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr8,  xr9
+    xvsub.h         xr5,   xr8,  xr9
+    xvadd.h         xr6,   xr10, xr11
+    xvsub.h         xr7,   xr10, xr11
+    xvilvl.h        xr8,   xr1,  xr0
+    xvilvl.h        xr9,   xr3,  xr2
+    xvilvl.h        xr10,  xr5,  xr4
+    xvilvl.h        xr11,  xr7,  xr6
+    xvilvh.h        xr0,   xr1,  xr0
+    xvilvh.h        xr1,   xr3,  xr2
+    xvilvh.h        xr2,   xr5,  xr4
+    xvilvh.h        xr3,   xr7,  xr6
+    xvadd.h         xr4,   xr8,  xr9
+    xvadd.h         xr6,   xr10, xr11
+    xvsub.h         xr5,   xr8,  xr9
+    xvsub.h         xr7,   xr10, xr11
+    xvadd.h         xr8,   xr4,  xr6
+    xvadd.h         xr9,   xr5,  xr7
+    xvsub.h         xr10,  xr4,  xr6
+    xvsub.h         xr11,  xr5,  xr7
+    xvadd.h         xr4,   xr0,  xr1
+    xvadd.h         xr6,   xr2,  xr3
+    xvsub.h         xr5,   xr0,  xr1
+    xvsub.h         xr7,   xr2,  xr3
+    xvadd.h         xr0,   xr4,  xr6
+    xvadd.h         xr1,   xr5,  xr7
+    xvsub.h         xr2,   xr4,  xr6
+    xvsub.h         xr3,   xr5,  xr7
+    xvadda.h        xr8,   xr8,  xr9
+    xvadda.h        xr9,   xr10, xr11
+    xvadda.h        xr0,   xr0,  xr1
+    xvadda.h        xr1,   xr2,  xr3
+    xvadd.h         xr8,   xr8,  xr9
+    xvadd.h         xr0,   xr0,  xr1
+    xvadd.h         xr0,   xr0,  xr8
+
+    xvhaddw.wu.hu   xr0,   xr0,  xr0
+    xvhaddw.du.wu   xr0,   xr0,  xr0
+    xvhaddw.qu.du   xr0,   xr0,  xr0
+    xvpickve2gr.wu  t0,    xr0,  0
+    xvpickve2gr.wu  t1,    xr0,  4
+    add.w           t0,    t0,   t1
+    srli.d          a0,    t0,   1
+endfunc
+
+/* int x264_pixel_satd_8x16_lasx(pixel *pix1, intptr_t i_pix1,
+ *                               pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_satd_8x16_lasx
+    slli.d          t2,    a1,   1
+    add.d           t3,    a1,   t2
+    slli.d          t4,    a1,   2
+    slli.d          t5,    a3,   1
+    add.d           t6,    a3,   t5
+    slli.d          t7,    a3,   2
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t3,  vr0,  vr1,  vr2,  vr3
+    add.d           a0,    a0,   t4
+    LSX_LOADX_4     a0,    a1,   t2,  t3,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4     a2,    a3,   t5,  t6,  vr8,  vr9,  vr10, vr11
+    add.d           a2,    a2,   t7
+    LSX_LOADX_4     a2,    a3,   t5,  t6,  vr12, vr13, vr14, vr15
+    vilvl.d         vr0,   vr1,  vr0
+    vilvl.d         vr1,   vr3,  vr2
+    vilvl.d         vr2,   vr5,  vr4
+    vilvl.d         vr3,   vr7,  vr6
+    xvpermi.q       xr0,   xr2,  0x02
+    xvpermi.q       xr1,   xr3,  0x02
+    vilvl.d         vr2,   vr9,  vr8
+    vilvl.d         vr3,   vr11, vr10
+    vilvl.d         vr4,   vr13, vr12
+    vilvl.d         vr5,   vr15, vr14
+    xvpermi.q       xr2,   xr4,  0x02
+    xvpermi.q       xr3,   xr5,  0x02
+
+    // HADAMARD4
+    xvsubwev.h.bu   xr4,   xr0,  xr2
+    xvsubwod.h.bu   xr5,   xr0,  xr2
+    xvsubwev.h.bu   xr6,   xr1,  xr3
+    xvsubwod.h.bu   xr7,   xr1,  xr3
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvpackev.h      xr4,   xr1,  xr0
+    xvpackod.h      xr5,   xr1,  xr0
+    xvpackev.h      xr6,   xr3,  xr2
+    xvpackod.h      xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvilvl.h        xr4,   xr1,  xr0
+    xvilvh.h        xr5,   xr1,  xr0
+    xvilvl.h        xr6,   xr3,  xr2
+    xvilvh.h        xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr1,   xr4,  xr5
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr0,  xr2
+    xvadd.h         xr5,   xr1,  xr3
+    xvsub.h         xr6,   xr0,  xr2
+    xvsub.h         xr7,   xr1,  xr3
+    xvadda.h        xr0,   xr4,  xr5
+    xvadda.h        xr1,   xr6,  xr7
+    xvadd.h         xr16,  xr0,  xr1
+    add.d           a0,    a0,   t4
+    add.d           a2,    a2,   t7
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t3,  vr0,  vr1,  vr2,  vr3
+    add.d           a0,    a0,   t4
+    LSX_LOADX_4     a0,    a1,   t2,  t3,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4     a2,    a3,   t5,  t6,  vr8,  vr9,  vr10, vr11
+    add.d           a2,    a2,   t7
+    LSX_LOADX_4     a2,    a3,   t5,  t6,  vr12, vr13, vr14, vr15
+    vilvl.d         vr0,   vr1,  vr0
+    vilvl.d         vr1,   vr3,  vr2
+    vilvl.d         vr2,   vr5,  vr4
+    vilvl.d         vr3,   vr7,  vr6
+    xvpermi.q       xr0,   xr2,  0x02
+    xvpermi.q       xr1,   xr3,  0x02
+    vilvl.d         vr2,   vr9,  vr8
+    vilvl.d         vr3,   vr11, vr10
+    vilvl.d         vr4,   vr13, vr12
+    vilvl.d         vr5,   vr15, vr14
+    xvpermi.q       xr2,   xr4,  0x02
+    xvpermi.q       xr3,   xr5,  0x02
+
+    // HADAMARD4
+    xvsubwev.h.bu   xr4,   xr0,  xr2
+    xvsubwod.h.bu   xr5,   xr0,  xr2
+    xvsubwev.h.bu   xr6,   xr1,  xr3
+    xvsubwod.h.bu   xr7,   xr1,  xr3
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvpackev.h      xr4,   xr1,  xr0
+    xvpackod.h      xr5,   xr1,  xr0
+    xvpackev.h      xr6,   xr3,  xr2
+    xvpackod.h      xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvilvl.h        xr4,   xr1,  xr0
+    xvilvh.h        xr5,   xr1,  xr0
+    xvilvl.h        xr6,   xr3,  xr2
+    xvilvh.h        xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr1,   xr4,  xr5
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr0,  xr2
+    xvadd.h         xr5,   xr1,  xr3
+    xvsub.h         xr6,   xr0,  xr2
+    xvsub.h         xr7,   xr1,  xr3
+    xvadda.h        xr0,   xr4,  xr5
+    xvadda.h        xr1,   xr6,  xr7
+    xvadd.h         xr0,   xr0,  xr1
+    xvadd.h         xr0,   xr0,  xr16
+    xvhaddw.wu.hu   xr0,   xr0,  xr0
+    xvhaddw.du.wu   xr0,   xr0,  xr0
+    xvhaddw.qu.du   xr0,   xr0,  xr0
+    xvpickve2gr.wu  t0,    xr0,  0
+    xvpickve2gr.wu  t1,    xr0,  4
+    add.w           t0,    t0,   t1
+    srli.d          a0,    t0,   1
+endfunc
+
+/* int x264_pixel_satd_8x8_lasx(pixel *pix1, intptr_t i_pix1,
+ *                              pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_satd_8x8_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t5,    a3,   1
+    add.d           t3,    a1,   t2
+    add.d           t6,    a3,   t5
+    slli.d          t4,    t2,   1
+    slli.d          t7,    t5,   1
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t3,  vr0,  vr1,  vr2,  vr3
+    add.d           a0,    a0,   t4
+    LSX_LOADX_4     a0,    a1,   t2,  t3,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4     a2,    a3,   t5,  t6,  vr8,  vr9,  vr10, vr11
+    add.d           a2,    a2,   t7
+    LSX_LOADX_4     a2,    a3,   t5,  t6,  vr12, vr13, vr14, vr15
+
+    vilvl.d         vr0,   vr1,  vr0
+    vilvl.d         vr1,   vr3,  vr2
+    vilvl.d         vr2,   vr5,  vr4
+    vilvl.d         vr3,   vr7,  vr6
+    xvpermi.q       xr0,   xr2,  0x02
+    xvpermi.q       xr1,   xr3,  0x02
+    vilvl.d         vr2,   vr9,  vr8
+    vilvl.d         vr3,   vr11, vr10
+    vilvl.d         vr4,   vr13, vr12
+    vilvl.d         vr5,   vr15, vr14
+    xvpermi.q       xr2,   xr4,  0x02
+    xvpermi.q       xr3,   xr5,  0x02
+
+    // HADAMARD4
+    xvsubwev.h.bu   xr4,   xr0,  xr2
+    xvsubwod.h.bu   xr5,   xr0,  xr2
+    xvsubwev.h.bu   xr6,   xr1,  xr3
+    xvsubwod.h.bu   xr7,   xr1,  xr3
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvpackev.h      xr4,   xr1,  xr0
+    xvpackod.h      xr5,   xr1,  xr0
+    xvpackev.h      xr6,   xr3,  xr2
+    xvpackod.h      xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvsub.h         xr1,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr3,   xr6,  xr7
+    xvilvl.h        xr4,   xr1,  xr0
+    xvilvh.h        xr5,   xr1,  xr0
+    xvilvl.h        xr6,   xr3,  xr2
+    xvilvh.h        xr7,   xr3,  xr2
+    xvadd.h         xr0,   xr4,  xr5
+    xvadd.h         xr2,   xr6,  xr7
+    xvsub.h         xr1,   xr4,  xr5
+    xvsub.h         xr3,   xr6,  xr7
+    xvadd.h         xr4,   xr0,  xr2
+    xvadd.h         xr5,   xr1,  xr3
+    xvsub.h         xr6,   xr0,  xr2
+    xvsub.h         xr7,   xr1,  xr3
+    xvadda.h        xr0,   xr4,  xr5
+    xvadda.h        xr1,   xr6,  xr7
+    xvadd.h         xr0,   xr0,  xr1
+    xvhaddw.wu.hu   xr0,   xr0,  xr0
+    xvhaddw.du.wu   xr0,   xr0,  xr0
+    xvhaddw.qu.du   xr0,   xr0,  xr0
+    xvpickve2gr.wu  t0,    xr0,  0
+    xvpickve2gr.wu  t1,    xr0,  4
+    add.w           t0,    t0,   t1
+    srli.d          a0,    t0,   1
+endfunc
+
+/* int x264_pixel_satd_8x4_lasx(pixel *pix1, intptr_t i_pix1,
+ *                              pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_satd_8x4_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    add.d           t4,    a1,   t2
+    add.d           t5,    a3,   t3
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
+    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr9,   xr11, xr13
+    xvsub.h         xr10,  xr11, xr13
+    xvpackev.d      xr11,  xr10, xr9
+    xvpackod.d      xr12,  xr10, xr9
+    xvadda.h        xr11,  xr11, xr12
+    xvhaddw.wu.hu   xr11,  xr11, xr11
+    xvhaddw.du.wu   xr11,  xr11, xr11
+    xvhaddw.qu.du   xr11,  xr11, xr11
+    xvpickve2gr.wu  t4,    xr11, 0
+    xvpickve2gr.wu  t5,    xr11, 4
+    add.d           t4,    t4,   t5
+    srli.d          a0,    t4,   1
+endfunc
+
+/* int x264_pixel_satd_4x8_lasx(pixel *pix1, intptr_t i_pix1,
+ *                              pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_satd_4x8_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    add.d           t4,    a1,   t2
+    add.d           t5,    a3,   t3
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
+    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+    vilvl.w         vr1,   vr2,  vr1
+    vilvl.w         vr3,   vr4,  vr3
+    vilvl.d         vr9,   vr3,  vr1
+    vilvl.w         vr5,   vr6,  vr5
+    vilvl.w         vr7,   vr8,  vr7
+    vilvl.d         vr10,  vr7,  vr5
+
+    slli.d          t0,    a1,   2
+    slli.d          t1,    a3,   2
+    add.d           a0,    a0,   t0
+    add.d           a2,    a2,   t1
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
+    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+    vilvl.w         vr1,   vr2,  vr1
+    vilvl.w         vr3,   vr4,  vr3
+    vilvl.d         vr1,   vr3,  vr1
+    vilvl.w         vr5,   vr6,  vr5
+    vilvl.w         vr7,   vr8,  vr7
+    vilvl.d         vr5,   vr7,  vr5
+    xvpermi.q       xr1,   xr9,  0x20
+    xvpermi.q       xr5,   xr10, 0x20
+
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* b0 + b1 */
+    xvsub.h         xr12,  xr9,  xr10  /* b0 - b1 */
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadda.h        xr9,   xr9,  xr10
+    xvhaddw.wu.hu   xr9,   xr9,  xr9
+    xvhaddw.du.wu   xr9,   xr9,  xr9
+    xvhaddw.qu.du   xr9,   xr9,  xr9
+    xvpickve2gr.wu  t6,    xr9,  0
+    xvpickve2gr.wu  t7,    xr9,  4
+    add.d           t6,    t6,   t7
+    srli.d          a0,    t6,   1
+endfunc
+
+/* int x264_pixel_satd_wxh4x4_lasx(pixel *pix1, intptr_t i_pix1,
+ *                                 pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_satd_4x4_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    add.d           t4,    a1,   t2
+    add.d           t5,    a3,   t3
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
+    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+    vilvl.w         vr1,   vr2,  vr1
+    vilvl.w         vr3,   vr4,  vr3
+    vilvl.d         vr1,   vr3,  vr1
+    vilvl.w         vr5,   vr6,  vr5
+    vilvl.w         vr7,   vr8,  vr7
+    vilvl.d         vr5,   vr7,  vr5
+
+    vsubwev.h.bu    vr9,   vr1,  vr5
+    vsubwod.h.bu    vr10,  vr1,  vr5
+    vadd.h          vr11,  vr9,  vr10  /* a0 + a1 */
+    vsub.h          vr12,  vr9,  vr10  /* a0 - a1 */
+    vpackev.h       vr9,   vr12, vr11
+    vpackod.h       vr10,  vr12, vr11
+    vadd.h          vr11,  vr9,  vr10  /* b0 + b1 */
+    vsub.h          vr12,  vr9,  vr10  /* b0 - b1 */
+    vpackev.w       vr9,   vr12, vr11
+    vpackod.w       vr10,  vr12, vr11
+    vadd.h          vr11,  vr9,  vr10  /* HADAMARD4 */
+    vsub.h          vr12,  vr9,  vr10
+    vpackev.d       vr9,   vr12, vr11
+    vpackod.d       vr10,  vr12, vr11
+    vadd.h          vr11,  vr9,  vr10
+    vsub.h          vr12,  vr9,  vr10
+    vpackev.d       vr9,   vr12, vr11
+    vpackod.d       vr10,  vr12, vr11
+    vadda.h         vr9,   vr9,  vr10
+    vhaddw.wu.hu    vr9,   vr9,  vr9
+    vhaddw.du.wu    vr9,   vr9,  vr9
+    vhaddw.qu.du    vr9,   vr9,  vr9
+    vpickve2gr.wu   t5,    vr9,  0
+    srli.d          a0,    t5,   1
+endfunc
+
+/*
+ * int pixel_ssd_16x16_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                          const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_ssd_16x16_lasx
+    slli.d         t0,     a1,    1
+    add.d          t1,     a1,    t0
+    add.d          t2,     a1,    t1
+    slli.d         t3,     a3,    1
+    add.d          t4,     a3,    t3
+    add.d          t5,     a3,    t4
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr2,    xr2
+    vext2xv.hu.bu  xr3,    xr3
+    vext2xv.hu.bu  xr4,    xr4
+    vext2xv.hu.bu  xr5,    xr5
+    vext2xv.hu.bu  xr6,    xr6
+    vext2xv.hu.bu  xr7,    xr7
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+    vext2xv.hu.bu  xr10,   xr10
+    vext2xv.hu.bu  xr11,   xr11
+    vext2xv.hu.bu  xr12,   xr12
+    vext2xv.hu.bu  xr13,   xr13
+    vext2xv.hu.bu  xr14,   xr14
+    vext2xv.hu.bu  xr15,   xr15
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvsub.h        xr2,    xr2,   xr10
+    xvsub.h        xr3,    xr3,   xr11
+    xvsub.h        xr4,    xr4,   xr12
+    xvsub.h        xr5,    xr5,   xr13
+    xvsub.h        xr6,    xr6,   xr14
+    xvsub.h        xr7,    xr7,   xr15
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvmul.h        xr2,    xr2,   xr2
+    xvmul.h        xr3,    xr3,   xr3
+    xvmul.h        xr4,    xr4,   xr4
+    xvmul.h        xr5,    xr5,   xr5
+    xvmul.h        xr6,    xr6,   xr6
+    xvmul.h        xr7,    xr7,   xr7
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr2,    xr2,   xr2
+    xvhaddw.wu.hu  xr3,    xr3,   xr3
+    xvhaddw.wu.hu  xr4,    xr4,   xr4
+    xvhaddw.wu.hu  xr5,    xr5,   xr5
+    xvhaddw.wu.hu  xr6,    xr6,   xr6
+    xvhaddw.wu.hu  xr7,    xr7,   xr7
+    xvadd.w        xr16,   xr0,   xr1
+    xvadd.w        xr17,   xr2,   xr3
+    xvadd.w        xr18,   xr4,   xr5
+    xvadd.w        xr19,   xr6,   xr7
+    xvadd.w        xr16,   xr16,  xr17
+    xvadd.w        xr18,   xr18,  xr19
+    xvadd.w        xr16,   xr16,  xr18
+
+    // Load data from pix1 and pix2
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr2,    xr2
+    vext2xv.hu.bu  xr3,    xr3
+    vext2xv.hu.bu  xr4,    xr4
+    vext2xv.hu.bu  xr5,    xr5
+    vext2xv.hu.bu  xr6,    xr6
+    vext2xv.hu.bu  xr7,    xr7
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+    vext2xv.hu.bu  xr10,   xr10
+    vext2xv.hu.bu  xr11,   xr11
+    vext2xv.hu.bu  xr12,   xr12
+    vext2xv.hu.bu  xr13,   xr13
+    vext2xv.hu.bu  xr14,   xr14
+    vext2xv.hu.bu  xr15,   xr15
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvsub.h        xr2,    xr2,   xr10
+    xvsub.h        xr3,    xr3,   xr11
+    xvsub.h        xr4,    xr4,   xr12
+    xvsub.h        xr5,    xr5,   xr13
+    xvsub.h        xr6,    xr6,   xr14
+    xvsub.h        xr7,    xr7,   xr15
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvmul.h        xr2,    xr2,   xr2
+    xvmul.h        xr3,    xr3,   xr3
+    xvmul.h        xr4,    xr4,   xr4
+    xvmul.h        xr5,    xr5,   xr5
+    xvmul.h        xr6,    xr6,   xr6
+    xvmul.h        xr7,    xr7,   xr7
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr2,    xr2,   xr2
+    xvhaddw.wu.hu  xr3,    xr3,   xr3
+    xvhaddw.wu.hu  xr4,    xr4,   xr4
+    xvhaddw.wu.hu  xr5,    xr5,   xr5
+    xvhaddw.wu.hu  xr6,    xr6,   xr6
+    xvhaddw.wu.hu  xr7,    xr7,   xr7
+    xvadd.w        xr0,    xr0,   xr1
+    xvadd.w        xr2,    xr2,   xr3
+    xvadd.w        xr4,    xr4,   xr5
+    xvadd.w        xr6,    xr6,   xr7
+    xvadd.w        xr0,    xr0,   xr2
+    xvadd.w        xr4,    xr4,   xr6
+    xvadd.w        xr0,    xr0,   xr4
+    xvadd.w        xr0,    xr0,   xr16
+
+    // Calculate the sum
+    xvhaddw.d.w    xr0,    xr0,   xr0
+    xvhaddw.q.d    xr0,    xr0,   xr0
+    xvpickve2gr.w  t2,     xr0,   0
+    xvpickve2gr.w  t3,     xr0,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+/*
+ * int pixel_ssd_8x8_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                        const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_ssd_8x8_lasx
+    slli.d         t0,     a1,    1
+    add.d          t1,     a1,    t0
+    add.d          t2,     a1,    t1
+    slli.d         t3,     a3,    1
+    add.d          t4,     a3,    t3
+    add.d          t5,     a3,    t4
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+
+    vilvl.d        vr0,    vr4,   vr0
+    vilvl.d        vr1,    vr5,   vr1
+    vilvl.d        vr2,    vr6,   vr2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr8,    vr12,  vr8
+    vilvl.d        vr9,    vr13,  vr9
+    vilvl.d        vr10,   vr14,  vr10
+    vilvl.d        vr11,   vr15,  vr11
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr2,    xr2
+    vext2xv.hu.bu  xr3,    xr3
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+    vext2xv.hu.bu  xr10,   xr10
+    vext2xv.hu.bu  xr11,   xr11
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvsub.h        xr2,    xr2,   xr10
+    xvsub.h        xr3,    xr3,   xr11
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvmul.h        xr2,    xr2,   xr2
+    xvmul.h        xr3,    xr3,   xr3
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr2,    xr2,   xr2
+    xvhaddw.wu.hu  xr3,    xr3,   xr3
+    xvadd.w        xr0,    xr0,   xr1
+    xvadd.w        xr2,    xr2,   xr3
+    xvadd.w        xr0,    xr0,   xr2
+
+    // Calculate the sum
+    xvhaddw.d.w    xr0,    xr0,   xr0
+    xvhaddw.q.d    xr0,    xr0,   xr0
+    xvpickve2gr.w  t2,     xr0,   0
+    xvpickve2gr.w  t3,     xr0,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+/*
+ * int pixel_ssd_4x4_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                        const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_ssd_4x4_lasx
+    slli.d         t0,     a1,    1
+    slli.d         t1,     a3,    1
+    add.d          t2,     t0,    a1
+    add.d          t3,     t1,    a3
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4    a0,     a1,    t0,  t2,  vr4, vr6, vr8, vr10
+    LSX_LOADX_4    a2,     a3,    t1,  t3,  vr5, vr7, vr9, vr11
+    vilvl.w        vr4,    vr6,   vr4
+    vilvl.w        vr5,    vr7,   vr5
+    vilvl.w        vr8,    vr10,  vr8
+    vilvl.w        vr9,    vr11,  vr9
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vext2xv.hu.bu  xr4,    xr4
+    vext2xv.hu.bu  xr5,    xr5
+
+    // Calculate the square of the difference
+    xvsub.h        xr6,    xr4,   xr5
+    xvmul.h        xr6,    xr6,   xr6
+    // Calculate the sum
+    xvhaddw.wu.hu  xr6,    xr6,   xr6
+    xvhaddw.du.wu  xr6,    xr6,   xr6
+    xvhaddw.qu.du  xr6,    xr6,   xr6
+    xvpickve2gr.wu t2,     xr6,   0
+    xvpickve2gr.wu t3,     xr6,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/pixel-c.c b/common/loongarch/pixel-c.c
index 99602106..a8bf5f21 100644
--- a/common/loongarch/pixel-c.c
+++ b/common/loongarch/pixel-c.c
@@ -128,60 +128,6 @@ static inline int32_t pixel_satd_4width_lasx( uint8_t *p_src, int32_t i_src_stri
     return ( u_sum >> 1 );
 }
 
-int32_t x264_pixel_satd_4x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                  uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    uint32_t sum;
-    intptr_t i_stride_2 = i_stride << 1;
-    intptr_t i_stride2_2 = i_stride2 << 1;
-    intptr_t i_stride_3 = i_stride_2 + i_stride;
-    intptr_t i_stride2_3 = i_stride2_2 + i_stride2;
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i diff0, diff1, tmp0, tmp1;
-
-    src0 = __lasx_xvld(p_pix1, 0);
-    src1 = __lasx_xvldx(p_pix1, i_stride);
-    src2 = __lasx_xvldx(p_pix1, i_stride_2);
-    src3 = __lasx_xvldx(p_pix1, i_stride_3);
-    ref0 = __lasx_xvld(p_pix2, 0);
-    ref1 = __lasx_xvldx(p_pix2, i_stride2);
-    ref2 = __lasx_xvldx(p_pix2, i_stride2_2);
-    ref3 = __lasx_xvldx(p_pix2, i_stride2_3);
-
-    src0 = __lasx_xvilvl_w(src1, src0);
-    src1 = __lasx_xvilvl_w(src3, src2);
-    ref0 = __lasx_xvilvl_w(ref1, ref0);
-    ref1 = __lasx_xvilvl_w(ref3, ref2);
-    src0 = __lasx_xvilvl_d(src1, src0);
-    ref0 = __lasx_xvilvl_d(ref1, ref0);
-
-    diff0 = __lasx_xvsubwev_h_bu(src0, ref0);
-    diff1 = __lasx_xvsubwod_h_bu(src0, ref0);
-    tmp0  = __lasx_xvadd_h(diff0, diff1);
-    tmp1  = __lasx_xvsub_h(diff0, diff1);
-    diff0 = __lasx_xvpackev_h(tmp1, tmp0);
-    diff1 = __lasx_xvpackod_h(tmp1, tmp0);
-    tmp0  = __lasx_xvadd_h(diff0, diff1);
-    tmp1  = __lasx_xvsub_h(diff0, diff1);
-    diff0 = __lasx_xvpackev_w(tmp1, tmp0);
-    diff1 = __lasx_xvpackod_w(tmp1, tmp0);
-    tmp0  = __lasx_xvadd_h(diff0, diff1);
-    tmp1  = __lasx_xvsub_h(diff0, diff1);
-    diff0 = __lasx_xvpackev_d(tmp1, tmp0);
-    diff1 = __lasx_xvpackod_d(tmp1, tmp0);
-    tmp0  = __lasx_xvadd_h(diff0, diff1);
-    tmp1  = __lasx_xvsub_h(diff0, diff1);
-    diff0 = __lasx_xvpackev_d(tmp1, tmp0);
-    diff1 = __lasx_xvpackod_d(tmp1, tmp0);
-    diff0 = __lasx_xvadda_h(diff0, diff1);
-    diff0 = __lasx_xvhaddw_wu_hu( diff0, diff0 );
-    diff0 = __lasx_xvhaddw_du_wu( diff0, diff0 );
-    diff0 = __lasx_xvhaddw_qu_du( diff0, diff0 );
-    sum   = __lasx_xvpickve2gr_wu(diff0, 0);
-    return ( sum >> 1 );
-}
-
 static inline int32_t pixel_satd_8width_lasx( uint8_t *p_pix1, int32_t i_stride,
                                               uint8_t *p_pix2, int32_t i_stride2,
                                               uint8_t i_height )
@@ -283,1094 +229,57 @@ static inline int32_t pixel_satd_8width_lasx( uint8_t *p_pix1, int32_t i_stride,
     return ( sum >> 1 );
 }
 
-int32_t x264_pixel_satd_4x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                  uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    return pixel_satd_4width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 8 );
-}
-
 int32_t x264_pixel_satd_4x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
                                    uint8_t *p_pix2, intptr_t i_stride2 )
 {
     return pixel_satd_4width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
 }
 
-int32_t x264_pixel_satd_8x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                  uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    uint32_t u_sum = 0, sum1, sum2;
-    intptr_t i_stride_2 = i_stride << 1;
-    intptr_t i_stride2_2 = i_stride2 << 1;
-    intptr_t i_stride_3 = i_stride_2 + i_stride;
-    intptr_t i_stride2_3 = i_stride2_2 + i_stride2;
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i dif0, dif1;
-    __m256i tmp0, tmp1;
-
-    LASX_LOAD_4(p_pix1, i_stride, i_stride_2, i_stride_3, src0, src1, src2, src3);
-    LASX_LOAD_4(p_pix2, i_stride2, i_stride2_2, i_stride2_3, ref0, ref1, ref2, ref3);
-
-    src0 = __lasx_xvilvl_d(src1, src0);
-    src1 = __lasx_xvilvl_d(src3, src2);
-    ref0 = __lasx_xvilvl_d(ref1, ref0);
-    ref1 = __lasx_xvilvl_d(ref3, ref2);
-    src0 = __lasx_xvpermi_q(src0, src1, 2);
-    ref0 = __lasx_xvpermi_q(ref0, ref1, 2);
-    dif0 = __lasx_xvsubwev_h_bu(src0, ref0);
-    dif1 = __lasx_xvsubwod_h_bu(src0, ref0);
-    tmp0 = __lasx_xvadd_h(dif0, dif1);
-    tmp1 = __lasx_xvsub_h(dif0, dif1);
-    dif0 = __lasx_xvpackev_h(tmp1, tmp0);
-    dif1 = __lasx_xvpackod_h(tmp1, tmp0);
-    tmp0 = __lasx_xvadd_h(dif0, dif1);
-    tmp1 = __lasx_xvsub_h(dif0, dif1);
-    dif0 = __lasx_xvpackev_d(tmp1, tmp0);
-    dif1 = __lasx_xvpackod_d(tmp1, tmp0);
-    tmp0 = __lasx_xvadd_h(dif0, dif1);
-    tmp1 = __lasx_xvsub_h(dif0, dif1);
-    dif0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);
-    dif1 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);
-    tmp0 = __lasx_xvadd_h(dif0, dif1);
-    tmp1 = __lasx_xvsub_h(dif0, dif1);
-    dif0 = __lasx_xvpackev_d(tmp1, tmp0);
-    dif1 = __lasx_xvpackod_d(tmp1, tmp0);
-    dif0 = __lasx_xvadda_h(dif0, dif1);
-    dif0 = __lasx_xvhaddw_wu_hu(dif0, dif0);
-    dif0 = __lasx_xvhaddw_du_wu(dif0, dif0);
-    dif0 = __lasx_xvhaddw_qu_du(dif0, dif0);
-    sum1 = __lasx_xvpickve2gr_wu(dif0, 0);
-    sum2 = __lasx_xvpickve2gr_wu(dif0, 4);
-    u_sum = sum1 + sum2;
-
-    return ( u_sum >> 1 );
-}
-
-int32_t x264_pixel_satd_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                  uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    uint32_t sum;
-    uint32_t sum1, sum2;
-    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
-    uint8_t *pix1, *pix2;
-
-    __asm__ volatile (
-    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
-    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
-    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
-    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
-    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
-    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
-    "add.d          %[pix1],          %[p_pix1],            %[stride_4]            \n\t"
-    "add.d          %[pix2],          %[p_pix2],            %[stride2_4]           \n\t"
-    "vld            $vr0,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
-    "vld            $vr4,             %[pix1],              0                      \n\t"
-    "vldx           $vr5,             %[pix1],              %[i_stride]            \n\t"
-    "vldx           $vr6,             %[pix1],              %[stride_2]            \n\t"
-    "vldx           $vr7,             %[pix1],              %[stride_3]            \n\t"
-    "vld            $vr8,             %[p_pix2],            0                      \n\t"
-    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
-    "vld            $vr12,            %[pix2],              0                      \n\t"
-    "vldx           $vr13,            %[pix2],              %[i_stride2]           \n\t"
-    "vldx           $vr14,            %[pix2],              %[stride2_2]           \n\t"
-    "vldx           $vr15,            %[pix2],              %[stride2_3]           \n\t"
-    "vilvl.d        $vr0,             $vr1,                 $vr0                   \n\t"
-    "vilvl.d        $vr1,             $vr3,                 $vr2                   \n\t"
-    "vilvl.d        $vr2,             $vr5,                 $vr4                   \n\t"
-    "vilvl.d        $vr3,             $vr7,                 $vr6                   \n\t"
-    "xvpermi.q      $xr0,             $xr2,                 2                      \n\t"
-    "xvpermi.q      $xr1,             $xr3,                 2                      \n\t"
-    "vilvl.d        $vr2,             $vr9,                 $vr8                   \n\t"
-    "vilvl.d        $vr3,             $vr11,                $vr10                  \n\t"
-    "vilvl.d        $vr4,             $vr13,                $vr12                  \n\t"
-    "vilvl.d        $vr5,             $vr15,                $vr14                  \n\t"
-    "xvpermi.q      $xr2,             $xr4,                 2                      \n\t"
-    "xvpermi.q      $xr3,             $xr5,                 2                      \n\t"
-    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr2                   \n\t"
-    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr2                   \n\t"
-    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr3                   \n\t"
-    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr3                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
-    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
-    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
-    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvilvl.h       $xr4,             $xr1,                 $xr0                   \n\t"
-    "xvilvh.h       $xr5,             $xr1,                 $xr0                   \n\t"
-    "xvilvl.h       $xr6,             $xr3,                 $xr2                   \n\t"
-    "xvilvh.h       $xr7,             $xr3,                 $xr2                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr0,                 $xr2                   \n\t"
-    "xvadd.h        $xr5,             $xr1,                 $xr3                   \n\t"
-    "xvsub.h        $xr6,             $xr0,                 $xr2                   \n\t"
-    "xvsub.h        $xr7,             $xr1,                 $xr3                   \n\t"
-    "xvadda.h       $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvadda.h       $xr1,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvhaddw.wu.hu  $xr0,             $xr0,                 $xr0                   \n\t"
-    "xvhaddw.du.wu  $xr0,             $xr0,                 $xr0                   \n\t"
-    "xvhaddw.qu.du  $xr0,             $xr0,                 $xr0                   \n\t"
-    "xvpickve2gr.wu %[sum1],          $xr0,                 0                      \n\t"
-    "xvpickve2gr.wu %[sum2],          $xr0,                 4                      \n\t"
-    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
-    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
-      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
-      [pix1]"=&r"(pix1), [pix2]"=&r"(pix2), [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum)
-    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2), [p_pix1]"r"(p_pix1), [p_pix2]"r"(p_pix2)
-    : "memory"
-    );
-
-    return ( sum >> 1 );
-}
-
-int32_t x264_pixel_satd_8x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                   uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    return pixel_satd_8width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
-}
-
-int32_t x264_pixel_satd_16x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                   uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    int32_t sum;
-    uint32_t sum1, sum2;
-    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
-
-    __asm__ volatile (
-    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
-    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
-    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
-    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
-    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
-    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
-    "vld            $vr0,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
-    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
-    "vld            $vr4,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
-    "vld            $vr8,             %[p_pix2],            0                      \n\t"
-    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
-    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
-    "vld            $vr12,            %[p_pix2],            0                      \n\t"
-    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
-    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
-    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
-    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
-    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
-    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
-    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
-    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
-    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
-    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
-    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
-    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
-    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
-    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
-    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
-    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
-    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
-
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
-    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
-
-    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
-    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
-    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
-    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
-    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
-    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
-    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
-    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
-
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
-    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
-
-    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
-    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
-    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
-    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
-    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
-    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
-    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
-    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
-
-
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
-    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
-    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
-    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
-    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
-
-    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
-    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
-    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
-    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
-    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
-    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
-
-    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
-    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
-    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
-
-    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr16,            $xr0,                 $xr8                   \n\t"
-    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
-    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
-    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
-    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
-      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
-      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2)
-    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2)
-    : "memory"
-    );
+#define SAD_LOAD                                                              \
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;                   \
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;                   \
+    __m256i diff;                                                             \
+    __m256i sad0 = __lasx_xvldi(0);                                           \
+    __m256i sad1 = __lasx_xvldi(0);                                           \
+    __m256i sad2 = __lasx_xvldi(0);                                           \
+    int32_t i_src_stride_x2 = FENC_STRIDE << 1;                               \
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;                              \
+    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;                  \
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;                 \
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;                           \
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
 
-    return ( sum >> 1 );
-}
 
-int32_t x264_pixel_satd_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                    uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    int32_t sum;
-    uint32_t sum1, sum2;
-    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+#define LOAD_REF_DATA_16W( p_ref, sad)                                        \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref0, ref1, ref2, ref3 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref4, ref5, ref6, ref7 );                                    \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );                              \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src2, ref2 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src3, ref3 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
 
-    __asm__ volatile (
-    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
-    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
-    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
-    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
-    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
-    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
-    "vld            $vr0,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
-    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
-    "vld            $vr4,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
-    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
-    "vld            $vr8,             %[p_pix2],            0                      \n\t"
-    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
-    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
-    "vld            $vr12,            %[p_pix2],            0                      \n\t"
-    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
-    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
-    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
-    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
-    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
-    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
-    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
-    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
-    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
-    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
-    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
-    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
-    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
-    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
-    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
-    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
-    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
-    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
 
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
-    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
-
-    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
-    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
-    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
-    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
-    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
-    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
-    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
-    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
 
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
-    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
-
-    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
-    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
-    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
-    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
-    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
-    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
-    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
-    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
-
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
-    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
-    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
-    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
-    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
-
-    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
-    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
-    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
-    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
-    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
-    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
-
-    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
-    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
-    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
-
-    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr16,            $xr0,                 $xr8                   \n\t"
-
-    "vld            $vr0,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
-    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
-    "vld            $vr4,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
-    "vld            $vr8,             %[p_pix2],            0                      \n\t"
-    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
-    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
-    "vld            $vr12,            %[p_pix2],            0                      \n\t"
-    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
-    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
-    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
-    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
-    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
-    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
-    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
-    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
-    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
-    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
-    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
-    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
-    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
-    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
-    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
-    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
-    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
-
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
-    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
-
-    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
-    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
-    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
-    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
-    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
-    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
-    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
-    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
-
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
-    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
-
-    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
-    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
-    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
-    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
-    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
-    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
-    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
-    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
-
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
-    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
-    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
-    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
-    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
-
-    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
-    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
-    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
-    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
-    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
-    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
-
-    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
-    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
-    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
-
-    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr16,            $xr16,                $xr8                   \n\t"
-    "xvadd.h        $xr16,            $xr16,                $xr0                   \n\t"
-
-    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
-    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
-    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
-    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
-      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
-      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2)
-    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2)
-    : "memory"
-    );
-
-    return ( sum >> 1 );
-}
-
-#define SAD_LOAD                                                        \
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;             \
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;             \
-    __m256i diff;                                                       \
-    __m256i sad0 = __lasx_xvldi( 0 );                                   \
-    __m256i sad1 = __lasx_xvldi( 0 );                                   \
-    __m256i sad2 = __lasx_xvldi( 0 );                                   \
-    __m256i sad3 = __lasx_xvldi( 0 );                                   \
-    int32_t i_src_stride_x2 = FENC_STRIDE << 1;                         \
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;                        \
-    int32_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;            \
-    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;           \
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;                     \
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;                     \
-
-#define LOAD_REF_DATA_16W( p_ref, sad)                                        \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
-                 ref0, ref1, ref2, ref3 );                                    \
-    p_ref += i_ref_stride_x4;                                                 \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
-                 ref4, ref5, ref6, ref7 );                                    \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
-    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
-    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );                              \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src1, ref1 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src2, ref2 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src3, ref3 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-
-#define ST_REF_DATA(sad)                                  \
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
-    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
-    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
-
-void x264_pixel_sad_x4_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                   uint8_t *p_ref1, uint8_t *p_ref2,
-                                   uint8_t *p_ref3, intptr_t i_ref_stride,
-                                   int32_t p_sad_array[4] )
-{
-    SAD_LOAD
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-    LOAD_REF_DATA_16W( p_ref0, sad0 );
-    LOAD_REF_DATA_16W( p_ref1, sad1 );
-    LOAD_REF_DATA_16W( p_ref2, sad2 );
-    LOAD_REF_DATA_16W( p_ref3, sad3 );
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-    p_ref0 += i_ref_stride_x4;
-    p_ref1 += i_ref_stride_x4;
-    p_ref2 += i_ref_stride_x4;
-    p_ref3 += i_ref_stride_x4;
-
-    LOAD_REF_DATA_16W( p_ref0, sad0 );
-    LOAD_REF_DATA_16W( p_ref1, sad1 );
-    LOAD_REF_DATA_16W( p_ref2, sad2 );
-    LOAD_REF_DATA_16W( p_ref3, sad3 );
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-    ST_REF_DATA(sad3);
-    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
-}
-
-void x264_pixel_sad_x4_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                  uint8_t *p_ref1, uint8_t *p_ref2,
-                                  uint8_t *p_ref3, intptr_t i_ref_stride,
-                                  int32_t p_sad_array[4] )
-{
-    SAD_LOAD
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-    LOAD_REF_DATA_16W( p_ref0, sad0 );
-    LOAD_REF_DATA_16W( p_ref1, sad1 );
-    LOAD_REF_DATA_16W( p_ref2, sad2 );
-    LOAD_REF_DATA_16W( p_ref3, sad3 );
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-    ST_REF_DATA(sad3);
-    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
-}
-
-#undef LOAD_REF_DATA_16W
-
-#define LOAD_REF_DATA_8W( p_ref, sad)                                             \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,           \
-                 ref0, ref1, ref2, ref3 );                                        \
-    p_ref += i_ref_stride_x4;                                                     \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,           \
-                 ref4, ref5, ref6, ref7 );                                        \
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                         \
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                         \
-    ref2 = __lasx_xvilvl_d( ref5, ref4 );                                         \
-    ref3 = __lasx_xvilvl_d( ref7, ref6 );                                         \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                                  \
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                                  \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                                        \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                    \
-    sad  = __lasx_xvadd_h( sad, diff );                                           \
-    diff = __lasx_xvabsd_bu( src1, ref1 );                                        \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                    \
-    sad  = __lasx_xvadd_h( sad, diff );
-
-void x264_pixel_sad_x4_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                  uint8_t *p_ref1, uint8_t *p_ref2,
-                                  uint8_t *p_ref3, intptr_t i_ref_stride,
-                                  int32_t p_sad_array[4] )
-{
-    SAD_LOAD
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    LOAD_REF_DATA_8W( p_ref0, sad0 );
-    LOAD_REF_DATA_8W( p_ref1, sad1 );
-    LOAD_REF_DATA_8W( p_ref2, sad2 );
-    LOAD_REF_DATA_8W( p_ref3, sad3 );
-
-    p_ref0 += i_ref_stride_x4;
-    p_ref1 += i_ref_stride_x4;
-    p_ref2 += i_ref_stride_x4;
-    p_ref3 += i_ref_stride_x4;
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    LOAD_REF_DATA_8W( p_ref0, sad0 );
-    LOAD_REF_DATA_8W( p_ref1, sad1 );
-    LOAD_REF_DATA_8W( p_ref2, sad2 );
-    LOAD_REF_DATA_8W( p_ref3, sad3 );
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-    ST_REF_DATA(sad3);
-    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
-}
-
-void x264_pixel_sad_x4_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 uint8_t *p_ref3, intptr_t i_ref_stride,
-                                 int32_t p_sad_array[4] )
-{
-    SAD_LOAD
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    LOAD_REF_DATA_8W( p_ref0, sad0 );
-    LOAD_REF_DATA_8W( p_ref1, sad1 );
-    LOAD_REF_DATA_8W( p_ref2, sad2 );
-    LOAD_REF_DATA_8W( p_ref3, sad3 );
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-    ST_REF_DATA(sad3);
-    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
-}
-
-#undef SAD_LOAD
-#undef LOAD_REF_DATA_8W
-#undef ST_REF_DATA
-
-void x264_pixel_sad_x4_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 uint8_t *p_ref3, intptr_t i_ref_stride,
-                                 int32_t p_sad_array[4] )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i diff;
-    __m256i sad0, sad1, sad2, sad3;
-    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
-    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
-    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
-    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-
-#define LOAD_REF_DATA_8W_4H( p_ref, sad)                                \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3, \
-                 ref0, ref1, ref2, ref3 );                              \
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );                               \
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );                               \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                        \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                              \
-    sad = __lasx_xvhaddw_hu_bu( diff, diff );                           \
-    sad = __lasx_xvhaddw_wu_hu( sad, sad );                             \
-    sad = __lasx_xvhaddw_du_wu( sad, sad );                             \
-    sad = __lasx_xvhaddw_qu_du( sad, sad );                             \
-
-    LOAD_REF_DATA_8W_4H( p_ref0, sad0 );
-    LOAD_REF_DATA_8W_4H( p_ref1, sad1 );
-    LOAD_REF_DATA_8W_4H( p_ref2, sad2 );
-    LOAD_REF_DATA_8W_4H( p_ref3, sad3 );
-
-#undef LOAD_REF_DATA_8W_4H
-
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
-}
-
-void x264_pixel_sad_x4_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 uint8_t *p_ref3, intptr_t i_ref_stride,
-                                 int32_t p_sad_array[4] )
-{
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad0, sad1, sad2, sad3;
-    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
-    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
-    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
-    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-    intptr_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    intptr_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-
-    src0 = __lasx_xvld( p_src, 0);
-    src1 = __lasx_xvld( p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx( p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx( p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld( p_src, 0);
-    src5 = __lasx_xvld( p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx( p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx( p_src, i_src_stride_x3);
-    src0 = __lasx_xvilvl_w( src1, src0 );
-    src1 = __lasx_xvilvl_w( src3, src2 );
-    src2 = __lasx_xvilvl_w( src5, src4 );
-    src3 = __lasx_xvilvl_w( src7, src6 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-
-#define LOAD_REF_DATA_4W_8H( p_ref, sad) \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
-                 ref0, ref1, ref2, ref3 );                                    \
-    p_ref += i_ref_stride_x4;                                                 \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
-                 ref4, ref5, ref6, ref7 );                                    \
-    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                     \
-    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                     \
-    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                     \
-    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                     \
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                     \
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                     \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
-    sad = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
-    sad = __lasx_xvhaddw_wu_hu( sad, sad );                                   \
-    sad = __lasx_xvhaddw_du_wu( sad, sad );                                   \
-    sad = __lasx_xvhaddw_qu_du( sad, sad );                                   \
-
-    LOAD_REF_DATA_4W_8H( p_ref0, sad0 );
-    LOAD_REF_DATA_4W_8H( p_ref1, sad1 );
-    LOAD_REF_DATA_4W_8H( p_ref2, sad2 );
-    LOAD_REF_DATA_4W_8H( p_ref3, sad3 );
-
-#undef LOAD_REF_DATA_4W_8H
-
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
-}
-
-void x264_pixel_sad_x4_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 uint8_t *p_ref3, intptr_t i_ref_stride,
-                                 int32_t p_sad_array[4] )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
-    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
-    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
-    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-
-    src0 = __lasx_xvld( p_src, 0 );
-    src1 = __lasx_xvld( p_src, FENC_STRIDE );
-    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-    src0 = __lasx_xvilvl_w( src1, src0 );
-    src1 = __lasx_xvilvl_w( src3, src2 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src0 = __lasx_xvpermi_q(src0, src0, 0x00);
-
-#define LOAD_REF_DATA_4W_4H( p0, p1 )                                    \
-    LASX_LOAD_4( p0, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,     \
-                 ref0, ref1, ref2, ref3 );                               \
-    LASX_LOAD_4( p1, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,     \
-                 ref4, ref5, ref6, ref7 );                               \
-    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                \
-    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                \
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                \
-    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                \
-    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                \
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                \
-    ref0 = __lasx_xvpermi_q(ref0, ref1, 0x02);                           \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                               \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                           \
-    diff = __lasx_xvhaddw_wu_hu( diff, diff );                           \
-    diff = __lasx_xvhaddw_du_wu( diff, diff );                           \
-    diff = __lasx_xvhaddw_qu_du( diff, diff );                           \
-
-    LOAD_REF_DATA_4W_4H( p_ref0, p_ref1 );
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(diff, 0);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(diff, 4);
-    LOAD_REF_DATA_4W_4H( p_ref2, p_ref3 );
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(diff, 0);
-    p_sad_array[3] = __lasx_xvpickve2gr_wu(diff, 4);
-
-#undef LOAD_REF_DATA_4W_4H
-
-}
-
-#define SAD_LOAD                                                              \
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;                   \
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;                   \
-    __m256i diff;                                                             \
-    __m256i sad0 = __lasx_xvldi(0);                                           \
-    __m256i sad1 = __lasx_xvldi(0);                                           \
-    __m256i sad2 = __lasx_xvldi(0);                                           \
-    int32_t i_src_stride_x2 = FENC_STRIDE << 1;                               \
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;                              \
-    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;                  \
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;                 \
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;                           \
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-
-
-#define LOAD_REF_DATA_16W( p_ref, sad)                                        \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
-                 ref0, ref1, ref2, ref3 );                                    \
-    p_ref += i_ref_stride_x4;                                                 \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
-                 ref4, ref5, ref6, ref7 );                                    \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
-    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
-    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );                              \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src1, ref1 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src2, ref2 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src3, ref3 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-
-
-#define ST_REF_DATA(sad)                                  \
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
-    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
-    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
-
-void x264_pixel_sad_x3_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                   uint8_t *p_ref1, uint8_t *p_ref2,
-                                   intptr_t i_ref_stride,
-                                   int32_t p_sad_array[3] )
-{
-    SAD_LOAD
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-    LOAD_REF_DATA_16W( p_ref0, sad0 );
-    LOAD_REF_DATA_16W( p_ref1, sad1 );
-    LOAD_REF_DATA_16W( p_ref2, sad2 );
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-    p_ref0 += i_ref_stride_x4;
-    p_ref1 += i_ref_stride_x4;
-    p_ref2 += i_ref_stride_x4;
-
-    LOAD_REF_DATA_16W( p_ref0, sad0 );
-    LOAD_REF_DATA_16W( p_ref1, sad1 );
-    LOAD_REF_DATA_16W( p_ref2, sad2 );
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-}
-
-void x264_pixel_sad_x3_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                  uint8_t *p_ref1, uint8_t *p_ref2,
-                                  intptr_t i_ref_stride,
-                                  int32_t p_sad_array[3] )
-{
-    SAD_LOAD
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-    LOAD_REF_DATA_16W( p_ref0, sad0 );
-    LOAD_REF_DATA_16W( p_ref1, sad1 );
-    LOAD_REF_DATA_16W( p_ref2, sad2 );
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-}
-
-#undef LOAD_REF_DATA_16W
+#undef LOAD_REF_DATA_16W
 
 #define LOAD_REF_DATA_8W( p_ref, sad)                                          \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
@@ -1379,619 +288,77 @@ void x264_pixel_sad_x3_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
                  ref4, ref5, ref6, ref7 );                                     \
     ref0 = __lasx_xvilvl_d( ref1, ref0 );                                      \
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
-    ref2 = __lasx_xvilvl_d( ref5, ref4 );                                      \
-    ref3 = __lasx_xvilvl_d( ref7, ref6 );                                      \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                               \
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                               \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                                     \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
-    sad  = __lasx_xvadd_h(sad, diff);                                          \
-    diff = __lasx_xvabsd_bu( src1, ref1 );                                     \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
-    sad  = __lasx_xvadd_h(sad, diff);                                          \
-
-void x264_pixel_sad_x3_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                  uint8_t *p_ref1, uint8_t *p_ref2,
-                                  intptr_t i_ref_stride,
-                                  int32_t p_sad_array[3] )
-{
-    SAD_LOAD
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    LOAD_REF_DATA_8W( p_ref0, sad0 );
-    LOAD_REF_DATA_8W( p_ref1, sad1 );
-    LOAD_REF_DATA_8W( p_ref2, sad2 );
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    p_ref0 += i_ref_stride_x4;
-    p_ref1 += i_ref_stride_x4;
-    p_ref2 += i_ref_stride_x4;
-
-    LOAD_REF_DATA_8W( p_ref0, sad0 );
-    LOAD_REF_DATA_8W( p_ref1, sad1 );
-    LOAD_REF_DATA_8W( p_ref2, sad2 );
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-}
-
-void x264_pixel_sad_x3_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 intptr_t i_ref_stride,
-                                 int32_t p_sad_array[3] )
-{
-    SAD_LOAD
-
-    src0 = __lasx_xvld(p_src, 0);
-    src1 = __lasx_xvld(p_src, FENC_STRIDE);
-    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld(p_src, 0);
-    src5 = __lasx_xvld(p_src, FENC_STRIDE);
-    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
-    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    LOAD_REF_DATA_8W( p_ref0, sad0 );
-    LOAD_REF_DATA_8W( p_ref1, sad1 );
-    LOAD_REF_DATA_8W( p_ref2, sad2 );
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-}
-
-#undef SAD_LOAD
-#undef LOAD_REF_DATA_8W
-
-void x264_pixel_sad_x3_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 intptr_t i_ref_stride,
-                                 int32_t p_sad_array[3] )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i diff;
-    __m256i sad0, sad1, sad2;
-    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
-    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
-    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
-    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
-
-    src0 = __lasx_xvld( p_src, 0 );
-    src1 = __lasx_xvld( p_src, FENC_STRIDE );
-    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-
-#define LOAD_REF_DATA_8W_4H( p_ref, sad)                                \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3, \
-                 ref0, ref1, ref2, ref3 );                              \
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );                               \
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );                               \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                        \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                              \
-    sad = __lasx_xvhaddw_hu_bu( diff, diff );
-
-    LOAD_REF_DATA_8W_4H( p_ref0, sad0 );
-    LOAD_REF_DATA_8W_4H( p_ref1, sad1 );
-    LOAD_REF_DATA_8W_4H( p_ref2, sad2 );
-
-#undef LOAD_REF_DATA_8W_4H
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-
-#undef ST_REF_DATA
-
-}
-
-void x264_pixel_sad_x3_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 intptr_t i_ref_stride,
-                                 int32_t p_sad_array[3] )
-{
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad0 = __lasx_xvldi( 0 );
-    __m256i sad1 = __lasx_xvldi( 0 );
-    __m256i sad2 = __lasx_xvldi( 0 );
-    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
-    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
-    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
-    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
-    intptr_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    intptr_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-
-    src0 = __lasx_xvld( p_src, 0 );
-    src1 = __lasx_xvld( p_src, FENC_STRIDE );
-    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-    p_src += i_src_stride_x4;
-    src4 = __lasx_xvld( p_src, 0 );
-    src5 = __lasx_xvld( p_src, FENC_STRIDE );
-    src6 = __lasx_xvldx( p_src, i_src_stride_x2 );
-    src7 = __lasx_xvldx( p_src, i_src_stride_x3 );
-    src0 = __lasx_xvilvl_w( src1, src0 );
-    src1 = __lasx_xvilvl_w( src3, src2 );
-    src2 = __lasx_xvilvl_w( src5, src4 );
-    src3 = __lasx_xvilvl_w( src7, src6 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-
-#define LOAD_REF_DATA_4W_8H( p_ref, sad)                                       \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
-                 ref0, ref1, ref2, ref3 );                                     \
-    p_ref += i_ref_stride_x4;                                                  \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
-                 ref4, ref5, ref6, ref7 );                                     \
-    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                      \
-    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                      \
-    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                      \
-    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                      \
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                      \
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                               \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                                     \
-    sad = __lasx_xvhaddw_hu_bu( diff, diff );
-
-    LOAD_REF_DATA_4W_8H( p_ref0, sad0 );
-    LOAD_REF_DATA_4W_8H( p_ref1, sad1 );
-    LOAD_REF_DATA_4W_8H( p_ref2, sad2 );
-
-#undef LOAD_REF_DATA_4W_8H
-
-#define ST_REF_DATA(sad)                                  \
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
-    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
-    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
-
-    ST_REF_DATA(sad0);
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-
-#undef ST_REF_DATA
-
-}
-
-void x264_pixel_sad_x3_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 intptr_t i_ref_stride,
-                                 int32_t p_sad_array[3] )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
-    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
-    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
-    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
-
-    src0 = __lasx_xvld( p_src, 0 );
-    src1 = __lasx_xvld( p_src, FENC_STRIDE );
-    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-    src0 = __lasx_xvilvl_w( src1, src0 );
-    src1 = __lasx_xvilvl_w( src3, src2 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src0 = __lasx_xvpermi_q(src0, src0, 0x00);
-
-    LASX_LOAD_4( p_ref0, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref0, ref1, ref2, ref3 );
-    LASX_LOAD_4( p_ref1, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref4, ref5, ref6, ref7 );
-    ref0 = __lasx_xvilvl_w( ref1, ref0 );
-    ref1 = __lasx_xvilvl_w( ref3, ref2 );
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );
-    ref2 = __lasx_xvilvl_w( ref5, ref4 );
-    ref3 = __lasx_xvilvl_w( ref7, ref6 );
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );
-    ref0 = __lasx_xvpermi_q(ref0, ref1, 0x02);
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    diff = __lasx_xvhaddw_wu_hu( diff, diff );
-    diff = __lasx_xvhaddw_du_wu( diff, diff );
-    diff = __lasx_xvhaddw_qu_du( diff, diff );
-
-    p_sad_array[0] = __lasx_xvpickve2gr_wu(diff, 0);
-    p_sad_array[1] = __lasx_xvpickve2gr_wu(diff, 4);
-    LASX_LOAD_4( p_ref2, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref0, ref1, ref2, ref3 );
-    ref0 = __lasx_xvilvl_w( ref1, ref0 );
-    ref1 = __lasx_xvilvl_w( ref3, ref2 );
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    diff = __lasx_xvhaddw_wu_hu( diff, diff );
-    diff = __lasx_xvhaddw_du_wu( diff, diff );
-    diff = __lasx_xvhaddw_qu_du( diff, diff );
-    p_sad_array[2] = __lasx_xvpickve2gr_wu(diff, 0);
-
-}
-
-static inline uint32_t sad_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                        uint8_t *p_ref, int32_t i_ref_stride,
-                                        int32_t i_height )
-{
-    int32_t i_ht_cnt;
-    uint32_t result;
-    uint8_t * p_src2;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad = __lasx_xvldi( 0 );
-    int32_t i_src_stride_x2 = FENC_STRIDE << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-    int32_t i_src_stride_x8 = i_src_stride << 3;
-
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src, 0 );
-        src1 = __lasx_xvld( p_src, FENC_STRIDE );
-        src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-        src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-        p_src2 = p_src + i_src_stride_x4;
-        src4 = __lasx_xvld( p_src2, 0 );
-        src5 = __lasx_xvld( p_src2, FENC_STRIDE );
-        src6 = __lasx_xvldx( p_src2, i_src_stride_x2 );
-        src7 = __lasx_xvldx( p_src2, i_src_stride_x3 );
-        p_src += i_src_stride_x8;
-        src0 = __lasx_xvilvl_w( src1, src0 );
-        src1 = __lasx_xvilvl_w( src3, src2 );
-        src2 = __lasx_xvilvl_w( src5, src4 );
-        src3 = __lasx_xvilvl_w( src7, src6 );
-        src0 = __lasx_xvilvl_d( src1, src0 );
-        src1 = __lasx_xvilvl_d( src3, src2 );
-        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref0, ref1, ref2, ref3 );
-        p_ref += i_ref_stride_x4;
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref4, ref5, ref6, ref7 );
-        p_ref += i_ref_stride_x4;
-        ref0 = __lasx_xvilvl_w( ref1, ref0 );
-        ref1 = __lasx_xvilvl_w( ref3, ref2 );
-        ref2 = __lasx_xvilvl_w( ref5, ref4 );
-        ref3 = __lasx_xvilvl_w( ref7, ref6 );
-        ref0 = __lasx_xvilvl_d( ref1, ref0 );
-        ref1 = __lasx_xvilvl_d( ref3, ref2 );
-        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-        diff = __lasx_xvabsd_bu( src0, ref0 );
-        diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h( sad, diff );
-    }
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);
-    sad = __lasx_xvhaddw_du_wu(sad, sad);
-    sad = __lasx_xvhaddw_qu_du(sad, sad);
-    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
-
-    return ( result );
-}
-
-int32_t x264_pixel_sad_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                   uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t result;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff, sad;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src0, src1, src2, src3 );
-    p_src += i_src_stride_x4;
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src4, src5, src6, src7 );
-    p_src += i_src_stride_x4;
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref0, ref1, ref2, ref3 );
-    p_ref += i_ref_stride_x4;
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref4, ref5, ref6, ref7 );
-    p_ref += i_ref_stride_x4;
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
-    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
-    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
-    diff = __lasx_xvabsd_bu( src1, ref1 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    diff = __lasx_xvabsd_bu( src2, ref2 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    diff = __lasx_xvabsd_bu( src3, ref3 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src0, src1, src2, src3 );
-    p_src += i_src_stride_x4;
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src4, src5, src6, src7 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref0, ref1, ref2, ref3 );
-    p_ref += i_ref_stride_x4;
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref4, ref5, ref6, ref7 );
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
-    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
-    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    diff = __lasx_xvabsd_bu( src1, ref1 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    diff = __lasx_xvabsd_bu( src2, ref2 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    diff = __lasx_xvabsd_bu( src3, ref3 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);
-    sad = __lasx_xvhaddw_du_wu(sad, sad);
-    sad = __lasx_xvhaddw_qu_du(sad, sad);
-    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
-    return result;
-}
-
-int32_t x264_pixel_sad_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                  uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t result;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff, sad;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src0, src1, src2, src3 );
-    p_src += i_src_stride_x4;
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src4, src5, src6, src7 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref0, ref1, ref2, ref3 );
-    p_ref += i_ref_stride_x4;
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref4, ref5, ref6, ref7 );
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
-    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
-    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
-    diff = __lasx_xvabsd_bu( src1, ref1 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    diff = __lasx_xvabsd_bu( src2, ref2 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    diff = __lasx_xvabsd_bu( src3, ref3 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);
-    sad = __lasx_xvhaddw_du_wu(sad, sad);
-    sad = __lasx_xvhaddw_qu_du(sad, sad);
-    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
-
-    return ( result );
-}
-
-int32_t x264_pixel_sad_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                  uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t result;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff, sad;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src0, src1, src2, src3 );
-    p_src += i_src_stride_x4;
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src4, src5, src6, src7 );
-    p_src += i_src_stride_x4;
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref0, ref1, ref2, ref3 );
-    p_ref += i_ref_stride_x4;
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref4, ref5, ref6, ref7 );
-    p_ref += i_ref_stride_x4;
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );
-    ref2 = __lasx_xvilvl_d( ref5, ref4 );
-    ref3 = __lasx_xvilvl_d( ref7, ref6 );
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
-    diff = __lasx_xvabsd_bu( src1, ref1 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src0, src1, src2, src3 );
-    p_src += i_src_stride_x4;
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src4, src5, src6, src7 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref0, ref1, ref2, ref3 );
-    p_ref += i_ref_stride_x4;
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref4, ref5, ref6, ref7 );
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );
-    ref2 = __lasx_xvilvl_d( ref5, ref4 );
-    ref3 = __lasx_xvilvl_d( ref7, ref6 );
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-    diff = __lasx_xvabsd_bu( src1, ref1 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
-
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);
-    sad = __lasx_xvhaddw_du_wu(sad, sad);
-    sad = __lasx_xvhaddw_qu_du(sad, sad);
-    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );                                      \
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );                                      \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                               \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                               \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                     \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
+    sad  = __lasx_xvadd_h(sad, diff);                                          \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                     \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
+    sad  = __lasx_xvadd_h(sad, diff);                                          \
 
-    return ( result );
-}
+#undef SAD_LOAD
+#undef LOAD_REF_DATA_8W
 
-int32_t x264_pixel_sad_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
+static inline uint32_t sad_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                        uint8_t *p_ref, int32_t i_ref_stride,
+                                        int32_t i_height )
 {
+    int32_t i_ht_cnt;
     uint32_t result;
+    uint8_t * p_src2;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff, sad;
+    __m256i diff;
+    __m256i sad = __lasx_xvldi( 0 );
     int32_t i_src_stride_x2 = i_src_stride << 1;
     int32_t i_ref_stride_x2 = i_ref_stride << 1;
     int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
     int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
     int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
     int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+    int32_t i_src_stride_x8 = i_src_stride << 3;
 
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src0, src1, src2, src3 );
-    p_src += i_src_stride_x4;
-    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                 src4, src5, src6, src7 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src2 = __lasx_xvilvl_d( src5, src4 );
-    src3 = __lasx_xvilvl_d( src7, src6 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref0, ref1, ref2, ref3 );
-    p_ref += i_ref_stride_x4;
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                 ref4, ref5, ref6, ref7 );
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );
-    ref2 = __lasx_xvilvl_d( ref5, ref4 );
-    ref3 = __lasx_xvilvl_d( ref7, ref6 );
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
-    diff = __lasx_xvabsd_bu( src1, ref1 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    sad  = __lasx_xvadd_h(sad, diff);
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src, 0 );
+        src1 = __lasx_xvldx( p_src, i_src_stride );
+        src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+        src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+        p_src2 = p_src + i_src_stride_x4;
+        src4 = __lasx_xvld( p_src2, 0 );
+        src5 = __lasx_xvldx( p_src2, i_src_stride );
+        src6 = __lasx_xvldx( p_src2, i_src_stride_x2 );
+        src7 = __lasx_xvldx( p_src2, i_src_stride_x3 );
+        p_src += i_src_stride_x8;
+        src0 = __lasx_xvilvl_w( src1, src0 );
+        src1 = __lasx_xvilvl_w( src3, src2 );
+        src2 = __lasx_xvilvl_w( src5, src4 );
+        src3 = __lasx_xvilvl_w( src7, src6 );
+        src0 = __lasx_xvilvl_d( src1, src0 );
+        src1 = __lasx_xvilvl_d( src3, src2 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref0, ref1, ref2, ref3 );
+        p_ref += i_ref_stride_x4;
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref4, ref5, ref6, ref7 );
+        p_ref += i_ref_stride_x4;
+        ref0 = __lasx_xvilvl_w( ref1, ref0 );
+        ref1 = __lasx_xvilvl_w( ref3, ref2 );
+        ref2 = __lasx_xvilvl_w( ref5, ref4 );
+        ref3 = __lasx_xvilvl_w( ref7, ref6 );
+        ref0 = __lasx_xvilvl_d( ref1, ref0 );
+        ref1 = __lasx_xvilvl_d( ref3, ref2 );
+        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+        diff = __lasx_xvabsd_bu( src0, ref0 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h( sad, diff );
+    }
     sad = __lasx_xvhaddw_wu_hu(sad, sad);
     sad = __lasx_xvhaddw_du_wu(sad, sad);
     sad = __lasx_xvhaddw_qu_du(sad, sad);
@@ -2000,91 +367,12 @@ int32_t x264_pixel_sad_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
     return ( result );
 }
 
-int32_t x264_pixel_sad_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i diff;
-    int32_t result;
-    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
-    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
-    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
-    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-
-    src0 = __lasx_xvld( p_src, 0 );
-    src1 = __lasx_xvld( p_src, FENC_STRIDE );
-    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-    src1 = __lasx_xvilvl_d( src3, src2 );
-    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-
-    ref0 = __lasx_xvld( p_ref, 0 );
-    ref1 = __lasx_xvldx( p_ref, i_ref_stride );
-    ref2 = __lasx_xvldx( p_ref, i_ref_stride_x2 );
-    ref3 = __lasx_xvldx( p_ref, i_ref_stride_x3 );
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    diff = __lasx_xvhaddw_wu_hu( diff, diff );
-    diff = __lasx_xvhaddw_du_wu( diff, diff );
-    diff = __lasx_xvhaddw_qu_du( diff, diff );
-    result = __lasx_xvpickve2gr_wu(diff, 0) + __lasx_xvpickve2gr_wu(diff, 4);
-    return ( result );
-}
-
 int32_t x264_pixel_sad_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                   uint8_t *p_ref, intptr_t i_ref_stride )
 {
     return sad_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
 }
 
-int32_t x264_pixel_sad_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    return sad_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
-}
-
-int32_t x264_pixel_sad_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i diff;
-    int32_t result;
-    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
-    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
-    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
-    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-
-    src0 = __lasx_xvld( p_src, 0);
-    src1 = __lasx_xvld( p_src, FENC_STRIDE );
-    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-    src0 = __lasx_xvilvl_w( src1, src0 );
-    src1 = __lasx_xvilvl_w( src3, src2 );
-    src0 = __lasx_xvilvl_d( src1, src0 );
-
-    ref0 = __lasx_xvld( p_ref, 0 );
-    ref1 = __lasx_xvldx( p_ref, i_ref_stride );
-    ref2 = __lasx_xvldx( p_ref, i_ref_stride_x2 );
-    ref3 = __lasx_xvldx( p_ref, i_ref_stride_x3 );
-    ref0 = __lasx_xvilvl_w( ref1, ref0 );
-    ref1 = __lasx_xvilvl_w( ref3, ref2 );
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );
-    diff = __lasx_xvabsd_bu( src0, ref0 );
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );
-    diff = __lasx_xvhaddw_wu_hu( diff, diff );
-    diff = __lasx_xvhaddw_du_wu( diff, diff );
-    diff = __lasx_xvhaddw_qu_du( diff, diff );
-    result = __lasx_xvpickve2gr_w(diff, 0);
-
-    return ( result );
-}
-
 static inline uint64_t pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix,
                                                    int32_t i_stride )
 {
@@ -2195,168 +483,6 @@ static inline uint64_t pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix,
     return ((uint64_t) u_sum8 << 32) + u_sum4;
 }
 
-static inline uint64_t pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix,
-                                                    int32_t i_stride )
-{
-    uint32_t u_sum4 = 0, u_sum8 = 0, u_dc;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
-    __m256i sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7;
-    int32_t i_stride2 = i_stride << 1;
-    int32_t i_stride3 = i_stride2 + i_stride;
-    int32_t i_stride4 = i_stride2 << 1;
-    v16i16  dc;
-
-    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src0, src1, src2, src3);
-    p_pix += i_stride4;
-    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src4, src5, src6, src7);
-
-    diff0 = __lasx_xvpermi_q(src0, src4, 0x02);
-    diff1 = __lasx_xvpermi_q(src1, src5, 0x02);
-    diff2 = __lasx_xvpermi_q(src2, src6, 0x02);
-    diff3 = __lasx_xvpermi_q(src3, src7, 0x02);
-
-    diff4 = __lasx_xvpickev_b(diff1, diff0);
-    diff5 = __lasx_xvpickod_b(diff1, diff0);
-    diff6 = __lasx_xvpickev_b(diff3, diff2);
-    diff7 = __lasx_xvpickod_b(diff3, diff2);
-
-    src0 = __lasx_xvaddwev_h_bu(diff4, diff5);
-    src1 = __lasx_xvaddwod_h_bu(diff4, diff5);
-    src2 = __lasx_xvsubwev_h_bu(diff4, diff5);
-    src3 = __lasx_xvsubwod_h_bu(diff4, diff5);
-    src4 = __lasx_xvaddwev_h_bu(diff6, diff7);
-    src5 = __lasx_xvaddwod_h_bu(diff6, diff7);
-    src6 = __lasx_xvsubwev_h_bu(diff6, diff7);
-    src7 = __lasx_xvsubwod_h_bu(diff6, diff7);
-
-    diff0 = __lasx_xvadd_h(src0, src1);
-    diff1 = __lasx_xvadd_h(src2, src3);
-    diff2 = __lasx_xvsub_h(src0, src1);
-    diff3 = __lasx_xvsub_h(src2, src3);
-    diff4 = __lasx_xvadd_h(src4, src5);
-    diff5 = __lasx_xvadd_h(src6, src7);
-    diff6 = __lasx_xvsub_h(src4, src5);
-    diff7 = __lasx_xvsub_h(src6, src7);
-
-    src0 = __lasx_xvilvl_h(diff1, diff0);
-    src1 = __lasx_xvilvh_h(diff1, diff0);
-    src2 = __lasx_xvilvl_h(diff3, diff2);
-    src3 = __lasx_xvilvh_h(diff3, diff2);
-
-    src4 = __lasx_xvilvl_h(diff5, diff4);
-    src5 = __lasx_xvilvh_h(diff5, diff4);
-    src6 = __lasx_xvilvl_h(diff7, diff6);
-    src7 = __lasx_xvilvh_h(diff7, diff6);
-
-    diff0 = __lasx_xvilvl_w(src2, src0);
-    diff1 = __lasx_xvilvh_w(src2, src0);
-    diff2 = __lasx_xvilvl_w(src3, src1);
-    diff3 = __lasx_xvilvh_w(src3, src1);
-
-    diff4 = __lasx_xvilvl_w(src6, src4);
-    diff5 = __lasx_xvilvh_w(src6, src4);
-    diff6 = __lasx_xvilvl_w(src7, src5);
-    diff7 = __lasx_xvilvh_w(src7, src5);
-
-    src0 = __lasx_xvadd_h(diff0, diff2);
-    src4 = __lasx_xvadd_h(diff1, diff3);
-    src2 = __lasx_xvadd_h(diff4, diff6);
-    src6 = __lasx_xvadd_h(diff5, diff7);
-    src1 = __lasx_xvsub_h(diff0, diff2);
-    src5 = __lasx_xvsub_h(diff1, diff3);
-    src3 = __lasx_xvsub_h(diff4, diff6);
-    src7 = __lasx_xvsub_h(diff5, diff7);
-
-    diff0 = __lasx_xvadd_h(src0, src2);
-    diff1 = __lasx_xvadd_h(src1, src3);
-    diff2 = __lasx_xvsub_h(src0, src2);
-    diff3 = __lasx_xvsub_h(src1, src3);
-    diff4 = __lasx_xvadd_h(src4, src6);
-    diff5 = __lasx_xvadd_h(src5, src7);
-    diff6 = __lasx_xvsub_h(src4, src6);
-    diff7 = __lasx_xvsub_h(src5, src7);
-
-    dc = (v16i16)diff0;
-    u_dc = (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
-    dc = (v16i16)diff4;
-    u_dc += (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
-
-    sub0 = __lasx_xvadda_h(diff0, diff1);
-    sub1 = __lasx_xvadda_h(diff2, diff3);
-    sub2 = __lasx_xvadda_h(diff4, diff5);
-    sub3 = __lasx_xvadda_h(diff6, diff7);
-    sub0 = __lasx_xvadd_h(sub0, sub1);
-    sub0 = __lasx_xvadd_h(sub0, sub2);
-    sub0 = __lasx_xvadd_h(sub0, sub3);
-    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
-    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
-    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
-    u_sum4 = __lasx_xvpickve2gr_wu(sub0, 0) + __lasx_xvpickve2gr_wu(sub0, 4);
-
-    sub0 = __lasx_xvpackev_h(diff1, diff0);
-    sub1 = __lasx_xvpackod_h(diff1, diff0);
-    sub2 = __lasx_xvpackev_h(diff3, diff2);
-    sub3 = __lasx_xvpackod_h(diff3, diff2);
-    sub4 = __lasx_xvpackev_h(diff5, diff4);
-    sub5 = __lasx_xvpackod_h(diff5, diff4);
-    sub6 = __lasx_xvpackev_h(diff7, diff6);
-    sub7 = __lasx_xvpackod_h(diff7, diff6);
-
-    src0 = __lasx_xvilvl_d(sub2, sub0);
-    src1 = __lasx_xvilvh_d(sub2, sub0);
-    src2 = __lasx_xvilvl_d(sub3, sub1);
-    src3 = __lasx_xvilvh_d(sub3, sub1);
-    src4 = __lasx_xvilvl_d(sub6, sub4);
-    src5 = __lasx_xvilvh_d(sub6, sub4);
-    src6 = __lasx_xvilvl_d(sub7, sub5);
-    src7 = __lasx_xvilvh_d(sub7, sub5);
-
-    diff0 = __lasx_xvpermi_q(src0, src4, 0x02);
-    diff1 = __lasx_xvpermi_q(src1, src5, 0x02);
-    diff2 = __lasx_xvpermi_q(src0, src4, 0x13);
-    diff3 = __lasx_xvpermi_q(src1, src5, 0x13);
-    diff4 = __lasx_xvpermi_q(src2, src6, 0x02);
-    diff5 = __lasx_xvpermi_q(src2, src6, 0x13);
-    diff6 = __lasx_xvpermi_q(src3, src7, 0x02);
-    diff7 = __lasx_xvpermi_q(src3, src7, 0x13);
-
-    src0 = __lasx_xvadd_h(diff0, diff1);
-    src1 = __lasx_xvsub_h(diff0, diff1);
-    src2 = __lasx_xvadd_h(diff2, diff3);
-    src3 = __lasx_xvsub_h(diff2, diff3);
-    src4 = __lasx_xvadd_h(diff4, diff5);
-    src5 = __lasx_xvsub_h(diff4, diff5);
-    src6 = __lasx_xvadd_h(diff6, diff7);
-    src7 = __lasx_xvsub_h(diff6, diff7);
-
-    diff0 = __lasx_xvadd_h(src0, src2);
-    diff1 = __lasx_xvadd_h(src1, src3);
-    diff2 = __lasx_xvsub_h(src0, src2);
-    diff3 = __lasx_xvsub_h(src1, src3);
-    diff4 = __lasx_xvadd_h(src4, src6);
-    diff5 = __lasx_xvadd_h(src5, src7);
-    diff6 = __lasx_xvsub_h(src4, src6);
-    diff7 = __lasx_xvsub_h(src5, src7);
-
-    sub0 = __lasx_xvadda_h(diff0, diff1);
-    sub1 = __lasx_xvadda_h(diff2, diff3);
-    sub2 = __lasx_xvadda_h(diff4, diff5);
-    sub3 = __lasx_xvadda_h(diff6, diff7);
-
-    sub0 = __lasx_xvadd_h(sub0, sub1);
-    sub0 = __lasx_xvadd_h(sub0, sub2);
-    sub0 = __lasx_xvadd_h(sub0, sub3);
-
-    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
-    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
-    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
-    u_sum8 = __lasx_xvpickve2gr_wu(sub0, 0) + __lasx_xvpickve2gr_wu(sub0, 4);
-    u_sum4 = u_sum4 - u_dc;
-    u_sum8 = u_sum8 - u_dc;
-    return ((uint64_t) u_sum8 << 32) + u_sum4;
-}
-
 uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
 {
     uint64_t u_sum;
@@ -2376,25 +502,6 @@ uint64_t x264_pixel_hadamard_ac_8x16_lasx( uint8_t *p_pix, intptr_t i_stride )
     return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
 }
 
-uint64_t x264_pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix, intptr_t i_stride )
-{
-    uint64_t u_sum;
-
-    u_sum = pixel_hadamard_ac_16x8_lasx( p_pix, i_stride );
-
-    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
-}
-
-uint64_t x264_pixel_hadamard_ac_16x16_lasx( uint8_t *p_pix, intptr_t i_stride )
-{
-    uint64_t u_sum;
-
-    u_sum = pixel_hadamard_ac_16x8_lasx( p_pix, i_stride );
-    u_sum += pixel_hadamard_ac_16x8_lasx( p_pix + ( i_stride << 3 ), i_stride );
-
-    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
-}
-
 static int32_t sa8d_8x8_lasx( uint8_t *p_src, int32_t i_src_stride,
                               uint8_t *p_ref, int32_t i_ref_stride )
 {
@@ -2758,85 +865,6 @@ void x264_intra_sad_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
     _ref7 = __lasx_xvsubwod_h_bu(_src6, _ref6);                                    \
 }
 
-
-int32_t x264_pixel_ssd_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                   uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t u_ssd;
-    intptr_t src_stride2 = i_src_stride << 1;
-    intptr_t ref_stride2 = i_ref_stride << 1;
-    intptr_t src_stride3 = i_src_stride + src_stride2;
-    intptr_t ref_stride3 = i_ref_stride + ref_stride2;
-    intptr_t src_stride4 = src_stride2 << 1;
-    intptr_t ref_stride4 = ref_stride2 << 1;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
-
-    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
-               src0, src1, src2, src3, src4, src5, src6, src7);
-    p_src += src_stride4;
-    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
-               src8, src9, src10, src11, src12, src13, src14, src15);
-
-    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
-               ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-    p_ref += ref_stride4;
-    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
-               ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
-
-    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
-                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
-                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
-
-    SSD_SUB_8(src0, src1, src2, src3, src4, src5, src6, src7,
-              ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-    SSD_SUB_8(src8, src9, src10, src11, src12, src13, src14, src15,
-              ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
-
-    src0 = __lasx_xvmulwev_w_h(src1, src1);
-    src0 = __lasx_xvmaddwod_w_h(src0, src1, src1);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref1, ref1);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref1, ref1);
-    src0 = __lasx_xvmaddwev_w_h(src0, src3, src3);
-    src0 = __lasx_xvmaddwod_w_h(src0, src3, src3);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref3, ref3);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref3, ref3);
-    src0 = __lasx_xvmaddwev_w_h(src0, src5, src5);
-    src0 = __lasx_xvmaddwod_w_h(src0, src5, src5);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref5, ref5);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref5, ref5);
-    src0 = __lasx_xvmaddwev_w_h(src0, src7, src7);
-    src0 = __lasx_xvmaddwod_w_h(src0, src7, src7);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref7, ref7);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref7, ref7);
-
-    src0 = __lasx_xvmaddwev_w_h(src0, src9, src9);
-    src0 = __lasx_xvmaddwod_w_h(src0, src9, src9);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref9, ref9);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref9, ref9);
-    src0 = __lasx_xvmaddwev_w_h(src0, src11, src11);
-    src0 = __lasx_xvmaddwod_w_h(src0, src11, src11);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref11, ref11);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref11, ref11);
-    src0 = __lasx_xvmaddwev_w_h(src0, src13, src13);
-    src0 = __lasx_xvmaddwod_w_h(src0, src13, src13);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref13, ref13);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref13, ref13);
-    src0 = __lasx_xvmaddwev_w_h(src0, src15, src15);
-    src0 = __lasx_xvmaddwod_w_h(src0, src15, src15);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref15, ref15);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref15, ref15);
-
-    ref0 = __lasx_xvhaddw_d_w(src0, src0);
-    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
-    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
-
-    return u_ssd;
-}
-
 int32_t x264_pixel_ssd_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                   uint8_t *p_ref, intptr_t i_ref_stride )
 {
@@ -2987,45 +1015,6 @@ int32_t x264_pixel_ssd_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
     return u_ssd;
 }
 
-int32_t x264_pixel_ssd_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t u_ssd;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-
-    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
-               src4, src5, src6, src7);
-    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
-               ref4, ref5, ref6, ref7);
-
-    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
-                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-
-    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
-    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
-    src4 = __lasx_xvpermi_q(src4, src5, 0x02);
-    src6 = __lasx_xvpermi_q(src6, src7, 0x02);
-
-    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
-    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
-    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
-    ref6 = __lasx_xvhsubw_hu_bu(src6, src6);
-    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref6, ref6);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref6, ref6);
-    ref0 = __lasx_xvhaddw_d_w(src0, src0);
-    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
-    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
-
-    return u_ssd;
-}
-
 int32_t x264_pixel_ssd_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride )
 {
@@ -3184,45 +1173,6 @@ int32_t x264_pixel_ssd_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
     return u_ssd;
 }
 
-int32_t x264_pixel_ssd_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t u_ssd;
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-
-    src0 = __lasx_xvldrepl_w( p_src, 0 );
-    p_src += i_src_stride;
-    src1 = __lasx_xvldrepl_w( p_src, 0 );
-    p_src += i_src_stride;
-    src2 = __lasx_xvldrepl_w( p_src, 0 );
-    p_src += i_src_stride;
-    src3 = __lasx_xvldrepl_w( p_src, 0 );
-
-    ref0 = __lasx_xvldrepl_w( p_ref, 0 );
-    p_ref += i_ref_stride;
-    ref1 = __lasx_xvldrepl_w( p_ref, 0 );
-    p_ref += i_ref_stride;
-    ref2 = __lasx_xvldrepl_w( p_ref, 0 );
-    p_ref += i_ref_stride;
-    ref3 = __lasx_xvldrepl_w( p_ref, 0 );
-
-    src0 = __lasx_xvilvl_b(src0, ref0);
-    src1 = __lasx_xvilvl_b(src1, ref1);
-    src2 = __lasx_xvilvl_b(src2, ref2);
-    src3 = __lasx_xvilvl_b(src3, ref3);
-    src0 = __lasx_xvilvl_d(src1, src0);
-    src2 = __lasx_xvilvl_d(src3, src2);
-    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
-    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
-    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
-    ref0 = __lasx_xvhaddw_d_w(src0, src0);
-    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
-    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
-
-    return u_ssd;
-}
 #undef SSD_LOAD_8
 #undef SSD_INSERT_8
 
diff --git a/common/loongarch/pixel.h b/common/loongarch/pixel.h
index ca56f2a6..067d5c76 100644
--- a/common/loongarch/pixel.h
+++ b/common/loongarch/pixel.h
@@ -82,11 +82,11 @@ void x264_pixel_sad_x4_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
                                  uint8_t *p_ref1, uint8_t *p_ref2,
                                  uint8_t *p_ref3, intptr_t i_ref_stride,
                                  int32_t p_sad_array[4] );
-#define x264_pixel_sad_x4_4x4_lasx x264_template(pixel_sad_x4_4x4_lasx)
-void x264_pixel_sad_x4_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 uint8_t *p_ref3, intptr_t i_ref_stride,
-                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_4x4_lsx x264_template(pixel_sad_x4_4x4_lsx)
+void x264_pixel_sad_x4_4x4_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                uint8_t *p_ref1, uint8_t *p_ref2,
+                                uint8_t *p_ref3, intptr_t i_ref_stride,
+                                int32_t p_sad_array[4] );
 
 #define x264_pixel_sad_x3_16x16_lasx x264_template(pixel_sad_x3_16x16_lasx)
 void x264_pixel_sad_x3_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
@@ -118,11 +118,11 @@ void x264_pixel_sad_x3_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
                                  uint8_t *p_ref1, uint8_t *p_ref2,
                                  intptr_t i_ref_stride,
                                  int32_t p_sad_array[3] );
-#define x264_pixel_sad_x3_4x4_lasx x264_template(pixel_sad_x3_4x4_lasx)
-void x264_pixel_sad_x3_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                 uint8_t *p_ref1, uint8_t *p_ref2,
-                                 intptr_t i_ref_stride,
-                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_4x4_lsx x264_template(pixel_sad_x3_4x4_lsx)
+void x264_pixel_sad_x3_4x4_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                uint8_t *p_ref1, uint8_t *p_ref2,
+                                intptr_t i_ref_stride,
+                                int32_t p_sad_array[3] );
 
 #define x264_pixel_sad_16x16_lasx x264_template(pixel_sad_16x16_lasx)
 int32_t x264_pixel_sad_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
diff --git a/common/loongarch/predict-a.S b/common/loongarch/predict-a.S
new file mode 100644
index 00000000..a3f21a8d
--- /dev/null
+++ b/common/loongarch/predict-a.S
@@ -0,0 +1,1105 @@
+/*****************************************************************************
+ * predict-a.S: loongarch predict functions
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *          Lu Wang <wanglu@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+
+#if !HIGH_BIT_DEPTH
+
+/****************************************************************************
+ * 4x4 prediction for intra luma block
+ ****************************************************************************/
+/* void x264_predict_4x4_v_c( pixel *src )
+ */
+function predict_4x4_v_lsx
+    ld.wu         t0,    a0,    -FDEC_STRIDE
+    st.w          t0,    a0,    0
+    st.w          t0,    a0,    FDEC_STRIDE
+    st.w          t0,    a0,    FDEC_STRIDE * 2
+    st.w          t0,    a0,    FDEC_STRIDE * 3
+endfunc
+
+/* void x264_predict_4x4_h_c( pixel *src )
+ */
+function predict_4x4_h_lsx
+    vldrepl.b     vr0,   a0,    -1
+    vldrepl.b     vr1,   a0,    FDEC_STRIDE - 1
+    vldrepl.b     vr2,   a0,    FDEC_STRIDE * 2 - 1
+    vldrepl.b     vr3,   a0,    FDEC_STRIDE * 3 - 1
+    fst.s         f0,    a0,    0
+    fst.s         f1,    a0,    FDEC_STRIDE
+    fst.s         f2,    a0,    FDEC_STRIDE * 2
+    fst.s         f3,    a0,    FDEC_STRIDE * 3
+endfunc
+
+/* void x264_predict_4x4_dc_c( pixel *src )
+ */
+function predict_4x4_dc_lsx
+    fld.s         f0,    a0,    -FDEC_STRIDE
+    ld.bu         t0,    a0,    -1
+    ld.bu         t1,    a0,    FDEC_STRIDE - 1
+    ld.bu         t2,    a0,    FDEC_STRIDE * 2 - 1
+    ld.bu         t3,    a0,    FDEC_STRIDE * 3 - 1
+
+    vhaddw.hu.bu  vr1,   vr0,   vr0
+    vhaddw.wu.hu  vr2,   vr1,   vr1
+    vpickve2gr.w  t4,    vr2,   0
+    add.w         t0,    t0,    t1
+    add.w         t0,    t0,    t2
+    add.w         t0,    t0,    t3
+    add.w         t0,    t0,    t4
+    addi.w        t0,    t0,    4
+    srai.w        t0,    t0,    3
+
+    vreplgr2vr.b  vr0,   t0
+    vstelm.w      vr0,   a0,    0,                 0
+    vstelm.w      vr0,   a0,    FDEC_STRIDE,       0
+    vstelm.w      vr0,   a0,    FDEC_STRIDE * 2,   0
+    vstelm.w      vr0,   a0,    FDEC_STRIDE * 3,   0
+endfunc
+
+/* void predict_4x4_dc_top_c( pixel *src )
+ */
+function predict_4x4_dc_top_lsx
+    fld.s         f0,    a0,    -FDEC_STRIDE
+    vhaddw.hu.bu  vr1,   vr0,   vr0
+    vhaddw.wu.hu  vr2,   vr1,   vr1
+    vsrari.w      vr2,   vr2,   2
+
+    vreplvei.b    vr3,   vr2,   0
+    fst.s         f3,    a0,    0
+    fst.s         f3,    a0,    FDEC_STRIDE
+    fst.s         f3,    a0,    FDEC_STRIDE * 2
+    fst.s         f3,    a0,    FDEC_STRIDE * 3
+endfunc
+
+/* void predict_4x4_dc_left_c( pixel *src )
+ */
+function predict_4x4_dc_left_lsx
+    ld.bu         t0,    a0,    -1
+    ld.bu         t1,    a0,    FDEC_STRIDE - 1
+    ld.bu         t2,    a0,    FDEC_STRIDE * 2 - 1
+    ld.bu         t3,    a0,    FDEC_STRIDE * 3 - 1
+    add.w         t0,    t0,    t1
+    add.w         t0,    t0,    t2
+    add.w         t0,    t0,    t3
+    addi.w        t0,    t0,    2
+    srai.w        t0,    t0,    2
+
+    vreplgr2vr.b  vr3,   t0
+    fst.s         f3,    a0,    0
+    fst.s         f3,    a0,    FDEC_STRIDE
+    fst.s         f3,    a0,    FDEC_STRIDE * 2
+    fst.s         f3,    a0,    FDEC_STRIDE * 3
+endfunc
+
+/* void predict_4x4_dc_128_c( pixel *src )
+ */
+function predict_4x4_dc_128_lsx
+    addi.w        t0,    zero,  1
+    slli.w        t0,    t0,    BIT_DEPTH - 1
+
+    vreplgr2vr.b  vr3,   t0
+    fst.s         f3,    a0,    0
+    fst.s         f3,    a0,    FDEC_STRIDE
+    fst.s         f3,    a0,    FDEC_STRIDE * 2
+    fst.s         f3,    a0,    FDEC_STRIDE * 3
+endfunc
+
+/* void predict_4x4_ddl_c( pixel *src )
+ */
+function predict_4x4_ddl_lsx
+    fld.d           f0,    a0,       -FDEC_STRIDE
+
+    vxor.v          vr10,  vr10,     vr10
+    vilvl.b         vr0,   vr10,     vr0
+    vbsrl.v         vr1,   vr0,      2
+    vbsrl.v         vr2,   vr0,      4
+
+    // t7
+    vextrins.h      vr2,   vr0,      0x67
+
+    vslli.h         vr1,   vr1,      1
+    vadd.h          vr0,   vr0,      vr1
+    vadd.h          vr2,   vr0,      vr2
+    vssrarni.bu.h   vr3,   vr2,      2
+
+    fst.s           f3,    a0,       0
+    vbsrl.v         vr4,   vr3,      1
+    fst.s           f4,    a0,       FDEC_STRIDE
+    vbsrl.v         vr4,   vr4,      1
+    fst.s           f4,    a0,       FDEC_STRIDE * 2
+    vbsrl.v         vr4,   vr4,      1
+    fst.s           f4,    a0,       FDEC_STRIDE * 3
+endfunc
+
+/****************************************************************************
+ * 8x8 prediction for intra chroma block (4:2:0)
+ ****************************************************************************/
+/* void x264_predict_8x8c_p_lsx( pixel *src )
+ */
+const mula
+.short 1, 2, 3, 4, 0, 0, 0, 0
+endconst
+
+const mulb
+.short 0, 1, 2, 3, 4, 5, 6, 7
+endconst
+
+function predict_8x8c_p_lsx
+    la.local      t0,    mula
+    fld.d         f3,    t0,    0
+    fld.s         f4,    a0,    4  - FDEC_STRIDE
+    fld.s         f5,    a0,    -1 - FDEC_STRIDE
+    vxor.v        vr0,   vr0,   vr0
+    vilvl.b       vr4,   vr0,   vr4
+    vilvl.b       vr5,   vr0,   vr5
+    vshuf4i.h     vr5,   vr5,   0x1b
+    vsub.h        vr4,   vr4,   vr5
+    vmul.h        vr4,   vr4,   vr3
+    vhaddw.w.h    vr4,   vr4,   vr4
+    vhaddw.d.w    vr4,   vr4,   vr4
+    vpickve2gr.w  t0,    vr4,   0       /* H */
+
+    fld.s         f6,    a0,    FDEC_STRIDE * 4 - 1
+    fld.s         f7,    a0,    FDEC_STRIDE * 5 - 1
+    fld.s         f8,    a0,    FDEC_STRIDE * 6 - 1
+    fld.s         f9,    a0,    FDEC_STRIDE * 7 - 1
+    fld.s         f10,   a0,    FDEC_STRIDE * 2 - 1
+    fld.s         f11,   a0,    FDEC_STRIDE - 1
+    fld.s         f12,   a0,    -1
+    fld.s         f13,   a0,    -1 - FDEC_STRIDE
+    vilvl.b       vr6,   vr7,   vr6
+    vilvl.b       vr9,   vr9,   vr8
+    vilvl.h       vr6,   vr9,   vr6
+    vilvl.b       vr10,  vr11,  vr10
+    vilvl.b       vr12,  vr13,  vr12
+    vilvl.h       vr10,  vr12,  vr10
+    vilvl.b       vr6,   vr0,   vr6
+    vilvl.b       vr10,  vr0,   vr10
+    vsub.h        vr6,   vr6,   vr10
+    vmul.h        vr6,   vr6,   vr3
+    vhaddw.w.h    vr6,   vr6,   vr6
+    vhaddw.d.w    vr6,   vr6,   vr6
+    vpickve2gr.w  t1,    vr6,   0       /* V */
+
+    ld.bu         t2,    a0,    FDEC_STRIDE * 7 - 1
+    ld.bu         t3,    a0,    7 - FDEC_STRIDE
+    add.w         t2,    t2,    t3
+    slli.w        t2,    t2,    4       /* a */
+
+    slli.w        t3,    t0,    4
+    add.w         t0,    t0,    t3
+    addi.w        t0,    t0,    16
+    srai.w        t0,    t0,    5       /* b */
+
+    slli.w        t3,    t1,    4
+    add.w         t1,    t1,    t3
+    addi.w        t1,    t1,    16
+    srai.w        t1,    t1,    5       /* c */
+
+    add.w         t3,    t0,    t1
+    slli.w        t4,    t3,    1
+    add.w         t4,    t4,    t3
+    sub.w         t5,    t2,    t4
+    addi.w        t5,    t5,    16      /* i00 */
+
+    la.local      t3,    mulb
+    vld           vr14,  t3,    0
+    vreplgr2vr.h  vr12,  t0
+    vmul.h        vr12,  vr12,  vr14
+
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr12,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    fst.d         f15,   a0,    0
+    add.w         t5,    t5,    t1
+
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr12,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    fst.d         f15,   a0,    FDEC_STRIDE
+    add.w         t5,    t5,    t1
+
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr12,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    fst.d         f15,   a0,    FDEC_STRIDE * 2
+    add.w         t5,    t5,    t1
+
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr12,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    fst.d         f15,   a0,    FDEC_STRIDE * 3
+    add.w         t5,    t5,    t1
+
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr12,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    fst.d         f15,   a0,    FDEC_STRIDE * 4
+    add.w         t5,    t5,    t1
+
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr12,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    fst.d         f15,   a0,    FDEC_STRIDE * 5
+    add.w         t5,    t5,    t1
+
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr12,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    fst.d         f15,   a0,    FDEC_STRIDE * 6
+    add.w         t5,    t5,    t1
+
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr12,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    fst.d         f15,   a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void x264_predict_8x8c_v_lsx( pixel *src )
+ */
+function predict_8x8c_v_lsx
+    fld.d       f0,    a0,    -FDEC_STRIDE
+    fst.d       f0,    a0,    0
+    fst.d       f0,    a0,    FDEC_STRIDE
+    fst.d       f0,    a0,    FDEC_STRIDE * 2
+    fst.d       f0,    a0,    FDEC_STRIDE * 3
+    fst.d       f0,    a0,    FDEC_STRIDE * 4
+    fst.d       f0,    a0,    FDEC_STRIDE * 5
+    fst.d       f0,    a0,    FDEC_STRIDE * 6
+    fst.d       f0,    a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void x264_predict_8x8c_h_lsx( pixel *src )
+ */
+function predict_8x8c_h_lsx
+    ld.bu           t0,    a0,    -1
+    ld.bu           t1,    a0,    FDEC_STRIDE - 1
+    ld.bu           t2,    a0,    FDEC_STRIDE * 2 - 1
+    ld.bu           t3,    a0,    FDEC_STRIDE * 3 - 1
+    ld.bu           t4,    a0,    FDEC_STRIDE * 4 - 1
+    ld.bu           t5,    a0,    FDEC_STRIDE * 5 - 1
+    ld.bu           t6,    a0,    FDEC_STRIDE * 6 - 1
+    ld.bu           t7,    a0,    FDEC_STRIDE * 7 - 1
+    vreplgr2vr.b    vr0,   t0
+    vreplgr2vr.b    vr1,   t1
+    vreplgr2vr.b    vr2,   t2
+    vreplgr2vr.b    vr3,   t3
+    vreplgr2vr.b    vr4,   t4
+    vreplgr2vr.b    vr5,   t5
+    vreplgr2vr.b    vr6,   t6
+    vreplgr2vr.b    vr7,   t7
+    fst.d           f0,    a0,    0
+    fst.d           f1,    a0,    FDEC_STRIDE
+    fst.d           f2,    a0,    FDEC_STRIDE * 2
+    fst.d           f3,    a0,    FDEC_STRIDE * 3
+    fst.d           f4,    a0,    FDEC_STRIDE * 4
+    fst.d           f5,    a0,    FDEC_STRIDE * 5
+    fst.d           f6,    a0,    FDEC_STRIDE * 6
+    fst.d           f7,    a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void x264_predict_8x8c_dc_lsx( pixel *src )
+ */
+function predict_8x8c_dc_lsx
+    fld.s           f0,     a0,    -FDEC_STRIDE
+    fld.s           f1,     a0,    4 - FDEC_STRIDE
+    vhaddw.hu.bu    vr2,    vr0,   vr0
+    vhaddw.wu.hu    vr2,    vr2,   vr2
+    vhaddw.hu.bu    vr3,    vr1,   vr1
+    vhaddw.wu.hu    vr3,    vr3,   vr3
+    vpickve2gr.w    t0,     vr2,   0     /* s0 */
+    vpickve2gr.w    t1,     vr3,   0     /* s1 */
+    ld.bu           t2,     a0,    -1
+    ld.bu           t3,     a0,    FDEC_STRIDE - 1
+    ld.bu           t4,     a0,    FDEC_STRIDE * 2 - 1
+    ld.bu           t5,     a0,    FDEC_STRIDE * 3 - 1
+    add.w           t2,     t2,    t3
+    add.w           t2,     t2,    t4
+    add.w           t2,     t2,    t5    /* s2 */
+    ld.bu           t3,     a0,    FDEC_STRIDE * 4 - 1
+    ld.bu           t4,     a0,    FDEC_STRIDE * 5 - 1
+    ld.bu           t5,     a0,    FDEC_STRIDE * 6 - 1
+    ld.bu           t6,     a0,    FDEC_STRIDE * 7 - 1
+    add.w           t3,     t3,    t4
+    add.w           t3,     t3,    t5
+    add.w           t3,     t3,    t6    /* s3 */
+
+    add.w           t4,     t0,    t2
+    addi.w          t4,     t4,    4
+    srai.w          t4,     t4,    3     /* ( s0 + s2 + 4 ) >> 3 */
+    addi.w          t5,     t1,    2
+    srai.w          t5,     t5,    2     /* ( s1 + 2 ) >> 2 */
+    addi.w          t6,     t3,    2
+    srai.w          t6,     t6,    2     /* ( s3 + 2 ) >> 2 */
+    add.w           t7,     t1,    t3
+    addi.w          t7,     t7,    4
+    srai.w          t7,     t7,    3     /* ( s1 + s3 + 4 ) >> 3 */
+    vreplgr2vr.b    vr4,    t4
+    vreplgr2vr.b    vr5,    t5
+    vreplgr2vr.b    vr6,    t6
+    vreplgr2vr.b    vr7,    t7
+    vpackev.w       vr4,    vr5,   vr4
+    vpackev.w       vr6,    vr7,   vr6
+
+    fst.d           f4,     a0,    0
+    fst.d           f4,     a0,    FDEC_STRIDE
+    fst.d           f4,     a0,    FDEC_STRIDE * 2
+    fst.d           f4,     a0,    FDEC_STRIDE * 3
+
+    fst.d           f6,     a0,    FDEC_STRIDE * 4
+    fst.d           f6,     a0,    FDEC_STRIDE * 5
+    fst.d           f6,     a0,    FDEC_STRIDE * 6
+    fst.d           f6,     a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void x264_predict_8x8c_dc_128_lsx( pixel *src )
+ */
+function predict_8x8c_dc_128_lsx
+    ori             t1,     t0,    1
+    slli.d          t1,     t1,    BIT_DEPTH - 1
+    vreplgr2vr.b    vr4,    t1
+    fst.d           f4,     a0,    0
+    fst.d           f4,     a0,    FDEC_STRIDE
+    fst.d           f4,     a0,    FDEC_STRIDE * 2
+    fst.d           f4,     a0,    FDEC_STRIDE * 3
+    fst.d           f4,     a0,    FDEC_STRIDE * 4
+    fst.d           f4,     a0,    FDEC_STRIDE * 5
+    fst.d           f4,     a0,    FDEC_STRIDE * 6
+    fst.d           f4,     a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void x264_predict_8x8c_dc_top_lsx( pixel *src )
+ */
+function predict_8x8c_dc_top_lsx
+    fld.s          f0,      a0,     -FDEC_STRIDE
+    fld.s          f1,      a0,     4 - FDEC_STRIDE
+    vhaddw.hu.bu   vr0,     vr0,    vr0
+    vhaddw.wu.hu   vr0,     vr0,    vr0
+    vhaddw.hu.bu   vr1,     vr1,    vr1
+    vhaddw.wu.hu   vr1,     vr1,    vr1
+    vpickve2gr.w   t0,      vr0,    0     /* dc0 */
+    vpickve2gr.w   t1,      vr1,    0     /* dc1 */
+
+    addi.w         t0,      t0,     2
+    srai.w         t0,      t0,     2
+    addi.w         t1,      t1,     2
+    srai.w         t1,      t1,     2
+    vreplgr2vr.b   vr4,     t0
+    vreplgr2vr.b   vr5,     t1
+    vpackev.w      vr4,     vr5,    vr4
+    fst.d           f4,     a0,    0
+    fst.d           f4,     a0,    FDEC_STRIDE
+    fst.d           f4,     a0,    FDEC_STRIDE * 2
+    fst.d           f4,     a0,    FDEC_STRIDE * 3
+    fst.d           f4,     a0,    FDEC_STRIDE * 4
+    fst.d           f4,     a0,    FDEC_STRIDE * 5
+    fst.d           f4,     a0,    FDEC_STRIDE * 6
+    fst.d           f4,     a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void x264_predict_8x8c_dc_left_lsx( pixel *src )
+ */
+function predict_8x8c_dc_left_lsx
+    ld.bu          t0,      a0,      -1
+    ld.bu          t1,      a0,      FDEC_STRIDE - 1
+    ld.bu          t2,      a0,      FDEC_STRIDE * 2 - 1
+    ld.bu          t3,      a0,      FDEC_STRIDE * 3 - 1
+    add.w          t0,      t0,      t1
+    add.w          t0,      t0,      t2
+    add.w          t0,      t0,      t3
+    ld.bu          t1,      a0,      FDEC_STRIDE * 4 - 1
+    ld.bu          t2,      a0,      FDEC_STRIDE * 5 - 1
+    ld.bu          t3,      a0,      FDEC_STRIDE * 6 - 1
+    ld.bu          t4,      a0,      FDEC_STRIDE * 7 - 1
+    add.w          t1,      t1,      t2
+    add.w          t1,      t1,      t3
+    add.w          t1,      t1,      t4
+    addi.w         t0,      t0,      2
+    srai.w         t0,      t0,      2
+    addi.w         t1,      t1,      2
+    srai.w         t1,      t1,      2
+    vreplgr2vr.b   vr4,     t0       /* ( dc0 + 2 ) >> 2 */
+    vreplgr2vr.b   vr5,     t1       /* ( dc1 + 2 ) >> 2 */
+    fst.d          f4,      a0,      0
+    fst.d          f4,      a0,      FDEC_STRIDE
+    fst.d          f4,      a0,      FDEC_STRIDE * 2
+    fst.d          f4,      a0,      FDEC_STRIDE * 3
+    fst.d          f5,      a0,      FDEC_STRIDE * 4
+    fst.d          f5,      a0,      FDEC_STRIDE * 5
+    fst.d          f5,      a0,      FDEC_STRIDE * 6
+    fst.d          f5,      a0,      FDEC_STRIDE * 7
+endfunc
+
+/****************************************************************************
+ * 8x8 prediction for intra luma block
+ ****************************************************************************/
+/* void predict_8x8_v_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_v_lsx
+    fld.d           f0,     a1,    16
+    fst.d           f0,     a0,    0
+    fst.d           f0,     a0,    FDEC_STRIDE
+    fst.d           f0,     a0,    FDEC_STRIDE * 2
+    fst.d           f0,     a0,    FDEC_STRIDE * 3
+    fst.d           f0,     a0,    FDEC_STRIDE * 4
+    fst.d           f0,     a0,    FDEC_STRIDE * 5
+    fst.d           f0,     a0,    FDEC_STRIDE * 6
+    fst.d           f0,     a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void predict_8x8_h_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_h_lasx
+    fld.d           f0,     a1,    7
+    xvinsve0.w      xr0,    xr0,   5
+    xvrepl128vei.b  xr4,    xr0,   7
+    xvrepl128vei.b  xr3,    xr0,   6
+    xvrepl128vei.b  xr2,    xr0,   5
+    xvrepl128vei.b  xr1,    xr0,   4
+
+    fst.d           f4,     a0,    0
+    fst.d           f3,     a0,    FDEC_STRIDE
+    fst.d           f2,     a0,    FDEC_STRIDE * 2
+    fst.d           f1,     a0,    FDEC_STRIDE * 3
+
+    xvstelm.d       xr4,    a0,    FDEC_STRIDE * 4,     2
+    xvstelm.d       xr3,    a0,    FDEC_STRIDE * 5,     2
+    xvstelm.d       xr2,    a0,    FDEC_STRIDE * 6,     2
+    xvstelm.d       xr1,    a0,    FDEC_STRIDE * 7,     2
+endfunc
+
+/* void predict_8x8_dc_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_dc_lsx
+    fld.d           f0,     a1,    7
+    fld.d           f1,     a1,    16
+    vilvl.d         vr0,    vr1,   vr0
+    vhaddw.hu.bu    vr1,    vr0,   vr0
+    vhaddw.wu.hu    vr2,    vr1,   vr1
+    vhaddw.du.wu    vr3,    vr2,   vr2
+    vhaddw.qu.du    vr4,    vr3,   vr3
+    vsrari.w        vr4,    vr4,   4
+
+    vreplvei.b      vr5,    vr4,   0
+    fst.d           f5,     a0,    0
+    fst.d           f5,     a0,    FDEC_STRIDE
+    fst.d           f5,     a0,    FDEC_STRIDE * 2
+    fst.d           f5,     a0,    FDEC_STRIDE * 3
+    fst.d           f5,     a0,    FDEC_STRIDE * 4
+    fst.d           f5,     a0,    FDEC_STRIDE * 5
+    fst.d           f5,     a0,    FDEC_STRIDE * 6
+    fst.d           f5,     a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void predict_8x8_dc_left_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_dc_left_lsx
+    fld.d           f0,     a1,    7
+    vhaddw.hu.bu    vr1,    vr0,   vr0
+    vhaddw.wu.hu    vr2,    vr1,   vr1
+    vhaddw.du.wu    vr3,    vr2,   vr2
+    vsrari.w        vr3,    vr3,   3
+
+    vreplvei.b      vr5,    vr3,   0
+    fst.d           f5,     a0,    0
+    fst.d           f5,     a0,    FDEC_STRIDE
+    fst.d           f5,     a0,    FDEC_STRIDE * 2
+    fst.d           f5,     a0,    FDEC_STRIDE * 3
+    fst.d           f5,     a0,    FDEC_STRIDE * 4
+    fst.d           f5,     a0,    FDEC_STRIDE * 5
+    fst.d           f5,     a0,    FDEC_STRIDE * 6
+    fst.d           f5,     a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void predict_8x8_dc_top_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_dc_top_lsx
+    fld.d           f0,     a1,    16
+    vhaddw.hu.bu    vr1,    vr0,   vr0
+    vhaddw.wu.hu    vr2,    vr1,   vr1
+    vhaddw.du.wu    vr3,    vr2,   vr2
+    vsrari.w        vr3,    vr3,   3
+
+    vreplvei.b      vr5,    vr3,   0
+    fst.d           f5,     a0,    0
+    fst.d           f5,     a0,    FDEC_STRIDE
+    fst.d           f5,     a0,    FDEC_STRIDE * 2
+    fst.d           f5,     a0,    FDEC_STRIDE * 3
+    fst.d           f5,     a0,    FDEC_STRIDE * 4
+    fst.d           f5,     a0,    FDEC_STRIDE * 5
+    fst.d           f5,     a0,    FDEC_STRIDE * 6
+    fst.d           f5,     a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void predict_8x8_dc_128_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_dc_128_lsx
+    addi.w          t0,     zero,  1
+    slli.d          t1,     t0,    (BIT_DEPTH-1)
+    vreplgr2vr.b    vr5,    t1
+    fst.d           f5,     a0,    0
+    fst.d           f5,     a0,    FDEC_STRIDE
+    fst.d           f5,     a0,    FDEC_STRIDE * 2
+    fst.d           f5,     a0,    FDEC_STRIDE * 3
+    fst.d           f5,     a0,    FDEC_STRIDE * 4
+    fst.d           f5,     a0,    FDEC_STRIDE * 5
+    fst.d           f5,     a0,    FDEC_STRIDE * 6
+    fst.d           f5,     a0,    FDEC_STRIDE * 7
+endfunc
+
+/* void predict_8x8_ddl_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_ddl_lasx
+    vld             vr1,    a1,    16
+    vbsrl.v         vr2,    vr1,   1
+    vbsrl.v         vr3,    vr1,   2
+
+    vextrins.b      vr3,    vr1,   0xef
+    vext2xv.hu.bu   xr5,    xr1
+    vext2xv.hu.bu   xr6,    xr2
+    vext2xv.hu.bu   xr7,    xr3
+
+    xvslli.h        xr6,    xr6,   1
+    xvadd.h         xr8,    xr5,   xr6
+    xvadd.h         xr9,    xr8,   xr7
+    xvssrarni.bu.h  xr9,    xr9,   2
+    xvpermi.d       xr9,    xr9,   0x08
+
+    vstelm.d        vr9,    a0,    0,                 0
+    vbsrl.v         vr10,   vr9,   1
+    vstelm.d        vr10,   a0,    FDEC_STRIDE,       0
+    vbsrl.v         vr10,   vr9,   2
+    vstelm.d        vr10,   a0,    FDEC_STRIDE * 2,   0
+    vbsrl.v         vr10,   vr9,   3
+    vstelm.d        vr10,   a0,    FDEC_STRIDE * 3,   0
+
+    vbsrl.v         vr10,   vr9,   4
+    vstelm.d        vr10,   a0,    FDEC_STRIDE * 4,  0
+    vbsrl.v         vr10,   vr9,   5
+    vstelm.d        vr10,   a0,    FDEC_STRIDE * 5,   0
+    vbsrl.v         vr10,   vr9,   6
+    vstelm.d        vr10,   a0,    FDEC_STRIDE * 6,   0
+    vbsrl.v         vr10,   vr9,   7
+    vstelm.d        vr10,   a0,    FDEC_STRIDE * 7,   0
+endfunc
+
+/* void predict_8x8_ddr_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_ddr_lasx
+    vld             vr1,    a1,    7
+    vbsrl.v         vr2,    vr1,   1
+    vbsrl.v         vr3,    vr1,   2
+
+    // edge[23]
+    ld.bu           t0,     a1,    23
+    vinsgr2vr.b     vr3,    t0,    0xe
+
+    vext2xv.hu.bu   xr1,    xr1
+    vext2xv.hu.bu   xr2,    xr2
+    vext2xv.hu.bu   xr3,    xr3
+    xvslli.h        xr2,    xr2,   1
+    xvadd.h         xr4,    xr1,   xr2
+    xvadd.h         xr5,    xr4,   xr3
+    xvssrarni.bu.h  xr5,    xr5,   2
+    xvpermi.d       xr6,    xr5,   0x08
+
+    vbsrl.v         vr7,    vr6,   7
+    vstelm.d        vr7,    a0,    0,                  0
+    vbsrl.v         vr7,    vr6,   6
+    vstelm.d        vr7,    a0,    FDEC_STRIDE,        0
+    vbsrl.v         vr7,    vr6,   5
+    vstelm.d        vr7,    a0,    FDEC_STRIDE * 2,    0
+    vbsrl.v         vr7,    vr6,   4
+    vstelm.d        vr7,    a0,    FDEC_STRIDE * 3,    0
+
+    vbsrl.v         vr7,    vr6,   3
+    vstelm.d        vr7,    a0,    FDEC_STRIDE * 4,    0
+    vbsrl.v         vr7,    vr6,   2
+    vstelm.d        vr7,    a0,    FDEC_STRIDE * 5,    0
+    vbsrl.v         vr7,    vr6,   1
+    vstelm.d        vr7,    a0,    FDEC_STRIDE * 6,    0
+    vstelm.d        vr6,    a0,    FDEC_STRIDE * 7,    0
+endfunc
+
+/* void predict_8x8_vr_c( pixel *src, pixel edge[36] )
+ */
+function predict_8x8_vr_lasx
+    vld             vr0,    a1,    8
+    vbsrl.v         vr1,    vr0,   1
+    vbsrl.v         vr2,    vr0,   2
+
+    vext2xv.hu.bu   xr5,    xr0
+    vext2xv.hu.bu   xr6,    xr1
+    vext2xv.hu.bu   xr7,    xr2
+
+    xvadd.h         xr10,   xr5,   xr6
+    xvadd.h         xr11,   xr10,  xr6
+    xvadd.h         xr12,   xr11,  xr7
+    xvssrarni.bu.h  xr12,   xr12,  2
+    xvssrarni.bu.h  xr10,   xr10,  1
+    xvpermi.d       xr13,   xr12,  0x08
+    xvpermi.d       xr14,   xr10,  0x08
+
+    vbsrl.v         vr15,   vr13,  6
+    vstelm.d        vr15,   a0,    FDEC_STRIDE,       0
+    vbsll.v         vr15,   vr15,  1
+    vextrins.b      vr15,   vr13,  0x04
+    vstelm.d        vr15,   a0,    FDEC_STRIDE * 3,   0
+    vbsll.v         vr15,   vr15,  1
+    vextrins.b      vr15,   vr13,  0x02
+    vstelm.d        vr15,   a0,    FDEC_STRIDE * 5,   0
+    vbsll.v         vr15,   vr15,  1
+    vextrins.b      vr15,   vr13,  0x00
+    vstelm.d        vr15,   a0,    FDEC_STRIDE * 7,   0
+
+    vbsrl.v         vr16,   vr14,  7
+    vstelm.d        vr16,   a0,    0,                 0
+    vbsll.v         vr16,   vr16,  1
+    vextrins.b      vr16,   vr13,  0x05
+    vstelm.d        vr16,   a0,    FDEC_STRIDE * 2,   0
+    vbsll.v         vr16,   vr16,  1
+    vextrins.b      vr16,   vr13,  0x03
+    vstelm.d        vr16,   a0,    FDEC_STRIDE * 4,   0
+    vbsll.v         vr16,   vr16,  1
+    vextrins.b      vr16,   vr13,  0x01
+    vstelm.d        vr16,   a0,    FDEC_STRIDE * 6,   0
+endfunc
+
+/* void predict_8x8_vl_c( pixel *src, pixel edge[36] );
+ */
+function predict_8x8_vl_lasx
+    vld             vr0,    a1,    16
+    vbsrl.v         vr1,    vr0,   1
+    vbsrl.v         vr2,    vr0,   2
+
+    vext2xv.hu.bu   xr0,    xr0
+    vext2xv.hu.bu   xr1,    xr1
+    vext2xv.hu.bu   xr2,    xr2
+
+    xvadd.h         xr3,    xr0,   xr1
+    xvadd.h         xr4,    xr3,   xr1
+    xvadd.h         xr5,    xr4,   xr2
+    xvssrarni.bu.h  xr3,    xr3,   1
+    xvssrarni.bu.h  xr5,    xr5,   2
+    xvpermi.d       xr6,    xr3,   0x8
+    xvpermi.d       xr7,    xr5,   0x8
+
+    vstelm.d        vr6,    a0,    0,                  0
+    vstelm.d        vr7,    a0,    FDEC_STRIDE,        0
+    vbsrl.v         vr8,    vr6,   1
+    vstelm.d        vr8,    a0,    FDEC_STRIDE * 2,    0
+    vbsrl.v         vr9,    vr7,   1
+    vstelm.d        vr9,    a0,    FDEC_STRIDE * 3,    0
+
+    vbsrl.v         vr8,    vr8,   1
+    vstelm.d        vr8,    a0,    FDEC_STRIDE * 4,    0
+    vbsrl.v         vr9,    vr9,   1
+    vstelm.d        vr9,    a0,    FDEC_STRIDE * 5,    0
+    vbsrl.v         vr8,    vr8,   1
+    vstelm.d        vr8,    a0,    FDEC_STRIDE * 6,    0
+    vbsrl.v         vr9,    vr9,   1
+    vstelm.d        vr9,    a0,    FDEC_STRIDE * 7,    0
+endfunc
+
+/****************************************************************************
+ * 16x16 prediction for intra luma block
+ ****************************************************************************/
+/* void x264_predict_16x16_dc_lsx( pixel *src )
+ */
+function predict_16x16_dc_lsx
+    ld.bu          t4,      a0,      -1
+    ld.bu          t5,      a0,      FDEC_STRIDE - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 2 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 3 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 4 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 5 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 6 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 7 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 8 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 9 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 10 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 11 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 12 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 13 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 14 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 15 - 1
+    add.d          t4,      t4,      t5
+
+    vld            vr4,     a0,      -FDEC_STRIDE
+    vhaddw.hu.bu   vr4,     vr4,     vr4
+    vhaddw.wu.hu   vr4,     vr4,     vr4
+    vhaddw.du.wu   vr4,     vr4,     vr4
+    vhaddw.qu.du   vr4,     vr4,     vr4
+    vpickve2gr.wu  t5,      vr4,     0
+    add.d          t4,      t4,      t5
+
+    addi.d         t5,      t4,      16
+    srai.w         t5,      t5,      5
+    vreplgr2vr.b   vr5,     t5
+
+    vst            vr5,     a0,      0
+    vst            vr5,     a0,      FDEC_STRIDE
+    vst            vr5,     a0,      FDEC_STRIDE * 2
+    vst            vr5,     a0,      FDEC_STRIDE * 3
+    vst            vr5,     a0,      FDEC_STRIDE * 4
+    vst            vr5,     a0,      FDEC_STRIDE * 5
+    vst            vr5,     a0,      FDEC_STRIDE * 6
+    vst            vr5,     a0,      FDEC_STRIDE * 7
+
+    vst            vr5,     a0,      FDEC_STRIDE * 8
+    vst            vr5,     a0,      FDEC_STRIDE * 9
+    vst            vr5,     a0,      FDEC_STRIDE * 10
+    vst            vr5,     a0,      FDEC_STRIDE * 11
+    vst            vr5,     a0,      FDEC_STRIDE * 12
+    vst            vr5,     a0,      FDEC_STRIDE * 13
+    vst            vr5,     a0,      FDEC_STRIDE * 14
+    vst            vr5,     a0,      FDEC_STRIDE * 15
+endfunc
+
+/* void x264_predict_16x16_dc_left_lsx( pixel *src )
+ */
+function predict_16x16_dc_left_lsx
+    ld.bu          t4,      a0,      -1
+    ld.bu          t5,      a0,      FDEC_STRIDE - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 2 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 3 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 4 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 5 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 6 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 7 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 8 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 9 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 10 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 11 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 12 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 13 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 14 - 1
+    add.d          t4,      t4,      t5
+    ld.bu          t5,      a0,      FDEC_STRIDE * 15 - 1
+    add.d          t4,      t4,      t5
+
+    addi.d         t5,      t4,      8
+    srai.w         t5,      t5,      4
+    vreplgr2vr.b   vr5,     t5
+
+    vst            vr5,     a0,      0
+    vst            vr5,     a0,      FDEC_STRIDE
+    vst            vr5,     a0,      FDEC_STRIDE * 2
+    vst            vr5,     a0,      FDEC_STRIDE * 3
+    vst            vr5,     a0,      FDEC_STRIDE * 4
+    vst            vr5,     a0,      FDEC_STRIDE * 5
+    vst            vr5,     a0,      FDEC_STRIDE * 6
+    vst            vr5,     a0,      FDEC_STRIDE * 7
+
+    vst            vr5,     a0,      FDEC_STRIDE * 8
+    vst            vr5,     a0,      FDEC_STRIDE * 9
+    vst            vr5,     a0,      FDEC_STRIDE * 10
+    vst            vr5,     a0,      FDEC_STRIDE * 11
+    vst            vr5,     a0,      FDEC_STRIDE * 12
+    vst            vr5,     a0,      FDEC_STRIDE * 13
+    vst            vr5,     a0,      FDEC_STRIDE * 14
+    vst            vr5,     a0,      FDEC_STRIDE * 15
+endfunc
+
+/* void x264_predict_16x16_dc_top_lsx( pixel *src )
+ */
+function predict_16x16_dc_top_lsx
+    vld            vr4,     a0,      -FDEC_STRIDE
+    vhaddw.hu.bu   vr4,     vr4,     vr4
+    vhaddw.wu.hu   vr4,     vr4,     vr4
+    vhaddw.du.wu   vr4,     vr4,     vr4
+    vhaddw.qu.du   vr4,     vr4,     vr4
+    vpickve2gr.wu  t5,      vr4,     0
+
+    addi.d         t5,      t5,      8
+    srai.w         t5,      t5,      4
+    vreplgr2vr.b   vr5,     t5
+
+    vst            vr5,     a0,      0
+    vst            vr5,     a0,      FDEC_STRIDE
+    vst            vr5,     a0,      FDEC_STRIDE * 2
+    vst            vr5,     a0,      FDEC_STRIDE * 3
+    vst            vr5,     a0,      FDEC_STRIDE * 4
+    vst            vr5,     a0,      FDEC_STRIDE * 5
+    vst            vr5,     a0,      FDEC_STRIDE * 6
+    vst            vr5,     a0,      FDEC_STRIDE * 7
+
+    vst            vr5,     a0,      FDEC_STRIDE * 8
+    vst            vr5,     a0,      FDEC_STRIDE * 9
+    vst            vr5,     a0,      FDEC_STRIDE * 10
+    vst            vr5,     a0,      FDEC_STRIDE * 11
+    vst            vr5,     a0,      FDEC_STRIDE * 12
+    vst            vr5,     a0,      FDEC_STRIDE * 13
+    vst            vr5,     a0,      FDEC_STRIDE * 14
+    vst            vr5,     a0,      FDEC_STRIDE * 15
+endfunc
+
+/* void x264_predict_16x16_dc_128_lsx( pixel *src )
+ */
+function predict_16x16_dc_128_lsx
+    ori            t1,      t0,      1
+    slli.d         t1,      t1,      BIT_DEPTH - 1
+    vreplgr2vr.b   vr5,     t1
+
+    vst            vr5,     a0,      0
+    vst            vr5,     a0,      FDEC_STRIDE
+    vst            vr5,     a0,      FDEC_STRIDE * 2
+    vst            vr5,     a0,      FDEC_STRIDE * 3
+    vst            vr5,     a0,      FDEC_STRIDE * 4
+    vst            vr5,     a0,      FDEC_STRIDE * 5
+    vst            vr5,     a0,      FDEC_STRIDE * 6
+    vst            vr5,     a0,      FDEC_STRIDE * 7
+
+    vst            vr5,     a0,      FDEC_STRIDE * 8
+    vst            vr5,     a0,      FDEC_STRIDE * 9
+    vst            vr5,     a0,      FDEC_STRIDE * 10
+    vst            vr5,     a0,      FDEC_STRIDE * 11
+    vst            vr5,     a0,      FDEC_STRIDE * 12
+    vst            vr5,     a0,      FDEC_STRIDE * 13
+    vst            vr5,     a0,      FDEC_STRIDE * 14
+    vst            vr5,     a0,      FDEC_STRIDE * 15
+endfunc
+
+/* void x264_predict_16x16_h_lsx( pixel *src )
+ */
+function predict_16x16_h_lsx
+    ld.bu           t0,    a0,    -1
+    ld.bu           t1,    a0,    FDEC_STRIDE - 1
+    ld.bu           t2,    a0,    FDEC_STRIDE * 2 - 1
+    ld.bu           t3,    a0,    FDEC_STRIDE * 3 - 1
+    ld.bu           t4,    a0,    FDEC_STRIDE * 4 - 1
+    ld.bu           t5,    a0,    FDEC_STRIDE * 5 - 1
+    ld.bu           t6,    a0,    FDEC_STRIDE * 6 - 1
+    ld.bu           t7,    a0,    FDEC_STRIDE * 7 - 1
+    vreplgr2vr.b    vr0,   t0
+    vreplgr2vr.b    vr1,   t1
+    vreplgr2vr.b    vr2,   t2
+    vreplgr2vr.b    vr3,   t3
+    vreplgr2vr.b    vr4,   t4
+    vreplgr2vr.b    vr5,   t5
+    vreplgr2vr.b    vr6,   t6
+    vreplgr2vr.b    vr7,   t7
+    vst             vr0,   a0,    0
+    vst             vr1,   a0,    FDEC_STRIDE
+    vst             vr2,   a0,    FDEC_STRIDE * 2
+    vst             vr3,   a0,    FDEC_STRIDE * 3
+    vst             vr4,   a0,    FDEC_STRIDE * 4
+    vst             vr5,   a0,    FDEC_STRIDE * 5
+    vst             vr6,   a0,    FDEC_STRIDE * 6
+    vst             vr7,   a0,    FDEC_STRIDE * 7
+
+    ld.bu           t0,    a0,    FDEC_STRIDE * 8 - 1
+    ld.bu           t1,    a0,    FDEC_STRIDE * 9 - 1
+    ld.bu           t2,    a0,    FDEC_STRIDE * 10 - 1
+    ld.bu           t3,    a0,    FDEC_STRIDE * 11 - 1
+    ld.bu           t4,    a0,    FDEC_STRIDE * 12 - 1
+    ld.bu           t5,    a0,    FDEC_STRIDE * 13 - 1
+    ld.bu           t6,    a0,    FDEC_STRIDE * 14 - 1
+    ld.bu           t7,    a0,    FDEC_STRIDE * 15 - 1
+    vreplgr2vr.b    vr0,   t0
+    vreplgr2vr.b    vr1,   t1
+    vreplgr2vr.b    vr2,   t2
+    vreplgr2vr.b    vr3,   t3
+    vreplgr2vr.b    vr4,   t4
+    vreplgr2vr.b    vr5,   t5
+    vreplgr2vr.b    vr6,   t6
+    vreplgr2vr.b    vr7,   t7
+    vst             vr0,   a0,    FDEC_STRIDE * 8
+    vst             vr1,   a0,    FDEC_STRIDE * 9
+    vst             vr2,   a0,    FDEC_STRIDE * 10
+    vst             vr3,   a0,    FDEC_STRIDE * 11
+    vst             vr4,   a0,    FDEC_STRIDE * 12
+    vst             vr5,   a0,    FDEC_STRIDE * 13
+    vst             vr6,   a0,    FDEC_STRIDE * 14
+    vst             vr7,   a0,    FDEC_STRIDE * 15
+endfunc
+
+/* void x264_predict_16x16_v_lsx( pixel *src )
+ */
+function predict_16x16_v_lsx
+    fld.d          f4,      a0,      -FDEC_STRIDE
+    fld.d          f5,      a0,      4 - FDEC_STRIDE
+    fld.d          f6,      a0,      8 - FDEC_STRIDE
+    fld.d          f7,      a0,      12 - FDEC_STRIDE
+    vilvl.w        vr4,     vr5,     vr4
+    vilvl.w        vr6,     vr7,     vr6
+    vilvl.d        vr4,     vr6,     vr4
+
+    vst            vr4,     a0,      0
+    vst            vr4,     a0,      FDEC_STRIDE
+    vst            vr4,     a0,      FDEC_STRIDE * 2
+    vst            vr4,     a0,      FDEC_STRIDE * 3
+    vst            vr4,     a0,      FDEC_STRIDE * 4
+    vst            vr4,     a0,      FDEC_STRIDE * 5
+    vst            vr4,     a0,      FDEC_STRIDE * 6
+    vst            vr4,     a0,      FDEC_STRIDE * 7
+
+    vst            vr4,     a0,      FDEC_STRIDE * 8
+    vst            vr4,     a0,      FDEC_STRIDE * 9
+    vst            vr4,     a0,      FDEC_STRIDE * 10
+    vst            vr4,     a0,      FDEC_STRIDE * 11
+    vst            vr4,     a0,      FDEC_STRIDE * 12
+    vst            vr4,     a0,      FDEC_STRIDE * 13
+    vst            vr4,     a0,      FDEC_STRIDE * 14
+    vst            vr4,     a0,      FDEC_STRIDE * 15
+endfunc
+
+/* void x264_predict_16x16_p_lasx( pixel *src )
+ */
+const mulc
+.short 1, 2, 3, 4, 5, 6, 7, 8
+endconst
+
+const muld
+.short 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15
+endconst
+
+function predict_16x16_p_lasx
+    la.local      t0,    mulc
+    vld           vr3,   t0,    0
+    fld.d         f4,    a0,    8  - FDEC_STRIDE
+    fld.d         f5,    a0,    -1 - FDEC_STRIDE
+    vxor.v        vr0,   vr0,   vr0
+    vilvl.b       vr4,   vr0,   vr4
+    vilvl.b       vr5,   vr0,   vr5
+    vshuf4i.h     vr5,   vr5,   0x1b
+    vbsll.v       vr6,   vr5,   8
+    vpackod.d     vr5,   vr6,   vr5
+    vsub.h        vr4,   vr4,   vr5
+    vmul.h        vr4,   vr4,   vr3
+    vhaddw.w.h    vr4,   vr4,   vr4
+    vhaddw.d.w    vr4,   vr4,   vr4
+    vhaddw.q.d    vr4,   vr4,   vr4
+    vpickve2gr.w  t0,    vr4,   0       /* H */
+
+    fld.d         f6,    a0,    FDEC_STRIDE * 8 - 1
+    fld.d         f7,    a0,    FDEC_STRIDE * 9  - 1
+    fld.d         f8,    a0,    FDEC_STRIDE * 10 - 1
+    fld.d         f9,    a0,    FDEC_STRIDE * 11 - 1
+    fld.d         f10,   a0,    FDEC_STRIDE * 12 - 1
+    fld.d         f11,   a0,    FDEC_STRIDE * 13 - 1
+    fld.d         f12,   a0,    FDEC_STRIDE * 14 - 1
+    fld.d         f13,   a0,    FDEC_STRIDE * 15 - 1
+    vilvl.b       vr6,   vr7,   vr6
+    vilvl.b       vr8,   vr9,   vr8
+    vilvl.b       vr10,  vr11,  vr10
+    vilvl.b       vr12,  vr13,  vr12
+    vilvl.h       vr6,   vr8,   vr6
+    vilvl.h       vr10,  vr12,  vr10
+    vilvl.w       vr6,   vr10,  vr6
+
+    fld.d         f7,    a0,    FDEC_STRIDE * 6 - 1
+    fld.d         f8,    a0,    FDEC_STRIDE * 5 - 1
+    fld.d         f9,    a0,    FDEC_STRIDE * 4 - 1
+    fld.d         f10,   a0,    FDEC_STRIDE * 3 - 1
+    fld.d         f11,   a0,    FDEC_STRIDE * 2 - 1
+    fld.d         f12,   a0,    FDEC_STRIDE - 1
+    fld.d         f13,   a0,    -1
+    fld.d         f14,   a0,    -FDEC_STRIDE - 1
+    vilvl.b       vr7,   vr8,   vr7
+    vilvl.b       vr9,   vr10,  vr9
+    vilvl.b       vr11,  vr12,  vr11
+    vilvl.b       vr13,  vr14,  vr13
+    vilvl.h       vr7,   vr9,   vr7
+    vilvl.h       vr11,  vr13,  vr11
+    vilvl.w       vr7,   vr11,  vr7
+
+    vilvl.b       vr6,   vr0,   vr6
+    vilvl.b       vr7,   vr0,   vr7
+    vsub.h        vr6,   vr6,   vr7
+    vmul.h        vr6,   vr6,   vr3
+    vhaddw.w.h    vr6,   vr6,   vr6
+    vhaddw.d.w    vr6,   vr6,   vr6
+    vhaddw.q.d    vr6,   vr6,   vr6
+    vpickve2gr.w  t1,    vr6,   0       /* V */
+
+    ld.bu         t2,    a0,    FDEC_STRIDE * 15 - 1
+    ld.bu         t3,    a0,    15 - FDEC_STRIDE
+    add.w         t2,    t2,    t3
+    slli.w        t2,    t2,    4       /* a */
+
+    slli.w        t3,    t0,    2
+    add.w         t0,    t0,    t3
+    addi.w        t0,    t0,    32
+    srai.w        t0,    t0,    6       /* b */
+
+    slli.w        t3,    t1,    2
+    add.w         t1,    t1,    t3
+    addi.w        t1,    t1,    32
+    srai.w        t1,    t1,    6       /* c */
+
+    add.w         t3,    t0,    t1
+    slli.w        t4,    t3,    3
+    sub.w         t4,    t4,    t3
+    sub.w         t5,    t2,    t4
+    addi.w        t5,    t5,    16      /* i00 */
+
+    la.local      t3,    muld
+    xvld          xr14,  t3,    0
+    xvreplgr2vr.h xr12,  t0
+    xvmul.h       xr12,  xr12,  xr14
+
+.rept 16
+    xvreplgr2vr.h xr14,  t5
+    xvadd.h       xr13,  xr12,  xr14
+    xvssrani.bu.h xr15,  xr13,  5
+    xvstelm.d     xr15,  a0,    0,    0
+    xvstelm.d     xr15,  a0,    8,    2
+    addi.d        a0,    a0,    FDEC_STRIDE
+    add.w         t5,    t5,    t1
+.endr
+endfunc
+
+#endif /* !HIGH_BIT_DEPT H */
diff --git a/common/loongarch/predict-c.c b/common/loongarch/predict-c.c
index d28e3e62..c90d8117 100644
--- a/common/loongarch/predict-c.c
+++ b/common/loongarch/predict-c.c
@@ -572,4 +572,15 @@ void x264_intra_predict_v_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
     intra_predict_vert_8x8_lasx( ( pu_xyz + 16 ), p_src, FDEC_STRIDE );
 }
 
+void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] )
+{
+    if ( cpu&X264_CPU_LASX ) {
+#if !HIGH_BIT_DEPTH
+        pf[I_PRED_16x16_V]    = x264_intra_predict_vert_16x16_lasx;
+        pf[I_PRED_16x16_H]    = x264_intra_predict_hor_16x16_lasx;
+        pf[I_PRED_16x16_DC]   = x264_intra_predict_dc_16x16_lasx;
+#endif
+    }
+}
+
 #endif
diff --git a/common/loongarch/predict.h b/common/loongarch/predict.h
index 8db72ad5..dc7e9a6a 100644
--- a/common/loongarch/predict.h
+++ b/common/loongarch/predict.h
@@ -55,4 +55,100 @@ void x264_intra_predict_h_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
 #define x264_intra_predict_v_8x8_lasx x264_template(intra_predict_v_8x8_lasx)
 void x264_intra_predict_v_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
 
+#define x264_predict_16x16_init_lasx x264_template(predict_16x16_init_lasx)
+void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] );
+
+#define x264_predict_8x8c_p_lsx x264_template(predict_8x8c_p_lsx)
+void x264_predict_8x8c_p_lsx(uint8_t *p_src);
+
+#define x264_predict_8x8c_v_lsx x264_template(predict_8x8c_v_lsx)
+void x264_predict_8x8c_v_lsx(uint8_t *p_src);
+
+#define x264_predict_8x8c_h_lsx x264_template(predict_8x8c_h_lsx)
+void x264_predict_8x8c_h_lsx(uint8_t *p_src);
+
+#define x264_predict_8x8c_dc_lsx x264_template(predict_8x8c_dc_lsx)
+void x264_predict_8x8c_dc_lsx(pixel *src);
+
+#define predict_8x8c_dc_128_lsx x264_template(predict_8x8c_dc_128_lsx)
+void predict_8x8c_dc_128_lsx(pixel *src);
+
+#define predict_8x8c_dc_top_lsx x264_template(predict_8x8c_dc_top_lsx)
+void predict_8x8c_dc_top_lsx(pixel *src);
+
+#define predict_8x8c_dc_left_lsx x264_template(predict_8x8c_dc_left_lsx)
+void predict_8x8c_dc_left_lsx(pixel *src);
+
+#define x264_predict_16x16_dc_lsx x264_template(predict_16x16_dc_lsx)
+void x264_predict_16x16_dc_lsx( pixel *src );
+
+#define predict_16x16_dc_left_lsx x264_template(predict_16x16_dc_left_lsx)
+void predict_16x16_dc_left_lsx( pixel *src );
+
+#define predict_16x16_dc_top_lsx x264_template(predict_16x16_dc_top_lsx)
+void predict_16x16_dc_top_lsx( pixel *src );
+
+#define predict_16x16_dc_128_lsx x264_template(predict_16x16_dc_128_lsx)
+void predict_16x16_dc_128_lsx( pixel *src );
+
+#define x264_predict_16x16_h_lsx x264_template(predict_16x16_h_lsx)
+void x264_predict_16x16_h_lsx( pixel *src );
+
+#define x264_predict_16x16_v_lsx x264_template(predict_16x16_v_lsx)
+void x264_predict_16x16_v_lsx( pixel *src );
+
+#define x264_predict_16x16_p_lasx x264_template(predict_16x16_p_lasx)
+void x264_predict_16x16_p_lasx( pixel *src );
+
+#define x264_predict_8x8_v_lsx x264_template(predict_8x8_v_lsx)
+void x264_predict_8x8_v_lsx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_h_lasx x264_template(predict_8x8_h_lasx)
+void x264_predict_8x8_h_lasx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_dc_lsx x264_template(predict_8x8_dc_lsx)
+void x264_predict_8x8_dc_lsx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_dc_left_lsx x264_template(predict_8x8_dc_left_lsx)
+void x264_predict_8x8_dc_left_lsx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_dc_top_lsx x264_template(predict_8x8_dc_top_lsx)
+void x264_predict_8x8_dc_top_lsx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_dc_128_lsx x264_template(predict_8x8_dc_128_lsx)
+void x264_predict_8x8_dc_128_lsx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_ddl_lasx x264_template(predict_8x8_ddl_lasx)
+void x264_predict_8x8_ddl_lasx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_ddr_lasx x264_template(predict_8x8_ddr_lasx)
+void x264_predict_8x8_ddr_lasx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_vr_lasx x264_template(predict_8x8_vr_lasx)
+void x264_predict_8x8_vr_lasx( pixel *src, pixel edge[36] );
+
+#define x264_predict_8x8_vl_lasx x264_template(predict_8x8_vl_lasx)
+void x264_predict_8x8_vl_lasx( pixel *src, pixel edge[36] );
+
+#define x264_predict_4x4_v_lsx x264_template(predict_4x4_v_lsx)
+void x264_predict_4x4_v_lsx( pixel *p_src );
+
+#define x264_predict_4x4_h_lsx x264_template(predict_4x4_h_lsx)
+void x264_predict_4x4_h_lsx( pixel *p_src );
+
+#define x264_predict_4x4_dc_lsx x264_template(predict_4x4_dc_lsx)
+void x264_predict_4x4_dc_lsx( pixel *p_src );
+
+#define x264_predict_4x4_ddl_lsx x264_template(predict_4x4_ddl_lsx)
+void x264_predict_4x4_ddl_lsx( pixel *p_src );
+
+#define x264_predict_4x4_dc_top_lsx x264_template(predict_4x4_dc_top_lsx)
+void x264_predict_4x4_dc_top_lsx( pixel *p_src );
+
+#define x264_predict_4x4_dc_left_lsx x264_template(predict_4x4_dc_left_lsx)
+void x264_predict_4x4_dc_left_lsx( pixel *p_src );
+
+#define x264_predict_4x4_dc_128_lsx x264_template(predict_4x4_dc_128_lsx)
+void x264_predict_4x4_dc_128_lsx( pixel *p_src );
+
 #endif
diff --git a/common/loongarch/quant-a.S b/common/loongarch/quant-a.S
new file mode 100644
index 00000000..49dcb7a8
--- /dev/null
+++ b/common/loongarch/quant-a.S
@@ -0,0 +1,60 @@
+/*****************************************************************************
+ * quant-a.S: LoongArch quantization and level-run
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+
+/*
+ * int quant_4x4x4( dctcoef dct[4][16], udctcoef mf[16], udctcoef bias[16] )
+ */
+.macro QUANT_ONE_LASX s1, s2, s3, s4
+    xvld        xr1,  \s1,  0  /* Load dctcoef */
+    xvadda.h    \s4,  xr1, \s3
+    xvmuh.hu    \s4,  \s4, \s2
+    xvsigncov.h \s4,  xr1, \s4
+    xvst        \s4,  \s1,  0
+.endm
+
+function quant_4x4x4_lasx
+    xvld    xr2,  a1,  0
+    xvld    xr3,  a2,  0
+    QUANT_ONE_LASX   a0,  xr2,  xr3,  xr4
+    addi.d  a0,   a0,  32
+    QUANT_ONE_LASX   a0,  xr2,  xr3,  xr0
+    xvssrlni.h.w  xr0, xr4, 0
+    addi.d  a0,   a0,  32
+    QUANT_ONE_LASX   a0,  xr2,  xr3,  xr4
+    addi.d  a0,   a0,  32
+    QUANT_ONE_LASX   a0,  xr2,  xr3,  xr5
+    xvssrlni.h.w  xr5, xr4, 0
+    xvssrlni.h.w  xr5, xr0, 0
+    xvseqi.w      xr5, xr5, 0
+    xvmskltz.w    xr5, xr5
+    xvpickve2gr.w t0,  xr5, 0
+    xvpickve2gr.w t1,  xr5, 4
+    alsl.d      t0,  t1,  t0,  4
+    and         t0,  t0,  t1
+    xori        a0,  t0,  0xf
+endfunc
diff --git a/common/loongarch/quant-c.c b/common/loongarch/quant-c.c
index e590c71e..0533ee5d 100644
--- a/common/loongarch/quant-c.c
+++ b/common/loongarch/quant-c.c
@@ -257,21 +257,6 @@ int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
     return avc_quant_4x4_lasx( p_dct, p_mf, p_bias );
 }
 
-int32_t x264_quant_4x4x4_lasx( int16_t p_dct[4][16],
-                               uint16_t pu_mf[16], uint16_t pu_bias[16] )
-{
-    int32_t i_non_zero, i_non_zero_acc = 0;
-
-    for( int32_t j = 0; j < 4; j++  )
-    {
-        i_non_zero = x264_quant_4x4_lasx( p_dct[j], pu_mf, pu_bias );
-
-        i_non_zero_acc |= ( !!i_non_zero ) << j;
-    }
-
-    return i_non_zero_acc;
-}
-
 int32_t x264_quant_8x8_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
 {
     return avc_quant_8x8_lasx( p_dct, p_mf, p_bias );
diff --git a/common/loongarch/sad-a.S b/common/loongarch/sad-a.S
new file mode 100644
index 00000000..f91715d2
--- /dev/null
+++ b/common/loongarch/sad-a.S
@@ -0,0 +1,2091 @@
+/*****************************************************************************
+ * sad-a.S: loongarch sad functions
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *          Lu Wang <wanglu@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+
+#if !HIGH_BIT_DEPTH
+
+
+/* void x264_pixel_sad_x4_16x16_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                   uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                   uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                   int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_16x16_lasx
+    slli.d         t1,     a5,    1
+    add.d          t2,     a5,    t1
+    slli.d         t3,     a5,    2
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    xvld           xr3,    a0,    0
+    xvld           xr16,   a0,    32
+    vld            vr4,    a1,    0
+    vldx           vr8,    a1,    a5
+    vld            vr5,    a2,    0
+    vldx           vr9,    a2,    a5
+    vld            vr6,    a3,    0
+    vldx           vr10,   a3,    a5
+    vld            vr7,    a4,    0
+    vldx           vr11,   a4,    a5
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr12,   xr8,   xr8
+    xvhaddw.hu.bu  xr13,   xr9,   xr9
+    xvhaddw.hu.bu  xr14,   xr10,  xr10
+    xvhaddw.hu.bu  xr15,   xr11,  xr11
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    vldx           vr4,    a1,    t1
+    vldx           vr8,    a1,    t2
+    vldx           vr5,    a2,    t1
+    vldx           vr9,    a2,    t2
+    vldx           vr6,    a3,    t1
+    vldx           vr10,   a3,    t2
+    vldx           vr7,    a4,    t1
+    vldx           vr11,   a4,    t2
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr16,  xr4
+    xvabsd.bu      xr9,    xr16,  xr5
+    xvabsd.bu      xr10,   xr16,  xr6
+    xvabsd.bu      xr11,   xr16,  xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    add.d          a4,     a4,    t3
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    xvld           xr3,    a0,    64
+    xvld           xr16,   a0,    96
+    vld            vr4,    a1,    0
+    vldx           vr8,    a1,    a5
+    vld            vr5,    a2,    0
+    vldx           vr9,    a2,    a5
+    vld            vr6,    a3,    0
+    vldx           vr10,   a3,    a5
+    vld            vr7,    a4,    0
+    vldx           vr11,   a4,    a5
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    vldx           vr4,    a1,    t1
+    vldx           vr8,    a1,    t2
+    vldx           vr5,    a2,    t1
+    vldx           vr9,    a2,    t2
+    vldx           vr6,    a3,    t1
+    vldx           vr10,   a3,    t2
+    vldx           vr7,    a4,    t1
+    vldx           vr11,   a4,    t2
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr16,  xr4
+    xvabsd.bu      xr9,    xr16,  xr5
+    xvabsd.bu      xr10,   xr16,  xr6
+    xvabsd.bu      xr11,   xr16,  xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    add.d          a4,     a4,    t3
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    xvld           xr3,    a0,    128
+    xvld           xr16,   a0,    160
+    vld            vr4,    a1,    0
+    vldx           vr8,    a1,    a5
+    vld            vr5,    a2,    0
+    vldx           vr9,    a2,    a5
+    vld            vr6,    a3,    0
+    vldx           vr10,   a3,    a5
+    vld            vr7,    a4,    0
+    vldx           vr11,   a4,    a5
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    vldx           vr4,    a1,    t1
+    vldx           vr8,    a1,    t2
+    vldx           vr5,    a2,    t1
+    vldx           vr9,    a2,    t2
+    vldx           vr6,    a3,    t1
+    vldx           vr10,   a3,    t2
+    vldx           vr7,    a4,    t1
+    vldx           vr11,   a4,    t2
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr16,  xr4
+    xvabsd.bu      xr9,    xr16,  xr5
+    xvabsd.bu      xr10,   xr16,  xr6
+    xvabsd.bu      xr11,   xr16,  xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    add.d          a4,     a4,    t3
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    xvld           xr3,    a0,    192
+    xvld           xr16,   a0,    224
+    vld            vr4,    a1,    0
+    vldx           vr8,    a1,    a5
+    vld            vr5,    a2,    0
+    vldx           vr9,    a2,    a5
+    vld            vr6,    a3,    0
+    vldx           vr10,   a3,    a5
+    vld            vr7,    a4,    0
+    vldx           vr11,   a4,    a5
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    vldx           vr4,    a1,    t1
+    vldx           vr8,    a1,    t2
+    vldx           vr5,    a2,    t1
+    vldx           vr9,    a2,    t2
+    vldx           vr6,    a3,    t1
+    vldx           vr10,   a3,    t2
+    vldx           vr7,    a4,    t1
+    vldx           vr11,   a4,    t2
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr16,  xr4
+    xvabsd.bu      xr9,    xr16,  xr5
+    xvabsd.bu      xr10,   xr16,  xr6
+    xvabsd.bu      xr11,   xr16,  xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    xvori.b        xr17,   xr12,  0
+    xvori.b        xr18,   xr13,  0
+    xvpermi.q      xr12,   xr14,  0x02
+    xvpermi.q      xr14,   xr17,  0x31
+    xvpermi.q      xr13,   xr15,  0x02
+    xvpermi.q      xr15,   xr18,  0x31
+    xvadd.h        xr12,   xr12,  xr14
+    xvadd.h        xr13,   xr13,  xr15
+    xvhaddw.w.h    xr12,   xr12,  xr12
+    xvhaddw.w.h    xr13,   xr13,  xr13
+    xvhaddw.d.w    xr12,   xr12,  xr12
+    xvhaddw.d.w    xr13,   xr13,  xr13
+    xvhaddw.q.d    xr12,   xr12,  xr12
+    xvhaddw.q.d    xr13,   xr13,  xr13
+    xvpackev.w     xr13,   xr13,  xr12
+    // Store data to p_sad_array
+    xvstelm.d      xr13,   a6,    0,    0
+    xvstelm.d      xr13,   a6,    8,    2
+endfunc
+
+/* void x264_pixel_sad_x4_16x8_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                  uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                  int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_16x8_lasx
+    slli.d         t1,     a5,    1
+    add.d          t2,     a5,    t1
+    slli.d         t3,     a5,    2
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    xvld           xr3,    a0,    0
+    vld            vr4,    a1,    0
+    vldx           vr8,    a1,    a5
+    vld            vr5,    a2,    0
+    vldx           vr9,    a2,    a5
+    vld            vr6,    a3,    0
+    vldx           vr10,   a3,    a5
+    vld            vr7,    a4,    0
+    vldx           vr11,   a4,    a5
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr12,   xr8,   xr8
+    xvhaddw.hu.bu  xr13,   xr9,   xr9
+    xvhaddw.hu.bu  xr14,   xr10,  xr10
+    xvhaddw.hu.bu  xr15,   xr11,  xr11
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    xvld           xr3,    a0,    32
+    vldx           vr4,    a1,    t1
+    vldx           vr8,    a1,    t2
+    vldx           vr5,    a2,    t1
+    vldx           vr9,    a2,    t2
+    vldx           vr6,    a3,    t1
+    vldx           vr10,   a3,    t2
+    vldx           vr7,    a4,    t1
+    vldx           vr11,   a4,    t2
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    add.d          a4,     a4,    t3
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    xvld           xr3,    a0,    64
+    vld            vr4,    a1,    0
+    vldx           vr8,    a1,    a5
+    vld            vr5,    a2,    0
+    vldx           vr9,    a2,    a5
+    vld            vr6,    a3,    0
+    vldx           vr10,   a3,    a5
+    vld            vr7,    a4,    0
+    vldx           vr11,   a4,    a5
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    xvld           xr3,    a0,    96
+    vldx           vr4,    a1,    t1
+    vldx           vr8,    a1,    t2
+    vldx           vr5,    a2,    t1
+    vldx           vr9,    a2,    t2
+    vldx           vr6,    a3,    t1
+    vldx           vr10,   a3,    t2
+    vldx           vr7,    a4,    t1
+    vldx           vr11,   a4,    t2
+    xvpermi.q      xr4,    xr8,   0x02
+    xvpermi.q      xr5,    xr9,   0x02
+    xvpermi.q      xr6,    xr10,  0x02
+    xvpermi.q      xr7,    xr11,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvadd.h        xr14,   xr14,  xr10
+    xvadd.h        xr15,   xr15,  xr11
+
+    xvori.b        xr17,   xr12,  0
+    xvori.b        xr18,   xr13,  0
+    xvpermi.q      xr12,   xr14,  0x02
+    xvpermi.q      xr14,   xr17,  0x31
+    xvpermi.q      xr13,   xr15,  0x02
+    xvpermi.q      xr15,   xr18,  0x31
+    xvadd.h        xr12,   xr12,  xr14
+    xvadd.h        xr13,   xr13,  xr15
+    xvhaddw.w.h    xr12,   xr12,  xr12
+    xvhaddw.w.h    xr13,   xr13,  xr13
+    xvhaddw.d.w    xr12,   xr12,  xr12
+    xvhaddw.d.w    xr13,   xr13,  xr13
+    xvhaddw.q.d    xr12,   xr12,  xr12
+    xvhaddw.q.d    xr13,   xr13,  xr13
+    xvpackev.w     xr13,   xr13,  xr12
+    // Store data to p_sad_array
+    xvstelm.d      xr13,   a6,    0,    0
+    xvstelm.d      xr13,   a6,    8,    2
+endfunc
+
+/* void x264_pixel_sad_x4_8x16_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                  uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                  int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_8x16_lasx
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    slli.d         t1,     a5,    1
+    add.d          t2,     t1,    a5
+    slli.d         t3,     a5,    2
+
+    fld.d          f2,     a0,    0
+    fld.d          f3,     a0,    16
+    fld.d          f12,    a0,    32
+    fld.d          f13,    a0,    48
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f14, f18
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f15, f19
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f16, f20
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f17, f21
+    vilvl.d        vr3,    vr3,   vr2
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr13,   vr13,  vr12
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr15,   vr19,  vr15
+    vilvl.d        vr16,   vr20,  vr16
+    vilvl.d        vr17,   vr21,  vr17
+    xvpermi.q      xr3,    xr13,  0x02
+    xvpermi.q      xr4,    xr14,  0x02
+    xvpermi.q      xr5,    xr15,  0x02
+    xvpermi.q      xr6,    xr16,  0x02
+    xvpermi.q      xr7,    xr17,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr0,    xr8,   xr8
+    xvhaddw.hu.bu  xr1,    xr9,   xr9
+    xvhaddw.hu.bu  xr2,    xr10,  xr10
+    xvhaddw.hu.bu  xr22,   xr11,  xr11
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    add.d          a4,     a4,    t3
+    fld.d          f12,    a0,    64
+    fld.d          f13,    a0,    80
+    vilvl.d        vr3,    vr13,  vr12
+    fld.d          f12,    a0,    96
+    fld.d          f13,    a0,    112
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f14, f18
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f15, f19
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f16, f20
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f17, f21
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr13,   vr13,  vr12
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr15,   vr19,  vr15
+    vilvl.d        vr16,   vr20,  vr16
+    vilvl.d        vr17,   vr21,  vr17
+    xvpermi.q      xr3,    xr13,  0x02
+    xvpermi.q      xr4,    xr14,  0x02
+    xvpermi.q      xr5,    xr15,  0x02
+    xvpermi.q      xr6,    xr16,  0x02
+    xvpermi.q      xr7,    xr17,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr0,    xr0,   xr8
+    xvadd.h        xr1,    xr1,   xr9
+    xvadd.h        xr2,    xr2,   xr10
+    xvadd.h        xr22,   xr22,  xr11
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    add.d          a4,     a4,    t3
+    fld.d          f12,    a0,    128
+    fld.d          f13,    a0,    144
+    vilvl.d        vr3,    vr13,  vr12
+    fld.d          f12,    a0,    160
+    fld.d          f13,    a0,    176
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f14, f18
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f15, f19
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f16, f20
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f17, f21
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr13,   vr13,  vr12
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr15,   vr19,  vr15
+    vilvl.d        vr16,   vr20,  vr16
+    vilvl.d        vr17,   vr21,  vr17
+    xvpermi.q      xr3,    xr13,  0x02
+    xvpermi.q      xr4,    xr14,  0x02
+    xvpermi.q      xr5,    xr15,  0x02
+    xvpermi.q      xr6,    xr16,  0x02
+    xvpermi.q      xr7,    xr17,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvadd.h        xr0,    xr0,   xr8
+    xvadd.h        xr1,    xr1,   xr9
+    xvadd.h        xr2,    xr2,   xr10
+    xvadd.h        xr22,   xr22,  xr11
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    add.d          a4,     a4,    t3
+    fld.d          f12,    a0,    192
+    fld.d          f13,    a0,    208
+    vilvl.d        vr3,    vr13,  vr12
+    fld.d          f12,    a0,    224
+    fld.d          f13,    a0,    240
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f14, f18
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f15, f19
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f16, f20
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f17, f21
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr13,   vr13,  vr12
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr15,   vr19,  vr15
+    vilvl.d        vr16,   vr20,  vr16
+    vilvl.d        vr17,   vr21,  vr17
+    xvpermi.q      xr3,    xr13,  0x02
+    xvpermi.q      xr4,    xr14,  0x02
+    xvpermi.q      xr5,    xr15,  0x02
+    xvpermi.q      xr6,    xr16,  0x02
+    xvpermi.q      xr7,    xr17,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+
+    xvadd.h        xr8,    xr0,   xr8
+    xvadd.h        xr9,    xr1,   xr9
+    xvadd.h        xr10,   xr2,   xr10
+    xvadd.h        xr11,   xr22,  xr11
+    xvori.b        xr0,    xr8,   0
+    xvori.b        xr1,    xr9,   0
+    xvpermi.q      xr8,    xr10,  0x02
+    xvpermi.q      xr10,   xr0,   0x31
+    xvpermi.q      xr9,    xr11,  0x02
+    xvpermi.q      xr11,   xr1,   0x31
+    xvadd.h        xr8,    xr8,   xr10
+    xvadd.h        xr9,    xr9,   xr11
+    xvhaddw.w.h    xr8,    xr8,   xr8
+    xvhaddw.w.h    xr9,    xr9,   xr9
+    xvhaddw.d.w    xr8,    xr8,   xr8
+    xvhaddw.d.w    xr9,    xr9,   xr9
+    xvhaddw.q.d    xr8,    xr8,   xr8
+    xvhaddw.q.d    xr9,    xr9,   xr9
+    xvpackev.w     xr9,    xr9,   xr8
+    // Store data to p_sad_array
+    xvstelm.d      xr9,    a6,    0,    0
+    xvstelm.d      xr9,    a6,    8,    2
+endfunc
+
+/* void x264_pixel_sad_x4_8x8_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_8x8_lasx
+    slli.d         t1,     a5,    1
+    add.d          t2,     t1,    a5
+    slli.d         t3,     a5,    2
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f14, f18
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f15, f19
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f16, f20
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f17, f21
+    vilvl.d        vr4,    vr5,   vr4
+    vilvl.d        vr6,    vr7,   vr6
+    vilvl.d        vr8,    vr9,   vr8
+    vilvl.d        vr10,   vr11,  vr10
+    vilvl.d        vr14,   vr15,  vr14
+    vilvl.d        vr16,   vr17,  vr16
+    vilvl.d        vr18,   vr19,  vr18
+    vilvl.d        vr20,   vr21,  vr20
+    xvpermi.q      xr4,    xr6,   0x02
+    xvpermi.q      xr8,    xr10,  0x02
+    xvpermi.q      xr14,   xr16,  0x02
+    xvpermi.q      xr18,   xr20,  0x02
+    // Calculate the absolute value of the difference
+    xvldrepl.d     xr3,    a0,    0
+    xvabsd.bu      xr5,    xr3,   xr4
+    xvldrepl.d     xr3,    a0,    16
+    xvabsd.bu      xr9,    xr3,   xr8
+    xvldrepl.d     xr3,    a0,    32
+    xvabsd.bu      xr10,   xr3,   xr14
+    xvldrepl.d     xr3,    a0,    48
+    xvabsd.bu      xr11,   xr3,   xr18
+    xvaddwev.h.bu  xr0,    xr5,   xr9
+    xvaddwod.h.bu  xr1,    xr5,   xr9
+    xvaddwev.h.bu  xr2,    xr10,  xr11
+    xvaddwod.h.bu  xr22,   xr10,  xr11
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    add.d          a4,     a4,    t3
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f14, f18
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f15, f19
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f16, f20
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f17, f21
+    vilvl.d        vr4,    vr5,   vr4
+    vilvl.d        vr6,    vr7,   vr6
+    vilvl.d        vr8,    vr9,   vr8
+    vilvl.d        vr10,   vr11,  vr10
+    vilvl.d        vr14,   vr15,  vr14
+    vilvl.d        vr16,   vr17,  vr16
+    vilvl.d        vr18,   vr19,  vr18
+    vilvl.d        vr20,   vr21,  vr20
+    xvpermi.q      xr4,    xr6,   0x02
+    xvpermi.q      xr8,    xr10,  0x02
+    xvpermi.q      xr14,   xr16,  0x02
+    xvpermi.q      xr18,   xr20,  0x02
+    // Calculate the absolute value of the difference
+    xvldrepl.d     xr3,    a0,    64
+    xvabsd.bu      xr5,    xr3,   xr4
+    xvldrepl.d     xr3,    a0,    80
+    xvabsd.bu      xr9,    xr3,   xr8
+    xvldrepl.d     xr3,    a0,    96
+    xvabsd.bu      xr10,   xr3,   xr14
+    xvldrepl.d     xr3,    a0,    112
+    xvabsd.bu      xr11,   xr3,   xr18
+    xvaddwev.h.bu  xr12,   xr5,   xr9
+    xvaddwod.h.bu  xr13,   xr5,   xr9
+    xvaddwev.h.bu  xr14,   xr10,  xr11
+    xvaddwod.h.bu  xr15,   xr10,  xr11
+    xvadd.h        xr5,    xr0,   xr12
+    xvadd.h        xr9,    xr1,   xr13
+    xvadd.h        xr10,   xr2,   xr14
+    xvadd.h        xr11,   xr22,  xr15
+    xvadd.h        xr5,    xr5,   xr9
+    xvadd.h        xr10,   xr10,  xr11
+    xvadd.h        xr10,   xr10,  xr5
+    xvhaddw.wu.hu  xr10,   xr10,  xr10
+    xvhaddw.du.wu  xr10,   xr10,  xr10
+    xvpermi.q      xr5,    xr10,  0x01
+    xvpickev.w     xr10,   xr5,   xr10
+    // Store data to p_sad_array
+    vst            vr10,   a6,    0
+endfunc
+
+/* void x264_pixel_sad_x4_8x4_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_8x4_lasx
+    slli.d         t1,     a5,    1
+    add.d          t2,     t1,    a5
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    fld.d          f2,     a0,    0
+    fld.d          f3,     a0,    16
+    fld.d          f12,    a0,    32
+    fld.d          f13,    a0,    48
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f14, f18
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f15, f19
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f16, f20
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f17, f21
+
+    vilvl.d        vr3,    vr3,   vr2
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr13,   vr13,  vr12
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr15,   vr19,  vr15
+    vilvl.d        vr16,   vr20,  vr16
+    vilvl.d        vr17,   vr21,  vr17
+    xvpermi.q      xr3,    xr13,  0x02
+    xvpermi.q      xr4,    xr16,  0x02
+    xvpermi.q      xr5,    xr17,  0x02
+    xvpermi.q      xr6,    xr14,  0x02
+    xvpermi.q      xr7,    xr15,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvpermi.d      xr10,   xr10,  0x4e
+    xvpermi.d      xr11,   xr11,  0x4e
+    xvadd.h        xr8,    xr8,   xr10
+    xvadd.h        xr9,    xr9,   xr11
+    xvhaddw.w.h    xr8,    xr8,   xr8
+    xvhaddw.w.h    xr9,    xr9,   xr9
+    xvhaddw.d.w    xr8,    xr8,   xr8
+    xvhaddw.d.w    xr9,    xr9,   xr9
+    xvhaddw.q.d    xr8,    xr8,   xr8
+    xvhaddw.q.d    xr9,    xr9,   xr9
+    xvpackev.w     xr9,    xr9,   xr8
+
+    // Store data to p_sad_array
+    xvstelm.d      xr9,    a6,    0,    0
+    xvstelm.d      xr9,    a6,    8,    2
+endfunc
+
+/* void x264_pixel_sad_x4_4x8_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_4x8_lasx
+    slli.d         t0,     a5,    1
+    add.d          t1,     a5,    t0
+    slli.d         t2,     a5,    2
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    fld.s          f2,     a0,    0
+    fld.s          f3,     a0,    16
+    fld.s          f4,     a1,    0
+    fldx.s         f8,     a1,    a5
+    fld.s          f5,     a2,    0
+    fldx.s         f9,     a2,    a5
+    fld.s          f6,     a3,    0
+    fldx.s         f10,    a3,    a5
+    fld.s          f7,     a4,    0
+    fldx.s         f11,    a4,    a5
+    vilvl.w        vr3,    vr3,   vr2
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr7,    vr11,  vr7
+
+    fld.s          f2,     a0,    32
+    fld.s          f0,     a0,    48
+    fldx.s         f8,     a1,    t0
+    fldx.s         f12,    a1,    t1
+    fldx.s         f9,     a2,    t0
+    fldx.s         f13,    a2,    t1
+    fldx.s         f10,    a3,    t0
+    fldx.s         f14,    a3,    t1
+    fldx.s         f11,    a4,    t0
+    fldx.s         f15,    a4,    t1
+    vilvl.w        vr2,    vr0,   vr2
+    vilvl.w        vr8,    vr12,  vr8
+    vilvl.w        vr9,    vr13,  vr9
+    vilvl.w        vr10,   vr14,  vr10
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.d        vr16,   vr2,   vr3
+    vilvl.d        vr17,   vr8,   vr4
+    vilvl.d        vr18,   vr9,   vr5
+    vilvl.d        vr19,   vr10,  vr6
+    vilvl.d        vr20,   vr11,  vr7
+
+    add.d          a1,     a1,    t2
+    add.d          a2,     a2,    t2
+    add.d          a3,     a3,    t2
+    add.d          a4,     a4,    t2
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    fld.s          f2,     a0,    64
+    fld.s          f3,     a0,    80
+    fld.s          f4,     a1,    0
+    fldx.s         f8,     a1,    a5
+    fld.s          f5,     a2,    0
+    fldx.s         f9,     a2,    a5
+    fld.s          f6,     a3,    0
+    fldx.s         f10,    a3,    a5
+    fld.s          f7,     a4,    0
+    fldx.s         f11,    a4,    a5
+    vilvl.w        vr3,    vr3,   vr2
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr7,    vr11,  vr7
+
+    fld.s          f2,     a0,    96
+    fld.s          f0,     a0,    112
+    fldx.s         f8,     a1,    t0
+    fldx.s         f12,    a1,    t1
+    fldx.s         f9,     a2,    t0
+    fldx.s         f13,    a2,    t1
+    fldx.s         f10,    a3,    t0
+    fldx.s         f14,    a3,    t1
+    fldx.s         f11,    a4,    t0
+    fldx.s         f15,    a4,    t1
+    vilvl.w        vr2,    vr0,   vr2
+    vilvl.w        vr8,    vr12,  vr8
+    vilvl.w        vr9,    vr13,  vr9
+    vilvl.w        vr10,   vr14,  vr10
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.d        vr3,    vr2,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr7,    vr11,  vr7
+    xvpermi.q      xr3,    xr16,  0x02
+    xvpermi.q      xr4,    xr17,  0x02
+    xvpermi.q      xr5,    xr18,  0x02
+    xvpermi.q      xr6,    xr19,  0x02
+    xvpermi.q      xr7,    xr20,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr8,    xr3,   xr4
+    xvabsd.bu      xr9,    xr3,   xr5
+    xvabsd.bu      xr10,   xr3,   xr6
+    xvabsd.bu      xr11,   xr3,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvori.b        xr0,    xr8,   0
+    xvori.b        xr1,    xr9,   0
+    xvpermi.q      xr8,    xr10,  0x02
+    xvpermi.q      xr10,   xr0,   0x31
+    xvpermi.q      xr9,    xr11,  0x02
+    xvpermi.q      xr11,   xr1,   0x31
+    xvadd.h        xr8,    xr8,   xr10
+    xvadd.h        xr9,    xr9,   xr11
+    xvhaddw.w.h    xr8,    xr8,   xr8
+    xvhaddw.w.h    xr9,    xr9,   xr9
+    xvhaddw.d.w    xr8,    xr8,   xr8
+    xvhaddw.d.w    xr9,    xr9,   xr9
+    xvhaddw.q.d    xr8,    xr8,   xr8
+    xvhaddw.q.d    xr9,    xr9,   xr9
+    xvpackev.w     xr9,    xr9,   xr8
+
+    // Store data to p_sad_array
+    xvstelm.d      xr9,    a6,    0,    0
+    xvstelm.d      xr9,    a6,    8,    2
+endfunc
+
+/* void x264_pixel_sad_x4_4x4_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_4x4_lsx
+    slli.d         t0,     a5,    1
+    add.d          t1,     a5,    t0
+    slli.d         t2,     a5,    2
+
+    // Load data from p_src, p_ref0, p_ref1, p_ref2 and p_ref3
+    fld.s          f2,     a0,    0
+    fld.s          f3,     a0,    16
+    fld.s          f4,     a1,    0
+    fldx.s         f8,     a1,    a5
+    fld.s          f5,     a2,    0
+    fldx.s         f9,     a2,    a5
+    fld.s          f6,     a3,    0
+    fldx.s         f10,    a3,    a5
+    fld.s          f7,     a4,    0
+    fldx.s         f11,    a4,    a5
+    vilvl.w        vr3,    vr3,   vr2
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr7,    vr11,  vr7
+
+    fld.s          f2,     a0,    32
+    fld.s          f0,     a0,    48
+    fldx.s         f8,     a1,    t0
+    fldx.s         f12,    a1,    t1
+    fldx.s         f9,     a2,    t0
+    fldx.s         f13,    a2,    t1
+    fldx.s         f10,    a3,    t0
+    fldx.s         f14,    a3,    t1
+    fldx.s         f11,    a4,    t0
+    fldx.s         f15,    a4,    t1
+    vilvl.w        vr2,    vr0,   vr2
+    vilvl.w        vr8,    vr12,  vr8
+    vilvl.w        vr9,    vr13,  vr9
+    vilvl.w        vr10,   vr14,  vr10
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.d        vr3,    vr2,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr7,    vr11,  vr7
+
+    // Calculate the absolute value of the difference
+    vabsd.bu       vr8,    vr3,   vr4
+    vabsd.bu       vr9,    vr3,   vr5
+    vabsd.bu       vr10,   vr3,   vr6
+    vabsd.bu       vr11,   vr3,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.wu.hu   vr8,    vr8,   vr8
+    vhaddw.wu.hu   vr9,    vr9,   vr9
+    vhaddw.wu.hu   vr10,   vr10,  vr10
+    vhaddw.wu.hu   vr11,   vr11,  vr11
+    vhaddw.du.wu   vr8,    vr8,   vr8
+    vhaddw.du.wu   vr9,    vr9,   vr9
+    vhaddw.du.wu   vr10,   vr10,  vr10
+    vhaddw.du.wu   vr11,   vr11,  vr11
+    vhaddw.qu.du   vr8,    vr8,   vr8
+    vhaddw.qu.du   vr9,    vr9,   vr9
+    vhaddw.qu.du   vr10,   vr10,  vr10
+    vhaddw.qu.du   vr11,   vr11,  vr11
+
+    // Store data to p_sad_array
+    vstelm.w       vr8,    a6,    0,  0
+    vstelm.w       vr9,    a6,    4,  0
+    vstelm.w       vr10,   a6,    8,  0
+    vstelm.w       vr11,   a6,   12,  0
+endfunc
+
+/* void x264_pixel_sad_x3_16x16_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                   uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                   intptr_t i_ref_stride,
+ *                                   int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_16x16_lasx
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+    slli.d         t3,     a4,    2
+
+    xvld           xr2,    a0,    0
+    xvld           xr3,    a0,    32
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    xvpermi.q      xr4,    xr7,   0x02
+    xvpermi.q      xr5,    xr8,   0x02
+    xvpermi.q      xr6,    xr9,   0x02
+    xvpermi.q      xr10,   xr13,  0x02
+    xvpermi.q      xr11,   xr14,  0x02
+    xvpermi.q      xr12,   xr15,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr2,   xr4
+    xvabsd.bu      xr8,    xr2,   xr5
+    xvabsd.bu      xr9,    xr2,   xr6
+    xvabsd.bu      xr10,   xr3,   xr10
+    xvabsd.bu      xr11,   xr3,   xr11
+    xvabsd.bu      xr12,   xr3,   xr12
+    xvhaddw.hu.bu  xr16,   xr7,   xr7
+    xvhaddw.hu.bu  xr17,   xr8,   xr8
+    xvhaddw.hu.bu  xr18,   xr9,   xr9
+    xvhaddw.hu.bu  xr19,   xr10,  xr10
+    xvhaddw.hu.bu  xr20,   xr11,  xr11
+    xvhaddw.hu.bu  xr21,   xr12,  xr12
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    xvld           xr2,    a0,    64
+    xvld           xr3,    a0,    96
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    xvpermi.q      xr4,    xr7,   0x02
+    xvpermi.q      xr5,    xr8,   0x02
+    xvpermi.q      xr6,    xr9,   0x02
+    xvpermi.q      xr10,   xr13,  0x02
+    xvpermi.q      xr11,   xr14,  0x02
+    xvpermi.q      xr12,   xr15,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr2,   xr4
+    xvabsd.bu      xr8,    xr2,   xr5
+    xvabsd.bu      xr9,    xr2,   xr6
+    xvabsd.bu      xr10,   xr3,   xr10
+    xvabsd.bu      xr11,   xr3,   xr11
+    xvabsd.bu      xr12,   xr3,   xr12
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvhaddw.hu.bu  xr12,   xr12,  xr12
+    xvadd.h        xr16,   xr16,  xr7
+    xvadd.h        xr17,   xr17,  xr8
+    xvadd.h        xr18,   xr18,  xr9
+    xvadd.h        xr19,   xr19,  xr10
+    xvadd.h        xr20,   xr20,  xr11
+    xvadd.h        xr21,   xr21,  xr12
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    xvld           xr2,    a0,    128
+    xvld           xr3,    a0,    160
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    xvpermi.q      xr4,    xr7,   0x02
+    xvpermi.q      xr5,    xr8,   0x02
+    xvpermi.q      xr6,    xr9,   0x02
+    xvpermi.q      xr10,   xr13,  0x02
+    xvpermi.q      xr11,   xr14,  0x02
+    xvpermi.q      xr12,   xr15,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr2,   xr4
+    xvabsd.bu      xr8,    xr2,   xr5
+    xvabsd.bu      xr9,    xr2,   xr6
+    xvabsd.bu      xr10,   xr3,   xr10
+    xvabsd.bu      xr11,   xr3,   xr11
+    xvabsd.bu      xr12,   xr3,   xr12
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvhaddw.hu.bu  xr12,   xr12,  xr12
+    xvadd.h        xr16,   xr16,  xr7
+    xvadd.h        xr17,   xr17,  xr8
+    xvadd.h        xr18,   xr18,  xr9
+    xvadd.h        xr19,   xr19,  xr10
+    xvadd.h        xr20,   xr20,  xr11
+    xvadd.h        xr21,   xr21,  xr12
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    xvld           xr2,    a0,    192
+    xvld           xr3,    a0,    224
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    xvpermi.q      xr4,    xr7,   0x02
+    xvpermi.q      xr5,    xr8,   0x02
+    xvpermi.q      xr6,    xr9,   0x02
+    xvpermi.q      xr10,   xr13,  0x02
+    xvpermi.q      xr11,   xr14,  0x02
+    xvpermi.q      xr12,   xr15,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr2,   xr4
+    xvabsd.bu      xr8,    xr2,   xr5
+    xvabsd.bu      xr9,    xr2,   xr6
+    xvabsd.bu      xr10,   xr3,   xr10
+    xvabsd.bu      xr11,   xr3,   xr11
+    xvabsd.bu      xr12,   xr3,   xr12
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvhaddw.hu.bu  xr12,   xr12,  xr12
+    xvadd.h        xr16,   xr16,  xr7
+    xvadd.h        xr17,   xr17,  xr8
+    xvadd.h        xr18,   xr18,  xr9
+    xvadd.h        xr19,   xr19,  xr10
+    xvadd.h        xr20,   xr20,  xr11
+    xvadd.h        xr21,   xr21,  xr12
+    xvadd.h        xr11,   xr16,  xr19
+    xvadd.h        xr12,   xr17,  xr20
+    xvadd.h        xr13,   xr18,  xr21
+
+    xvhaddw.wu.hu  xr11,   xr11,  xr11
+    xvhaddw.wu.hu  xr12,   xr12,  xr12
+    xvhaddw.wu.hu  xr13,   xr13,  xr13
+    xvhaddw.du.wu  xr11,   xr11,  xr11
+    xvhaddw.du.wu  xr12,   xr12,  xr12
+    xvhaddw.du.wu  xr13,   xr13,  xr13
+    xvhaddw.qu.du  xr11,   xr11,  xr11
+    xvhaddw.qu.du  xr12,   xr12,  xr12
+    xvhaddw.qu.du  xr13,   xr13,  xr13
+    xvpickve.w     xr17,   xr11,  4
+    xvpickve.w     xr18,   xr12,  4
+    xvpickve.w     xr19,   xr13,  4
+    xvadd.w        xr11,   xr11,  xr17
+    xvadd.w        xr12,   xr12,  xr18
+    xvadd.w        xr13,   xr13,  xr19
+
+    // Store data to p_sad_array
+    vstelm.w       vr11,   a5,    0,  0
+    vstelm.w       vr12,   a5,    4,  0
+    vstelm.w       vr13,   a5,    8,  0
+endfunc
+
+/* void x264_pixel_sad_x3_16x8_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                  uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                  intptr_t i_ref_stride,
+ *                                  int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_16x8_lasx
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+    slli.d         t3,     a4,    2
+
+    xvld           xr2,    a0,    0
+    xvld           xr3,    a0,    32
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    xvpermi.q      xr4,    xr7,   0x02
+    xvpermi.q      xr5,    xr8,   0x02
+    xvpermi.q      xr6,    xr9,   0x02
+    xvpermi.q      xr10,   xr13,  0x02
+    xvpermi.q      xr11,   xr14,  0x02
+    xvpermi.q      xr12,   xr15,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr2,   xr4
+    xvabsd.bu      xr8,    xr2,   xr5
+    xvabsd.bu      xr9,    xr2,   xr6
+    xvabsd.bu      xr10,   xr3,   xr10
+    xvabsd.bu      xr11,   xr3,   xr11
+    xvabsd.bu      xr12,   xr3,   xr12
+    xvhaddw.hu.bu  xr16,   xr7,   xr7
+    xvhaddw.hu.bu  xr17,   xr8,   xr8
+    xvhaddw.hu.bu  xr18,   xr9,   xr9
+    xvhaddw.hu.bu  xr19,   xr10,  xr10
+    xvhaddw.hu.bu  xr20,   xr11,  xr11
+    xvhaddw.hu.bu  xr21,   xr12,  xr12
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    xvld           xr2,    a0,    64
+    xvld           xr3,    a0,    96
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    xvpermi.q      xr4,    xr7,   0x02
+    xvpermi.q      xr5,    xr8,   0x02
+    xvpermi.q      xr6,    xr9,   0x02
+    xvpermi.q      xr10,   xr13,  0x02
+    xvpermi.q      xr11,   xr14,  0x02
+    xvpermi.q      xr12,   xr15,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr2,   xr4
+    xvabsd.bu      xr8,    xr2,   xr5
+    xvabsd.bu      xr9,    xr2,   xr6
+    xvabsd.bu      xr10,   xr3,   xr10
+    xvabsd.bu      xr11,   xr3,   xr11
+    xvabsd.bu      xr12,   xr3,   xr12
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.hu.bu  xr10,   xr10,  xr10
+    xvhaddw.hu.bu  xr11,   xr11,  xr11
+    xvhaddw.hu.bu  xr12,   xr12,  xr12
+    xvadd.h        xr16,   xr16,  xr7
+    xvadd.h        xr17,   xr17,  xr8
+    xvadd.h        xr18,   xr18,  xr9
+    xvadd.h        xr19,   xr19,  xr10
+    xvadd.h        xr20,   xr20,  xr11
+    xvadd.h        xr21,   xr21,  xr12
+    xvadd.h        xr11,   xr16,  xr19
+    xvadd.h        xr12,   xr17,  xr20
+    xvadd.h        xr13,   xr18,  xr21
+
+    xvhaddw.wu.hu  xr11,   xr11,  xr11
+    xvhaddw.wu.hu  xr12,   xr12,  xr12
+    xvhaddw.wu.hu  xr13,   xr13,  xr13
+    xvhaddw.du.wu  xr11,   xr11,  xr11
+    xvhaddw.du.wu  xr12,   xr12,  xr12
+    xvhaddw.du.wu  xr13,   xr13,  xr13
+    xvhaddw.qu.du  xr11,   xr11,  xr11
+    xvhaddw.qu.du  xr12,   xr12,  xr12
+    xvhaddw.qu.du  xr13,   xr13,  xr13
+    xvpickve.w     xr17,   xr11,  4
+    xvpickve.w     xr18,   xr12,  4
+    xvpickve.w     xr19,   xr13,  4
+    xvadd.w        xr11,   xr11,  xr17
+    xvadd.w        xr12,   xr12,  xr18
+    xvadd.w        xr13,   xr13,  xr19
+
+    // Store data to p_sad_array
+    vstelm.w       vr11,   a5,    0,  0
+    vstelm.w       vr12,   a5,    4,  0
+    vstelm.w       vr13,   a5,    8,  0
+endfunc
+
+/* void x264_pixel_sad_x3_8x16_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                  uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                  intptr_t i_ref_stride,
+ *                                  int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_8x16_lasx
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+    slli.d         t3,     a4,    2
+
+    fld.d          f3,     a0,    0
+    fld.d          f7,     a0,    16
+    fld.d          f4,     a1,    0
+    fldx.d         f8,     a1,    a4
+    fld.d          f5,     a2,    0
+    fldx.d         f9,     a2,    a4
+    fld.d          f6,     a3,    0
+    fldx.d         f10,    a3,    a4
+    vilvl.d        vr14,   vr7,   vr3
+    vilvl.d        vr15,   vr8,   vr4
+    vilvl.d        vr16,   vr9,   vr5
+    vilvl.d        vr17,   vr10,  vr6
+    fld.d          f3,     a0,    32
+    fld.d          f7,     a0,    48
+    fldx.d         f4,     a1,    t1
+    fldx.d         f8,     a1,    t2
+    fldx.d         f5,     a2,    t1
+    fldx.d         f9,     a2,    t2
+    fldx.d         f6,     a3,    t1
+    fldx.d         f10,    a3,    t2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    xvpermi.q      xr3,    xr14,  0x02
+    xvpermi.q      xr4,    xr15,  0x02
+    xvpermi.q      xr5,    xr16,  0x02
+    xvpermi.q      xr6,    xr17,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr3,   xr4
+    xvabsd.bu      xr8,    xr3,   xr5
+    xvabsd.bu      xr9,    xr3,   xr6
+    xvhaddw.hu.bu  xr11,   xr7,   xr7
+    xvhaddw.hu.bu  xr12,   xr8,   xr8
+    xvhaddw.hu.bu  xr13,   xr9,   xr9
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    64
+    fld.d          f7,     a0,    80
+    fld.d          f4,     a1,    0
+    fldx.d         f8,     a1,    a4
+    fld.d          f5,     a2,    0
+    fldx.d         f9,     a2,    a4
+    fld.d          f6,     a3,    0
+    fldx.d         f10,    a3,    a4
+    vilvl.d        vr14,   vr7,   vr3
+    vilvl.d        vr15,   vr8,   vr4
+    vilvl.d        vr16,   vr9,   vr5
+    vilvl.d        vr17,   vr10,  vr6
+    fld.d          f3,     a0,    96
+    fld.d          f7,     a0,    112
+    fldx.d         f4,     a1,    t1
+    fldx.d         f8,     a1,    t2
+    fldx.d         f5,     a2,    t1
+    fldx.d         f9,     a2,    t2
+    fldx.d         f6,     a3,    t1
+    fldx.d         f10,    a3,    t2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    xvpermi.q      xr3,    xr14,  0x02
+    xvpermi.q      xr4,    xr15,  0x02
+    xvpermi.q      xr5,    xr16,  0x02
+    xvpermi.q      xr6,    xr17,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr3,   xr4
+    xvabsd.bu      xr8,    xr3,   xr5
+    xvabsd.bu      xr9,    xr3,   xr6
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvadd.h        xr11,   xr11,  xr7
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    128
+    fld.d          f7,     a0,    144
+    fld.d          f4,     a1,    0
+    fldx.d         f8,     a1,    a4
+    fld.d          f5,     a2,    0
+    fldx.d         f9,     a2,    a4
+    fld.d          f6,     a3,    0
+    fldx.d         f10,    a3,    a4
+    vilvl.d        vr14,   vr7,   vr3
+    vilvl.d        vr15,   vr8,   vr4
+    vilvl.d        vr16,   vr9,   vr5
+    vilvl.d        vr17,   vr10,  vr6
+    fld.d          f3,     a0,    160
+    fld.d          f7,     a0,    176
+    fldx.d         f4,     a1,    t1
+    fldx.d         f8,     a1,    t2
+    fldx.d         f5,     a2,    t1
+    fldx.d         f9,     a2,    t2
+    fldx.d         f6,     a3,    t1
+    fldx.d         f10,    a3,    t2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    xvpermi.q      xr3,    xr14,  0x02
+    xvpermi.q      xr4,    xr15,  0x02
+    xvpermi.q      xr5,    xr16,  0x02
+    xvpermi.q      xr6,    xr17,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr3,   xr4
+    xvabsd.bu      xr8,    xr3,   xr5
+    xvabsd.bu      xr9,    xr3,   xr6
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvadd.h        xr11,   xr11,  xr7
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    192
+    fld.d          f7,     a0,    208
+    fld.d          f4,     a1,    0
+    fldx.d         f8,     a1,    a4
+    fld.d          f5,     a2,    0
+    fldx.d         f9,     a2,    a4
+    fld.d          f6,     a3,    0
+    fldx.d         f10,    a3,    a4
+    vilvl.d        vr14,   vr7,   vr3
+    vilvl.d        vr15,   vr8,   vr4
+    vilvl.d        vr16,   vr9,   vr5
+    vilvl.d        vr17,   vr10,  vr6
+    fld.d          f3,     a0,    224
+    fld.d          f7,     a0,    240
+    fldx.d         f4,     a1,    t1
+    fldx.d         f8,     a1,    t2
+    fldx.d         f5,     a2,    t1
+    fldx.d         f9,     a2,    t2
+    fldx.d         f6,     a3,    t1
+    fldx.d         f10,    a3,    t2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    xvpermi.q      xr3,    xr14,  0x02
+    xvpermi.q      xr4,    xr15,  0x02
+    xvpermi.q      xr5,    xr16,  0x02
+    xvpermi.q      xr6,    xr17,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr3,   xr4
+    xvabsd.bu      xr8,    xr3,   xr5
+    xvabsd.bu      xr9,    xr3,   xr6
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvadd.h        xr11,   xr11,  xr7
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+
+    xvhaddw.w.h    xr11,   xr11,  xr11
+    xvhaddw.w.h    xr12,   xr12,  xr12
+    xvhaddw.w.h    xr13,   xr13,  xr13
+    xvhaddw.d.w    xr11,   xr11,  xr11
+    xvhaddw.d.w    xr12,   xr12,  xr12
+    xvhaddw.d.w    xr13,   xr13,  xr13
+    xvhaddw.q.d    xr11,   xr11,  xr11
+    xvhaddw.q.d    xr12,   xr12,  xr12
+    xvhaddw.q.d    xr13,   xr13,  xr13
+    xvpickve.w     xr14,   xr11,  4
+    xvpickve.w     xr15,   xr12,  4
+    xvpickve.w     xr16,   xr13,  4
+    xvadd.w        xr11,   xr11,  xr14
+    xvadd.w        xr12,   xr12,  xr15
+    xvadd.w        xr13,   xr13,  xr16
+
+    // Store data to p_sad_array
+    vstelm.w       vr11,   a5,    0,  0
+    vstelm.w       vr12,   a5,    4,  0
+    vstelm.w       vr13,   a5,    8,  0
+endfunc
+
+/* void x264_pixel_sad_x3_8x8_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_8x8_lasx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+    slli.d         t3,     a4,    2
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    0
+    fld.d          f7,     a0,    16
+    fld.d          f4,     a1,    0
+    fldx.d         f8,     a1,    a4
+    fld.d          f5,     a2,    0
+    fldx.d         f9,     a2,    a4
+    fld.d          f6,     a3,    0
+    fldx.d         f10,    a3,    a4
+    vilvl.d        vr14,   vr7,   vr3
+    vilvl.d        vr15,   vr8,   vr4
+    vilvl.d        vr16,   vr9,   vr5
+    vilvl.d        vr17,   vr10,  vr6
+    fld.d          f3,     a0,    32
+    fld.d          f7,     a0,    48
+    fldx.d         f4,     a1,    t1
+    fldx.d         f8,     a1,    t2
+    fldx.d         f5,     a2,    t1
+    fldx.d         f9,     a2,    t2
+    fldx.d         f6,     a3,    t1
+    fldx.d         f10,    a3,    t2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    xvpermi.q      xr3,    xr14,  0x02
+    xvpermi.q      xr4,    xr15,  0x02
+    xvpermi.q      xr5,    xr16,  0x02
+    xvpermi.q      xr6,    xr17,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr3,   xr4
+    xvabsd.bu      xr8,    xr3,   xr5
+    xvabsd.bu      xr9,    xr3,   xr6
+    xvhaddw.hu.bu  xr11,   xr7,   xr7
+    xvhaddw.hu.bu  xr12,   xr8,   xr8
+    xvhaddw.hu.bu  xr13,   xr9,   xr9
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    64
+    fld.d          f7,     a0,    80
+    fld.d          f4,     a1,    0
+    fldx.d         f8,     a1,    a4
+    fld.d          f5,     a2,    0
+    fldx.d         f9,     a2,    a4
+    fld.d          f6,     a3,    0
+    fldx.d         f10,    a3,    a4
+    vilvl.d        vr14,   vr7,   vr3
+    vilvl.d        vr15,   vr8,   vr4
+    vilvl.d        vr16,   vr9,   vr5
+    vilvl.d        vr17,   vr10,  vr6
+    fld.d          f3,     a0,    96
+    fld.d          f7,     a0,    112
+    fldx.d         f4,     a1,    t1
+    fldx.d         f8,     a1,    t2
+    fldx.d         f5,     a2,    t1
+    fldx.d         f9,     a2,    t2
+    fldx.d         f6,     a3,    t1
+    fldx.d         f10,    a3,    t2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    xvpermi.q      xr3,    xr14,  0x02
+    xvpermi.q      xr4,    xr15,  0x02
+    xvpermi.q      xr5,    xr16,  0x02
+    xvpermi.q      xr6,    xr17,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr3,   xr4
+    xvabsd.bu      xr8,    xr3,   xr5
+    xvabsd.bu      xr9,    xr3,   xr6
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvadd.h        xr11,   xr11,  xr7
+    xvadd.h        xr12,   xr12,  xr8
+    xvadd.h        xr13,   xr13,  xr9
+    xvhaddw.w.h    xr11,   xr11,  xr11
+    xvhaddw.w.h    xr12,   xr12,  xr12
+    xvhaddw.w.h    xr13,   xr13,  xr13
+    xvhaddw.d.w    xr11,   xr11,  xr11
+    xvhaddw.d.w    xr12,   xr12,  xr12
+    xvhaddw.d.w    xr13,   xr13,  xr13
+    xvhaddw.q.d    xr11,   xr11,  xr11
+    xvhaddw.q.d    xr12,   xr12,  xr12
+    xvhaddw.q.d    xr13,   xr13,  xr13
+    xvpickve.w     xr18,   xr11,  4
+    xvpickve.w     xr19,   xr12,  4
+    xvpickve.w     xr20,   xr13,  4
+    xvadd.w        xr11,   xr11,  xr18
+    xvadd.w        xr12,   xr12,  xr19
+    xvadd.w        xr13,   xr13,  xr20
+
+    // Store data to p_sad_array
+    vstelm.w       vr11,   a5,    0,  0
+    vstelm.w       vr12,   a5,    4,  0
+    vstelm.w       vr13,   a5,    8,  0
+endfunc
+
+/* void x264_pixel_sad_x3_8x4_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_8x4_lasx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+    slli.d         t3,     a4,    2
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    0
+    fld.d          f7,     a0,    16
+    fld.d          f4,     a1,    0
+    fldx.d         f8,     a1,    a4
+    fld.d          f5,     a2,    0
+    fldx.d         f9,     a2,    a4
+    fld.d          f6,     a3,    0
+    fldx.d         f10,    a3,    a4
+    vilvl.d        vr14,   vr7,   vr3
+    vilvl.d        vr15,   vr8,   vr4
+    vilvl.d        vr16,   vr9,   vr5
+    vilvl.d        vr17,   vr10,  vr6
+    fld.d          f3,     a0,    32
+    fld.d          f7,     a0,    48
+    fldx.d         f4,     a1,    t1
+    fldx.d         f8,     a1,    t2
+    fldx.d         f5,     a2,    t1
+    fldx.d         f9,     a2,    t2
+    fldx.d         f6,     a3,    t1
+    fldx.d         f10,    a3,    t2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    xvpermi.q      xr3,    xr14,  0x02
+    xvpermi.q      xr4,    xr15,  0x02
+    xvpermi.q      xr5,    xr16,  0x02
+    xvpermi.q      xr6,    xr17,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr3,   xr4
+    xvabsd.bu      xr8,    xr3,   xr5
+    xvabsd.bu      xr9,    xr3,   xr6
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.wu.hu  xr7,    xr7,   xr7
+    xvhaddw.wu.hu  xr8,    xr8,   xr8
+    xvhaddw.wu.hu  xr9,    xr9,   xr9
+    xvhaddw.du.wu  xr7,    xr7,   xr7
+    xvhaddw.du.wu  xr8,    xr8,   xr8
+    xvhaddw.du.wu  xr9,    xr9,   xr9
+    xvhaddw.qu.du  xr7,    xr7,   xr7
+    xvhaddw.qu.du  xr8,    xr8,   xr8
+    xvhaddw.qu.du  xr9,    xr9,   xr9
+    xvpickve.w     xr18,   xr7,   4
+    xvpickve.w     xr19,   xr8,   4
+    xvpickve.w     xr20,   xr9,   4
+    xvadd.w        xr7,    xr7,   xr18
+    xvadd.w        xr8,    xr8,   xr19
+    xvadd.w        xr9,    xr9,   xr20
+
+    // Store data to p_sad_array
+    vstelm.w       vr7,    a5,    0,  0
+    vstelm.w       vr8,    a5,    4,  0
+    vstelm.w       vr9,    a5,    8,  0
+endfunc
+
+/* void x264_pixel_sad_x3_4x8_lasx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_4x8_lasx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+    slli.d         t3,     a4,    2
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.s          f3,     a0,    0
+    fld.s          f7,     a0,    16
+    fld.s          f11,    a0,    32
+    fld.s          f15,    a0,    48
+    FLDS_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDS_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDS_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.w        vr3,    vr7,   vr3
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.w        vr12,   vr16,  vr12
+    vilvl.w        vr13,   vr17,  vr13
+    vilvl.w        vr14,   vr18,  vr14
+    vilvl.d        vr19,   vr11,  vr3
+    vilvl.d        vr20,   vr12,  vr4
+    vilvl.d        vr21,   vr13,  vr5
+    vilvl.d        vr22,   vr14,  vr6
+
+    add.d          a1,     a1,    t3
+    add.d          a2,     a2,    t3
+    add.d          a3,     a3,    t3
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.s          f3,     a0,    64
+    fld.s          f7,     a0,    80
+    fld.s          f11,    a0,    96
+    fld.s          f15,    a0,    112
+    FLDS_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDS_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDS_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.w        vr3,    vr7,   vr3
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.w        vr12,   vr16,  vr12
+    vilvl.w        vr13,   vr17,  vr13
+    vilvl.w        vr14,   vr18,  vr14
+    vilvl.d        vr3,    vr11,  vr3
+    vilvl.d        vr4,    vr12,  vr4
+    vilvl.d        vr5,    vr13,  vr5
+    vilvl.d        vr6,    vr14,  vr6
+    xvpermi.q      xr3,    xr19,  0x02
+    xvpermi.q      xr4,    xr20,  0x02
+    xvpermi.q      xr5,    xr21,  0x02
+    xvpermi.q      xr6,    xr22,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu      xr7,    xr3,   xr4
+    xvabsd.bu      xr8,    xr3,   xr5
+    xvabsd.bu      xr9,    xr3,   xr6
+    xvhaddw.hu.bu  xr7,    xr7,   xr7
+    xvhaddw.hu.bu  xr8,    xr8,   xr8
+    xvhaddw.hu.bu  xr9,    xr9,   xr9
+    xvhaddw.wu.hu  xr7,    xr7,   xr7
+    xvhaddw.wu.hu  xr8,    xr8,   xr8
+    xvhaddw.wu.hu  xr9,    xr9,   xr9
+    xvhaddw.du.wu  xr7,    xr7,   xr7
+    xvhaddw.du.wu  xr8,    xr8,   xr8
+    xvhaddw.du.wu  xr9,    xr9,   xr9
+    xvhaddw.qu.du  xr7,    xr7,   xr7
+    xvhaddw.qu.du  xr8,    xr8,   xr8
+    xvhaddw.qu.du  xr9,    xr9,   xr9
+    xvpickve.w     xr10,   xr7,   4
+    xvpickve.w     xr11,   xr8,   4
+    xvpickve.w     xr12,   xr9,   4
+    xvadd.w        xr7,    xr7,   xr10
+    xvadd.w        xr8,    xr8,   xr11
+    xvadd.w        xr9,    xr9,   xr12
+
+    // Store data to p_sad_array
+    vstelm.w       vr7,    a5,    0,  0
+    vstelm.w       vr8,    a5,    4,  0
+    vstelm.w       vr9,    a5,    8,  0
+endfunc
+
+/* void x264_pixel_sad_x3_4x4_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                intptr_t i_ref_stride,
+ *                                int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_4x4_lsx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.s          f3,     a0,    0
+    fld.s          f7,     a0,    16
+    fld.s          f11,    a0,    32
+    fld.s          f15,    a0,    48
+    FLDS_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDS_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDS_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+
+    vilvl.w        vr3,    vr7,   vr3
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.w        vr12,   vr16,  vr12
+    vilvl.w        vr13,   vr17,  vr13
+    vilvl.w        vr14,   vr18,  vr14
+    vilvl.d        vr3,    vr11,  vr3
+    vilvl.d        vr4,    vr12,  vr4
+    vilvl.d        vr5,    vr13,  vr5
+    vilvl.d        vr6,    vr14,  vr6
+
+    // Calculate the absolute value of the difference
+    vabsd.bu       vr7,    vr3,   vr4
+    vabsd.bu       vr8,    vr3,   vr5
+    vabsd.bu       vr9,    vr3,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.wu.hu   vr7,    vr7,   vr7
+    vhaddw.wu.hu   vr8,    vr8,   vr8
+    vhaddw.wu.hu   vr9,    vr9,   vr9
+    vhaddw.du.wu   vr7,    vr7,   vr7
+    vhaddw.du.wu   vr8,    vr8,   vr8
+    vhaddw.du.wu   vr9,    vr9,   vr9
+    vhaddw.qu.du   vr7,    vr7,   vr7
+    vhaddw.qu.du   vr8,    vr8,   vr8
+    vhaddw.qu.du   vr9,    vr9,   vr9
+
+    // Store data to p_sad_array
+    vstelm.w       vr7,    a5,    0,  0
+    vstelm.w       vr8,    a5,    4,  0
+    vstelm.w       vr9,    a5,    8,  0
+endfunc
+
+/* int32_t x264_pixel_sad_16x16_lasx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                   uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_16x16_lasx
+    slli.d         t0,     a1,    1
+    add.d          t1,     a1,    t0
+    add.d          t2,     a1,    t1
+    slli.d         t3,     a3,    1
+    add.d          t4,     a3,    t3
+    add.d          t5,     a3,    t4
+
+    // Load data from p_src and p_ref
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+
+    xvpermi.q      xr0,    xr4,   0x02
+    xvpermi.q      xr1,    xr5,   0x02
+    xvpermi.q      xr2,    xr6,   0x02
+    xvpermi.q      xr3,    xr7,   0x02
+    xvpermi.q      xr8,    xr12,  0x02
+    xvpermi.q      xr9,    xr13,  0x02
+    xvpermi.q      xr10,   xr14,  0x02
+    xvpermi.q      xr11,   xr15,  0x02
+
+    // Calculate the square of the difference
+    xvabsd.bu      xr0,    xr0,   xr8
+    xvabsd.bu      xr1,    xr1,   xr9
+    xvabsd.bu      xr2,    xr2,   xr10
+    xvabsd.bu      xr3,    xr3,   xr11
+    xvhaddw.hu.bu  xr16,   xr0,   xr0
+    xvhaddw.hu.bu  xr17,   xr1,   xr1
+    xvhaddw.hu.bu  xr18,   xr2,   xr2
+    xvhaddw.hu.bu  xr19,   xr3,   xr3
+
+    // Load data from p_src and p_ref
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,   vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,   vr7
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10,  vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14,  vr15
+
+    xvpermi.q      xr0,    xr4,   0x02
+    xvpermi.q      xr1,    xr5,   0x02
+    xvpermi.q      xr2,    xr6,   0x02
+    xvpermi.q      xr3,    xr7,   0x02
+    xvpermi.q      xr8,    xr12,  0x02
+    xvpermi.q      xr9,    xr13,  0x02
+    xvpermi.q      xr10,   xr14,  0x02
+    xvpermi.q      xr11,   xr15,  0x02
+
+    // Calculate the square of the difference
+    xvabsd.bu      xr0,    xr0,   xr8
+    xvabsd.bu      xr1,    xr1,   xr9
+    xvabsd.bu      xr2,    xr2,   xr10
+    xvabsd.bu      xr3,    xr3,   xr11
+    xvhaddw.hu.bu  xr0,    xr0,   xr0
+    xvhaddw.hu.bu  xr1,    xr1,   xr1
+    xvhaddw.hu.bu  xr2,    xr2,   xr2
+    xvhaddw.hu.bu  xr3,    xr3,   xr3
+    xvadd.h        xr0,    xr0,   xr16
+    xvadd.h        xr1,    xr1,   xr17
+    xvadd.h        xr2,    xr2,   xr18
+    xvadd.h        xr3,    xr3,   xr19
+    xvadd.h        xr0,    xr0,   xr1
+    xvadd.h        xr2,    xr2,   xr3
+    xvadd.h        xr0,    xr0,   xr2
+
+    // Calculate the sum
+    xvhaddw.w.h    xr0,    xr0,   xr0
+    xvhaddw.d.w    xr0,    xr0,   xr0
+    xvhaddw.q.d    xr0,    xr0,   xr0
+    xvpickve2gr.w  t2,     xr0,   0
+    xvpickve2gr.w  t3,     xr0,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+/* int32_t x264_pixel_sad_16x8_lasx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                  uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_16x8_lasx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    // Load data from p_src and p_ref
+    vld             vr3,   a0,   0
+    vld             vr4,   a2,   0
+    vldx            vr5,   a0,   a1
+    vldx            vr6,   a2,   a3
+    xvpermi.q       xr3,   xr5,  0x02
+    xvpermi.q       xr4,   xr6,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr5,   xr3,  xr4
+    xvhaddw.hu.bu   xr0,   xr5,  xr5
+    add.d           a0,    a0,   t1
+    add.d           a2,    a2,   t2
+.rept 3
+    // Load data from p_src and p_ref
+    vld             vr3,   a0,   0
+    vld             vr4,   a2,   0
+    vldx            vr5,   a0,   a1
+    vldx            vr6,   a2,   a3
+    xvpermi.q       xr3,   xr5,  0x02
+    xvpermi.q       xr4,   xr6,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr5,   xr3,  xr4
+    xvhaddw.hu.bu   xr6,   xr5,  xr5
+    xvadd.h         xr0,   xr0,  xr6
+    add.d           a0,    a0,   t1
+    add.d           a2,    a2,   t2
+.endr
+    // Calculate the sum
+    xvhaddw.w.h     xr0,   xr0,  xr0
+    xvhaddw.d.w     xr0,   xr0,  xr0
+    xvhaddw.q.d     xr0,   xr0,  xr0
+    xvpickve2gr.w   t2,    xr0,  0
+    xvpickve2gr.w   t3,    xr0,  4
+    add.d           a0,    t2,   t3
+endfunc
+
+/* int32_t x264_pixel_sad_8x16_lasx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                  uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_8x16_lasx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+    slli.d          t5,    a1,   2
+    slli.d          t6,    a3,   2
+
+    // Load data from p_src and p_ref
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr8,   vr10, vr8
+    xvpermi.q       xr3,   xr7,  0x02
+    xvpermi.q       xr4,   xr8,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr5,   xr3,  xr4
+    xvhaddw.hu.bu   xr0,   xr5,  xr5
+    add.d           a0,    a0,   t5
+    add.d           a2,    a2,   t6
+.rept 3
+    // Load data from p_src and p_ref
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr8,   vr10, vr8
+    xvpermi.q       xr3,   xr7,  0x02
+    xvpermi.q       xr4,   xr8,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr5,   xr3,  xr4
+    xvhaddw.hu.bu   xr6,   xr5,  xr5
+    xvadd.h         xr0,   xr0,  xr6
+    add.d           a0,    a0,   t5
+    add.d           a2,    a2,   t6
+.endr
+    // Calculate the sum
+    xvhaddw.w.h     xr0,   xr0,  xr0
+    xvhaddw.d.w     xr0,   xr0,  xr0
+    xvhaddw.q.d     xr0,   xr0,  xr0
+    xvpickve2gr.wu  t2,    xr0,  0
+    xvpickve2gr.wu  t3,    xr0,  4
+    add.d           a0,    t2,   t3
+endfunc
+
+/* int32_t x264_pixel_sad_8x8_lasx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                 uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_8x8_lasx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+    slli.d          t5,    a1,   2
+    slli.d          t6,    a3,   2
+
+    // Load data from p_src and p_ref
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    add.d           a0,    a0,   t5
+    add.d           a2,    a2,   t6
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr8,   vr10, vr8
+    xvpermi.q       xr3,   xr7,  0x02
+    xvpermi.q       xr4,   xr8,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr5,   xr3,  xr4
+    xvhaddw.hu.bu   xr0,   xr5,  xr5
+
+    // Load data from p_src and p_ref
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr8,   vr10, vr8
+    xvpermi.q       xr3,   xr7,  0x02
+    xvpermi.q       xr4,   xr8,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr5,   xr3,  xr4
+    xvhaddw.hu.bu   xr6,   xr5,  xr5
+    xvadd.h         xr0,   xr0,  xr6
+    xvhaddw.w.h     xr0,   xr0,  xr0
+    xvhaddw.d.w     xr0,   xr0,  xr0
+    xvhaddw.q.d     xr0,   xr0,  xr0
+
+    xvpickve2gr.wu  t2,    xr0,  0
+    xvpickve2gr.wu  t3,    xr0,  4
+    add.d           a0,    t2,   t3
+endfunc
+
+/* int32_t x264_pixel_sad_8x4_lasx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                 uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_8x4_lasx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    // Load data from p_src and p_ref
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr8,   vr10, vr8
+    xvpermi.q       xr3,   xr7,  0x02
+    xvpermi.q       xr4,   xr8,  0x02
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr5,   xr3,  xr4
+    xvhaddw.hu.bu   xr6,   xr5,  xr5
+    xvhaddw.wu.hu   xr6,   xr6,  xr6
+    xvhaddw.du.wu   xr6,   xr6,  xr6
+    xvhaddw.qu.du   xr6,   xr6,  xr6
+
+    xvpickve2gr.wu  t2,    xr6,  0
+    xvpickve2gr.wu  t3,    xr6,  4
+    add.d           a0,    t2,   t3
+endfunc
+
+/* int32_t x264_pixel_sad_4x8_lasx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                 uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_4x8_lasx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+    slli.d          t5,    a1,   2
+    slli.d          t6,    a3,   2
+
+    // Load data from p_src and p_ref
+    FLDS_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDS_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    add.d           a0,    a0,   t5
+    add.d           a2,    a2,   t6
+    vilvl.w         vr3,   vr5,  vr3
+    vilvl.w         vr4,   vr6,  vr4
+    vilvl.w         vr7,   vr9,  vr7
+    vilvl.w         vr8,   vr10, vr8
+    vilvl.d         vr1,   vr7,  vr3
+    vilvl.d         vr2,   vr8,  vr4
+
+    // Load data from p_src and p_ref
+    FLDS_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDS_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.w         vr3,   vr5,  vr3
+    vilvl.w         vr4,   vr6,  vr4
+    vilvl.w         vr7,   vr9,  vr7
+    vilvl.w         vr8,   vr10, vr8
+    vilvl.d         vr3,   vr7,  vr3
+    vilvl.d         vr4,   vr8,  vr4
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr2,   xr4,  0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr5,   xr1,  xr2
+    xvhaddw.hu.bu   xr6,   xr5,  xr5
+    xvhaddw.wu.hu   xr6,   xr6,  xr6
+    xvhaddw.du.wu   xr6,   xr6,  xr6
+    xvhaddw.qu.du   xr6,   xr6,  xr6
+    xvpickve2gr.wu  t2,    xr6,  0
+    xvpickve2gr.wu  t3,    xr6,  4
+    add.d           a0,    t2,   t3
+endfunc
+
+/* int32_t x264_pixel_sad_4x4_lasx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                 uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_4x4_lasx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    // Load data from p_src and p_ref
+    FLDS_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDS_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.w         vr3,   vr5,  vr3
+    vilvl.w         vr4,   vr6,  vr4
+    vilvl.w         vr7,   vr9,  vr7
+    vilvl.w         vr8,   vr10, vr8
+    vilvl.d         vr3,   vr7,  vr3
+    vilvl.d         vr4,   vr8,  vr4
+
+    // Calculate the absolute value of the difference
+    vabsd.bu        vr5,   vr3,  vr4
+    vhaddw.hu.bu    vr6,   vr5,  vr5
+    vhaddw.wu.hu    vr6,   vr6,  vr6
+    vhaddw.du.wu    vr6,   vr6,  vr6
+    vhaddw.qu.du    vr6,   vr6,  vr6
+    vpickve2gr.wu   a0,    vr6,  0
+endfunc
+
+.end     sad-a.S
+
+#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/osdep.h b/common/osdep.h
index fbc48019..39cce04f 100644
--- a/common/osdep.h
+++ b/common/osdep.h
@@ -141,7 +141,7 @@ int x264_is_pipe( const char *path );
 
 #define EXPAND(x) x
 
-#if ARCH_X86 || ARCH_X86_64
+#if ARCH_X86 || ARCH_X86_64 || ARCH_LOONGARCH
 #define NATIVE_ALIGN 64
 #define ALIGNED_32( var ) DECLARE_ALIGNED( var, 32 )
 #define ALIGNED_64( var ) DECLARE_ALIGNED( var, 64 )
@@ -293,6 +293,13 @@ static ALWAYS_INLINE uint32_t endian_fix32( uint32_t x )
     asm("rev %0, %0":"+r"(x));
     return x;
 }
+// TODO: "revh.d" has negative effect on LA, why?
+//#elif ARCH_LOONGARCH
+//static ALWAYS_INLINE uint32_t endian_fix32( uint32_t x )
+//{
+//    asm("revh.d %0, %0":"+r"(x));
+//    return x;
+//}
 #else
 static ALWAYS_INLINE uint32_t endian_fix32( uint32_t x )
 {
@@ -364,6 +371,12 @@ static ALWAYS_INLINE void x264_prefetch( void *p )
 {
     asm volatile( "prefetcht0 %0"::"m"(*(uint8_t*)p) );
 }
+// TODO: x264_prefetch has negative effect on LA, why?
+//#elif ARCH_LOONGARCH
+//static ALWAYS_INLINE void x264_prefetch( void *p )
+//{
+//    asm volatile( "preld 0, %[p], 0 \n\t"::[p]"r"(*(uint8_t*)p) );
+//}
 /* We require that prefetch not fault on invalid reads, so we only enable it on
  * known architectures. */
 #elif defined(__GNUC__) && (__GNUC__ > 3 || __GNUC__ == 3 && __GNUC_MINOR__ > 1) &&\
diff --git a/common/pixel.c b/common/pixel.c
index 0d84d2c7..a8af2f95 100644
--- a/common/pixel.c
+++ b/common/pixel.c
@@ -1517,11 +1517,23 @@ void x264_pixel_init( int cpu, x264_pixel_function_t *pixf )
         INIT8( sad, _lasx );
         INIT8_NAME( sad_aligned, sad, _lasx );
         INIT8( ssd, _lasx );
-        INIT7( sad_x3, _lasx );
-        INIT7( sad_x4, _lasx );
         INIT8( satd, _lasx );
         INIT4( hadamard_ac, _lasx );
 
+        pixf->sad_x4[PIXEL_16x16] = x264_pixel_sad_x4_16x16_lasx;
+        pixf->sad_x4[PIXEL_16x8] = x264_pixel_sad_x4_16x8_lasx;
+        pixf->sad_x4[PIXEL_8x16] = x264_pixel_sad_x4_8x16_lasx;
+        pixf->sad_x4[PIXEL_8x8] = x264_pixel_sad_x4_8x8_lasx;
+        pixf->sad_x4[PIXEL_8x4] = x264_pixel_sad_x4_8x4_lasx;
+        pixf->sad_x4[PIXEL_4x8] = x264_pixel_sad_x4_4x8_lasx;
+        pixf->sad_x4[PIXEL_4x4] = x264_pixel_sad_x4_4x4_lsx;
+        pixf->sad_x3[PIXEL_16x16] = x264_pixel_sad_x3_16x16_lasx;
+        pixf->sad_x3[PIXEL_16x8] = x264_pixel_sad_x3_16x8_lasx;
+        pixf->sad_x3[PIXEL_8x16] = x264_pixel_sad_x3_8x16_lasx;
+        pixf->sad_x3[PIXEL_8x8] = x264_pixel_sad_x3_8x8_lasx;
+        pixf->sad_x3[PIXEL_8x4] = x264_pixel_sad_x3_8x4_lasx;
+        pixf->sad_x3[PIXEL_4x8] = x264_pixel_sad_x3_4x8_lasx;
+        pixf->sad_x3[PIXEL_4x4] = x264_pixel_sad_x3_4x4_lsx;
         pixf->intra_sad_x3_4x4   = x264_intra_sad_x3_4x4_lasx;
         pixf->intra_sad_x3_8x8   = x264_intra_sad_x3_8x8_lasx;
         pixf->intra_sad_x3_8x8c  = x264_intra_sad_x3_8x8c_lasx;
diff --git a/common/predict.c b/common/predict.c
index 18a996ec..d9fb024c 100644
--- a/common/predict.c
+++ b/common/predict.c
@@ -46,7 +46,9 @@
 #if ARCH_MIPS
 #   include "mips/predict.h"
 #endif
-
+#if ARCH_LOONGARCH
+#   include "loongarch/predict.h"
+#endif
 /****************************************************************************
  * 16x16 prediction for intra luma block
  ****************************************************************************/
@@ -923,6 +925,22 @@ void x264_predict_16x16_init( int cpu, x264_predict_t pf[7] )
         pf[I_PRED_16x16_DC_128 ]= x264_intra_predict_dc_128_16x16_msa;
     }
 #endif
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf[I_PRED_16x16_V ]     = x264_predict_16x16_v_lsx;
+        pf[I_PRED_16x16_H ]     = x264_predict_16x16_h_lsx;
+        pf[I_PRED_16x16_DC]     = x264_predict_16x16_dc_lsx;
+        pf[I_PRED_16x16_P ]     = x264_predict_16x16_p_lasx;
+        pf[I_PRED_16x16_DC_LEFT]= predict_16x16_dc_left_lsx;
+        pf[I_PRED_16x16_DC_TOP ]= predict_16x16_dc_top_lsx;
+        pf[I_PRED_16x16_DC_128 ]= predict_16x16_dc_128_lsx;
+    }
+#endif
+#endif
+
+#if ARCH_LOONGARCH64
+    x264_predict_16x16_init_lasx( cpu, pf );
 #endif
 }
 
@@ -961,6 +979,21 @@ void x264_predict_8x8c_init( int cpu, x264_predict_t pf[7] )
     }
 #endif
 #endif
+
+#if !HIGH_BIT_DEPTH
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf[I_PRED_CHROMA_P]      = x264_predict_8x8c_p_lsx;
+        pf[I_PRED_CHROMA_V]      = x264_predict_8x8c_v_lsx;
+        pf[I_PRED_CHROMA_H]      = x264_predict_8x8c_h_lsx;
+        pf[I_PRED_CHROMA_DC]     = x264_predict_8x8c_dc_lsx;
+        pf[I_PRED_CHROMA_DC_128] = predict_8x8c_dc_128_lsx;
+        pf[I_PRED_CHROMA_DC_TOP] = predict_8x8c_dc_top_lsx;
+        pf[I_PRED_CHROMA_DC_LEFT]= predict_8x8c_dc_left_lsx;
+    }
+#endif
+#endif
 }
 
 void x264_predict_8x16c_init( int cpu, x264_predict_t pf[7] )
@@ -1022,6 +1055,24 @@ void x264_predict_8x8_init( int cpu, x264_predict8x8_t pf[12], x264_predict_8x8_
     }
 #endif
 #endif
+
+#if !HIGH_BIT_DEPTH
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf[I_PRED_8x8_V]      = x264_predict_8x8_v_lsx;
+        pf[I_PRED_8x8_H]      = x264_predict_8x8_h_lasx;
+        pf[I_PRED_8x8_DC]     = x264_predict_8x8_dc_lsx;
+        pf[I_PRED_8x8_DDL]    = x264_predict_8x8_ddl_lasx;
+        pf[I_PRED_8x8_DDR]    = x264_predict_8x8_ddr_lasx;
+        pf[I_PRED_8x8_VR]     = x264_predict_8x8_vr_lasx;
+        pf[I_PRED_8x8_VL]     = x264_predict_8x8_vl_lasx;
+        pf[I_PRED_8x8_DC_LEFT]= x264_predict_8x8_dc_left_lsx;
+        pf[I_PRED_8x8_DC_TOP] = x264_predict_8x8_dc_top_lsx;
+        pf[I_PRED_8x8_DC_128] = x264_predict_8x8_dc_128_lsx;
+    }
+#endif
+#endif
 }
 
 void x264_predict_4x4_init( int cpu, x264_predict_t pf[12] )
@@ -1050,5 +1101,20 @@ void x264_predict_4x4_init( int cpu, x264_predict_t pf[12] )
 #if ARCH_AARCH64
     x264_predict_4x4_init_aarch64( cpu, pf );
 #endif
+
+#if !HIGH_BIT_DEPTH
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf[I_PRED_4x4_V]      = x264_predict_4x4_v_lsx;
+        pf[I_PRED_4x4_H]      = x264_predict_4x4_h_lsx;
+        pf[I_PRED_4x4_DC]     = x264_predict_4x4_dc_lsx;
+        pf[I_PRED_4x4_DDL]    = x264_predict_4x4_ddl_lsx;
+        pf[I_PRED_4x4_DC_LEFT]= x264_predict_4x4_dc_left_lsx;
+        pf[I_PRED_4x4_DC_TOP] = x264_predict_4x4_dc_top_lsx;
+        pf[I_PRED_4x4_DC_128] = x264_predict_4x4_dc_128_lsx;
+    }
+#endif
+#endif
 }
 
diff --git a/common/x86/dct-a.asm b/common/x86/dct-a.asm
index 01311c49..c6cac510 100644
--- a/common/x86/dct-a.asm
+++ b/common/x86/dct-a.asm
@@ -350,7 +350,7 @@ cglobal sub4x4_dct, 3,3
 %else
 
 %macro SUB_DCT4 0
-cglobal sub4x4_dct, 3,3
+cglobal sub4x4_dct, 3,3 ; Here
 .skip_prologue:
 %if cpuflag(ssse3)
     mova m5, [hsub_mul]
@@ -531,7 +531,7 @@ cglobal_label .skip_prologue
     ret
 
 ; 2xdst, 2xtmp, 4xsrcrow, 1xzero
-%macro LOAD_DIFF8x2_AVX2 9
+%macro LOAD_DIFF8x2_AVX2 9 ; Here
     movq    xm%1, [r1+%5*FENC_STRIDE]
     movq    xm%2, [r1+%6*FENC_STRIDE]
     vinserti128 m%1, m%1, [r1+%7*FENC_STRIDE], 1
@@ -579,7 +579,7 @@ INIT_YMM avx2
 cglobal sub8x8_dct, 3,3,7
     pxor m6, m6
     add r2, 4*FDEC_STRIDE
-    LOAD_DIFF8x2_AVX2 0, 1, 4, 5, 0, 1, 4, 5, 6
+    LOAD_DIFF8x2_AVX2 0, 1, 4, 5, 0, 1, 4, 5, 6 ; Here
     LOAD_DIFF8x2_AVX2 2, 3, 4, 5, 2, 3, 6, 7, 6
     DCT4_1D 0, 1, 2, 3, 4
     TRANSPOSE2x4x4W 0, 1, 2, 3, 4
diff --git a/common/x86/x86util.asm b/common/x86/x86util.asm
index b8671e8c..619505e5 100644
--- a/common/x86/x86util.asm
+++ b/common/x86/x86util.asm
@@ -100,7 +100,7 @@
     SWAP %2, %3
 %endmacro
 
-%macro TRANSPOSE2x4x4W 5
+%macro TRANSPOSE2x4x4W 5  ; Here
     SBUTTERFLY wd,  %1, %2, %5
     SBUTTERFLY wd,  %3, %4, %5
     SBUTTERFLY dq,  %1, %3, %5
diff --git a/configure b/configure
index a5f0ca70..44b6aeb0 100755
--- a/configure
+++ b/configure
@@ -791,6 +791,7 @@ case $host_cpu in
     loongarch*)
         ARCH="LOONGARCH"
         AS="${AS-${CC}}"
+        ASFLAGS="$ASFLAGS -c"
         AS_EXT=".c"
         ;;
     arm*)
diff --git a/tools/checkasm.c b/tools/checkasm.c
index 5f1e275f..cdcaa5b5 100644
--- a/tools/checkasm.c
+++ b/tools/checkasm.c
@@ -106,6 +106,9 @@ static inline uint32_t read_time(void)
     a = b;
 #elif ARCH_MIPS
     asm volatile( "rdhwr %0, $2" : "=r"(a) :: "memory" );
+#elif ARCH_LOONGARCH
+    uint32_t id = 0;
+    asm volatile ( "rdtimel.w  %0, %1" : "=r"(a), "=r"(id) :: "memory" );
 #endif
     return a;
 }
@@ -204,6 +207,8 @@ static void print_bench(void)
                     b->cpu&X264_CPU_ARMV8 ? "armv8" :
 #elif ARCH_MIPS
                     b->cpu&X264_CPU_MSA ? "msa" :
+#elif ARCH_LOONGARCH
+                    b->cpu&X264_CPU_LASX ? "lasx" :
 #endif
                     "c",
 #if HAVE_MMX
@@ -2909,6 +2914,9 @@ static int check_all_flags( void )
 #elif ARCH_MIPS
     if( cpu_detect & X264_CPU_MSA )
         ret |= add_flags( &cpu0, &cpu1, X264_CPU_MSA, "MSA" );
+#elif ARCH_LOONGARCH
+    if( cpu_detect & X264_CPU_LASX )
+        ret |= add_flags( &cpu0, &cpu1, X264_CPU_LASX, "LASX" );
 #endif
     return ret;
 }
@@ -2922,7 +2930,7 @@ static int main_internal( int argc, char **argv )
 
     if( argc > 1 && !strncmp( argv[1], "--bench", 7 ) )
     {
-#if !ARCH_X86 && !ARCH_X86_64 && !ARCH_PPC && !ARCH_ARM && !ARCH_AARCH64 && !ARCH_MIPS
+#if !ARCH_X86 && !ARCH_X86_64 && !ARCH_PPC && !ARCH_ARM && !ARCH_AARCH64 && !ARCH_MIPS && !ARCH_LOONGARCH
         fprintf( stderr, "no --bench for your cpu until you port rdtsc\n" );
         return 1;
 #endif
