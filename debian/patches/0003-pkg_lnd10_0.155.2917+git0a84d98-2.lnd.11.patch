diff --git a/Makefile b/Makefile
index 9b27df65..ff40f97e 100644
--- a/Makefile
+++ b/Makefile
@@ -212,8 +212,7 @@ SRCASM_X += common/loongarch/mc-a.S \
 
 SRCS_X += common/loongarch/pixel-c.c \
           common/loongarch/predict-c.c \
-          common/loongarch/mc-c.c \
-          common/loongarch/deblock-c.c
+          common/loongarch/mc-c.c
 
 OBJASM +=
 ifneq ($(findstring HAVE_BITDEPTH8 1, $(CONFIG)),)
diff --git a/common/loongarch/asm.S b/common/loongarch/asm.S
index bb42e6b2..44937ca1 100644
--- a/common/loongarch/asm.S
+++ b/common/loongarch/asm.S
@@ -344,6 +344,10 @@ ASM_PREF\name:
     vstelm.d   \vd,  \rk,  0, \si
 .endm
 
+.macro vmov xd, xj
+    vor.v  \xd,  \xj,  \xj
+.endm
+
 .macro xmov xd, xj
     xvor.v  \xd,  \xj,  \xj
 .endm
diff --git a/common/loongarch/deblock-a.S b/common/loongarch/deblock-a.S
index 44df8b21..c7f9e85a 100644
--- a/common/loongarch/deblock-a.S
+++ b/common/loongarch/deblock-a.S
@@ -665,6 +665,273 @@ function deblock_h_luma_intra_lasx
     addi.d          sp,    sp,    64
 endfunc
 
-.end     deblock-a.S
+/*
+ * void deblock_strength_c( uint8_t nnz[X264_SCAN8_SIZE], int8_t ref[2][X264_SCAN8_LUMA_SIZE],
+ *                          int16_t mv[2][X264_SCAN8_LUMA_SIZE][2], uint8_t bs[2][8][4],
+ *                          int mvy_limit, int bframe )
+ */
+const shuf_loc
+.byte 1, 9, 17, 25, 2, 10, 18, 26, 3, 11, 19, 27, 4, 12, 20, 28
+endconst
+
+const shuf_locn
+.byte 0, 8, 16, 24, 1, 9, 17, 25, 2, 10, 18, 26, 3, 11, 19, 27
+endconst
+
+function deblock_strength_lasx
+    // dir = 0 s1 = 8 s2 = 1
+    vldi            vr18,  2
+    vldi            vr19,  1
+    addi.d          t0,    zero,  4
+    xvreplgr2vr.h   xr20,  t0
+    xvreplgr2vr.h   xr21,  a4
+
+    vld             vr0,   a0,    11
+    vld             vr1,   a0,    27
+    la.local        t0,    shuf_loc
+    la.local        t1,    shuf_locn
+    vld             vr2,   t0,    0
+    vld             vr3,   t1,    0
+    vshuf.b         vr4,   vr1,   vr0,   vr2
+    vshuf.b         vr5,   vr1,   vr0,   vr3
+    vor.v           vr6,   vr4,   vr5
+    vseqi.b         vr6,   vr6,   0
+    vmov            vr15,  vr6
+    vxor.v          vr8,   vr8,   vr8
+    vbitsel.v       vr8,   vr18,  vr8,   vr6
+
+    vld             vr0,   a1,    11
+    vld             vr1,   a1,    27
+    vshuf.b         vr4,   vr1,   vr0,   vr2
+    vshuf.b         vr5,   vr1,   vr0,   vr3
+    vseq.b          vr4,   vr4,   vr5
+    vseqi.b         vr4,   vr4,   0
+
+    vld             vr0,   a2,    44
+    vld             vr1,   a2,    76
+    vld             vr5,   a2,    108
+    vld             vr6,   a2,    140
+
+    vilvl.h         vr9,   vr1,   vr0
+    vilvl.h         vr10,  vr6,   vr5
+    vilvl.w         vr11,  vr10,  vr9
+    vilvh.w         vr12,  vr10,  vr9
+
+    vilvh.h         vr9,   vr1,   vr0
+    vilvh.h         vr10,  vr6,   vr5
+    vilvl.w         vr13,  vr10,  vr9
+    vilvh.w         vr14,  vr10,  vr9
+
+    vilvl.d         vr0,   vr13,  vr12
+    ld.h            t0,    a2,    60
+    ld.h            t1,    a2,    92
+    ld.h            t2,    a2,    124
+    ld.h            t3,    a2,    156
+    vmov            vr6,   vr14
+    vinsgr2vr.h     vr6,   t0,    4
+    vinsgr2vr.h     vr6,   t1,    5
+    vinsgr2vr.h     vr6,   t2,    6
+    vinsgr2vr.h     vr6,   t3,    7
+    vilvl.d         vr1,   vr12,  vr11
+    vilvl.d         vr5,   vr14,  vr13
+    xvpermi.q       xr0,   xr6,   0x02  // mv[0][loc][0]
+    xvpermi.q       xr5,   xr1,   0x20  // mv[0][locn][0]
+    xvabsd.h        xr5,   xr0,   xr5
+    xvsle.h         xr5,   xr20,  xr5
+
+    vilvh.d         vr0,   vr13,  vr12
+    ld.h            t0,    a2,    62
+    ld.h            t1,    a2,    94
+    ld.h            t2,    a2,    126
+    ld.h            t3,    a2,    158
+    vbsrl.v         vr7,   vr14,  8
+    vinsgr2vr.h     vr7,   t0,    4
+    vinsgr2vr.h     vr7,   t1,    5
+    vinsgr2vr.h     vr7,   t2,    6
+    vinsgr2vr.h     vr7,   t3,    7
+    vilvh.d         vr1,   vr12,  vr11
+    vilvh.d         vr6,   vr14,  vr13
+    xvpermi.q       xr0,   xr7,   0x02  // mv[0][loc][1]
+    xvpermi.q       xr6,   xr1,   0x20  // mv[0][locn][1]
+    xvabsd.h        xr6,   xr0,   xr6
+    xvsle.h         xr6,   xr21,  xr6
+
+    xvor.v          xr5,   xr5,   xr6
+    xvpickev.b      xr5,   xr5,   xr5
+    xvpermi.d       xr5,   xr5,   0xd8
+    vor.v           vr17,  vr4,   vr5
+
+    beqz            a5,    .bframe_iszero_0
+    // bframe != 0
+    vld             vr0,   a1,    51
+    vld             vr1,   a1,    67
+    vshuf.b         vr4,   vr1,   vr0,   vr2   // ref[1][loc]
+    vshuf.b         vr5,   vr1,   vr0,   vr3   // ref[1][locn]
+    vseq.b          vr4,   vr4,   vr5
+    vseqi.b         vr4,   vr4,   0
+
+    vld             vr0,   a2,    204
+    vld             vr1,   a2,    236
+    vld             vr5,   a2,    268
+    vld             vr6,   a2,    300
+
+    vilvl.h         vr9,   vr1,   vr0
+    vilvl.h         vr10,  vr6,   vr5
+    vilvl.w         vr11,  vr10,  vr9
+    vilvh.w         vr12,  vr10,  vr9
+    vilvh.h         vr9,   vr1,   vr0
+    vilvh.h         vr10,  vr6,   vr5
+    vilvl.w         vr13,  vr10,  vr9
+    vilvh.w         vr14,  vr10,  vr9
+
+    vilvl.d         vr0,   vr13,  vr12
+    ld.h            t0,    a2,    220
+    ld.h            t1,    a2,    252
+    ld.h            t2,    a2,    284
+    ld.h            t3,    a2,    316
+    vmov            vr6,   vr14
+    vinsgr2vr.h     vr6,   t0,    4
+    vinsgr2vr.h     vr6,   t1,    5
+    vinsgr2vr.h     vr6,   t2,    6
+    vinsgr2vr.h     vr6,   t3,    7
+    vilvl.d         vr1,   vr12,  vr11
+    vilvl.d         vr5,   vr14,  vr13
+    xvpermi.q       xr0,   xr6,   0x02  // mv[1][loc][0]
+    xvpermi.q       xr5,   xr1,   0x20  // mv[1][locn][0]
+    xvabsd.h        xr5,   xr0,   xr5
+    xvsle.h         xr5,   xr20,  xr5
+
+    vilvh.d         vr0,   vr13,  vr12
+    ld.h            t0,    a2,    222
+    ld.h            t1,    a2,    254
+    ld.h            t2,    a2,    286
+    ld.h            t3,    a2,    318
+    vbsrl.v         vr7,   vr14,  8
+    vinsgr2vr.h     vr7,   t0,    4
+    vinsgr2vr.h     vr7,   t1,    5
+    vinsgr2vr.h     vr7,   t2,    6
+    vinsgr2vr.h     vr7,   t3,    7
+    vilvh.d         vr1,   vr12,  vr11
+    vilvh.d         vr6,   vr14,  vr13
+    xvpermi.q       xr0,   xr7,   0x02  // mv[1][loc][1]
+    xvpermi.q       xr6,   xr1,   0x20  // mv[1][locn][1]
+    xvabsd.h        xr6,   xr0,   xr6
+    xvsle.h         xr6,   xr21,  xr6
+
+    xvor.v          xr5,   xr5,   xr6
+    xvpickev.b      xr5,   xr5,   xr5
+    xvpermi.d       xr5,   xr5,   0xd8
+    vor.v           vr5,   vr5,   vr4
+    vor.v           vr17,  vr5,   vr17
+
+.bframe_iszero_0:
+    vxor.v          vr22,  vr22,  vr22
+    vbitsel.v       vr22,  vr22,  vr19,  vr17
+    vbitsel.v       vr22,  vr8,   vr22,  vr15
+    vstelm.w        vr22,  a3,    0,     0
+    vstelm.w        vr22,  a3,    4,     1
+    vstelm.w        vr22,  a3,    8,     2
+    vstelm.w        vr22,  a3,    12,    3
+
+    // dir = 1 s1 = 1 s2 = 8
+    vld             vr0,   a0,    4
+    vld             vr1,   a0,    20
+    ld.wu           t0,    a0,    36
+    vpickev.w       vr2,   vr1,   vr0
+    vbsrl.v         vr3,   vr2,   4
+    vinsgr2vr.w     vr3,   t0,    3
+    vor.v           vr2,   vr3,   vr2
+    vseqi.b         vr2,   vr2,   0
+    vmov            vr15,  vr2
+    vxor.v          vr3,   vr3,   vr3
+    vbitsel.v       vr3,   vr18,  vr3,   vr2
+
+    vld             vr0,   a1,    4
+    vld             vr1,   a1,    20
+    ld.w            t0,    a1,    36
+    vpickev.w       vr2,   vr1,   vr0
+    vbsrl.v         vr4,   vr2,   4
+    vinsgr2vr.w     vr4,   t0,    3
+    vseq.b          vr2,   vr4,   vr2
+    vseqi.b         vr2,   vr2,   0
+
+    vld             vr0,   a2,    16
+    vld             vr1,   a2,    48
+    vld             vr12,  a2,    80
+    vld             vr13,  a2,    112
+    vld             vr4,   a2,    144
+    vpickev.h       vr5,   vr1,   vr0
+    vpickev.h       vr14,  vr13,  vr12
+    xvpermi.q       xr5,   xr14,  0x02  // mv[0][locn][0]
+    vpickev.h       vr7,   vr4,   vr4
+    xvpermi.d       xr6,   xr5,   0x39
+    xvinsve0.d      xr6,   xr7,   3     // mv[0][loc][0]
+    xvabsd.h        xr5,   xr6,   xr5
+    xvsle.h         xr5,   xr20,  xr5
+
+    vpickod.h       vr6,   vr1,   vr0
+    vpickod.h       vr14,  vr13,  vr12
+    xvpermi.q       xr6,   xr14,  0x02  // mv[0][locn][1]
+    vpickod.h       vr7,   vr4,   vr4
+    xvpermi.d       xr8,   xr6,   0x39
+    xvinsve0.d      xr8,   xr7,   3     // mv[0][loc][1]
+    xvabsd.h        xr6,   xr8,   xr6
+    xvsle.h         xr6,   xr21,  xr6
+
+    xvor.v          xr5,   xr6,   xr5
+    xvpickev.b      xr6,   xr5,   xr5
+    xvpermi.d       xr6,   xr6,   0xd8
+    vor.v           vr2,   vr6,   vr2
+
+    beqz            a5,    .bframe_iszero_1
+    // bframe != 0 ref[1]
+    vld             vr0,   a1,    44
+    vld             vr1,   a1,    60
+    ld.w            t0,    a1,    76
+    vpickev.w       vr0,   vr1,   vr0
+    vbsrl.v         vr1,   vr0,   4
+    vinsgr2vr.w     vr1,   t0,    3
+    vseq.b          vr11,  vr1,   vr0
+    vseqi.b         vr11,  vr11,  0
+
+    vld             vr0,   a2,    176
+    vld             vr1,   a2,    208
+    vld             vr12,  a2,    240
+    vld             vr13,  a2,    272
+    vld             vr4,   a2,    304
+    vpickev.h       vr5,   vr1,   vr0
+    vpickev.h       vr14,  vr13,  vr12
+    xvpermi.q       xr5,   xr14,  0x02  // mv[1][locn][0]
+    vpickev.h       vr7,   vr4,   vr4
+    xvpermi.d       xr6,   xr5,   0x39
+    xvinsve0.d      xr6,   xr7,   3     // mv[1][loc][0]
+    xvabsd.h        xr5,   xr6,   xr5
+    xvsle.h         xr5,   xr20,  xr5
+
+    vpickod.h       vr6,   vr1,   vr0
+    vpickod.h       vr14,  vr13,  vr12
+    xvpermi.q       xr6,   xr14,  0x02  // mv[1][locn][1]
+    vpickod.h       vr7,   vr4,   vr4
+    xvpermi.d       xr8,   xr6,   0x39
+    xvinsve0.d      xr8,   xr7,   3     // mv[1][loc][1]
+    xvabsd.h        xr6,   xr8,   xr6
+    xvsle.h         xr6,   xr21,  xr6
+
+    xvor.v          xr5,   xr6,   xr5
+    xvpickev.b      xr6,   xr5,   xr5
+    xvpermi.d       xr6,   xr6,   0xd8
+    vor.v           vr6,   vr6,   vr11
+    vor.v           vr2,   vr6,   vr2
+
+.bframe_iszero_1:
+    vxor.v          vr22,  vr22,  vr22
+    vbitsel.v       vr22,  vr22,  vr19,  vr2
+    vbitsel.v       vr22,  vr3,   vr22,  vr15
+
+    vstelm.w        vr22,  a3,    32,    0
+    vstelm.w        vr22,  a3,    36,    1
+    vstelm.w        vr22,  a3,    40,    2
+    vstelm.w        vr22,  a3,    44,    3
+endfunc
 
 #endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/deblock-c.c b/common/loongarch/deblock-c.c
deleted file mode 100644
index 5748f70e..00000000
--- a/common/loongarch/deblock-c.c
+++ /dev/null
@@ -1,417 +0,0 @@
-/*****************************************************************************
- * deblock-c.c: loongarch deblocking
- *****************************************************************************
- * Copyright (C) 2015-2018 x264 project
- * Copyright (C) 2020 Loongson Technology Corporation Limited
- *
- * Authors: zhou peng <zhoupeng@loongson.cn>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
- *
- * This program is also available under a commercial proprietary license.
- * For more information, contact us at licensing@x264.com.
- *****************************************************************************/
-
-#include "common/common.h"
-#include "loongson_intrinsics.h"
-#include "deblock.h"
-
-#if !HIGH_BIT_DEPTH
-
-static void avc_deblock_strength_lasx( uint8_t *nnz,
-                                       int8_t pi_lef[2][X264_SCAN8_LUMA_SIZE],
-                                       int16_t pi_mv[2][X264_SCAN8_LUMA_SIZE][2],
-                                       uint8_t pu_bs[2][8][4],
-                                       int32_t i_mvy_himit )
-{
-    __m256i nnz0, nnz1, nnz2, nnz3, nnz4;
-    __m256i nnz_mask, ref_mask, mask, one, two, dst = { 0 };
-    __m256i ref0, ref1, ref2, ref3, ref4;
-    __m256i temp_vec0, temp_vec1, temp_vec2;
-    __m256i mv0, mv1, mv2, mv3, mv4, mv5, mv6, mv7, mv8, mv9, mv_a, mv_b;
-    __m256i four, mvy_himit_vec, sub0, sub1;
-    int8_t* p_lef = pi_lef[0];
-    int16_t* p_mv = pi_mv[0][0];
-
-    DUP2_ARG2(__lasx_xvld, nnz, 4, nnz, 20, nnz0, nnz2 );
-    nnz4 = __lasx_xvld( nnz, 36 );
-
-    DUP2_ARG2(__lasx_xvld, p_lef, 4, p_lef, 20, ref0, ref2 );
-    ref4 = __lasx_xvld( p_lef, 36 );
-
-    DUP4_ARG2(__lasx_xvld, p_mv, 16, p_mv, 48, p_mv, 80, p_mv, 112, mv0, mv1, mv2, mv3 );
-    mv4 = __lasx_xvld( p_mv, 144 );
-
-    mvy_himit_vec = __lasx_xvreplgr2vr_h( i_mvy_himit );
-    four = __lasx_xvreplgr2vr_h( 4 );
-    mask = __lasx_xvldi( 0 );
-    one = __lasx_xvldi( 1 );
-    two = __lasx_xvldi( 2 );
-
-    mv5 = __lasx_xvpickod_h( mv0, mv0 );
-    mv6 = __lasx_xvpickod_h( mv1, mv1 );
-    mv_a = __lasx_xvpickev_h( mv0, mv0 );
-    mv_b = __lasx_xvpickev_h( mv1, mv1 );
-    nnz1 = __lasx_xvrepl128vei_w( nnz0, 2 );
-    ref1 = __lasx_xvrepl128vei_w( ref0, 2 );
-    nnz_mask = __lasx_xvor_v( nnz0, nnz1 );
-    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
-
-    ref_mask = __lasx_xvseq_b( ref0, ref1 );
-    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
-
-    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
-    sub1 = __lasx_xvabsd_h( mv6, mv5 );
-
-    sub0 = __lasx_xvsle_hu( four, sub0 );
-    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
-
-    sub0 = __lasx_xvpickev_b( sub0, sub0 );
-    sub1 = __lasx_xvpickev_b( sub1, sub1 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
-
-    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
-    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
-
-    __lasx_xvstelm_w( dst, pu_bs[1][0], 0, 0 );
-
-    dst = __lasx_xvldi( 0 );
-    two = __lasx_xvldi( 2 );
-
-    mv5 = __lasx_xvpickod_h( mv1, mv1 );
-    mv6 = __lasx_xvpickod_h( mv2, mv2 );
-    mv_a = __lasx_xvpickev_h( mv1, mv1 );
-    mv_b = __lasx_xvpickev_h( mv2, mv2 );
-
-    nnz_mask = __lasx_xvor_v( nnz2, nnz1 );
-    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
-
-    ref_mask = __lasx_xvseq_b( ref1, ref2 );
-    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
-
-    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
-    sub1 = __lasx_xvabsd_h( mv6, mv5 );
-    sub0 = __lasx_xvsle_hu( four, sub0 );
-    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
-
-    sub0 = __lasx_xvpickev_b( sub0, sub0 );
-    sub1 = __lasx_xvpickev_b( sub1, sub1 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
-
-    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
-    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
-
-    __lasx_xvstelm_w( dst, pu_bs[1][1], 0, 0 );
-
-    dst = __lasx_xvldi( 0 );
-    two = __lasx_xvldi( 2 );
-
-    mv5 = __lasx_xvpickod_h( mv2, mv2 );
-    mv6 = __lasx_xvpickod_h( mv3, mv3 );
-    mv_a = __lasx_xvpickev_h( mv2, mv2 );
-    mv_b = __lasx_xvpickev_h( mv3, mv3 );
-
-    nnz3 = __lasx_xvrepl128vei_w( nnz2, 2 );
-    ref3 = __lasx_xvrepl128vei_w( ref2, 2 );
-
-    nnz_mask = __lasx_xvor_v( nnz3, nnz2 );
-    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
-
-    ref_mask = __lasx_xvseq_b( ref2, ref3 );
-    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
-
-    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
-    sub1 = __lasx_xvabsd_h( mv6, mv5 );
-
-    sub0 = __lasx_xvsle_hu( four, sub0 );
-    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
-
-    sub0 = __lasx_xvpickev_b( sub0, sub0 );
-    sub1 = __lasx_xvpickev_b( sub1, sub1 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
-
-    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
-    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
-
-    __lasx_xvstelm_w( dst, pu_bs[1][2], 0, 0 );
-
-    dst = __lasx_xvldi( 0 );
-    two = __lasx_xvldi( 2 );
-
-    mv5 = __lasx_xvpickod_h( mv3, mv3 );
-    mv6 = __lasx_xvpickod_h( mv4, mv4 );
-    mv_a = __lasx_xvpickev_h( mv3, mv3 );
-    mv_b = __lasx_xvpickev_h( mv4, mv4 );
-
-    nnz_mask = __lasx_xvor_v( nnz4, nnz3 );
-    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
-
-    ref_mask = __lasx_xvseq_b( ref3, ref4 );
-    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
-
-    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
-    sub1 = __lasx_xvabsd_h( mv6, mv5 );
-
-    sub0 = __lasx_xvsle_hu( four, sub0 );
-    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
-
-    sub0 = __lasx_xvpickev_b( sub0, sub0 );
-    sub1 = __lasx_xvpickev_b( sub1, sub1 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
-
-    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
-    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
-
-    __lasx_xvstelm_w( dst, pu_bs[1][3], 0, 0 );
-
-    DUP2_ARG2( __lasx_xvld, nnz, 8, nnz, 24, nnz0, nnz2 );
-    DUP2_ARG2( __lasx_xvld, p_lef, 8, p_lef, 24, ref0, ref2);
-
-    DUP4_ARG2(__lasx_xvld, p_mv, 32, p_mv, 48, p_mv, 64, p_mv, 80, mv0, mv1, mv2, mv3 );
-    DUP4_ARG2(__lasx_xvld, p_mv, 96, p_mv, 112, p_mv, 128, p_mv, 144, mv4, mv7, mv8, mv9 );
-
-    nnz1 = __lasx_xvrepl128vei_d( nnz0, 1 );
-    nnz3 = __lasx_xvrepl128vei_d( nnz2, 1 );
-
-    DUP2_ARG2( __lasx_xvilvl_b, nnz2, nnz0, nnz3, nnz1, temp_vec0, temp_vec1 );
-
-    temp_vec2 = __lasx_xvilvl_b( temp_vec1, temp_vec0 );
-    nnz1 = __lasx_xvilvh_b( temp_vec1, temp_vec0 );
-
-    nnz0 = __lasx_xvrepl128vei_w( temp_vec2, 3 );
-    nnz2 = __lasx_xvrepl128vei_w( nnz1, 1 );
-    nnz3 = __lasx_xvrepl128vei_w( nnz1, 2 );
-    nnz4 = __lasx_xvrepl128vei_w( nnz1, 3 );
-
-    ref1 = __lasx_xvrepl128vei_d( ref0, 1 );
-    ref3 = __lasx_xvrepl128vei_d( ref2, 1 );
-
-    DUP2_ARG2( __lasx_xvilvl_b, ref2, ref0, ref3, ref1, temp_vec0, temp_vec1 );
-
-    temp_vec2 = __lasx_xvilvl_b( temp_vec1, temp_vec0 );
-    ref1 = __lasx_xvilvh_b( temp_vec1, temp_vec0 );
-
-    ref0 = __lasx_xvrepl128vei_w( temp_vec2, 3 );
-
-    ref2 = __lasx_xvrepl128vei_w( ref1, 1 );
-    ref3 = __lasx_xvrepl128vei_w( ref1, 2 );
-    ref4 = __lasx_xvrepl128vei_w( ref1, 3 );
-
-    LASX_TRANSPOSE8X4_H( mv0, mv2, mv4, mv8, mv5, mv5, mv5, mv0 );
-    LASX_TRANSPOSE8X4_H( mv1, mv3, mv7, mv9, mv1, mv2, mv3, mv4 );
-
-    mvy_himit_vec = __lasx_xvreplgr2vr_h( i_mvy_himit );
-    four = __lasx_xvreplgr2vr_h( 4 );
-    mask = __lasx_xvldi( 0 );
-    one = __lasx_xvldi( 1 );
-    two = __lasx_xvldi( 2 );
-    dst = __lasx_xvldi( 0 );
-
-    mv5 = __lasx_xvrepl128vei_d( mv0, 1 );
-    mv6 = __lasx_xvrepl128vei_d( mv1, 1 );
-
-    nnz_mask = __lasx_xvor_v( nnz0, nnz1 );
-    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
-
-    ref_mask = __lasx_xvseq_b( ref0, ref1 );
-    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
-
-    sub0 = __lasx_xvabsd_h( mv1, mv0 );
-    sub1 = __lasx_xvabsd_h( mv6, mv5 );
-
-    sub0 = __lasx_xvsle_hu( four, sub0 );
-    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
-
-    sub0 = __lasx_xvpickev_b( sub0, sub0 );
-    sub1 = __lasx_xvpickev_b( sub1, sub1 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
-
-    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
-    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
-
-    __lasx_xvstelm_w( dst, pu_bs[0][0], 0, 0 );
-
-    two = __lasx_xvldi( 2 );
-    dst = __lasx_xvldi( 0 );
-
-    mv5 = __lasx_xvrepl128vei_d( mv1, 1 );
-    mv6 = __lasx_xvrepl128vei_d( mv2, 1 );
-
-    nnz_mask = __lasx_xvor_v( nnz1, nnz2 );
-    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
-
-    ref_mask = __lasx_xvseq_b( ref1, ref2 );
-    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
-
-    sub0 = __lasx_xvabsd_h( mv2, mv1 );
-    sub1 = __lasx_xvabsd_h( mv6, mv5 );
-    sub0 = __lasx_xvsle_hu( four, sub0 );
-    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
-
-    sub0 = __lasx_xvpickev_b( sub0, sub0 );
-    sub1 = __lasx_xvpickev_b( sub1, sub1 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
-
-    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
-    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
-
-    __lasx_xvstelm_w( dst, pu_bs[0][1], 0, 0 );
-
-    two = __lasx_xvldi( 2 );
-    dst = __lasx_xvldi( 0 );
-
-    mv5 = __lasx_xvrepl128vei_d( mv2, 1 );
-    mv6 = __lasx_xvrepl128vei_d( mv3, 1 );
-
-    nnz_mask = __lasx_xvor_v( nnz2, nnz3 );
-    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
-
-    ref_mask = __lasx_xvseq_b( ref2, ref3 );
-    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
-
-    sub0 = __lasx_xvabsd_h( mv3, mv2 );
-    sub1 = __lasx_xvabsd_h( mv6, mv5 );
-    sub0 = __lasx_xvsle_hu( four, sub0 );
-    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
-
-    sub0 = __lasx_xvpickev_b( sub0, sub0 );
-    sub1 = __lasx_xvpickev_b( sub1, sub1 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
-
-    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
-    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
-
-    __lasx_xvstelm_w( dst, pu_bs[0][2], 0, 0 );
-
-    two = __lasx_xvldi( 2 );
-    dst = __lasx_xvldi( 0 );
-
-    mv5 = __lasx_xvrepl128vei_d( mv3, 1 );
-    mv6 = __lasx_xvrepl128vei_d( mv4, 1 );
-
-    nnz_mask = __lasx_xvor_v( nnz3, nnz4 );
-    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
-
-    ref_mask = __lasx_xvseq_b( ref3, ref4 );
-    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
-
-    sub0 = __lasx_xvabsd_h( mv4, mv3 );
-    sub1 = __lasx_xvabsd_h( mv6, mv5 );
-    sub0 = __lasx_xvsle_hu( four, sub0 );
-    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
-
-    sub0 = __lasx_xvpickev_b( sub0, sub0 );
-    sub1 = __lasx_xvpickev_b( sub1, sub1 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
-    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
-
-    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
-    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
-
-    __lasx_xvstelm_w( dst, pu_bs[0][3], 0, 0 );
-}
-
-void x264_deblock_strength_lasx( uint8_t u_nnz[X264_SCAN8_SIZE],
-                                 int8_t pi_lef[2][X264_SCAN8_LUMA_SIZE],
-                                 int16_t pi_mv[2][X264_SCAN8_LUMA_SIZE][2],
-                                 uint8_t pu_bs[2][8][4], int32_t i_mvy_himit,
-                                 int32_t i_bframe )
-{
-    int32_t i_edge, i, loc, locn;
-    int8_t* p_lef0 = pi_lef[0];
-    int8_t* p_lef1 = pi_lef[1];
-    uint8_t (*p_bs0)[4] = pu_bs[0];
-    uint8_t (*p_bs1)[4] = pu_bs[1];
-    int16_t (*p_mv0)[2] = pi_mv[0];
-    int16_t (*p_mv1)[2] = pi_mv[1];
-
-    if( i_bframe )
-    {
-        for( i_edge = 0; i_edge < 4; i_edge++ )
-        {
-            loc = X264_SCAN8_0 + i_edge;
-            for( i = 0; i < 4; i++, loc += 8 )
-            {
-                locn = loc - 1;
-                if( u_nnz[loc] || u_nnz[locn] )
-                {
-                    p_bs0[i_edge][i] = 2;
-                }
-                else if( p_lef0[loc] != p_lef0[locn] ||
-                         abs( p_mv0[loc][0] - p_mv0[locn][0] ) >= 4 ||
-                         abs( p_mv0[loc][1] - p_mv0[locn][1] ) >= i_mvy_himit ||
-                         ( p_lef1[loc] != p_lef1[locn] ||
-                           abs( p_mv1[loc][0] - p_mv1[locn][0] ) >= 4 ||
-                           abs( p_mv1[loc][1] - p_mv1[locn][1] ) >= i_mvy_himit )
-                       )
-                {
-                    p_bs0[i_edge][i] = 1;
-                }
-                else
-                {
-                    p_bs0[i_edge][i] = 0;
-                }
-            }
-        }
-
-        for( i_edge = 0; i_edge < 4; i_edge++ )
-        {
-            loc = X264_SCAN8_0 + ( i_edge << 3 );
-            for( i = 0; i < 4; i++, loc++ )
-            {
-                locn = loc - 8;
-                if( u_nnz[loc] || u_nnz[locn] )
-                {
-                    p_bs1[i_edge][i] = 2;
-                }
-                else if( p_lef0[loc] != p_lef0[locn] ||
-                         abs( p_mv0[loc][0] - p_mv0[locn][0] ) >= 4 ||
-                         abs( p_mv0[loc][1] - p_mv0[locn][1] ) >= i_mvy_himit ||
-                         ( p_lef1[loc] != p_lef1[locn] ||
-                           abs( p_mv1[loc][0] - p_mv1[locn][0] ) >= 4 ||
-                           abs( p_mv1[loc][1] - p_mv1[locn][1] ) >= i_mvy_himit )
-                       )
-                {
-                    p_bs1[i_edge][i] = 1;
-                }
-                else
-                {
-                    p_bs1[i_edge][i] = 0;
-                }
-            }
-        }
-    }
-    else
-    {
-        avc_deblock_strength_lasx( u_nnz, pi_lef, pi_mv, pu_bs, i_mvy_himit );
-    }
-}
-
-#endif
diff --git a/common/loongarch/loongson_intrinsics.h b/common/loongarch/loongson_intrinsics.h
deleted file mode 100644
index a21a68a3..00000000
--- a/common/loongarch/loongson_intrinsics.h
+++ /dev/null
@@ -1,1965 +0,0 @@
-/*****************************************************************************
- * loongson_intrinsics.h: loongarch macros
- *****************************************************************************
- * Copyright (C) 2020 x264 project
- * Copyright (C) 2020 Loongson Technology Corporation Limited
- *
- * Authors: Peng Zhou    <zhoupeng@loongson.cn>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
- *
- * This program is also available under a commercial proprietary license.
- * For more information, contact us at licensing@x264.com.
- *****************************************************************************/
-
-/*
- * Copyright (c) 2021 Loongson Technology Corporation Limited
- * All rights reserved.
- * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
- *                Xiwei Gu   <guxiwei-hf@loongson.cn>
- *                Lu Wang    <wanglu@loongson.cn>
- *
- * This file is maintained in LSOM project, don't change it directly.
- * You can get the latest version of this header from: ***
- *
- */
-
-#ifndef LOONGSON_INTRINSICS_H
-#define LOONGSON_INTRINSICS_H
-
-/**
- * MAJOR version: Macro usage changes.
- * MINOR version: Add new functions, or bug fix.
- * MICRO version: Comment changes or implementation changes.
- */
-#define LSOM_VERSION_MAJOR 1
-#define LSOM_VERSION_MINOR 0
-#define LSOM_VERSION_MICRO 1
-
-#define DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _INS(_IN0); \
-    _OUT1 = _INS(_IN1); \
-}
-
-#define DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _INS(_IN0, _IN1); \
-    _OUT1 = _INS(_IN2, _IN3); \
-}
-
-#define DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1) \
-{ \
-    _OUT0 = _INS(_IN0, _IN1, _IN2); \
-    _OUT1 = _INS(_IN3, _IN4, _IN5); \
-}
-
-#define DUP4_ARG1(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1); \
-    DUP2_ARG1(_INS, _IN2, _IN3, _OUT2, _OUT3); \
-}
-
-#define DUP4_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
-                  _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1); \
-    DUP2_ARG2(_INS, _IN4, _IN5, _IN6, _IN7, _OUT2, _OUT3); \
-}
-
-#define DUP4_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
-                  _IN8, _IN9, _IN10, _IN11, _OUT0, _OUT1, _OUT2, _OUT3) \
-{ \
-    DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4,  _IN5,  _OUT0, _OUT1); \
-    DUP2_ARG3(_INS, _IN6, _IN7, _IN8, _IN9, _IN10, _IN11, _OUT2, _OUT3); \
-}
-
-/*
- * =============================================================================
- * Description : Print out elements in vector.
- * Arguments   : Inputs  - RTYPE, _element_num, _in0, _enter
- *               Outputs -
- * Details     : Print out '_element_num' elements in 'RTYPE' vector '_in0', if
- *               '_enter' is TRUE, prefix "\nVP:" will be added first.
- * Example     : VECT_PRINT(v4i32,4,in0,1); // in0: 1,2,3,4
- *               VP:1,2,3,4,
- * =============================================================================
- */
-#define VECT_PRINT(RTYPE, element_num, in0, enter)    \
-{                                                     \
-    RTYPE _tmp0 = (RTYPE)in0;                         \
-    int _i = 0;                                       \
-    if (enter)                                        \
-        printf("\nVP:");                              \
-    for(_i = 0; _i < element_num; _i++)               \
-        printf("%d,",_tmp0[_i]);                      \
-}
-
-#ifdef __loongarch_sx
-#include <lsxintrin.h>
-/*
- * =============================================================================
- * Description : Dot product & addition of byte vector elements
- * Arguments   : Inputs  - in_c, in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in_h are multiplied by
- *               signed byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- *               Then the results plus to signed half word elements from in_c.
- * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
- *        in_c : 1,2,3,4, 1,2,3,4
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 23,40,41,26, 23,40,41,26
- * =============================================================================
- */
-static inline __m128i __lsx_vdp2add_h_b(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmaddwev_h_b(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product & addition of byte vector elements
- * Arguments   : Inputs  - in_c, in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Unsigned byte elements from in_h are multiplied by
- *               unsigned byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- *               The results plus to signed half word elements from in_c.
- * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
- *        in_c : 1,2,3,4, 1,2,3,4
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 23,40,41,26, 23,40,41,26
- * =============================================================================
- */
-static inline __m128i __lsx_vdp2add_h_bu(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmaddwev_h_bu(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product & addition of half word vector elements
- * Arguments   : Inputs  - in_c, in_h, in_l
- *               Outputs - out
- *               Retrun Type - __m128i
- * Details     : Signed half word elements from in_h are multiplied by
- *               signed half word elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- *               Then the results plus to signed word elements from in_c.
- * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
- *        in_c : 1,2,3,4
- *        in_h : 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1
- *         out : 23,40,41,26
- * =============================================================================
- */
-static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmaddwev_w_h(in_c, in_h, in_l);
-    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in_h are multiplied by
- *               signed byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- * Example     : out = __lsx_vdp2_h_b(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 22,38,38,22, 22,38,38,22
- * =============================================================================
- */
-static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmulwev_h_b(in_h, in_l);
-    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Unsigned byte elements from in_h are multiplied by
- *               unsigned byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- * Example     : out = __lsx_vdp2_h_bu(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 22,38,38,22, 22,38,38,22
- * =============================================================================
- */
-static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmulwev_h_bu(in_h, in_l);
-    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Unsigned byte elements from in_h are multiplied by
- *               signed byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- * Example     : out = __lsx_vdp2_h_bu_b(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,-1
- *         out : 22,38,38,22, 22,38,38,6
- * =============================================================================
- */
-static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmulwev_h_bu_b(in_h, in_l);
-    out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in_h, in_l
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in_h are multiplied by
- *               signed byte elements from in_l, and then added adjacent to
- *               each other to get results with the twice size of input.
- * Example     : out = __lsx_vdp2_w_h(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1
- *         out : 22,38,38,22
- * =============================================================================
- */
-static inline __m128i __lsx_vdp2_w_h(__m128i in_h, __m128i in_l)
-{
-    __m128i out;
-
-    out = __lsx_vmulwev_w_h(in_h, in_l);
-    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Clip all halfword elements of input vector between min & max
- *               out = ((_in) < (min)) ? (min) : (((_in) > (max)) ? (max) : (_in))
- * Arguments   : Inputs  - _in  (input vector)
- *                       - min  (min threshold)
- *                       - max  (max threshold)
- *               Outputs - out  (output vector with clipped elements)
- *               Return Type - signed halfword
- * Example     : out = __lsx_vclip_h(_in)
- *         _in : -8,2,280,249, -8,255,280,249
- *         min : 1,1,1,1, 1,1,1,1
- *         max : 9,9,9,9, 9,9,9,9
- *         out : 1,2,9,9, 1,9,9,9
- * =============================================================================
- */
-static inline __m128i __lsx_vclip_h(__m128i _in, __m128i min, __m128i max)
-{
-    __m128i out;
-
-    out = __lsx_vmax_h(min, _in);
-    out = __lsx_vmin_h(max, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Set each element of vector between 0 and 255
- * Arguments   : Inputs  - _in
- *               Outputs - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from _in are clamped between 0 and 255.
- * Example     : out = __lsx_vclip255_h(_in)
- *         _in : -8,255,280,249, -8,255,280,249
- *         out : 0,255,255,249, 0,255,255,249
- * =============================================================================
- */
-static inline __m128i __lsx_vclip255_h(__m128i _in)
-{
-    __m128i out;
-
-    out = __lsx_vmaxi_h(_in, 0);
-    out = __lsx_vsat_hu(out, 7);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Set each element of vector between 0 and 255
- * Arguments   : Inputs  - _in
- *               Outputs - out
- *               Retrun Type - word
- * Details     : Signed byte elements from _in are clamped between 0 and 255.
- * Example     : out = __lsx_vclip255_w(_in)
- *         _in : -8,255,280,249
- *         out : 0,255,255,249
- * =============================================================================
- */
-static inline __m128i __lsx_vclip255_w(__m128i _in)
-{
-    __m128i out;
-
-    out = __lsx_vmaxi_w(_in, 0);
-    out = __lsx_vsat_wu(out, 7);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Swap two variables
- * Arguments   : Inputs  - _in0, _in1
- *               Outputs - _in0, _in1 (in-place)
- * Details     : Swapping of two input variables using xor
- * Example     : LSX_SWAP(_in0, _in1)
- *        _in0 : 1,2,3,4
- *        _in1 : 5,6,7,8
- *   _in0(out) : 5,6,7,8
- *   _in1(out) : 1,2,3,4
- * =============================================================================
- */
-#define LSX_SWAP(_in0, _in1)                                            \
-{                                                                       \
-    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
-    _in1 = __lsx_vxor_v(_in0, _in1);                                    \
-    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
-}                                                                       \
-
-/*
- * =============================================================================
- * Description : Transpose 4x4 block with word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * Details     :
- * Example     :
- *               1, 2, 3, 4            1, 5, 9,13
- *               5, 6, 7, 8    to      2, 6,10,14
- *               9,10,11,12  =====>    3, 7,11,15
- *              13,14,15,16            4, 8,12,16
- * =============================================================================
- */
-#define LSX_TRANSPOSE4x4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                              \
-    __m128i _t0, _t1, _t2, _t3;                                                \
-                                                                               \
-    _t0   = __lsx_vilvl_w(_in1, _in0);                                         \
-    _t1   = __lsx_vilvh_w(_in1, _in0);                                         \
-    _t2   = __lsx_vilvl_w(_in3, _in2);                                         \
-    _t3   = __lsx_vilvh_w(_in3, _in2);                                         \
-    _out0 = __lsx_vilvl_d(_t2, _t0);                                           \
-    _out1 = __lsx_vilvh_d(_t2, _t0);                                           \
-    _out2 = __lsx_vilvl_d(_t3, _t1);                                           \
-    _out3 = __lsx_vilvh_d(_t3, _t1);                                           \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 8x8 block with byte elements in vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : LSX_TRANSPOSE8x8_B
- *        _in0 : 00,01,02,03,04,05,06,07, 00,00,00,00,00,00,00,00
- *        _in1 : 10,11,12,13,14,15,16,17, 00,00,00,00,00,00,00,00
- *        _in2 : 20,21,22,23,24,25,26,27, 00,00,00,00,00,00,00,00
- *        _in3 : 30,31,32,33,34,35,36,37, 00,00,00,00,00,00,00,00
- *        _in4 : 40,41,42,43,44,45,46,47, 00,00,00,00,00,00,00,00
- *        _in5 : 50,51,52,53,54,55,56,57, 00,00,00,00,00,00,00,00
- *        _in6 : 60,61,62,63,64,65,66,67, 00,00,00,00,00,00,00,00
- *        _in7 : 70,71,72,73,74,75,76,77, 00,00,00,00,00,00,00,00
- *
- *      _ out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
- *      _ out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
- *      _ out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
- *      _ out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
- *      _ out4 : 04,14,24,34,44,54,64,74, 00,00,00,00,00,00,00,00
- *      _ out5 : 05,15,25,35,45,55,65,75, 00,00,00,00,00,00,00,00
- *      _ out6 : 06,16,26,36,46,56,66,76, 00,00,00,00,00,00,00,00
- *      _ out7 : 07,17,27,37,47,57,67,77, 00,00,00,00,00,00,00,00
- * =============================================================================
- */
-#define LSX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-   __m128i zero = {0};                                                            \
-   __m128i shuf8 = {0x0F0E0D0C0B0A0908, 0x1716151413121110};                      \
-   __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
-                                                                                  \
-   _t0 = __lsx_vilvl_b(_in2, _in0);                                               \
-   _t1 = __lsx_vilvl_b(_in3, _in1);                                               \
-   _t2 = __lsx_vilvl_b(_in6, _in4);                                               \
-   _t3 = __lsx_vilvl_b(_in7, _in5);                                               \
-   _t4 = __lsx_vilvl_b(_t1, _t0);                                                 \
-   _t5 = __lsx_vilvh_b(_t1, _t0);                                                 \
-   _t6 = __lsx_vilvl_b(_t3, _t2);                                                 \
-   _t7 = __lsx_vilvh_b(_t3, _t2);                                                 \
-   _out0 = __lsx_vilvl_w(_t6, _t4);                                               \
-   _out2 = __lsx_vilvh_w(_t6, _t4);                                               \
-   _out4 = __lsx_vilvl_w(_t7, _t5);                                               \
-   _out6 = __lsx_vilvh_w(_t7, _t5);                                               \
-   _out1 = __lsx_vshuf_b(zero, _out0, shuf8);                                     \
-   _out3 = __lsx_vshuf_b(zero, _out2, shuf8);                                     \
-   _out5 = __lsx_vshuf_b(zero, _out4, shuf8);                                     \
-   _out7 = __lsx_vshuf_b(zero, _out6, shuf8);                                     \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 8x8 block with half word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * Details     :
- * Example     :
- *              00,01,02,03,04,05,06,07           00,10,20,30,40,50,60,70
- *              10,11,12,13,14,15,16,17           01,11,21,31,41,51,61,71
- *              20,21,22,23,24,25,26,27           02,12,22,32,42,52,62,72
- *              30,31,32,33,34,35,36,37    to     03,13,23,33,43,53,63,73
- *              40,41,42,43,44,45,46,47  ======>  04,14,24,34,44,54,64,74
- *              50,51,52,53,54,55,56,57           05,15,25,35,45,55,65,75
- *              60,61,62,63,64,65,66,67           06,16,26,36,46,56,66,76
- *              70,71,72,73,74,75,76,77           07,17,27,37,47,57,67,77
- * =============================================================================
- */
-#define LSX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    __m128i _s0, _s1, _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                     \
-                                                                                  \
-    _s0 = __lsx_vilvl_h(_in6, _in4);                                              \
-    _s1 = __lsx_vilvl_h(_in7, _in5);                                              \
-    _t0 = __lsx_vilvl_h(_s1, _s0);                                                \
-    _t1 = __lsx_vilvh_h(_s1, _s0);                                                \
-    _s0 = __lsx_vilvh_h(_in6, _in4);                                              \
-    _s1 = __lsx_vilvh_h(_in7, _in5);                                              \
-    _t2 = __lsx_vilvl_h(_s1, _s0);                                                \
-    _t3 = __lsx_vilvh_h(_s1, _s0);                                                \
-    _s0 = __lsx_vilvl_h(_in2, _in0);                                              \
-    _s1 = __lsx_vilvl_h(_in3, _in1);                                              \
-    _t4 = __lsx_vilvl_h(_s1, _s0);                                                \
-    _t5 = __lsx_vilvh_h(_s1, _s0);                                                \
-    _s0 = __lsx_vilvh_h(_in2, _in0);                                              \
-    _s1 = __lsx_vilvh_h(_in3, _in1);                                              \
-    _t6 = __lsx_vilvl_h(_s1, _s0);                                                \
-    _t7 = __lsx_vilvh_h(_s1, _s0);                                                \
-                                                                                  \
-    _out0 = __lsx_vpickev_d(_t0, _t4);                                            \
-    _out2 = __lsx_vpickev_d(_t1, _t5);                                            \
-    _out4 = __lsx_vpickev_d(_t2, _t6);                                            \
-    _out6 = __lsx_vpickev_d(_t3, _t7);                                            \
-    _out1 = __lsx_vpickod_d(_t0, _t4);                                            \
-    _out3 = __lsx_vpickod_d(_t1, _t5);                                            \
-    _out5 = __lsx_vpickod_d(_t2, _t6);                                            \
-    _out7 = __lsx_vpickod_d(_t3, _t7);                                            \
-}
-
-/*
- * =============================================================================
- * Description : Transpose input 8x4 byte block into 4x8
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3      (input 8x4 byte block)
- *               Outputs - _out0, _out1, _out2, _out3  (output 4x8 byte block)
- *               Return Type - as per RTYPE
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : LSX_TRANSPOSE8x4_B
- *        _in0 : 00,01,02,03,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in1 : 10,11,12,13,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in2 : 20,21,22,23,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in3 : 30,31,32,33,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in4 : 40,41,42,43,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in5 : 50,51,52,53,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in6 : 60,61,62,63,00,00,00,00, 00,00,00,00,00,00,00,00
- *        _in7 : 70,71,72,73,00,00,00,00, 00,00,00,00,00,00,00,00
- *
- *       _out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
- *       _out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
- *       _out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
- *       _out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
- * =============================================================================
- */
-#define LSX_TRANSPOSE8x4_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,       \
-                           _out0, _out1, _out2, _out3)                           \
-{                                                                                \
-    __m128i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                  \
-                                                                                 \
-    _tmp0_m = __lsx_vpackev_w(_in4, _in0);                                       \
-    _tmp1_m = __lsx_vpackev_w(_in5, _in1);                                       \
-    _tmp2_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
-    _tmp0_m = __lsx_vpackev_w(_in6, _in2);                                       \
-    _tmp1_m = __lsx_vpackev_w(_in7, _in3);                                       \
-                                                                                 \
-    _tmp3_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
-    _tmp0_m = __lsx_vilvl_h(_tmp3_m, _tmp2_m);                                   \
-    _tmp1_m = __lsx_vilvh_h(_tmp3_m, _tmp2_m);                                   \
-                                                                                 \
-    _out0 = __lsx_vilvl_w(_tmp1_m, _tmp0_m);                                     \
-    _out2 = __lsx_vilvh_w(_tmp1_m, _tmp0_m);                                     \
-    _out1 = __lsx_vilvh_d(_out2, _out0);                                         \
-    _out3 = __lsx_vilvh_d(_out0, _out2);                                         \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 16x8 block with byte elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7, in8
- *                         in9, in10, in11, in12, in13, in14, in15
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * Details     :
- * Example     :
- *              000,001,002,003,004,005,006,007
- *              008,009,010,011,012,013,014,015
- *              016,017,018,019,020,021,022,023
- *              024,025,026,027,028,029,030,031
- *              032,033,034,035,036,037,038,039
- *              040,041,042,043,044,045,046,047        000,008,...,112,120
- *              048,049,050,051,052,053,054,055        001,009,...,113,121
- *              056,057,058,059,060,061,062,063   to   002,010,...,114,122
- *              064,068,066,067,068,069,070,071 =====> 003,011,...,115,123
- *              072,073,074,075,076,077,078,079        004,012,...,116,124
- *              080,081,082,083,084,085,086,087        005,013,...,117,125
- *              088,089,090,091,092,093,094,095        006,014,...,118,126
- *              096,097,098,099,100,101,102,103        007,015,...,119,127
- *              104,105,106,107,108,109,110,111
- *              112,113,114,115,116,117,118,119
- *              120,121,122,123,124,125,126,127
- * =============================================================================
- */
-#define LSX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _in8,  \
-                            _in9, _in10, _in11, _in12, _in13, _in14, _in15, _out0, \
-                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)       \
-{                                                                                  \
-    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
-    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
-    DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5,       \
-              _tmp0, _tmp1, _tmp2, _tmp3);                                         \
-    DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,        \
-              _in13, _tmp4, _tmp5, _tmp6, _tmp7);                                  \
-    DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);                \
-    DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);                \
-    DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);                \
-    DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);                \
-    DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);                    \
-    DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);                    \
-    DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);                    \
-    DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);                    \
-    DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);            \
-    DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);            \
-    DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);            \
-    DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);            \
-}
-
-/*
- * =============================================================================
- * Description : Butterfly of 4 input vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * Details     : Butterfly operation
- * Example     :
- *               out0 = in0 + in3;
- *               out1 = in1 + in2;
- *               out2 = in1 - in2;
- *               out3 = in0 - in3;
- * =============================================================================
- */
-#define LSX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                             \
-    _out0 = __lsx_vadd_b(_in0, _in3);                                         \
-    _out1 = __lsx_vadd_b(_in1, _in2);                                         \
-    _out2 = __lsx_vsub_b(_in1, _in2);                                         \
-    _out3 = __lsx_vsub_b(_in0, _in3);                                         \
-}
-#define LSX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                             \
-    _out0 = __lsx_vadd_h(_in0, _in3);                                         \
-    _out1 = __lsx_vadd_h(_in1, _in2);                                         \
-    _out2 = __lsx_vsub_h(_in1, _in2);                                         \
-    _out3 = __lsx_vsub_h(_in0, _in3);                                         \
-}
-#define LSX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                             \
-    _out0 = __lsx_vadd_w(_in0, _in3);                                         \
-    _out1 = __lsx_vadd_w(_in1, _in2);                                         \
-    _out2 = __lsx_vsub_w(_in1, _in2);                                         \
-    _out3 = __lsx_vsub_w(_in0, _in3);                                         \
-}
-#define LSX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                             \
-    _out0 = __lsx_vadd_d(_in0, _in3);                                         \
-    _out1 = __lsx_vadd_d(_in1, _in2);                                         \
-    _out2 = __lsx_vsub_d(_in1, _in2);                                         \
-    _out3 = __lsx_vsub_d(_in0, _in3);                                         \
-}
-
-/*
- * =============================================================================
- * Description : Butterfly of 8 input vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
- *               Outputs - _out0, _out1, _out2, _out3, ~
- * Details     : Butterfly operation
- * Example     :
- *              _out0 = _in0 + _in7;
- *              _out1 = _in1 + _in6;
- *              _out2 = _in2 + _in5;
- *              _out3 = _in3 + _in4;
- *              _out4 = _in3 - _in4;
- *              _out5 = _in2 - _in5;
- *              _out6 = _in1 - _in6;
- *              _out7 = _in0 - _in7;
- * =============================================================================
- */
-#define LSX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                \
-    _out0 = __lsx_vadd_b(_in0, _in7);                                            \
-    _out1 = __lsx_vadd_b(_in1, _in6);                                            \
-    _out2 = __lsx_vadd_b(_in2, _in5);                                            \
-    _out3 = __lsx_vadd_b(_in3, _in4);                                            \
-    _out4 = __lsx_vsub_b(_in3, _in4);                                            \
-    _out5 = __lsx_vsub_b(_in2, _in5);                                            \
-    _out6 = __lsx_vsub_b(_in1, _in6);                                            \
-    _out7 = __lsx_vsub_b(_in0, _in7);                                            \
-}
-
-#define LSX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                \
-    _out0 = __lsx_vadd_h(_in0, _in7);                                            \
-    _out1 = __lsx_vadd_h(_in1, _in6);                                            \
-    _out2 = __lsx_vadd_h(_in2, _in5);                                            \
-    _out3 = __lsx_vadd_h(_in3, _in4);                                            \
-    _out4 = __lsx_vsub_h(_in3, _in4);                                            \
-    _out5 = __lsx_vsub_h(_in2, _in5);                                            \
-    _out6 = __lsx_vsub_h(_in1, _in6);                                            \
-    _out7 = __lsx_vsub_h(_in0, _in7);                                            \
-}
-
-#define LSX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                \
-    _out0 = __lsx_vadd_w(_in0, _in7);                                            \
-    _out1 = __lsx_vadd_w(_in1, _in6);                                            \
-    _out2 = __lsx_vadd_w(_in2, _in5);                                            \
-    _out3 = __lsx_vadd_w(_in3, _in4);                                            \
-    _out4 = __lsx_vsub_w(_in3, _in4);                                            \
-    _out5 = __lsx_vsub_w(_in2, _in5);                                            \
-    _out6 = __lsx_vsub_w(_in1, _in6);                                            \
-    _out7 = __lsx_vsub_w(_in0, _in7);                                            \
-}
-
-#define LSX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                \
-    _out0 = __lsx_vadd_d(_in0, _in7);                                            \
-    _out1 = __lsx_vadd_d(_in1, _in6);                                            \
-    _out2 = __lsx_vadd_d(_in2, _in5);                                            \
-    _out3 = __lsx_vadd_d(_in3, _in4);                                            \
-    _out4 = __lsx_vsub_d(_in3, _in4);                                            \
-    _out5 = __lsx_vsub_d(_in2, _in5);                                            \
-    _out6 = __lsx_vsub_d(_in1, _in6);                                            \
-    _out7 = __lsx_vsub_d(_in0, _in7);                                            \
-}
-
-#endif //LSX
-
-#ifdef __loongarch_asx
-#include <lasxintrin.h>
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- *               Return Type - signed halfword
- * Details     : Unsigned byte elements from in_h are multiplied with
- *               unsigned byte elements from in_l producing a result
- *               twice the size of input i.e. signed halfword.
- *               Then this multiplied results of adjacent odd-even elements
- *               are added to the out vector
- * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2_h_bu(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_h_bu(in_h, in_l);
-    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of byte vector elements
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- *               Return Type - signed halfword
- * Details     : Signed byte elements from in_h are multiplied with
- *               signed byte elements from in_l producing a result
- *               twice the size of input i.e. signed halfword.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added to the out vector
- * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2_h_b(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_h_b(in_h, in_l);
-    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of halfword vector elements
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- *               Return Type - signed word
- * Details     : Signed halfword elements from in_h are multiplied with
- *               signed halfword elements from in_l producing a result
- *               twice the size of input i.e. signed word.
- *               Then this multiplied results of adjacent odd-even elements
- *               are added to the out vector.
- * Example     : out = __lasx_xvdp2_w_h(in_h, in_l)
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *         out : 22,38,38,22, 22,38,38,22
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_w_h(in_h, in_l);
-    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of word vector elements
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- *               Retrun Type - signed double
- * Details     : Signed word elements from in_h are multiplied with
- *               signed word elements from in_l producing a result
- *               twice the size of input i.e. signed double word.
- *               Then this multiplied results of adjacent odd-even elements
- *               are added to the out vector.
- * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2_d_w(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_d_w(in_h, in_l);
-    out = __lasx_xvmaddwod_d_w(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of halfword vector elements
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- *               Return Type - signed word
- * Details     : Unsigned halfword elements from in_h are multiplied with
- *               signed halfword elements from in_l producing a result
- *               twice the size of input i.e. unsigned word.
- *               Multiplication result of adjacent odd-even elements
- *               are added to the out vector
- * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_w_hu_h(in_h, in_l);
-    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product & addition of byte vector elements
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in_h are multiplied with
- *               signed byte elements from in_l producing a result
- *               twice the size of input i.e. signed halfword.
- *               Then this multiplied results of adjacent odd-even elements
- *               are added to the in_c vector.
- * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2add_h_b(__m256i in_c,__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmaddwev_h_b(in_c, in_h, in_l);
-    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of halfword vector elements
- * Arguments   : Inputs - in_c, in_h, in_l
- *               Output - out
- *               Return Type - per RTYPE
- * Details     : Signed halfword elements from in_h are multiplied with
- *               signed halfword elements from in_l producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added to the in_c vector.
- * Example     : out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
- *        in_c : 1,2,3,4, 1,2,3,4
- *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
- *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
- *         out : 23,40,41,26, 23,40,41,26
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2add_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmaddwev_w_h(in_c, in_h, in_l);
-    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of halfword vector elements
- * Arguments   : Inputs - in_c, in_h, in_l
- *               Output - out
- *               Return Type - signed word
- * Details     : Unsigned halfword elements from in_h are multiplied with
- *               unsigned halfword elements from in_l producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added to the in_c vector.
- * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2add_w_hu(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmaddwev_w_hu(in_c, in_h, in_l);
-    out = __lasx_xvmaddwod_w_hu(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of halfword vector elements
- * Arguments   : Inputs - in_c, in_h, in_l
- *               Output - out
- *               Return Type - signed word
- * Details     : Unsigned halfword elements from in_h are multiplied with
- *               signed halfword elements from in_l producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added to the in_c vector
- * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2add_w_hu_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmaddwev_w_hu_h(in_c, in_h, in_l);
-    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Vector Unsigned Dot Product and Subtract
- * Arguments   : Inputs - in_c, in_h, in_l
- *               Output - out
- *               Return Type - signed halfword
- * Details     : Unsigned byte elements from in_h are multiplied with
- *               unsigned byte elements from in_l producing a result
- *               twice the size of input i.e. signed halfword.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and subtracted from double width elements
- *               in_c vector.
- * Example     : See out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2sub_h_bu(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_h_bu(in_h, in_l);
-    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
-    out = __lasx_xvsub_h(in_c, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Vector Signed Dot Product and Subtract
- * Arguments   : Inputs - in_c, in_h, in_l
- *               Output - out
- *               Return Type - signed word
- * Details     : Signed halfword elements from in_h are multiplied with
- *               Signed halfword elements from in_l producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and subtracted from double width elements
- *               in_c vector.
- * Example     : out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
- *        in_c : 0,0,0,0, 0,0,0,0
- *        in_h : 3,1,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
- *        in_l : 2,1,1,0, 1,0,0,0, 0,0,1,0, 1,0,0,1
- *         out : -7,-3,0,0, 0,-1,0,-1
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_w_h(in_h, in_l);
-    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
-    out = __lasx_xvsub_w(in_c, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Dot product of halfword vector elements
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- *               Return Type - signed word
- * Details     : Signed halfword elements from in_h are iniplied with
- *               signed halfword elements from in_l producing a result
- *               four times the size of input i.e. signed doubleword.
- *               Then this iniplication results of four adjacent elements
- *               are added together and stored to the out vector.
- * Example     : out = __lasx_xvdp4_d_h(in_h, in_l)
- *        in_h :  3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1
- *        in_l : -2,1,1,0, 1,0,0,0, 0,0,1, 0, 1,0,0,1
- *         out : -2,0,1,1
- * =============================================================================
- */
-static inline __m256i __lasx_xvdp4_d_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvmulwev_w_h(in_h, in_l);
-    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
-    out = __lasx_xvhaddw_d_w(out, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : The high half of the vector elements are expanded and
- *               added after being doubled.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_h vector and the in_l vector are added after the
- *               higher half of the two-fold sign extension (signed byte
- *               to signed halfword) and stored to the out vector.
- * Example     : See out = __lasx_xvaddwh_w_h(in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvilvh_b(in_h, in_l);
-    out = __lasx_xvhaddw_h_b(out, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : The high half of the vector elements are expanded and
- *               added after being doubled.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_h vector and the in_l vector are added after the
- *               higher half of the two-fold sign extension (signed halfword
- *               to signed word) and stored to the out vector.
- * Example     : out = __lasx_xvaddwh_w_h(in_h, in_l)
- *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
- *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
- *         out : 1,0,0,-1, 1,0,0, 2
- * =============================================================================
- */
- static inline __m256i __lasx_xvaddwh_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvilvh_h(in_h, in_l);
-    out = __lasx_xvhaddw_w_h(out, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : The low half of the vector elements are expanded and
- *               added after being doubled.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_h vector and the in_l vector are added after the
- *               lower half of the two-fold sign extension (signed byte
- *               to signed halfword) and stored to the out vector.
- * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvaddwl_h_b(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvilvl_b(in_h, in_l);
-    out = __lasx_xvhaddw_h_b(out, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : The low half of the vector elements are expanded and
- *               added after being doubled.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_h vector and the in_l vector are added after the
- *               lower half of the two-fold sign extension (signed halfword
- *               to signed word) and stored to the out vector.
- * Example     : out = __lasx_xvaddwl_w_h(in_h, in_l)
- *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
- *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
- *         out : 5,-1,4,2, 1,0,2,-1
- * =============================================================================
- */
-static inline __m256i __lasx_xvaddwl_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvilvl_h(in_h, in_l);
-    out = __lasx_xvhaddw_w_h(out, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : The low half of the vector elements are expanded and
- *               added after being doubled.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The out vector and the out vector are added after the
- *               lower half of the two-fold zero extension (unsigned byte
- *               to unsigned halfword) and stored to the out vector.
- * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvaddwl_h_bu(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvilvl_b(in_h, in_l);
-    out = __lasx_xvhaddw_hu_bu(out, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : The low half of the vector elements are expanded and
- *               added after being doubled.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_l vector after double zero extension (unsigned byte to
- *               signed halfword)，added to the in_h vector.
- * Example     : See out = __lasx_xvaddw_w_w_h(in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvaddw_h_h_bu(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvsllwil_hu_bu(in_l, 0);
-    out = __lasx_xvadd_h(in_h, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : The low half of the vector elements are expanded and
- *               added after being doubled.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_l vector after double sign extension (signed halfword to
- *               signed word), added to the in_h vector.
- * Example     : out = __lasx_xvaddw_w_w_h(in_h, in_l)
- *        in_h : 0, 1,0,0, -1,0,0,1,
- *        in_l : 2,-1,1,2,  1,0,0,0, 0,0,1,0, 1,0,0,1,
- *         out : 2, 0,1,2, -1,0,1,1,
- * =============================================================================
- */
-static inline __m256i __lasx_xvaddw_w_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i out;
-
-    out = __lasx_xvsllwil_w_h(in_l, 0);
-    out = __lasx_xvadd_w(in_h, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Multiplication and addition calculation after expansion
- *               of the lower half of the vector.
- * Arguments   : Inputs - in_c, in_h, in_l
- *               Output - out
- * Details     : The in_h vector and the in_l vector are multiplied after
- *               the lower half of the two-fold sign extension (signed halfword
- *               to signed word), and the result is added to the vector in_c,
- *               then stored to the out vector.
- * Example     : out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
- *        in_c : 1,2,3,4, 5,6,7,8
- *        in_h : 1,2,3,4, 1,2,3,4, 5,6,7,8, 5,6,7,8
- *        in_l : 200, 300, 400, 500,  2000, 3000, 4000, 5000,
- *              -200,-300,-400,-500, -2000,-3000,-4000,-5000
- *         out : 201, 602,1203,2004, -995, -1794,-2793,-3992
- * =============================================================================
- */
-static inline __m256i __lasx_xvmaddwl_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i tmp0, tmp1, out;
-
-    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
-    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
-    tmp0 = __lasx_xvmul_w(tmp0, tmp1);
-    out  = __lasx_xvadd_w(tmp0, in_c);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Multiplication and addition calculation after expansion
- *               of the higher half of the vector.
- * Arguments   : Inputs - in_c, in_h, in_l
- *               Output - out
- * Details     : The in_h vector and the in_l vector are multiplied after
- *               the higher half of the two-fold sign extension (signed
- *               halfword to signed word), and the result is added to
- *               the vector in_c, then stored to the out vector.
- * Example     : See out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
- * =============================================================================
- */
-static inline __m256i __lasx_xvmaddwh_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
-{
-    __m256i tmp0, tmp1, out;
-
-    tmp0 = __lasx_xvilvh_h(in_h, in_h);
-    tmp1 = __lasx_xvilvh_h(in_l, in_l);
-    tmp0 = __lasx_xvmulwev_w_h(tmp0, tmp1);
-    out  = __lasx_xvadd_w(tmp0, in_c);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Multiplication calculation after expansion of the lower
- *               half of the vector.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_h vector and the in_l vector are multiplied after
- *               the lower half of the two-fold sign extension (signed
- *               halfword to signed word), then stored to the out vector.
- * Example     : out = __lasx_xvmulwl_w_h(in_h, in_l)
- *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
- *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
- *         out : 6,1,3,0, 0,0,1,0
- * =============================================================================
- */
-static inline __m256i __lasx_xvmulwl_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i tmp0, tmp1, out;
-
-    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
-    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
-    out  = __lasx_xvmul_w(tmp0, tmp1);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Multiplication calculation after expansion of the lower
- *               half of the vector.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_h vector and the in_l vector are multiplied after
- *               the lower half of the two-fold sign extension (signed
- *               halfword to signed word), then stored to the out vector.
- * Example     : out = __lasx_xvmulwh_w_h(in_h, in_l)
- *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
- *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
- *         out : 0,0,0,0, 0,0,0,1
- * =============================================================================
- */
-static inline __m256i __lasx_xvmulwh_w_h(__m256i in_h, __m256i in_l)
-{
-    __m256i tmp0, tmp1, out;
-
-    tmp0 = __lasx_xvilvh_h(in_h, in_h);
-    tmp1 = __lasx_xvilvh_h(in_l, in_l);
-    out  = __lasx_xvmulwev_w_h(tmp0, tmp1);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : The low half of the vector elements are expanded and
- *               added saturately after being doubled.
- * Arguments   : Inputs - in_h, in_l
- *               Output - out
- * Details     : The in_h vector adds the in_l vector saturately after the lower
- *               half of the two-fold zero extension (unsigned byte to unsigned
- *               halfword) and the results are stored to the out vector.
- * Example     : out = __lasx_xvsaddw_hu_hu_bu(in_h, in_l)
- *        in_h : 2,65532,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1
- *        in_l : 3,6,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1, 3,18,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
- *         out : 5,65535,4,2, 1,0,0,1, 3,18,4,0, 1,0,0,2,
- * =============================================================================
- */
-static inline __m256i __lasx_xvsaddw_hu_hu_bu(__m256i in_h, __m256i in_l)
-{
-    __m256i tmp1, out;
-    __m256i zero = {0};
-
-    tmp1 = __lasx_xvilvl_b(zero, in_l);
-    out  = __lasx_xvsadd_hu(in_h, tmp1);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Clip all halfword elements of input vector between min & max
- *               out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
- * Arguments   : Inputs  - in    (input vector)
- *                       - min   (min threshold)
- *                       - max   (max threshold)
- *               Outputs - in    (output vector with clipped elements)
- *               Return Type - signed halfword
- * Example     : out = __lasx_xvclip_h(in, min, max)
- *          in : -8,2,280,249, -8,255,280,249, 4,4,4,4, 5,5,5,5
- *         min : 1,1,1,1, 1,1,1,1, 1,1,1,1, 1,1,1,1
- *         max : 9,9,9,9, 9,9,9,9, 9,9,9,9, 9,9,9,9
- *         out : 1,2,9,9, 1,9,9,9, 4,4,4,4, 5,5,5,5
- * =============================================================================
- */
-static inline __m256i __lasx_xvclip_h(__m256i in, __m256i min, __m256i max)
-{
-    __m256i out;
-
-    out = __lasx_xvmax_h(min, in);
-    out = __lasx_xvmin_h(max, out);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Clip all signed halfword elements of input vector
- *               between 0 & 255
- * Arguments   : Inputs  - in   (input vector)
- *               Outputs - out  (output vector with clipped elements)
- *               Return Type - signed halfword
- * Example     : See out = __lasx_xvclamp255_w(in)
- * =============================================================================
- */
-static inline __m256i __lasx_xvclip255_h(__m256i in)
-{
-    __m256i out;
-
-    out = __lasx_xvmaxi_h(in, 0);
-    out = __lasx_xvsat_hu(out, 7);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Clip all signed word elements of input vector
- *               between 0 & 255
- * Arguments   : Inputs - in   (input vector)
- *               Output - out  (output vector with clipped elements)
- *               Return Type - signed word
- * Example     : out = __lasx_xvclamp255_w(in)
- *          in : -8,255,280,249, -8,255,280,249
- *         out :  0,255,255,249,  0,255,255,249
- * =============================================================================
- */
-static inline __m256i __lasx_xvclip255_w(__m256i in)
-{
-    __m256i out;
-
-    out = __lasx_xvmaxi_w(in, 0);
-    out = __lasx_xvsat_wu(out, 7);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Indexed halfword element values are replicated to all
- *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
- *               if 'indx >= 8' use xvsplati_h_*.
- * Arguments   : Inputs - in, idx
- *               Output - out
- * Details     : Idx element value from in vector is replicated to all
- *               elements in out vector.
- *               Valid index range for halfword operation is 0-7
- * Example     : out = __lasx_xvsplati_l_h(in, idx)
- *          in : 20,10,11,12, 13,14,15,16, 0,0,2,0, 0,0,0,0
- *         idx : 0x02
- *         out : 11,11,11,11, 11,11,11,11, 11,11,11,11, 11,11,11,11
- * =============================================================================
- */
-static inline __m256i __lasx_xvsplati_l_h(__m256i in, int idx)
-{
-    __m256i out;
-
-    out = __lasx_xvpermi_q(in, in, 0x02);
-    out = __lasx_xvreplve_h(out, idx);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Indexed halfword element values are replicated to all
- *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
- *               if 'indx >= 8' use xvsplati_h_*.
- * Arguments   : Inputs - in, idx
- *               Output - out
- * Details     : Idx element value from in vector is replicated to all
- *               elements in out vector.
- *               Valid index range for halfword operation is 0-7
- * Example     : out = __lasx_xvsplati_h_h(in, idx)
- *          in : 20,10,11,12, 13,14,15,16, 0,2,0,0, 0,0,0,0
- *         idx : 0x09
- *         out : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
- * =============================================================================
- */
-static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
-{
-    __m256i out;
-
-    out = __lasx_xvpermi_q(in, in, 0x13);
-    out = __lasx_xvreplve_h(out, idx);
-    return out;
-}
-
-/*
- * =============================================================================
- * Description : Transpose 4x4 block with double word elements in vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3
- *               Outputs - _out0, _out1, _out2, _out3
- * Example     : LASX_TRANSPOSE4x4_D
- *         _in0 : 1,2,3,4
- *         _in1 : 1,2,3,4
- *         _in2 : 1,2,3,4
- *         _in3 : 1,2,3,4
- *
- *        _out0 : 1,1,1,1
- *        _out1 : 2,2,2,2
- *        _out2 : 3,3,3,3
- *        _out3 : 4,4,4,4
- * =============================================================================
- */
-#define LASX_TRANSPOSE4x4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                               \
-    __m256i _tmp0, _tmp1, _tmp2, _tmp3;                                         \
-    _tmp0 = __lasx_xvilvl_d(_in1, _in0);                                        \
-    _tmp1 = __lasx_xvilvh_d(_in1, _in0);                                        \
-    _tmp2 = __lasx_xvilvl_d(_in3, _in2);                                        \
-    _tmp3 = __lasx_xvilvh_d(_in3, _in2);                                        \
-    _out0 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x20);                               \
-    _out2 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x31);                               \
-    _out1 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x20);                               \
-    _out3 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x31);                               \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 8x8 block with word elements in vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- * Example     : LASX_TRANSPOSE8x8_W
- *         _in0 : 1,2,3,4,5,6,7,8
- *         _in1 : 2,2,3,4,5,6,7,8
- *         _in2 : 3,2,3,4,5,6,7,8
- *         _in3 : 4,2,3,4,5,6,7,8
- *         _in4 : 5,2,3,4,5,6,7,8
- *         _in5 : 6,2,3,4,5,6,7,8
- *         _in6 : 7,2,3,4,5,6,7,8
- *         _in7 : 8,2,3,4,5,6,7,8
- *
- *        _out0 : 1,2,3,4,5,6,7,8
- *        _out1 : 2,2,2,2,2,2,2,2
- *        _out2 : 3,3,3,3,3,3,3,3
- *        _out3 : 4,4,4,4,4,4,4,4
- *        _out4 : 5,5,5,5,5,5,5,5
- *        _out5 : 6,6,6,6,6,6,6,6
- *        _out6 : 7,7,7,7,7,7,7,7
- *        _out7 : 8,8,8,8,8,8,8,8
- * =============================================================================
- */
-#define LASX_TRANSPOSE8x8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-{                                                                                   \
-    __m256i _s0_m, _s1_m;                                                           \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
-                                                                                    \
-    _s0_m   = __lasx_xvilvl_w(_in2, _in0);                                          \
-    _s1_m   = __lasx_xvilvl_w(_in3, _in1);                                          \
-    _tmp0_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
-    _tmp1_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvh_w(_in2, _in0);                                          \
-    _s1_m   = __lasx_xvilvh_w(_in3, _in1);                                          \
-    _tmp2_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
-    _tmp3_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvl_w(_in6, _in4);                                          \
-    _s1_m   = __lasx_xvilvl_w(_in7, _in5);                                          \
-    _tmp4_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
-    _tmp5_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvh_w(_in6, _in4);                                          \
-    _s1_m   = __lasx_xvilvh_w(_in7, _in5);                                          \
-    _tmp6_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
-    _tmp7_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
-    _out0 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x20);                               \
-    _out1 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x20);                               \
-    _out2 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x20);                               \
-    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x20);                               \
-    _out4 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x31);                               \
-    _out5 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x31);                               \
-    _out6 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x31);                               \
-    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x31);                               \
-}
-
-/*
- * =============================================================================
- * Description : Transpose input 16x8 byte block
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
- *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
- *                         (input 16x8 byte block)
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- *                         (output 8x16 byte block)
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : See LASX_TRANSPOSE16x8_H
- * =============================================================================
- */
-#define LASX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
-                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-{                                                                                    \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
-                                                                                     \
-    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                           \
-    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                           \
-    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                           \
-    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                           \
-    _tmp4_m = __lasx_xvilvl_b(_in10, _in8);                                          \
-    _tmp5_m = __lasx_xvilvl_b(_in11, _in9);                                          \
-    _tmp6_m = __lasx_xvilvl_b(_in14, _in12);                                         \
-    _tmp7_m = __lasx_xvilvl_b(_in15, _in13);                                         \
-    _out0 = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                       \
-    _out1 = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                       \
-    _out2 = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                       \
-    _out3 = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                       \
-    _out4 = __lasx_xvilvl_b(_tmp5_m, _tmp4_m);                                       \
-    _out5 = __lasx_xvilvh_b(_tmp5_m, _tmp4_m);                                       \
-    _out6 = __lasx_xvilvl_b(_tmp7_m, _tmp6_m);                                       \
-    _out7 = __lasx_xvilvh_b(_tmp7_m, _tmp6_m);                                       \
-    _tmp0_m = __lasx_xvilvl_w(_out2, _out0);                                         \
-    _tmp2_m = __lasx_xvilvh_w(_out2, _out0);                                         \
-    _tmp4_m = __lasx_xvilvl_w(_out3, _out1);                                         \
-    _tmp6_m = __lasx_xvilvh_w(_out3, _out1);                                         \
-    _tmp1_m = __lasx_xvilvl_w(_out6, _out4);                                         \
-    _tmp3_m = __lasx_xvilvh_w(_out6, _out4);                                         \
-    _tmp5_m = __lasx_xvilvl_w(_out7, _out5);                                         \
-    _tmp7_m = __lasx_xvilvh_w(_out7, _out5);                                         \
-    _out0 = __lasx_xvilvl_d(_tmp1_m, _tmp0_m);                                       \
-    _out1 = __lasx_xvilvh_d(_tmp1_m, _tmp0_m);                                       \
-    _out2 = __lasx_xvilvl_d(_tmp3_m, _tmp2_m);                                       \
-    _out3 = __lasx_xvilvh_d(_tmp3_m, _tmp2_m);                                       \
-    _out4 = __lasx_xvilvl_d(_tmp5_m, _tmp4_m);                                       \
-    _out5 = __lasx_xvilvh_d(_tmp5_m, _tmp4_m);                                       \
-    _out6 = __lasx_xvilvl_d(_tmp7_m, _tmp6_m);                                       \
-    _out7 = __lasx_xvilvh_d(_tmp7_m, _tmp6_m);                                       \
-}
-
-/*
- * =============================================================================
- * Description : Transpose input 16x8 byte block
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
- *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
- *                         (input 16x8 byte block)
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- *                         (output 8x16 byte block)
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : LASX_TRANSPOSE16x8_H
- *        _in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in2 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in3 : 4,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in4 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in5 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in6 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in7 : 8,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in8 : 9,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        _in9 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *       _in10 : 0,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *       _in11 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *       _in12 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *       _in13 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *       _in14 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *       _in15 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *
- *       _out0 : 1,2,3,4,5,6,7,8,9,1,0,2,3,7,5,6
- *       _out1 : 2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
- *       _out2 : 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
- *       _out3 : 4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4
- *       _out4 : 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
- *       _out5 : 6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
- *       _out6 : 7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
- *       _out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
- * =============================================================================
- */
-#define LASX_TRANSPOSE16x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
-                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
-                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
-   {                                                                                 \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
-    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                  \
-                                                                                     \
-    _tmp0_m = __lasx_xvilvl_h(_in2, _in0);                                           \
-    _tmp1_m = __lasx_xvilvl_h(_in3, _in1);                                           \
-    _tmp2_m = __lasx_xvilvl_h(_in6, _in4);                                           \
-    _tmp3_m = __lasx_xvilvl_h(_in7, _in5);                                           \
-    _tmp4_m = __lasx_xvilvl_h(_in10, _in8);                                          \
-    _tmp5_m = __lasx_xvilvl_h(_in11, _in9);                                          \
-    _tmp6_m = __lasx_xvilvl_h(_in14, _in12);                                         \
-    _tmp7_m = __lasx_xvilvl_h(_in15, _in13);                                         \
-    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
-    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
-    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
-    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
-    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
-    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
-    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
-    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
-    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
-    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
-    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
-    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
-    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
-    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
-    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
-    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
-    _out0 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
-    _out1 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
-    _out2 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
-    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
-                                                                                     \
-    _tmp0_m = __lasx_xvilvh_h(_in2, _in0);                                           \
-    _tmp1_m = __lasx_xvilvh_h(_in3, _in1);                                           \
-    _tmp2_m = __lasx_xvilvh_h(_in6, _in4);                                           \
-    _tmp3_m = __lasx_xvilvh_h(_in7, _in5);                                           \
-    _tmp4_m = __lasx_xvilvh_h(_in10, _in8);                                          \
-    _tmp5_m = __lasx_xvilvh_h(_in11, _in9);                                          \
-    _tmp6_m = __lasx_xvilvh_h(_in14, _in12);                                         \
-    _tmp7_m = __lasx_xvilvh_h(_in15, _in13);                                         \
-    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
-    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
-    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
-    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
-    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
-    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
-    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
-    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
-    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
-    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
-    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
-    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
-    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
-    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
-    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
-    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
-    _out4 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
-    _out5 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
-    _out6 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
-    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 4x4 block with halfword elements in vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3
- *               Outputs - _out0, _out1, _out2, _out3
- *               Return Type - signed halfword
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : See LASX_TRANSPOSE8x8_H
- * =============================================================================
- */
-#define LASX_TRANSPOSE4x4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)     \
-{                                                                                   \
-    __m256i _s0_m, _s1_m;                                                           \
-                                                                                    \
-    _s0_m = __lasx_xvilvl_h(_in1, _in0);                                            \
-    _s1_m = __lasx_xvilvl_h(_in3, _in2);                                            \
-    _out0 = __lasx_xvilvl_w(_s1_m, _s0_m);                                          \
-    _out2 = __lasx_xvilvh_w(_s1_m, _s0_m);                                          \
-    _out1 = __lasx_xvilvh_d(_out0, _out0);                                          \
-    _out3 = __lasx_xvilvh_d(_out2, _out2);                                          \
-}
-
-/*
- * =============================================================================
- * Description : Transpose input 8x8 byte block
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
- *                         (input 8x8 byte block)
- *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
- *                         (output 8x8 byte block)
- * Example     : See LASX_TRANSPOSE8x8_H
- * =============================================================================
- */
-#define LASX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
-                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
-{                                                                                   \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
-    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                          \
-    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                          \
-    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                          \
-    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                          \
-    _tmp4_m = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                    \
-    _tmp5_m = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                    \
-    _tmp6_m = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                    \
-    _tmp7_m = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                    \
-    _out0 = __lasx_xvilvl_w(_tmp6_m, _tmp4_m);                                      \
-    _out2 = __lasx_xvilvh_w(_tmp6_m, _tmp4_m);                                      \
-    _out4 = __lasx_xvilvl_w(_tmp7_m, _tmp5_m);                                      \
-    _out6 = __lasx_xvilvh_w(_tmp7_m, _tmp5_m);                                      \
-    _out1 = __lasx_xvbsrl_v(_out0, 8);                                              \
-    _out3 = __lasx_xvbsrl_v(_out2, 8);                                              \
-    _out5 = __lasx_xvbsrl_v(_out4, 8);                                              \
-    _out7 = __lasx_xvbsrl_v(_out6, 8);                                              \
-}
-
-/*
- * =============================================================================
- * Description : Transpose 8x8 block with halfword elements in vectors.
- * Arguments   : Inputs  - _in0, _in1, ~
- *               Outputs - _out0, _out1, ~
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : LASX_TRANSPOSE8x8_H
- *        _in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        _in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
- *        _in2 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
- *        _in3 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        _in4 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
- *        _in5 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        _in6 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *        _in7 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
- *
- *       _out0 : 1,8,8,1, 9,1,1,9, 1,8,8,1, 9,1,1,9
- *       _out1 : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
- *       _out2 : 3,3,3,3, 3,3,3,3, 3,3,3,3, 3,3,3,3
- *       _out3 : 4,4,4,4, 4,4,4,4, 4,4,4,4, 4,4,4,4
- *       _out4 : 5,5,5,5, 5,5,5,5, 5,5,5,5, 5,5,5,5
- *       _out5 : 6,6,6,6, 6,6,6,6, 6,6,6,6, 6,6,6,6
- *       _out6 : 7,7,7,7, 7,7,7,7, 7,7,7,7, 7,7,7,7
- *       _out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
- * =============================================================================
- */
-#define LASX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
-                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
-{                                                                                   \
-    __m256i _s0_m, _s1_m;                                                           \
-    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
-    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
-                                                                                    \
-    _s0_m   = __lasx_xvilvl_h(_in6, _in4);                                          \
-    _s1_m   = __lasx_xvilvl_h(_in7, _in5);                                          \
-    _tmp0_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
-    _tmp1_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvh_h(_in6, _in4);                                          \
-    _s1_m   = __lasx_xvilvh_h(_in7, _in5);                                          \
-    _tmp2_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
-    _tmp3_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
-                                                                                    \
-    _s0_m   = __lasx_xvilvl_h(_in2, _in0);                                          \
-    _s1_m   = __lasx_xvilvl_h(_in3, _in1);                                          \
-    _tmp4_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
-    _tmp5_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
-    _s0_m   = __lasx_xvilvh_h(_in2, _in0);                                          \
-    _s1_m   = __lasx_xvilvh_h(_in3, _in1);                                          \
-    _tmp6_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
-    _tmp7_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
-                                                                                    \
-    _out0 = __lasx_xvpickev_d(_tmp0_m, _tmp4_m);                                    \
-    _out2 = __lasx_xvpickev_d(_tmp1_m, _tmp5_m);                                    \
-    _out4 = __lasx_xvpickev_d(_tmp2_m, _tmp6_m);                                    \
-    _out6 = __lasx_xvpickev_d(_tmp3_m, _tmp7_m);                                    \
-    _out1 = __lasx_xvpickod_d(_tmp0_m, _tmp4_m);                                    \
-    _out3 = __lasx_xvpickod_d(_tmp1_m, _tmp5_m);                                    \
-    _out5 = __lasx_xvpickod_d(_tmp2_m, _tmp6_m);                                    \
-    _out7 = __lasx_xvpickod_d(_tmp3_m, _tmp7_m);                                    \
-}
-
-/*
- * =============================================================================
- * Description : Butterfly of 4 input vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3
- *               Outputs - _out0, _out1, _out2, _out3
- * Details     : Butterfly operation
- * Example     : LASX_BUTTERFLY_4
- *               _out0 = _in0 + _in3;
- *               _out1 = _in1 + _in2;
- *               _out2 = _in1 - _in2;
- *               _out3 = _in0 - _in3;
- * =============================================================================
- */
-#define LASX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
-{                                                                               \
-    _out0 = __lasx_xvadd_b(_in0, _in3);                                         \
-    _out1 = __lasx_xvadd_b(_in1, _in2);                                         \
-    _out2 = __lasx_xvsub_b(_in1, _in2);                                         \
-    _out3 = __lasx_xvsub_b(_in0, _in3);                                         \
-}
-#define LASX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
-{                                                                               \
-    _out0 = __lasx_xvadd_h(_in0, _in3);                                         \
-    _out1 = __lasx_xvadd_h(_in1, _in2);                                         \
-    _out2 = __lasx_xvsub_h(_in1, _in2);                                         \
-    _out3 = __lasx_xvsub_h(_in0, _in3);                                         \
-}
-#define LASX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
-{                                                                               \
-    _out0 = __lasx_xvadd_w(_in0, _in3);                                         \
-    _out1 = __lasx_xvadd_w(_in1, _in2);                                         \
-    _out2 = __lasx_xvsub_w(_in1, _in2);                                         \
-    _out3 = __lasx_xvsub_w(_in0, _in3);                                         \
-}
-#define LASX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
-{                                                                               \
-    _out0 = __lasx_xvadd_d(_in0, _in3);                                         \
-    _out1 = __lasx_xvadd_d(_in1, _in2);                                         \
-    _out2 = __lasx_xvsub_d(_in1, _in2);                                         \
-    _out3 = __lasx_xvsub_d(_in0, _in3);                                         \
-}
-
-/*
- * =============================================================================
- * Description : Butterfly of 8 input vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
- *               Outputs - _out0, _out1, _out2, _out3, ~
- * Details     : Butterfly operation
- * Example     : LASX_BUTTERFLY_8
- *               _out0 = _in0 + _in7;
- *               _out1 = _in1 + _in6;
- *               _out2 = _in2 + _in5;
- *               _out3 = _in3 + _in4;
- *               _out4 = _in3 - _in4;
- *               _out5 = _in2 - _in5;
- *               _out6 = _in1 - _in6;
- *               _out7 = _in0 - _in7;
- * =============================================================================
- */
-#define LASX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    _out0 = __lasx_xvadd_b(_in0, _in7);                                           \
-    _out1 = __lasx_xvadd_b(_in1, _in6);                                           \
-    _out2 = __lasx_xvadd_b(_in2, _in5);                                           \
-    _out3 = __lasx_xvadd_b(_in3, _in4);                                           \
-    _out4 = __lasx_xvsub_b(_in3, _in4);                                           \
-    _out5 = __lasx_xvsub_b(_in2, _in5);                                           \
-    _out6 = __lasx_xvsub_b(_in1, _in6);                                           \
-    _out7 = __lasx_xvsub_b(_in0, _in7);                                           \
-}
-
-#define LASX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    _out0 = __lasx_xvadd_h(_in0, _in7);                                           \
-    _out1 = __lasx_xvadd_h(_in1, _in6);                                           \
-    _out2 = __lasx_xvadd_h(_in2, _in5);                                           \
-    _out3 = __lasx_xvadd_h(_in3, _in4);                                           \
-    _out4 = __lasx_xvsub_h(_in3, _in4);                                           \
-    _out5 = __lasx_xvsub_h(_in2, _in5);                                           \
-    _out6 = __lasx_xvsub_h(_in1, _in6);                                           \
-    _out7 = __lasx_xvsub_h(_in0, _in7);                                           \
-}
-
-#define LASX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    _out0 = __lasx_xvadd_w(_in0, _in7);                                           \
-    _out1 = __lasx_xvadd_w(_in1, _in6);                                           \
-    _out2 = __lasx_xvadd_w(_in2, _in5);                                           \
-    _out3 = __lasx_xvadd_w(_in3, _in4);                                           \
-    _out4 = __lasx_xvsub_w(_in3, _in4);                                           \
-    _out5 = __lasx_xvsub_w(_in2, _in5);                                           \
-    _out6 = __lasx_xvsub_w(_in1, _in6);                                           \
-    _out7 = __lasx_xvsub_w(_in0, _in7);                                           \
-}
-
-#define LASX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
-                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
-{                                                                                 \
-    _out0 = __lasx_xvadd_d(_in0, _in7);                                           \
-    _out1 = __lasx_xvadd_d(_in1, _in6);                                           \
-    _out2 = __lasx_xvadd_d(_in2, _in5);                                           \
-    _out3 = __lasx_xvadd_d(_in3, _in4);                                           \
-    _out4 = __lasx_xvsub_d(_in3, _in4);                                           \
-    _out5 = __lasx_xvsub_d(_in2, _in5);                                           \
-    _out6 = __lasx_xvsub_d(_in1, _in6);                                           \
-    _out7 = __lasx_xvsub_d(_in0, _in7);                                           \
-}
-
-/*
- ****************************************************************************
- ***************  Non-generic macro definition ******************************
- ****************************************************************************
- */
-
-/* Description : Horizontal addition of 8 signed word elements of input vector
- * Arguments   : Input  - in       (signed word vector)
- *               Output - sum_m    (s32 sum)
- * Details     : 8 signed word elements of 'in' vector are added together and
- *               the resulting integer sum is returned
- */
-#define LASX_HADD_SW_S32( in )                               \
-( {                                                          \
-    int32_t s_sum_m;                                         \
-    v4i64  out;                                              \
-                                                             \
-    out = __lasx_xvhaddw_d_w( in, in );                      \
-    s_sum_m = out[0] + out[1] + out[2] + out[3];             \
-    s_sum_m;                                                 \
-} )
-
-/* Description : Horizontal addition of 16 half word elements of input vector
- * Arguments   : Input  - in       (half word vector)
- *               Output - sum_m    (i32 sum)
- * Details     : 16 half word elements of 'in' vector are added together and
- *               the resulting integer sum is returned
- */
-#define LASX_HADD_UH_U32( in )                               \
-( {                                                          \
-    uint32_t u_sum_m;                                        \
-    v4u64  out;                                              \
-    __m256i res_m;                                           \
-                                                             \
-    res_m = __lasx_xvhaddw_wu_hu( in, in );                  \
-    out = ( v4u64 )__lasx_xvhaddw_du_wu( res_m, res_m );     \
-    u_sum_m = out[0] + out[1] + out[2] + out[3];             \
-    u_sum_m;                                                 \
-} )
-
-/* Description : Sign extend halfword elements from input vector and return
- *               the result in pair of vectors
- * Arguments   : Input  - in            (halfword vector)
- *               Outputs - out0, out1   (sign extended word vectors)
- *               Return Type - signed word
- * Details     : Sign bit of halfword elements from input vector 'in' is
- *               extracted and interleaved right with same vector 'in0' to
- *               generate 4 signed word elements in 'out0'
- *               Then interleaved left with same vector 'in0' to
- *               generate 4 signed word elements in 'out1'
- */
-#define LASX_UNPCK_SH( in, out0, out1 )         \
-{                                               \
-    __m256i tmp_m;                              \
-                                                \
-    tmp_m = __lasx_xvslti_h( in, 0 );           \
-    out0 = __lasx_xvilvl_h( tmp_m, in );        \
-    out1 = __lasx_xvilvh_h( tmp_m, in );        \
-}
-
-/*
- * Description : Transpose 8x4 block with half word elements in vectors
- * Arguments   : Inputs  - _in0, _in1, _in2, _in3
- *               Outputs - _out0, _out1, _out2, _out3
- * Example     : LASX_TRANSPOSE8x4_H
- *        _in0 : 1,2,3,4, 5,6,7,8, 2,2,2,2, 2,2,2,2
- *        _in1 : 8,2,3,4, 5,6,7,8, 3,3,3,3, 3,3,3,3
- *        _in2 : 8,2,3,4, 5,6,7,8, 4,4,4,4, 4,4,4,4
- *        _in3 : 1,2,3,4, 5,6,7,8, 0,0,0,0, 0,0,0,0
- *
- *       _out0 : 1,8,8,1, 2,2,2,2, 2,3,4,0, 2,3,4,0
- *       _out1 : 3,3,3,3, 4,4,4,4, 2,3,4,0, 2,3,4,0
- *       _out2 : 5,5,5,5, 6,6,6,6, 2,3,4,0, 2,3,4,0
- *       _out3 : 7,7,7,7, 8,8,8,8, 2,3,4,0, 2,3,4,0
- */
-#define LASX_TRANSPOSE8X4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
-{                                                                               \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                     \
-                                                                                \
-    tmp0_m = __lasx_xvilvl_h(_in1, _in0);                                       \
-    tmp1_m = __lasx_xvilvl_h(_in3, _in2);                                       \
-    tmp2_m = __lasx_xvilvh_h(_in1, _in0);                                       \
-    tmp3_m = __lasx_xvilvh_h(_in3, _in2);                                       \
-    _out0 = __lasx_xvilvl_w(tmp1_m, tmp0_m);                                    \
-    _out2 = __lasx_xvilvl_w(tmp3_m, tmp2_m);                                    \
-    _out1 = __lasx_xvilvh_w(tmp1_m, tmp0_m);                                    \
-    _out3 = __lasx_xvilvh_w(tmp3_m, tmp2_m);                                    \
-}
-
-#endif //LASX
-
-#endif /* LOONGSON_INTRINSICS_H */
-
diff --git a/common/loongarch/mc-a.S b/common/loongarch/mc-a.S
index 7a373ffa..25246077 100644
--- a/common/loongarch/mc-a.S
+++ b/common/loongarch/mc-a.S
@@ -828,7 +828,7 @@ function mc_weight_w8_lasx
     xvsll.h          xr1,     xr1,   xr2
     xvxor.v          xr10,    xr10,  xr10
     xvaddi.hu        xr10,    xr10,  1
-    xvsub.h        xr9,     xr2,   xr10
+    xvsub.h          xr9,     xr2,   xr10
     xvsll.h          xr11,    xr10,  xr9
     xvadd.h          xr1,     xr1,   xr11
 .LOOP_WEIGHTW8:
@@ -876,4 +876,529 @@ function mc_weight_w4_lasx
     addi.w           a5,      a5,    -2
     blt              zero,    a5,    .LOOP_WEIGHTW4
 endfunc
+
+/*
+ * void x264_pixel_avg2_w4(uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+ *                         intptr_t i_src_stride, uint8_t *src2, int i_height)
+ */
+function pixel_avg2_w4_lasx
+.avg2w4_loop_2:
+    addi.d           a5,      a5,    -2
+    fld.s            f0,      a2,    0
+    fld.s            f1,      a4,    0
+    fldx.s           f2,      a2,    a3
+    fldx.s           f3,      a4,    a3
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a3,    a4,   1
+    vavgr.bu         vr0,     vr0,   vr1
+    vavgr.bu         vr1,     vr2,   vr3
+    fst.s            f0,      a0,    0
+    fstx.s           f1,      a0,    a1
+    alsl.d           a0,      a1,    a0,     1
+    blt              zero,    a5,    .avg2w4_loop_2
+endfunc
+
+/*
+ * void x264_pixel_avg2_w8(uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+ *                         intptr_t i_src_stride, uint8_t *src2, int i_height)
+ */
+function pixel_avg2_w8_lasx
+.avg2w8_loop_2:
+    addi.d           a5,      a5,    -2
+    fld.d            f0,      a2,    0
+    fld.d            f1,      a4,    0
+    fldx.d           f2,      a2,    a3
+    fldx.d           f3,      a4,    a3
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a3,    a4,   1
+    vavgr.bu         vr0,     vr0,   vr1
+    vavgr.bu         vr1,     vr2,   vr3
+    fst.d            f0,      a0,    0
+    fstx.d           f1,      a0,    a1
+    alsl.d           a0,      a1,    a0,   1
+    blt              zero,    a5,    .avg2w8_loop_2
+endfunc
+
+/*
+ * void x264_pixel_avg2_w16(uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+ *                          intptr_t i_src_stride, uint8_t *src2, int i_height)
+ */
+function pixel_avg2_w16_lasx
+.avg2w16_loop_2:
+    addi.d           a5,      a5,    -2
+    vld              vr0,     a2,    0
+    vldx             vr1,     a2,    a3
+    vld              vr2,     a4,    0
+    vldx             vr3,     a4,    a3
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a3,    a4,   1
+    vavgr.bu         vr0,     vr0,   vr2
+    vavgr.bu         vr1,     vr1,   vr3
+    vst              vr0,     a0,    0
+    vstx             vr1,     a0,    a1
+    alsl.d           a0,      a1,    a0,   1
+    blt              zero,    a5,    .avg2w16_loop_2
+endfunc
+
+/*
+ * void x264_pixel_avg2_w20(uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+ *                          intptr_t i_src_stride, uint8_t *src2, int i_height)
+ */
+function pixel_avg2_w20_lasx
+.avg2w20_loop_2:
+    addi.d           a5,      a5,    -2
+    xvld             xr0,     a2,    0
+    xvldx            xr1,     a2,    a3
+    xvld             xr2,     a4,    0
+    xvldx            xr3,     a4,    a3
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a3,    a4,   1
+    xvavgr.bu        xr0,     xr0,   xr2
+    xvavgr.bu        xr1,     xr1,   xr3
+    vst              vr0,     a0,    0
+    xvstelm.w        xr0,     a0,    16,    4
+    add.d            a0,      a0,    a1
+    vst              vr1,     a0,    0
+    xvstelm.w        xr1,     a0,    16,    4
+    add.d            a0,      a0,    a1
+    blt              zero,    a5,    .avg2w20_loop_2
+endfunc
+
+/*
+ * void mc_copy_width16( uint8_t *p_dst, int32_t i_dst_stride,
+ *                       uint8_t *p_src, int32_t i_src_stride,
+ *                       int32_t i_height )
+ */
+function mc_copy_w16_lasx
+    slli.d            t0,     a3,     1
+    add.d             t1,     t0,     a3
+    slli.d            t2,     a1,     1
+    add.d             t3,     t2,     a1
+.LOOP_COPYW16:
+    vld               vr1,    a2,    0
+    vldx              vr2,    a2,    a3
+    vldx              vr3,    a2,    t0
+    vldx              vr4,    a2,    t1
+
+    vst               vr1,    a0,    0
+    vstx              vr2,    a0,    a1
+    vstx              vr3,    a0,    t2
+    vstx              vr4,    a0,    t3
+    alsl.d            a0,     a1,    a0,   2
+    alsl.d            a2,     a3,    a2,   2
+    addi.w            a4,     a4,    -4
+    bnez              a4,    .LOOP_COPYW16
+endfunc
+
+/*
+ * void mc_copy_w8( uint8_t *p_dst, intptr_t i_dst_stride,
+ *                  uint8_t *p_src, intptr_t i_src_stride,
+ *                  int32_t i_height )
+ */
+function mc_copy_w8_lasx
+    slli.d            t0,     a3,     1
+    add.d             t1,     t0,     a3
+    slli.d            t2,     a1,     1
+    add.d             t3,     t2,     a1
+.LOOP_COPYW8:
+    fld.d             f0,     a2,     0
+    fldx.d            f1,     a2,     a3
+    fldx.d            f2,     a2,     t0
+    fldx.d            f3,     a2,     t1
+
+    fst.d             f0,     a0,     0
+    fstx.d            f1,     a0,     a1
+    fstx.d            f2,     a0,     t2
+    fstx.d            f3,     a0,     t3
+    alsl.d            a0,     a1,     a0,   2
+    alsl.d            a2,     a3,     a2,   2
+    addi.w            a4,     a4,     -4
+    bnez              a4,     .LOOP_COPYW8
+endfunc
+
+/*
+ * void mc_copy_w4( uint8_t *p_dst, intptr_t i_dst_stride,
+ *                  uint8_t *p_src, intptr_t i_src_stride,
+ *                  int32_t i_height )
+ */
+function mc_copy_w4_lasx
+    slli.d            t0,     a3,     1
+    add.d             t1,     t0,     a3
+    slli.d            t2,     a1,     1
+    add.d             t3,     t2,     a1
+.LOOP_COPYW4:
+    fld.s             f0,     a2,     0
+    fldx.s            f1,     a2,     a3
+    fldx.s            f2,     a2,     t0
+    fldx.s            f3,     a2,     t1
+
+    fst.s             f0,     a0,     0
+    fstx.s            f1,     a0,     a1
+    fstx.s            f2,     a0,     t2
+    fstx.s            f3,     a0,     t3
+    alsl.d            a0,     a1,     a0,   2
+    alsl.d            a2,     a3,     a2,   2
+    addi.w            a4,     a4,     -4
+    bnez              a4,     .LOOP_COPYW4
+endfunc
+
+/*
+ * void store_interleave_chroma( uint8_t *p_dst, intptr_t i_dst_stride,
+ *                               uint8_t *p_src0, uint8_t *p_src1,
+ *                               int32_t i_height )
+ */
+function store_interleave_chroma_lasx
+.LOOP_interleave_chroma:
+    fld.d             f0,     a2,    0
+    fld.d             f1,     a3,    0
+    addi.d            a2,     a2,    FDEC_STRIDE
+    addi.d            a3,     a3,    FDEC_STRIDE
+    vilvl.b           vr0,    vr1,   vr0
+    vst               vr0,    a0,    0
+    add.d             a0,     a0,    a1
+    addi.w            a4,     a4,    -1
+    blt               zero,   a4,    .LOOP_interleave_chroma
+endfunc
+
+/*
+ * void load_deinterleave_chroma_fenc( pixel *dst, pixel *src,
+ *                                     intptr_t i_src, int height )
+ */
+function load_deinterleave_chroma_fenc_lasx
+    addi.d            t0,     a0,     FENC_STRIDE/2
+    andi              t1,     a3,     1
+    sub.w             t2,     a3,     t1
+.LOOP_deinterleave_fenc:
+    vld               vr0,    a1,     0
+    vldx              vr1,    a1,     a2
+    vpickev.b         vr2,    vr1,    vr0
+    vpickod.b         vr3,    vr1,    vr0
+    fst.d             f2,     a0,     0
+    fst.d             f3,     t0,     0
+    vstelm.d          vr2,    a0,     FENC_STRIDE,   1
+    vstelm.d          vr3,    t0,     FENC_STRIDE,   1
+    addi.d            a0,     a0,     FENC_STRIDE * 2
+    addi.d            t0,     t0,     FENC_STRIDE * 2
+    alsl.d            a1,     a2,     a1,    1
+    addi.w            t2,     t2,     -2
+    blt               zero,   t2,     .LOOP_deinterleave_fenc
+
+    beqz              t1,     .LOOP_deinterleave_fenc_end
+    vld               vr0,    a1,     0
+    vpickev.b         vr1,    vr0,    vr0
+    vpickod.b         vr2,    vr0,    vr0
+    fst.d             f1,     a0,     0
+    fst.d             f2,     t0,     0
+.LOOP_deinterleave_fenc_end:
+endfunc
+
+/*
+ * void load_deinterleave_chroma_fdec( pixel *dst, pixel *src,
+ *                                     intptr_t i_src, int height )
+ */
+function load_deinterleave_chroma_fdec_lasx
+    addi.d            t0,     a0,     FDEC_STRIDE/2
+    andi              t1,     a3,     1
+    sub.w             t2,     a3,     t1
+.LOOP_deinterleave_fdec:
+    vld               vr0,    a1,     0
+    vldx              vr1,    a1,     a2
+    vpickev.b         vr2,    vr1,    vr0
+    vpickod.b         vr3,    vr1,    vr0
+    fst.d             f2,     a0,     0
+    fst.d             f3,     t0,     0
+    vstelm.d          vr2,    a0,     FDEC_STRIDE,   1
+    vstelm.d          vr3,    t0,     FDEC_STRIDE,   1
+    addi.d            a0,     a0,     FDEC_STRIDE * 2
+    addi.d            t0,     t0,     FDEC_STRIDE * 2
+    alsl.d            a1,     a2,     a1,    1
+    addi.w            t2,     t2,     -2
+    blt               zero,   t2,     .LOOP_deinterleave_fdec
+
+    beqz              t1,     .LOOP_deinterleave_fdec_end
+    vld               vr0,    a1,     0
+    vpickev.b         vr1,    vr0,    vr0
+    vpickod.b         vr2,    vr0,    vr0
+    fst.d             f1,     a0,     0
+    fst.d             f2,     t0,     0
+.LOOP_deinterleave_fdec_end:
+endfunc
+
+/*
+ * x264_plane_copy_interleave_c( pixel *dst,  intptr_t i_dst,
+ *                               pixel *srcu, intptr_t i_srcu,
+ *                               pixel *srcv, intptr_t i_srcv, int w, int h )
+ */
+function plane_copy_interleave_core_lasx
+.LOOP_h:
+    add.d             t0,     a0,     zero
+    add.d             t2,     a2,     zero
+    add.d             t4,     a4,     zero
+    add.d             t6,     a6,     zero
+.LOOP_copy_interleavew16:
+    vld               vr0,    t2,     0
+    vld               vr1,    t4,     0
+    vilvl.b           vr2,    vr1,    vr0
+    vilvh.b           vr3,    vr1,    vr0
+    vst               vr2,    t0,     0
+    vst               vr3,    t0,     16
+    addi.d            t2,     t2,     16
+    addi.d            t4,     t4,     16
+    addi.d            t0,     t0,     32
+    addi.w            t6,     t6,     -16
+    blt               zero,   t6,     .LOOP_copy_interleavew16
+
+    add.d             a2,     a2,     a3
+    add.d             a4,     a4,     a5
+    add.d             a0,     a0,     a1
+    addi.w            a7,     a7,     -1
+    blt               zero,   a7,     .LOOP_h
+endfunc
+
+/*
+ * void x264_plane_copy_deinterleave_c( pixel *dsta, intptr_t i_dsta,
+ *                                      pixel *dstb, intptr_t i_dstb,
+ *                                      pixel *src,  intptr_t i_src, int w, int h )
+ */
+function plane_copy_deinterleave_lasx
+.LOOP_deinterleave_h:
+    add.d             t0,     a0,     zero
+    add.d             t2,     a2,     zero
+    add.d             t4,     a4,     zero
+    add.d             t6,     a6,     zero
+.LOOP_copy_deinterleavew16:
+    vld               vr0,    t4,     0
+    vld               vr1,    t4,     16
+    vpickev.b         vr2,    vr1,    vr0
+    vpickod.b         vr3,    vr1,    vr0
+    vst               vr2,    t0,     0
+    vst               vr3,    t2,     0
+    addi.d            t4,     t4,     32
+    addi.d            t0,     t0,     16
+    addi.d            t2,     t2,     16
+    addi.w            t6,     t6,     -16
+    blt               zero,   t6,     .LOOP_copy_deinterleavew16
+
+    add.d             a2,     a2,     a3
+    add.d             a4,     a4,     a5
+    add.d             a0,     a0,     a1
+    addi.w            a7,     a7,     -1
+    blt               zero,   a7,     .LOOP_deinterleave_h
+endfunc
+
+/*
+ * void prefetch_ref( uint8_t *pix, intptr_t stride, int32_t parity )
+ */
+function prefetch_ref_lasx
+    addi.d            a2,     a2,     -1
+    addi.d            a0,     a0,     64
+    and               a2,     a2,     a1
+    alsl.d            t1,     a2,     a0,   3
+    alsl.d            a2,     a1,     a1,   1
+    preld             0,      t1,     0
+    add.d             t2,     t1,     a1
+    preld             0,      t2,     0
+    add.d             t2,     t2,     a1
+    preld             0,      t2,     0
+    add.d             t1,     t1,     a2
+    preld             0,      t1,     0
+    alsl.d            a0,     a1,     t2,   1
+    preld             0,      a0,     0
+    add.d             t1,     a0,     a1
+    preld             0,      t1,     0
+    add.d             t1,     t1,     a1
+    preld             0,      t1,     0
+    add.d             a0,     a0,     a2
+    preld             0,      a0,     0
+endfunc
+
+/*
+ * void prefetch_fenc_422( uint8_t *pix_y, intptr_t stride_y,
+ *                         uint8_t *pix_uv, intptr_t stride_uv,
+ *                         int32_t mb_x )
+ */
+function prefetch_fenc_422_lasx
+    andi              t0,     a4,     3
+    mul.d             t0,     t0,     a1
+    andi              a4,     a4,     6
+    mul.d             t1,     a4,     a3
+    addi.d            a0,     a0,     64
+    addi.d            a2,     a2,     64
+    alsl.d            a0,     t0,     a0,   2
+    preld             0,      a0,     0
+    add.d             t2,     a0,     a1
+    preld             0,      t2,     0
+    add.d             a0,     t2,     a1
+    preld             0,      a0,     0
+    add.d             a0,     a0,     a1
+    preld             0,      a0,     0
+    alsl.d            a2,     t1,     a2,   2
+    preld             0,      a2,     0
+    add.d             t3,     a2,     a3
+    preld             0,      t3,     0
+    add.d             a2,     t3,     a3
+    preld             0,      a2,     0
+    add.d             a2,     a2,     a3
+    preld             0,      a2,     0
+endfunc
+
+/*
+ * void prefetch_fenc_420( uint8_t *pix_y, intptr_t stride_y,
+ *                         uint8_t *pix_uv, intptr_t stride_uv,
+ *                         int32_t mb_x )
+ */
+function prefetch_fenc_420_lasx
+    andi             t0,      a4,     3
+    mul.d            t0,      t0,     a1
+    andi             a4,      a4,     6
+    mul.d            t1,      a4,     a3
+    addi.d           a0,      a0,     64
+    addi.d           a2,      a2,     64
+    alsl.d           a0,      t0,     a0,   2
+    preld            0,       a0,     0
+    add.d            t2,      a0,     a1
+    preld            0,       t2,     0
+    add.d            a0,      t2,     a1
+    preld            0,       a0,     0
+    add.d            a0,      a0,     a1
+    preld            0,       a0,     0
+    alsl.d           a2,      t1,     a2,   2
+    preld            0,       a2,     0
+    add.d            a2,      a2,     a3
+    preld            0,       a2,     0
+endfunc
+
+/*
+ * void *memcpy_aligned(void *dst, const void *src, size_t n)
+ */
+function memcpy_aligned_lasx
+    andi             t0,      a2,     16
+    beqz             t0,      2f
+    addi.d           a2,      a2,     -16
+    vld              vr0,     a1,     0
+    vst              vr0,     a0,     0
+    addi.d           a1,      a1,     16
+    addi.d           a0,      a0,     16
+2:
+    andi             t0,      a2,     32
+    beqz             t0,      3f
+    addi.d           a2,      a2,     -32
+    xvld             xr0,     a1,     0
+    xvst             xr0,     a0,     0
+    addi.d           a1,      a1,     32
+    addi.d           a0,      a0,     32
+3:
+    beqz             a2,      5f
+4:
+    addi.d           a2,      a2,     -64
+    xvld             xr0,     a1,     32
+    xvld             xr1,     a1,     0
+    xvst             xr0,     a0,     32
+    xvst             xr1,     a0,     0
+    addi.d           a1,      a1,     64
+    addi.d           a0,      a0,     64
+    blt              zero,    a2,     4b
+5:
+endfunc
+
+/*
+ * void memzero_aligned( void *p_dst, size_t n )
+ */
+function memzero_aligned_lasx
+    xvxor.v          xr1,     xr1,    xr1
+.memzero_loop:
+    addi.d           a1,      a1,     -128
+.rept 4
+    xvst             xr1,     a0,     0
+    addi.d           a0,      a0,     32
+.endr
+    blt              zero,    a1,     .memzero_loop
+endfunc
+
+/*
+ * void frame_init_lowres_core( pixel *src0, pixel *dst0, pixel *dsth,
+ *                              pixel *dstv, pixel *dstc, intptr_t src_stride,
+ *                              intptr_t dst_stride, int width, int height )
+ */
+function frame_init_lowres_core_lasx
+    andi             t1,      a7,     15
+    sub.w            t0,      a7,     t1
+    slli.d           t2,      a5,     1
+    ldptr.w          a7,      sp,     0  // use a7 as height variable
+
+.height_loop:
+    add.d            t4,      zero,   t0
+    addi.d           t3,      a0,     0
+    addi.d           t5,      a1,     0
+    addi.d           t6,      a2,     0
+    addi.d           t7,      a3,     0
+    addi.d           t8,      a4,     0
+.width16_loop:
+    xvld             xr0,     t3,     0
+    xvldx            xr1,     t3,     a5
+    xvldx            xr2,     t3,     t2
+    xvavgr.bu        xr3,     xr0,    xr1
+    xvavgr.bu        xr4,     xr1,    xr2
+    xvhaddw.hu.bu    xr5,     xr3,    xr3
+    xvhaddw.hu.bu    xr6,     xr4,    xr4
+    xvssrarni.bu.h   xr6,     xr5,    1
+    xvpermi.d        xr7,     xr6,    0xd8
+    vst              vr7,     t5,     0
+    xvpermi.q        xr7,     xr7,    0x11
+    vst              vr7,     t7,     0
+
+    addi.d           t3,      t3,     1
+    xvld             xr0,     t3,     0
+    xvldx            xr1,     t3,     a5
+    xvldx            xr2,     t3,     t2
+    xvavgr.bu        xr3,     xr0,    xr1
+    xvavgr.bu        xr4,     xr1,    xr2
+    xvhaddw.hu.bu    xr5,     xr3,    xr3
+    xvhaddw.hu.bu    xr6,     xr4,    xr4
+    xvssrarni.bu.h   xr6,     xr5,    1
+    xvpermi.d        xr7,     xr6,    0xd8
+    vst              vr7,     t6,     0
+    xvpermi.q        xr7,     xr7,    0x11
+    vst              vr7,     t8,     0
+    addi.d           t3,      t3,     31
+    addi.d           t5,      t5,     16
+    addi.d           t6,      t6,     16
+    addi.d           t7,      t7,     16
+    addi.d           t8,      t8,     16
+    addi.w           t4,      t4,     -16
+    blt              zero,    t4,     .width16_loop
+
+    beqz             t1,      .width16_end
+    vld              vr0,     t3,     0
+    vldx             vr1,     t3,     a5
+    vldx             vr2,     t3,     t2
+    vavgr.bu         vr3,     vr0,    vr1
+    vavgr.bu         vr4,     vr1,    vr2
+    vhaddw.hu.bu     vr5,     vr3,    vr3
+    vhaddw.hu.bu     vr6,     vr4,    vr4
+    vssrarni.bu.h    vr6,     vr5,    1
+    fst.d            f6,      t5,     0
+    vstelm.d         vr6,     t7,     0,    1
+
+    addi.d           t3,      t3,     1
+    vld              vr0,     t3,     0
+    vldx             vr1,     t3,     a5
+    vldx             vr2,     t3,     t2
+    vavgr.bu         vr3,     vr0,    vr1
+    vavgr.bu         vr4,     vr1,    vr2
+    vhaddw.hu.bu     vr5,     vr3,    vr3
+    vhaddw.hu.bu     vr6,     vr4,    vr4
+    vssrarni.bu.h    vr6,     vr5,    1
+    fst.d            f6,      t6,     0
+    vstelm.d         vr6,     t8,     0,    1
+
+.width16_end:
+    add.d            a0,      a0,     t2
+    add.d            a1,      a1,     a6
+    add.d            a2,      a2,     a6
+    add.d            a3,      a3,     a6
+    add.d            a4,      a4,     a6
+    addi.w           a7,      a7,     -1
+    blt              zero,    a7,     .height_loop
+endfunc
 #endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/mc-c.c b/common/loongarch/mc-c.c
index 8933b8c2..6d63ee59 100644
--- a/common/loongarch/mc-c.c
+++ b/common/loongarch/mc-c.c
@@ -25,26 +25,19 @@
  *****************************************************************************/
 
 #include "common/common.h"
-#include "loongson_intrinsics.h"
 #include "mc.h"
 
 #if !HIGH_BIT_DEPTH
 
-static const uint8_t pu_core_mask_arr[16 * 2] =
-{
-    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
-    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
-};
-
 #define MC_WEIGHT(func)                                                                                             \
 static void (* mc##func##_wtab_lasx[6])( uint8_t *, intptr_t, uint8_t *, intptr_t, const x264_weight_t *, int ) =   \
 {                                                                                                                   \
     x264_mc_weight_w4##func##_lasx,                                                                                 \
     x264_mc_weight_w4##func##_lasx,                                                                                 \
     x264_mc_weight_w8##func##_lasx,                                                                                 \
-    x264_mc_weight_w16##func##_lasx,                                                                           \
-    x264_mc_weight_w16##func##_lasx,                                                                           \
-    x264_mc_weight_w20##func##_lasx,                                                                           \
+    x264_mc_weight_w16##func##_lasx,                                                                                \
+    x264_mc_weight_w16##func##_lasx,                                                                                \
+    x264_mc_weight_w20##func##_lasx,                                                                                \
 };
 
 #if !HIGH_BIT_DEPTH
@@ -72,569 +65,6 @@ static weight_fn_t mc_weight_wtab_lasx[6] =
     x264_mc_weight_w20_lasx,
 };
 
-static void *x264_memcpy_aligned_lasx(void *dst, const void *src, size_t n)
-{
-    int64_t zero = 0, d;
-
-    __asm__ volatile(
-    "andi      %[d],            %[n],              16                   \n\t"
-    "beqz      %[d],            2f                                      \n\t"
-    "addi.d    %[n],            %[n],              -16                  \n\t"
-    "vld       $vr0,            %[src],            0                    \n\t"
-    "vst       $vr0,            %[dst],            0                    \n\t"
-    "addi.d    %[src],          %[src],            16                   \n\t"
-    "addi.d    %[dst],          %[dst],            16                   \n\t"
-    "2:                                                                 \n\t"
-    "andi      %[d],            %[n],              32                   \n\t"
-    "beqz      %[d],            3f                                      \n\t"
-    "addi.d    %[n],            %[n],              -32                  \n\t"
-    "xvld      $xr0,            %[src],            0                    \n\t"
-    "xvst      $xr0,            %[dst],            0                    \n\t"
-    "addi.d    %[src],          %[src],            32                   \n\t"
-    "addi.d    %[dst],          %[dst],            32                   \n\t"
-    "3:                                                                 \n\t"
-    "beqz      %[n],            5f                                      \n\t"
-    "4:                                                                 \n\t"
-    "addi.d    %[n],            %[n],              -64                  \n\t"
-    "xvld      $xr0,            %[src],            32                   \n\t"
-    "xvld      $xr1,            %[src],            0                    \n\t"
-    "xvst      $xr0,            %[dst],            32                   \n\t"
-    "xvst      $xr1,            %[dst],            0                    \n\t"
-    "addi.d    %[src],          %[src],            64                   \n\t"
-    "addi.d    %[dst],          %[dst],            64                   \n\t"
-    "blt       %[zero],         %[n],              4b                   \n\t"
-    "5:                                                                 \n\t"
-    : [dst]"+&r"(dst), [src]"+&r"(src), [n]"+&r"(n),
-      [d]"=&r"(d)
-    : [zero]"r"(zero)
-    : "memory"
-    );
-    return NULL;
-}
-
-static inline void avg_src_width16_no_align_lasx( uint8_t *p_src1,
-                                                  int32_t i_src1_stride,
-                                                  uint8_t *p_src2,
-                                                  int32_t i_src2_stride,
-                                                  uint8_t *p_dst,
-                                                  int32_t i_dst_stride,
-                                                  int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1;
-
-    for( i_cnt = i_height; i_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src1 = __lasx_xvld( p_src2, 0 );
-        p_src2 += i_src2_stride;
-
-        src0 = __lasx_xvavgr_bu( src0, src1 );
-        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src0, p_dst, 0, 1 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static inline void avg_src_width20_no_align_lasx( uint8_t *p_src1,
-                                                  int32_t i_src1_stride,
-                                                  uint8_t *p_src2,
-                                                  int32_t i_src2_stride,
-                                                  uint8_t *p_dst,
-                                                  int32_t i_dst_stride,
-                                                  int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1;
-
-    for( i_cnt = i_height; i_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src1 = __lasx_xvld( p_src2, 0 );
-        p_src2 += i_src2_stride;
-
-        src0 = __lasx_xvavgr_bu( src0, src1 );
-        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src0, p_dst, 8, 1 );
-        __lasx_xvstelm_w( src0, p_dst, 16, 4 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static inline void avg_src_width12_no_align_lasx( uint8_t *p_src1,
-                                                  int32_t i_src1_stride,
-                                                  uint8_t *p_src2,
-                                                  int32_t i_src2_stride,
-                                                  uint8_t *p_dst,
-                                                  int32_t i_dst_stride,
-                                                  int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1;
-
-    for( i_cnt = i_height; i_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src1 = __lasx_xvld( p_src2, 0 );
-        p_src2 += i_src2_stride;
-
-        src0 = __lasx_xvavgr_bu( src0, src1 );
-        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-        __lasx_xvstelm_w( src0, p_dst, 8, 2 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static inline void avg_src_width8_no_align_lasx( uint8_t *p_src1,
-                                                 int32_t i_src1_stride,
-                                                 uint8_t *p_src2,
-                                                 int32_t i_src2_stride,
-                                                 uint8_t *p_dst,
-                                                 int32_t i_dst_stride,
-                                                 int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1;
-
-    for( i_cnt = i_height; i_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src1 = __lasx_xvld( p_src2, 0 );
-        p_src2 += i_src2_stride;
-
-        src0 = __lasx_xvavgr_bu( src0, src1 );
-        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static inline void avg_src_width4_no_align_lasx( uint8_t *p_src1,
-                                                 int32_t i_src1_stride,
-                                                 uint8_t *p_src2,
-                                                 int32_t i_src2_stride,
-                                                 uint8_t *p_dst,
-                                                 int32_t i_dst_stride,
-                                                 int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1;
-
-    for( i_cnt = i_height; i_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src1 = __lasx_xvld( p_src2, 0 );
-        p_src2 += i_src2_stride;
-
-        src0 = __lasx_xvavgr_bu( src0, src1 );
-        __lasx_xvstelm_w( src0, p_dst, 0, 0 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static inline void mc_weight_w16_no_align_lasx( uint8_t *p_dst,
-                                                intptr_t i_dst_stride,
-                                                uint8_t *p_src,
-                                                intptr_t i_src_stride,
-                                                const x264_weight_t *pWeight,
-                                                int32_t i_height )
-{
-    int32_t i_log2_denom = pWeight->i_denom;
-    int32_t i_offset = pWeight->i_offset;
-    int32_t i_weight = pWeight->i_scale;
-    uint8_t u_cnt;
-    __m256i zero = __lasx_xvldi( 0 );
-    __m256i src;
-    __m256i wgt, denom, offset;
-
-    i_offset <<= ( i_log2_denom );
-
-    if( i_log2_denom )
-    {
-        i_offset += ( 1 << ( i_log2_denom - 1 ) );
-    }
-
-    wgt =  __lasx_xvreplgr2vr_h( i_weight );
-    offset = __lasx_xvreplgr2vr_h( i_offset );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
-
-    for( u_cnt = i_height; u_cnt--; )
-    {
-        src = __lasx_xvld( p_src, 0 );
-        p_src += i_src_stride;
-
-        src = __lasx_xvpermi_d( src, 0x50 );
-        src = __lasx_xvilvl_b( zero, src );
-
-        src = __lasx_xvmul_h( src, wgt );
-        src = __lasx_xvsadd_h( src, offset );
-        src = __lasx_xvmaxi_h( src, 0 );
-        src = __lasx_xvssrln_bu_h(src, denom);
-
-        __lasx_xvstelm_d( src, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src, p_dst, 8, 2 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static inline void mc_weight_w8_no_align_lasx( uint8_t *p_dst,
-                                               intptr_t i_dst_stride,
-                                               uint8_t *p_src,
-                                               intptr_t i_src_stride,
-                                               const x264_weight_t *pWeight,
-                                               int32_t i_height )
-{
-    int32_t i_log2_denom = pWeight->i_denom;
-    int32_t i_offset = pWeight->i_offset;
-    int32_t i_weight = pWeight->i_scale;
-    uint8_t u_cnt;
-    __m256i zero = __lasx_xvldi( 0 );
-    __m256i src;
-    __m256i wgt, denom, offset;
-
-    i_offset <<= ( i_log2_denom );
-
-    if( i_log2_denom )
-    {
-        i_offset += ( 1 << ( i_log2_denom - 1 ) );
-    }
-
-    wgt =  __lasx_xvreplgr2vr_h( i_weight );
-    offset = __lasx_xvreplgr2vr_h( i_offset );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
-
-    for( u_cnt = i_height; u_cnt--; )
-    {
-        src = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-
-        src = __lasx_xvilvl_b( zero, src );
-
-        src = __lasx_xvmul_h( src, wgt );
-        src = __lasx_xvsadd_h( src, offset );
-        src = __lasx_xvmaxi_h( src, 0 );
-        src = __lasx_xvssrln_bu_h(src, denom);
-
-        __lasx_xvstelm_d( src, p_dst, 0, 0 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static inline void mc_weight_w4_no_align_lasx( uint8_t *p_dst,
-                                               intptr_t i_dst_stride,
-                                               uint8_t *p_src,
-                                               intptr_t i_src_stride,
-                                               const x264_weight_t *pWeight,
-                                               int32_t i_height )
-{
-    int32_t i_log2_denom = pWeight->i_denom;
-    int32_t i_offset = pWeight->i_offset;
-    int32_t i_weight = pWeight->i_scale;
-    uint8_t u_cnt;
-    __m256i zero = __lasx_xvldi( 0 );
-    __m256i src;
-    __m256i wgt, denom, offset;
-
-    i_offset <<= ( i_log2_denom );
-
-    if( i_log2_denom )
-    {
-        i_offset += ( 1 << ( i_log2_denom - 1 ) );
-    }
-
-    wgt =  __lasx_xvreplgr2vr_h( i_weight );
-    offset = __lasx_xvreplgr2vr_h( i_offset );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
-
-    for( u_cnt = i_height; u_cnt--; )
-    {
-        src = __lasx_xvldrepl_w( p_src, 0 );
-        p_src += i_src_stride;
-
-        src = __lasx_xvilvl_b( zero, src );
-
-        src = __lasx_xvmul_h( src, wgt );
-        src = __lasx_xvsadd_h( src, offset );
-        src = __lasx_xvmaxi_h( src, 0 );
-        src = __lasx_xvssrln_bu_h(src, denom);
-
-        __lasx_xvstelm_w( src, p_dst, 0, 0 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static inline void mc_weight_w20_no_align_lasx( uint8_t *p_dst,
-                                                intptr_t i_dst_stride,
-                                                uint8_t *p_src,
-                                                intptr_t i_src_stride,
-                                                const x264_weight_t *pWeight,
-                                                int32_t i_height )
-{
-    mc_weight_w16_no_align_lasx( p_dst, i_dst_stride,
-                                 p_src, i_src_stride,
-                                 pWeight, i_height );
-    mc_weight_w4_no_align_lasx( p_dst + 16, i_dst_stride,
-                                p_src + 16, i_src_stride,
-                                pWeight, i_height );
-}
-
-void x264_pixel_avg2_w4_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
-                              intptr_t i_src_stride, uint8_t *src2, int i_height)
-{
-    int64_t zero = 2, i_4 = 4;
-
-    __asm__ volatile(
-    "1:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
-    "vldrepl.w      $vr0,             %[src1],              0                      \n\t"
-    "vldrepl.w      $vr1,             %[src2],              0                      \n\t"
-    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
-    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
-    "vldrepl.w      $vr2,             %[src1],              0                      \n\t"
-    "vldrepl.w      $vr3,             %[src2],              0                      \n\t"
-    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
-    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
-    "vldrepl.w      $vr4,             %[src1],              0                      \n\t"
-    "vldrepl.w      $vr5,             %[src2],              0                      \n\t"
-    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
-    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
-    "vldrepl.w      $vr6,             %[src1],              0                      \n\t"
-    "vldrepl.w      $vr7,             %[src2],              0                      \n\t"
-    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
-    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
-    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
-    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
-    "vavgr.bu       $vr2,             $vr4,                 $vr5                   \n\t"
-    "vavgr.bu       $vr3,             $vr6,                 $vr7                   \n\t"
-    "vstelm.w       $vr0,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vstelm.w       $vr1,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vstelm.w       $vr2,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vstelm.w       $vr3,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "bge            %[i_height],      %[i_4],               1b                     \n\t"
-    "beqz           %[i_height],      3f                                           \n\t"
-    "2:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
-    "vldrepl.w      $vr0,             %[src1],              0                      \n\t"
-    "vldrepl.w      $vr1,             %[src2],              0                      \n\t"
-    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
-    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
-    "vldrepl.w      $vr2,             %[src1],              0                      \n\t"
-    "vldrepl.w      $vr3,             %[src2],              0                      \n\t"
-    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
-    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
-    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
-    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
-    "vstelm.w       $vr0,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vstelm.w       $vr1,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "blt            %[zero],          %[i_height],          2b                     \n\t"
-    "3:                                                                            \n\t"
-    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
-      [dst]"+&r"(dst)
-    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
-      [zero]"r"(zero), [i_4]"r"(i_4)
-    : "memory"
-    );
-}
-
-void x264_pixel_avg2_w8_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
-                              intptr_t i_src_stride, uint8_t *src2, int i_height)
-{
-    int64_t zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
-
-    __asm__ volatile(
-    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
-    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
-    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "1:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
-    "vld            $vr0,             %[src1],              0                      \n\t"
-    "vld            $vr1,             %[src2],              0                      \n\t"
-    "vldx           $vr2,             %[src1],              %[i_src_stride]        \n\t"
-    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
-    "vldx           $vr4,             %[src1],              %[src_stride2]         \n\t"
-    "vldx           $vr5,             %[src2],              %[src_stride2]         \n\t"
-    "vldx           $vr6,             %[src1],              %[src_stride3]         \n\t"
-    "vldx           $vr7,             %[src2],              %[src_stride3]         \n\t"
-    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
-    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
-    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
-    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
-    "vavgr.bu       $vr2,             $vr4,                 $vr5                   \n\t"
-    "vavgr.bu       $vr3,             $vr6,                 $vr7                   \n\t"
-    "vstelm.d       $vr0,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vstelm.d       $vr1,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vstelm.d       $vr2,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vstelm.d       $vr3,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "bge            %[i_height],      %[i_4],               1b                     \n\t"
-    "beqz           %[i_height],      3f                                           \n\t"
-    "2:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
-    "vld            $vr0,             %[src1],              0                      \n\t"
-    "vld            $vr1,             %[src2],              0                      \n\t"
-    "vldx           $vr2,             %[src1],              %[i_src_stride]        \n\t"
-    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
-    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
-    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
-    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
-    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
-    "vstelm.d       $vr0,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vstelm.d       $vr1,             %[dst],               0,           0         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "blt            %[zero],          %[i_height],          2b                     \n\t"
-    "3:                                                                            \n\t"
-    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
-      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2), [src_stride3]"=&r"(src_stride3),
-      [src_stride4]"=&r"(src_stride4)
-    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
-      [zero]"r"(zero), [i_4]"r"(i_4)
-    : "memory"
-    );
-}
-
-void x264_pixel_avg2_w16_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
-                               intptr_t i_src_stride, uint8_t *src2, int i_height)
-{
-    int64_t src_stride2, dst_stride2, dst_stride3, src_stride3, src_stride4, dst_stride4;
-    int64_t zero = 0, i_4 = 4;
-
-    __asm__ volatile(
-    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
-    "slli.d         %[dst_stride2],   %[i_dst_stride],      1                      \n\t"
-    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
-    "add.d          %[dst_stride3],   %[dst_stride2],       %[i_dst_stride]        \n\t"
-    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "slli.d         %[dst_stride4],   %[dst_stride2],       1                      \n\t"
-    "1:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
-    "vld            $vr0,             %[src1],              0                      \n\t"
-    "vldx           $vr1,             %[src1],              %[i_src_stride]        \n\t"
-    "vldx           $vr2,             %[src1],              %[src_stride2]         \n\t"
-    "vldx           $vr3,             %[src1],              %[src_stride3]         \n\t"
-    "vld            $vr4,             %[src2],              0                      \n\t"
-    "vldx           $vr5,             %[src2],              %[i_src_stride]        \n\t"
-    "vldx           $vr6,             %[src2],              %[src_stride2]         \n\t"
-    "vldx           $vr7,             %[src2],              %[src_stride3]         \n\t"
-    "vavgr.bu       $vr0,             $vr0,                 $vr4                   \n\t"
-    "vavgr.bu       $vr1,             $vr1,                 $vr5                   \n\t"
-    "vavgr.bu       $vr2,             $vr2,                 $vr6                   \n\t"
-    "vavgr.bu       $vr3,             $vr3,                 $vr7                   \n\t"
-    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
-    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
-    "vst            $vr0,             %[dst],               0                      \n\t"
-    "vstx           $vr1,             %[dst],               %[i_dst_stride]        \n\t"
-    "vstx           $vr2,             %[dst],               %[dst_stride2]         \n\t"
-    "vstx           $vr3,             %[dst],               %[dst_stride3]         \n\t"
-    "add.d          %[dst],           %[dst],               %[dst_stride4]         \n\t"
-    "bge            %[i_height],      %[i_4],               1b                     \n\t"
-    "beqz           %[i_height],      3f                                           \n\t"
-    "2:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
-    "vld            $vr0,             %[src1],              0                      \n\t"
-    "vldx           $vr1,             %[src1],              %[i_src_stride]        \n\t"
-    "vld            $vr2,             %[src2],              0                      \n\t"
-    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
-    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
-    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
-    "vavgr.bu       $vr0,             $vr0,                 $vr2                   \n\t"
-    "vavgr.bu       $vr1,             $vr1,                 $vr3                   \n\t"
-    "vst            $vr0,             %[dst],               0                      \n\t"
-    "vstx           $vr1,             %[dst],               %[i_dst_stride]        \n\t"
-    "add.d          %[dst],           %[dst],               %[dst_stride2]         \n\t"
-    "blt            %[zero],          %[i_height],          2b                     \n\t"
-    "3:                                                                            \n\t"
-    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
-      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2), [dst_stride2]"=&r"(dst_stride2),
-      [src_stride3]"=&r"(src_stride3), [dst_stride3]"=&r"(dst_stride3),
-      [src_stride4]"=&r"(src_stride4), [dst_stride4]"=&r"(dst_stride4)
-    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
-      [zero]"r"(zero), [i_4]"r"(i_4)
-    : "memory"
-    );
-}
-
-void x264_pixel_avg2_w20_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
-                               intptr_t i_src_stride, uint8_t *src2, int i_height)
-{
-    int64_t zero = 0, i_4 = 4;
-    int64_t src_stride2, src_stride3, src_stride4;
-
-    __asm__ volatile(
-    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
-    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
-    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "1:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
-    "xvld           $xr0,             %[src1],              0                      \n\t"
-    "xvldx          $xr1,             %[src1],              %[i_src_stride]        \n\t"
-    "xvldx          $xr2,             %[src1],              %[src_stride2]         \n\t"
-    "xvldx          $xr3,             %[src1],              %[src_stride3]         \n\t"
-    "xvld           $xr4,             %[src2],              0                      \n\t"
-    "xvldx          $xr5,             %[src2],              %[i_src_stride]        \n\t"
-    "xvldx          $xr6,             %[src2],              %[src_stride2]         \n\t"
-    "xvldx          $xr7,             %[src2],              %[src_stride3]         \n\t"
-    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
-    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
-
-    "xvavgr.bu      $xr0,             $xr0,                 $xr4                   \n\t"
-    "xvavgr.bu      $xr1,             $xr1,                 $xr5                   \n\t"
-    "xvavgr.bu      $xr2,             $xr2,                 $xr6                   \n\t"
-    "xvavgr.bu      $xr3,             $xr3,                 $xr7                   \n\t"
-    "vst            $vr0,             %[dst],               0                      \n\t"
-    "xvstelm.w      $xr0,             %[dst],               16,          4         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vst            $vr1,             %[dst],               0                      \n\t"
-    "xvstelm.w      $xr1,             %[dst],               16,          4         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vst            $vr2,             %[dst],               0                      \n\t"
-    "xvstelm.w      $xr2,             %[dst],               16,          4         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vst            $vr3,             %[dst],               0                      \n\t"
-    "xvstelm.w      $xr3,             %[dst],               16,          4         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "bge            %[i_height],      %[i_4],          1b                     \n\t"
-    "beqz           %[i_height],      3f                                           \n\t"
-    "2:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
-    "xvld           $xr0,             %[src1],              0                      \n\t"
-    "xvldx          $xr1,             %[src1],              %[i_src_stride]        \n\t"
-    "xvld           $xr2,             %[src2],              0                      \n\t"
-    "xvldx          $xr3,             %[src2],              %[i_src_stride]        \n\t"
-    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
-    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
-    "xvavgr.bu      $xr0,             $xr0,                 $xr2                   \n\t"
-    "xvavgr.bu      $xr1,             $xr1,                 $xr3                   \n\t"
-    "vst            $vr0,             %[dst],               0                      \n\t"
-    "xvstelm.w      $xr0,             %[dst],               16,          4         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "vst            $vr1,             %[dst],               0                      \n\t"
-    "xvstelm.w      $xr1,             %[dst],               16,          4         \n\t"
-    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
-    "blt            %[zero],          %[i_height],          2b                     \n\t"
-    "3:                                                                            \n\t"
-    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
-      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
-      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
-    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
-      [zero]"r"(zero), [i_4]"r"(i_4)
-    : "memory"
-    );
-}
-
 static void (* const pixel_avg_wtab_lasx[6])(uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int ) =
 {
     NULL,
@@ -645,6 +75,15 @@ static void (* const pixel_avg_wtab_lasx[6])(uint8_t *, intptr_t, uint8_t *, int
     x264_pixel_avg2_w20_lasx,
 };
 
+static void (* const mc_copy_wtab_lasx[5])( uint8_t *, intptr_t, uint8_t *, intptr_t, int ) =
+{
+    NULL,
+    x264_mc_copy_w4_lasx,
+    x264_mc_copy_w8_lasx,
+    NULL,
+    x264_mc_copy_w16_lasx,
+};
+
 static uint8_t *get_ref_lasx( uint8_t *p_dst, intptr_t *p_dst_stride,
                               uint8_t *p_src[4], intptr_t i_src_stride,
                               int32_t m_vx, int32_t m_vy,
@@ -689,162 +128,6 @@ static uint8_t *get_ref_lasx( uint8_t *p_dst, intptr_t *p_dst_stride,
     }
 }
 
-static void copy_width4_lasx( uint8_t *p_src, int32_t i_src_stride,
-                              uint8_t *p_dst, int32_t i_dst_stride,
-                              int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1;
-
-    for( i_cnt = ( i_height >> 1 ); i_cnt--;  )
-    {
-        src0 = __lasx_xvldrepl_w( p_src, 0 );
-        p_src += i_src_stride;
-        src1 = __lasx_xvldrepl_w( p_src, 0 );
-        p_src += i_src_stride;
-
-        __lasx_xvstelm_w( src0, p_dst, 0, 0 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_w( src1, p_dst, 0, 0 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static void copy_width8_lasx( uint8_t *p_src, int32_t i_src_stride,
-                              uint8_t *p_dst, int32_t i_dst_stride,
-                              int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1, src2, src3;
-
-#define COPY_W8_H4                                  \
-    src0 = __lasx_xvldrepl_d( p_src, 0 );           \
-    p_src += i_src_stride;                          \
-    src1 = __lasx_xvldrepl_d( p_src, 0 );           \
-    p_src += i_src_stride;                          \
-    src2 = __lasx_xvldrepl_d( p_src, 0 );           \
-    p_src += i_src_stride;                          \
-    src3 = __lasx_xvldrepl_d( p_src, 0 );           \
-    p_src += i_src_stride;                          \
-                                                    \
-    __lasx_xvstelm_d( src0, p_dst, 0, 0 );          \
-    p_dst += i_dst_stride;                          \
-    __lasx_xvstelm_d( src1, p_dst, 0, 0 );          \
-    p_dst += i_dst_stride;                          \
-    __lasx_xvstelm_d( src2, p_dst, 0, 0 );          \
-    p_dst += i_dst_stride;                          \
-    __lasx_xvstelm_d( src3, p_dst, 0, 0 );          \
-    p_dst += i_dst_stride;
-
-    if( 0 == i_height % 12 )
-    {
-        for( i_cnt = i_height; 0 < i_cnt; i_cnt -= 12 )
-        {
-            COPY_W8_H4;
-            COPY_W8_H4;
-            COPY_W8_H4;
-        }
-    }
-    else if( 0 == ( i_height & 7 ) )
-    {
-        for( i_cnt = ( i_height >> 3 ); i_cnt--; )
-        {
-            COPY_W8_H4;
-            COPY_W8_H4;
-        }
-    }
-    else if( 0 == ( i_height & 3 ) )
-    {
-        for( i_cnt = ( i_height >> 2 ); i_cnt--; )
-        {
-            COPY_W8_H4;
-        }
-    }
-
-#undef COPY_W8_H4
-
-}
-
-static void copy_width16_lasx( uint8_t *p_src, int32_t i_src_stride,
-                               uint8_t *p_dst, int32_t i_dst_stride,
-                               int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1, src2, src3;
-
-#define COPY_W16_H4                                 \
-    src0 = __lasx_xvld(p_src, 0);                   \
-    p_src += i_src_stride;                          \
-    src1 = __lasx_xvld(p_src, 0);                   \
-    p_src += i_src_stride;                          \
-    src2 = __lasx_xvld(p_src, 0);                   \
-    p_src += i_src_stride;                          \
-    src3 = __lasx_xvld(p_src, 0);                   \
-    p_src += i_src_stride;                          \
-                                                    \
-    __lasx_xvstelm_d( src0, p_dst, 0, 0 );          \
-    __lasx_xvstelm_d( src0, p_dst, 8, 1 );          \
-    p_dst += i_dst_stride;                          \
-    __lasx_xvstelm_d( src1, p_dst, 0, 0 );          \
-    __lasx_xvstelm_d( src1, p_dst, 8, 1 );          \
-    p_dst += i_dst_stride;                          \
-    __lasx_xvstelm_d( src2, p_dst, 0, 0 );          \
-    __lasx_xvstelm_d( src2, p_dst, 8, 1 );          \
-    p_dst += i_dst_stride;                          \
-    __lasx_xvstelm_d( src3, p_dst, 0, 0 );          \
-    __lasx_xvstelm_d( src3, p_dst, 8, 1 );          \
-    p_dst += i_dst_stride;
-
-    if( 0 == i_height % 12 )
-    {
-        for( i_cnt = i_height; 0 < i_cnt; i_cnt -= 12 )
-        {
-            COPY_W16_H4;
-            COPY_W16_H4;
-            COPY_W16_H4;
-        }
-    }
-    else if( 0 == ( i_height & 7 ) )
-    {
-        for( i_cnt = ( i_height >> 3 ); i_cnt--; )
-        {
-            COPY_W16_H4;
-            COPY_W16_H4;
-        }
-    }
-    else if( 0 == ( i_height & 3 ) )
-    {
-        for( i_cnt = ( i_height >> 2 ); i_cnt--; )
-        {
-            COPY_W16_H4;
-        }
-    }
-
-#undef COPY_W16_H4
-
-}
-
-static void mc_copy_w16_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
-                              uint8_t *p_src, intptr_t i_src_stride,
-                              int32_t i_height )
-{
-    copy_width16_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
-}
-
-static void mc_copy_w8_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
-                             uint8_t *p_src, intptr_t i_src_stride,
-                             int32_t i_height )
-{
-    copy_width8_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
-}
-
-static void mc_copy_w4_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
-                             uint8_t *p_src, intptr_t i_src_stride,
-                             int32_t i_height )
-{
-    copy_width4_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
-}
-
 static void mc_luma_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
                           uint8_t *p_src[4], intptr_t i_src_stride,
                           int32_t m_vx, int32_t m_vy,
@@ -880,1034 +163,12 @@ static void mc_luma_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
     }
     else
     {
-        if( 16 == i_width )
-        {
-            copy_width16_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
-                               i_height );
-        }
-        else if( 8 == i_width )
-        {
-            copy_width8_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
-                              i_height );
-        }
-        else if( 4 == i_width )
-        {
-            copy_width4_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
-                              i_height );
-        }
-    }
-}
-
-#define LASX_HORZ_FILTER_SH( in, mask0, mask1, mask2 )         \
-( {                                                            \
-    __m256i out0_m;                                            \
-    __m256i tmp0_m, tmp1_m;                                    \
-                                                               \
-    tmp0_m = __lasx_xvshuf_b(in, in, mask0);                   \
-    out0_m = __lasx_xvhaddw_h_b( tmp0_m, tmp0_m );             \
-                                                               \
-    tmp0_m = __lasx_xvshuf_b(in, in, mask1);                   \
-    out0_m = __lasx_xvdp2add_h_b( out0_m, minus5b, tmp0_m );   \
-                                                               \
-    tmp1_m = __lasx_xvshuf_b(in, in, mask2);                   \
-    out0_m = __lasx_xvdp2add_h_b( out0_m, plus20b, tmp1_m );   \
-                                                               \
-    out0_m;                                                    \
-} )
-
-#define LASX_CALC_DPADD_H_6PIX_2COEFF_SH( in0, in1, in2, in3, in4, in5 )   \
-( {                                                                        \
-    __m256i tmp0_m, tmp1_m;                                                \
-    __m256i out0_m, out1_m, out2_m, out3_m;                                \
-                                                                           \
-    tmp0_m = __lasx_xvilvh_h( in5, in0 );                                  \
-    tmp1_m = __lasx_xvilvl_h( in5, in0 );                                  \
-                                                                           \
-    tmp0_m = __lasx_xvhaddw_w_h( tmp0_m, tmp0_m );                         \
-    tmp1_m = __lasx_xvhaddw_w_h( tmp1_m, tmp1_m );                         \
-                                                                           \
-    out0_m = __lasx_xvilvh_h( in1, in4 );                                  \
-    out1_m = __lasx_xvilvl_h( in1, in4 );                                  \
-    DUP2_ARG3( __lasx_xvdp2add_w_h, tmp0_m, out0_m, minus5h, tmp1_m,       \
-               out1_m, minus5h, tmp0_m, tmp1_m );                          \
-    out2_m = __lasx_xvilvh_h( in2, in3 );                                  \
-    out3_m = __lasx_xvilvl_h( in2, in3 );                                  \
-    DUP2_ARG3( __lasx_xvdp2add_w_h, tmp0_m, out2_m, plus20h, tmp1_m,       \
-               out3_m, plus20h, tmp0_m, tmp1_m );                          \
-                                                                           \
-    out0_m = __lasx_xvssrarni_h_w(tmp0_m, tmp1_m, 10);                     \
-    out0_m = __lasx_xvsat_h(out0_m, 7);                                    \
-                                                                           \
-    out0_m;                                                                \
-} )
-
-static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
-                                              int32_t i_src_stride,
-                                              uint8_t *p_dst0,
-                                              uint8_t *p_dst1,
-                                              uint8_t *p_dst2,
-                                              uint8_t *p_dst3,
-                                              int32_t i_dst_stride,
-                                              int32_t i_width,
-                                              int32_t i_height )
-{
-    int32_t i_loop_width, i_loop_height, i_w16_mul;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
-    __m256i sld1_vec0, sld1_vec1, sld1_vec2, sld1_vec3, sld1_vec4, sld1_vec5;
-    __m256i pckev_vec0, pckev_vec1, pckev_vec2;
-    __m256i pckod_vec0, pckod_vec1, pckod_vec2;
-    __m256i tmp0, tmp1, tmp2, tmp3;
-    __m256i mask;
-
-    mask = __lasx_xvld( pu_core_mask_arr, 0 );
-
-    i_w16_mul = i_width - i_width % 16;
-    for( i_loop_height = i_height; i_loop_height--; )
-    {
-        src0  = __lasx_xvld( p_src, 0 );
-        DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src1, src2 );
-        p_src += 16;
-        for( i_loop_width = 0; i_loop_width < ( i_w16_mul >> 4 ); i_loop_width++ )
-        {
-            src3  = __lasx_xvld( p_src, 0 );
-            DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src4, src5 );
-            src6 = __lasx_xvpermi_q( src3, src3, 0x11 );
-            src7 = __lasx_xvpermi_q( src4, src4, 0x11 );
-            src8 = __lasx_xvpermi_q( src5, src5, 0x11 );
-            p_src += 32;
-
-            pckev_vec0 = __lasx_xvpickev_b( src3, src0 );
-            pckod_vec0 = __lasx_xvpickod_b( src3, src0 );
-            pckev_vec1 = __lasx_xvpickev_b( src4, src1 );
-            pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
-            pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
-            pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
-            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
-                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
-                       tmp3 );
-            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
-            __lasx_xvstelm_d( tmp0, p_dst0, 0, 0 );
-            __lasx_xvstelm_d( tmp0, p_dst0, 8, 1 );
-            __lasx_xvstelm_d( tmp1, p_dst2, 0, 0 );
-            __lasx_xvstelm_d( tmp1, p_dst2, 8, 1 );
-
-            DUP2_ARG3( __lasx_xvshuf_b, src3, src0, mask, src4, src1,
-                       mask, sld1_vec0, sld1_vec1 );
-            DUP2_ARG3( __lasx_xvshuf_b, src5, src2, mask, src6, src3,
-                       mask, sld1_vec2, sld1_vec3 );
-            DUP2_ARG3( __lasx_xvshuf_b, src7, src4, mask, src8, src5,
-                       mask, sld1_vec4, sld1_vec5 );
-            pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
-            pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
-            pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
-            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
-                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
-                       tmp3 );
-            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
-            __lasx_xvstelm_d( tmp0, p_dst1, 0, 0 );
-            __lasx_xvstelm_d( tmp0, p_dst1, 8, 1 );
-            __lasx_xvstelm_d( tmp1, p_dst3, 0, 0 );
-            __lasx_xvstelm_d( tmp1, p_dst3, 8, 1 );
-
-            src0 = src6;
-            src1 = src7;
-            src2 = src8;
-            p_dst0 += 16;
-            p_dst1 += 16;
-            p_dst2 += 16;
-            p_dst3 += 16;
-        }
-
-        if (i_w16_mul < i_width)
-        {
-            src3  = __lasx_xvld( p_src, 0 );
-            DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src4, src5 );
-            src6 = __lasx_xvpermi_q( src3, src3, 0x11 );
-            src7 = __lasx_xvpermi_q( src4, src4, 0x11 );
-            src8 = __lasx_xvpermi_q( src5, src5, 0x11 );
-            p_src += 16;
-
-            pckev_vec0 = __lasx_xvpickev_b( src3, src0 );
-            pckod_vec0 = __lasx_xvpickod_b( src3, src0 );
-            pckev_vec1 = __lasx_xvpickev_b( src4, src1 );
-            pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
-            pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
-            pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
-            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1,
-                       pckod_vec0, pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
-                       tmp0, tmp1, tmp2, tmp3 );
-            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
-            __lasx_xvstelm_d( tmp0, p_dst0, 0, 0 );
-            __lasx_xvstelm_d( tmp1, p_dst2, 0, 0 );
-
-            DUP2_ARG3( __lasx_xvshuf_b, src3, src0, mask, src4, src1,
-                       mask, sld1_vec0, sld1_vec1 );
-            DUP2_ARG3( __lasx_xvshuf_b, src5, src2, mask, src6, src3,
-                       mask, sld1_vec2, sld1_vec3 );
-            DUP2_ARG3( __lasx_xvshuf_b, src7, src4, mask, src8, src5,
-                       mask, sld1_vec4, sld1_vec5 );
-            pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
-            pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
-            pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
-            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
-                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
-                       tmp3 );
-            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
-            __lasx_xvstelm_d( tmp0, p_dst1, 0, 0 );
-            __lasx_xvstelm_d( tmp1, p_dst3, 0, 0 );
-            p_dst0 += 8;
-            p_dst1 += 8;
-            p_dst2 += 8;
-            p_dst3 += 8;
-        }
-
-        p_src += ( ( i_src_stride << 1 ) - ( ( i_width << 1 ) + 16 ) );
-        p_dst0 += ( i_dst_stride - i_width );
-        p_dst1 += ( i_dst_stride - i_width );
-        p_dst2 += ( i_dst_stride - i_width );
-        p_dst3 += ( i_dst_stride - i_width );
+        mc_copy_wtab_lasx[i_width>>2]( p_dst, i_dst_stride, p_src1, i_src_stride, i_height );
     }
 }
 
-static void frame_init_lowres_core_lasx( uint8_t *p_src, uint8_t *p_dst0,
-                                         uint8_t *p_dst1, uint8_t *p_dst2,
-                                         uint8_t *p_dst3, intptr_t i_src_stride,
-                                         intptr_t i_dst_stride, int32_t i_width,
-                                         int32_t i_height )
-{
-    core_frame_init_lowres_core_lasx( p_src, i_src_stride, p_dst0,
-                                      p_dst1, p_dst2, p_dst3,
-                                      i_dst_stride, i_width, i_height );
-}
-static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
-                                               int32_t i_src_stride,
-                                               uint8_t *p_dst0,
-                                               int32_t dst0_stride,
-                                               uint8_t *p_dst1,
-                                               int32_t dst1_stride,
-                                               int32_t i_width,
-                                               int32_t i_height )
-{
-    int32_t i_loop_width, i_loop_height;
-    int32_t i_w_mul4, i_w_mul16, i_w_mul32, i_h4w;
-    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
-    __m256i vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3;
-    __m256i vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7;
-    __m256i vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3;
-    __m256i vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7;
-    uint8_t *p_dst, *p_dstA, *p_dstB, *p_srcA;
-    int32_t dst0_stride_x2 = dst0_stride << 1;
-    int32_t dst0_stride_x3 = dst0_stride_x2 + dst0_stride;
-    int32_t dst0_stride_x4 = dst0_stride << 2;
-    int32_t dst0_stride_x5 = dst0_stride_x4 + dst0_stride;
-    int32_t dst0_stride_x6 = dst0_stride_x4 + dst0_stride_x2;
-    int32_t dst0_stride_x7 = dst0_stride_x4 + dst0_stride_x3;
-    int32_t dst1_stride_x2 = dst1_stride << 1;
-    int32_t dst1_stride_x3 = dst1_stride_x2 + dst1_stride;
-    int32_t dst1_stride_x4 = dst1_stride << 2;
-    int32_t dst1_stride_x5 = dst1_stride_x4 + dst1_stride;
-    int32_t dst1_stride_x6 = dst1_stride_x4 + dst1_stride_x2;
-    int32_t dst1_stride_x7 = dst1_stride_x4 + dst1_stride_x3;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-    int32_t i_src_stride_x4 = i_src_stride << 2;
-    int32_t i_src_stride_x5 = i_src_stride_x4 + i_src_stride;
-    int32_t i_src_stride_x6 = i_src_stride_x4 + i_src_stride_x2;
-    int32_t i_src_stride_x7 = i_src_stride_x4 + i_src_stride_x3;
-
-    i_w_mul32 = i_width - ( i_width & 31 );
-    i_w_mul16 = i_width - ( i_width & 15 );
-    i_w_mul4 = i_width - ( i_width & 3 );
-    i_h4w = i_height - ( i_height & 7 );
-
-    for( i_loop_height = ( i_h4w >> 3 ); i_loop_height--; )
-    {
-        for( i_loop_width = ( i_w_mul32 >> 5 ); i_loop_width--; )
-        {
-            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
-                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
-            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
-                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
-            p_src += 32;
-            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-
-            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
-                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
-            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
-                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
-            p_src += 32;
-            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
-            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
-
-            in0 = __lasx_xvpermi_q( vec_pckev0, vec_pckev4, 0x02 );
-            in1 = __lasx_xvpermi_q( vec_pckev0, vec_pckev4, 0x13 );
-            in2 = __lasx_xvpermi_q( vec_pckev1, vec_pckev5, 0x02 );
-            in3 = __lasx_xvpermi_q( vec_pckev1, vec_pckev5, 0x13 );
-            in4 = __lasx_xvpermi_q( vec_pckev2, vec_pckev6, 0x02 );
-            in5 = __lasx_xvpermi_q( vec_pckev2, vec_pckev6, 0x13 );
-            in6 = __lasx_xvpermi_q( vec_pckev3, vec_pckev7, 0x02 );
-            in7 = __lasx_xvpermi_q( vec_pckev3, vec_pckev7, 0x13 );
-
-            DUP4_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            DUP4_ARG2( __lasx_xvilvh_d, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
-
-            __lasx_xvst( vec_pckev0, p_dst0, 0 );
-            __lasx_xvstx( vec_pckev4, p_dst0, dst0_stride );
-            __lasx_xvstx( vec_pckev1, p_dst0, dst0_stride_x2 );
-            __lasx_xvstx( vec_pckev5, p_dst0, dst0_stride_x3 );
-            __lasx_xvstx( vec_pckev2, p_dst0, dst0_stride_x4 );
-            __lasx_xvstx( vec_pckev6, p_dst0, dst0_stride_x5 );
-            __lasx_xvstx( vec_pckev3, p_dst0, dst0_stride_x6 );
-            __lasx_xvstx( vec_pckev7, p_dst0, dst0_stride_x7 );
-
-            in0 = __lasx_xvpermi_q( vec_pckod0, vec_pckod4, 0x02 );
-            in1 = __lasx_xvpermi_q( vec_pckod0, vec_pckod4, 0x13 );
-            in2 = __lasx_xvpermi_q( vec_pckod1, vec_pckod5, 0x02 );
-            in3 = __lasx_xvpermi_q( vec_pckod1, vec_pckod5, 0x13 );
-            in4 = __lasx_xvpermi_q( vec_pckod2, vec_pckod6, 0x02 );
-            in5 = __lasx_xvpermi_q( vec_pckod2, vec_pckod6, 0x13 );
-            in6 = __lasx_xvpermi_q( vec_pckod3, vec_pckod7, 0x02 );
-            in7 = __lasx_xvpermi_q( vec_pckod3, vec_pckod7, 0x13 );
-
-            DUP4_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-            DUP4_ARG2( __lasx_xvilvh_d, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
-
-            __lasx_xvst( vec_pckod0, p_dst1, 0 );
-            __lasx_xvstx( vec_pckod4, p_dst1, dst1_stride );
-            __lasx_xvstx( vec_pckod1, p_dst1, dst1_stride_x2 );
-            __lasx_xvstx( vec_pckod5, p_dst1, dst1_stride_x3 );
-            __lasx_xvstx( vec_pckod2, p_dst1, dst1_stride_x4 );
-            __lasx_xvstx( vec_pckod6, p_dst1, dst1_stride_x5 );
-            __lasx_xvstx( vec_pckod3, p_dst1, dst1_stride_x6 );
-            __lasx_xvstx( vec_pckod7, p_dst1, dst1_stride_x7 );
-
-            p_dst0 += 32;
-            p_dst1 += 32;
-        }
-
-        for( i_loop_width = ( ( i_width & 31 ) >> 4 ); i_loop_width--; )
-        {
-            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
-                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
-            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
-                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
-            p_src += 32;
-            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-
-            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev0, p_dst0, 8, 2 );
-            p_dst = p_dst0 + dst0_stride;
-            __lasx_xvstelm_d( vec_pckev0, p_dst, 0, 1 );
-            __lasx_xvstelm_d( vec_pckev0, p_dst, 8, 3 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev1, p_dst, 8, 2 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 1 );
-            __lasx_xvstelm_d( vec_pckev1, p_dst, 8, 3 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev2, p_dst, 8, 2 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 1 );
-            __lasx_xvstelm_d( vec_pckev2, p_dst, 8, 3 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev3, p_dst, 8, 2 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 1 );
-            __lasx_xvstelm_d( vec_pckev3, p_dst, 8, 3 );
-
-            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod0, p_dst1, 8, 2 );
-            p_dst = p_dst1 + dst0_stride;
-            __lasx_xvstelm_d( vec_pckod0, p_dst, 0, 1 );
-            __lasx_xvstelm_d( vec_pckod0, p_dst, 8, 3 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod1, p_dst, 8, 2 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 1 );
-            __lasx_xvstelm_d( vec_pckod1, p_dst, 8, 3 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod2, p_dst, 8, 2 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 1 );
-            __lasx_xvstelm_d( vec_pckod2, p_dst, 8, 3 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod3, p_dst, 8, 2 );
-            p_dst = p_dst + dst0_stride;
-            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 1 );
-            __lasx_xvstelm_d( vec_pckod3, p_dst, 8, 3 );
-
-            p_dst0 += 16;
-            p_dst1 += 16;
-        }
-
-        for( i_loop_width = ( ( i_width & 15 ) >> 3 ); i_loop_width--; )
-        {
-            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
-                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
-            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
-                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
-            p_src += 16;
-            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-
-            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev0, p_dst0 + dst0_stride, 0, 1 );
-            p_dst = p_dst0 + dst0_stride_x2;
-            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev1, p_dst + dst0_stride, 0, 1 );
-            p_dst = p_dst + dst0_stride_x2;
-            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev2, p_dst + dst0_stride, 0, 1 );
-            p_dst = p_dst + dst0_stride_x2;
-            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev3, p_dst + dst0_stride, 0, 1 );
-
-            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod0, p_dst1 + dst0_stride, 0, 1 );
-            p_dst = p_dst1 + dst1_stride_x2;
-            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod1, p_dst + dst0_stride, 0, 1 );
-            p_dst = p_dst + dst1_stride_x2;
-            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod2, p_dst + dst0_stride, 0, 1 );
-            p_dst = p_dst + dst1_stride_x2;
-            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod3, p_dst + dst0_stride, 0, 1 );
-
-            p_dst0 += 8;
-            p_dst1 += 8;
-        }
-
-
-        for( i_loop_width = ( ( i_width & 7 ) >> 2 ); i_loop_width--; )
-        {
-            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
-                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
-            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
-                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
-            p_src += 8;
-            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
-                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-
-            __lasx_xvstelm_w( vec_pckev0, p_dst0, 0, 0 );
-            __lasx_xvstelm_w( vec_pckev0, p_dst0 + dst0_stride, 0, 2 );
-            p_dst = p_dst0 + dst0_stride_x2;
-            __lasx_xvstelm_w( vec_pckev1, p_dst, 0, 0 );
-            __lasx_xvstelm_w( vec_pckev1, p_dst + dst0_stride, 0, 2 );
-            p_dst = p_dst + dst0_stride_x2;
-            __lasx_xvstelm_w( vec_pckev2, p_dst, 0, 0 );
-            __lasx_xvstelm_w( vec_pckev2, p_dst + dst0_stride, 0, 2 );
-            p_dst = p_dst + dst0_stride_x2;
-            __lasx_xvstelm_w( vec_pckev3, p_dst, 0, 0 );
-            __lasx_xvstelm_w( vec_pckev3, p_dst + dst0_stride, 0, 2 );
-
-            __lasx_xvstelm_w( vec_pckod0, p_dst1, 0, 0 );
-            __lasx_xvstelm_w( vec_pckod0, p_dst1 + dst0_stride, 0, 2 );
-            p_dst = p_dst1 + dst1_stride_x2;
-            __lasx_xvstelm_w( vec_pckod1, p_dst, 0, 0 );
-            __lasx_xvstelm_w( vec_pckod1, p_dst + dst0_stride, 0, 2 );
-            p_dst = p_dst + dst1_stride_x2;
-            __lasx_xvstelm_w( vec_pckod2, p_dst, 0, 0 );
-            __lasx_xvstelm_w( vec_pckod2, p_dst + dst0_stride, 0, 2 );
-            p_dst = p_dst + dst1_stride_x2;
-            __lasx_xvstelm_w( vec_pckod3, p_dst, 0, 0 );
-            __lasx_xvstelm_w( vec_pckod3, p_dst + dst0_stride, 0, 2 );
-
-            p_dst0 += 4;
-            p_dst1 += 4;
-        }
-
-        for( i_loop_width = i_w_mul4; i_loop_width < i_width; i_loop_width++ )
-        {
-            p_dst0[0] = p_src[0];
-            p_dst1[0] = p_src[1];
-
-            p_dstA = p_dst0 + dst0_stride;
-            p_dstB = p_dst1 + dst1_stride;
-            p_srcA = p_src + i_src_stride;
-            p_dstA[0] = p_srcA[0];
-            p_dstB[0] = p_srcA[1];
-
-            p_dstA += dst0_stride;
-            p_dstB += dst1_stride;
-            p_srcA += i_src_stride;
-            p_dstA[0] = p_srcA[0];
-            p_dstB[0] = p_srcA[1];
-
-            p_dstA += dst0_stride;
-            p_dstB += dst1_stride;
-            p_srcA += i_src_stride;
-            p_dstA[0] = p_srcA[0];
-            p_dstB[0] = p_srcA[1];
-
-            p_dstA += dst0_stride;
-            p_dstB += dst1_stride;
-            p_srcA += i_src_stride;
-            p_dstA[0] = p_srcA[0];
-            p_dstB[0] = p_srcA[1];
-
-            p_dstA += dst0_stride;
-            p_dstB += dst1_stride;
-            p_srcA += i_src_stride;
-            p_dstA[0] = p_srcA[0];
-            p_dstB[0] = p_srcA[1];
-
-            p_dstA += dst0_stride;
-            p_dstB += dst1_stride;
-            p_srcA += i_src_stride;
-            p_dstA[0] = p_srcA[0];
-            p_dstB[0] = p_srcA[1];
-
-            p_dstA += dst0_stride;
-            p_dstB += dst1_stride;
-            p_srcA += i_src_stride;
-            p_dstA[0] = p_srcA[0];
-            p_dstB[0] = p_srcA[1];
-
-            p_dst0 += 1;
-            p_dst1 += 1;
-            p_src += 2;
-        }
-
-        p_src += ( ( i_src_stride << 3 ) - ( i_width << 1 ) );
-        p_dst0 += ( ( dst0_stride << 3 ) - i_width );
-        p_dst1 += ( ( dst1_stride << 3) - i_width );
-    }
-
-    for( i_loop_height = i_h4w; i_loop_height < i_height; i_loop_height++ )
-    {
-        for( i_loop_width = ( i_w_mul16 >> 4 ); i_loop_width--; )
-        {
-            in0 = __lasx_xvld( p_src, 0 );
-            p_src += 32;
-            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
-            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
-            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
-            __lasx_xvstelm_d( vec_pckev0, p_dst0, 8, 2 );
-            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod0, p_dst1, 8, 2 );
-            p_dst0 += 16;
-            p_dst1 += 16;
-        }
-
-        for( i_loop_width = ( ( i_width & 15 ) >> 3 ); i_loop_width--; )
-        {
-            in0 = __lasx_xvld( p_src, 0 );
-            p_src += 16;
-            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
-            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
-            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
-            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
-            p_dst0 += 8;
-            p_dst1 += 8;
-        }
-
-        for( i_loop_width = ( ( i_width & 7 ) >> 2 ); i_loop_width--; )
-        {
-            in0 = __lasx_xvld( p_src, 0 );
-            p_src += 8;
-            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
-            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
-            __lasx_xvstelm_w( vec_pckev0, p_dst0, 0, 0 );
-            __lasx_xvstelm_w( vec_pckod0, p_dst1, 0, 0 );
-            p_dst0 += 4;
-            p_dst1 += 4;
-        }
-
-        for( i_loop_width = i_w_mul4; i_loop_width < i_width; i_loop_width++ )
-        {
-            p_dst0[0] = p_src[0];
-            p_dst1[0] = p_src[1];
-            p_dst0 += 1;
-            p_dst1 += 1;
-            p_src += 2;
-        }
-
-        p_src += ( ( i_src_stride ) - ( i_width << 1 ) );
-        p_dst0 += ( ( dst0_stride ) - i_width );
-        p_dst1 += ( ( dst1_stride ) - i_width );
-    }
-}
-
-static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
-                                             int32_t i_src0_stride,
-                                             uint8_t *p_src1,
-                                             int32_t i_src1_stride,
-                                             uint8_t *p_dst,
-                                             int32_t i_dst_stride,
-                                             int32_t i_width, int32_t i_height )
-{
-    int32_t i_loop_width, i_loop_height, i_w_mul8, i_h4w;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3;
-    __m256i vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3;
-    uint8_t *p_dst_t, *p_srcA, *p_srcB;
-    int32_t i_src0_stride_x2 = i_src0_stride << 1;
-    int32_t i_src1_stride_x2 = i_src1_stride << 1;
-    int32_t i_src0_stride_x3 = i_src0_stride_x2 + i_src0_stride;
-    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
-
-    i_w_mul8 = i_width - ( i_width & 7 );
-    i_h4w = i_height - ( i_height & 3 );
-
-    for( i_loop_height = ( i_h4w >> 2 ); i_loop_height--; )
-    {
-        for( i_loop_width = ( i_width >> 5 ); i_loop_width--; )
-        {
-            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
-                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
-            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
-                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
-
-            DUP4_ARG2( __lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                       vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
-            DUP4_ARG2( __lasx_xvilvh_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                       vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
-
-            src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
-            src1 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x02 );
-            src2 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x02 );
-            src3 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x02 );
-
-            src4 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x13 );
-            src5 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x13 );
-            src6 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x13 );
-            src7 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x13 );
-
-            __lasx_xvst( src0, p_dst, 0 );
-            __lasx_xvstx( src1, p_dst, i_dst_stride );
-            __lasx_xvstx( src2, p_dst, i_dst_stride_x2 );
-            __lasx_xvstx( src3, p_dst, i_dst_stride_x3 );
-            __lasx_xvst( src4, p_dst, 32 );
-            __lasx_xvstx( src5, p_dst, 32 + i_dst_stride );
-            __lasx_xvstx( src6, p_dst, 32 + i_dst_stride_x2 );
-            __lasx_xvstx( src7, p_dst, 32 + i_dst_stride_x3 );
-
-            p_src0 += 32;
-            p_src1 += 32;
-            p_dst += 64;
-        }
-
-        for( i_loop_width = ( ( i_width & 31 ) >> 4 ); i_loop_width--; )
-        {
-            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
-                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
-            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
-                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
-            DUP4_ARG2(__lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                      vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
-            DUP4_ARG2(__lasx_xvilvh_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                      vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
-
-            vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
-            vec_ilv_l1 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x02 );
-            vec_ilv_l2 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x02 );
-            vec_ilv_l3 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x02 );
-
-            __lasx_xvst( vec_ilv_l0, p_dst, 0 );
-            __lasx_xvstx( vec_ilv_l1, p_dst, i_dst_stride );
-            __lasx_xvstx( vec_ilv_l2, p_dst, i_dst_stride_x2 );
-            __lasx_xvstx( vec_ilv_l3, p_dst, i_dst_stride_x3 );
-
-            p_src0 += 16;
-            p_src1 += 16;
-            p_dst += 32;
-        }
-
-        for( i_loop_width = ( i_width & 15 ) >> 3; i_loop_width--; )
-        {
-            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
-                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
-            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
-                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
-            DUP4_ARG2(__lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
-                      vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
-
-            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 8, 1 );
-            p_dst_t = p_dst + i_dst_stride;
-            __lasx_xvstelm_d( vec_ilv_l1, p_dst_t, 0, 0 );
-            __lasx_xvstelm_d( vec_ilv_l1, p_dst_t, 8, 1 );
-            p_dst_t = p_dst_t + i_dst_stride;
-            __lasx_xvstelm_d( vec_ilv_l2, p_dst_t, 0, 0 );
-            __lasx_xvstelm_d( vec_ilv_l2, p_dst_t, 8, 1 );
-            p_dst_t = p_dst_t + i_dst_stride;
-            __lasx_xvstelm_d( vec_ilv_l3, p_dst_t, 0, 0 );
-            __lasx_xvstelm_d( vec_ilv_l3, p_dst_t, 8, 1 );
-
-            p_src0 += 8;
-            p_src1 += 8;
-            p_dst += 16;
-        }
-
-        for( i_loop_width = i_w_mul8; i_loop_width < i_width; i_loop_width++ )
-        {
-            p_dst[0] = p_src0[0];
-            p_dst[1] = p_src1[0];
-
-            p_dst_t = p_dst + i_dst_stride;
-            p_srcA = p_src0 + i_src0_stride;
-            p_srcB = p_src1 + i_src1_stride;
-            p_dst_t[0] = p_srcA[0];
-            p_dst_t[1] = p_srcB[0];
-
-            p_dst_t += i_dst_stride;
-            p_srcA += i_src0_stride;
-            p_srcB += i_src1_stride;
-            p_dst_t[0] = p_srcA[0];
-            p_dst_t[1] = p_srcB[0];
-
-            p_dst_t += i_dst_stride;
-            p_srcA += i_src0_stride;
-            p_srcB += i_src1_stride;
-            p_dst_t[0] = p_srcA[0];
-            p_dst_t[1] = p_srcB[0];
-
-            p_src0 += 1;
-            p_src1 += 1;
-            p_dst += 2;
-        }
-
-        p_src0 += ( ( i_src0_stride << 2 ) - i_width );
-        p_src1 += ( ( i_src1_stride << 2 ) - i_width );
-        p_dst += ( ( i_dst_stride << 2 ) - ( i_width << 1 ) );
-    }
-
-    for( i_loop_height = i_h4w; i_loop_height < i_height; i_loop_height++ )
-    {
-        for( i_loop_width = ( i_width >> 5 ); i_loop_width--; )
-        {
-            src0 = __lasx_xvld( p_src0, 0 );
-            src4 = __lasx_xvld( p_src1, 0 );
-            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
-            vec_ilv_h0 = __lasx_xvilvh_b( src4, src0 );
-
-            src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
-            src1 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x13 );
-            __lasx_xvst( src0, p_dst, 0 );
-            __lasx_xvst( src1, p_dst, 32 );
-
-            p_src0 += 32;
-            p_src1 += 32;
-            p_dst += 64;
-        }
-
-        for( i_loop_width = ( ( i_width &  31 )  >> 4 ); i_loop_width--; )
-        {
-            src0 = __lasx_xvld( p_src0, 0 );
-            src4 = __lasx_xvld( p_src1, 0 );
-            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
-            vec_ilv_h0 = __lasx_xvilvh_b( src4, src0 );
-
-            vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
-            __lasx_xvst( vec_ilv_l0, p_dst, 0 );
-
-            p_src0 += 16;
-            p_src1 += 16;
-            p_dst += 32;
-        }
-
-        for( i_loop_width = ( i_width & 15 ) >> 3; i_loop_width--; )
-        {
-            src0 = __lasx_xvld( p_src0, 0 );
-            src4 = __lasx_xvld( p_src1, 0 );
-            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
-            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 0, 0 );
-            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 8, 1 );
-
-            p_src0 += 8;
-            p_src1 += 8;
-            p_dst += 16;
-        }
-
-        for( i_loop_width = i_w_mul8; i_loop_width < i_width; i_loop_width++ )
-        {
-            p_dst[0] = p_src0[0];
-            p_dst[1] = p_src1[0];
-            p_src0 += 1;
-            p_src1 += 1;
-            p_dst += 2;
-        }
-
-        p_src0 += ( i_src0_stride - i_width );
-        p_src1 += ( i_src1_stride - i_width );
-        p_dst += ( i_dst_stride - ( i_width << 1 ) );
-    }
-}
-
-static void core_store_interleave_chroma_lasx( uint8_t *p_src0,
-                                               int32_t i_src0_stride,
-                                               uint8_t *p_src1,
-                                               int32_t i_src1_stride,
-                                               uint8_t *p_dst,
-                                               int32_t i_dst_stride,
-                                               int32_t i_height )
-{
-    int32_t i_loop_height, i_h4w;
-    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
-    __m256i tmp0, tmp1, tmp2, tmp3;
-    int32_t i_src0_stride_x2 = i_src0_stride << 1;
-    int32_t i_src1_stride_x2 = i_src1_stride << 1;
-    int32_t i_src0_stride_x4 = i_src0_stride << 2;
-    int32_t i_src1_stride_x4 = i_src1_stride << 2;
-    int32_t i_src0_stride_x3 = i_src0_stride_x2 + i_src0_stride;
-    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
-
-    i_h4w = i_height & 3;
-    for( i_loop_height = ( i_height >> 2 ); i_loop_height--; )
-    {
-        DUP4_ARG2( __lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
-                   i_src0_stride_x2, p_src0, i_src0_stride_x3, in0, in1, in2, in3 );
-        p_src0 += i_src0_stride_x4;
-        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
-                   i_src1_stride_x2, p_src1, i_src1_stride_x3, in4, in5, in6, in7 );
-        p_src1 += i_src1_stride_x4;
-        DUP4_ARG2( __lasx_xvilvl_b, in4, in0, in5, in1, in6, in2, in7, in3,
-                   tmp0, tmp1, tmp2, tmp3 );
-
-        __lasx_xvstelm_d( tmp0, p_dst, 0, 0 );
-        __lasx_xvstelm_d( tmp0, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( tmp1, p_dst, 0, 0 );
-        __lasx_xvstelm_d( tmp1, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( tmp2, p_dst, 0, 0 );
-        __lasx_xvstelm_d( tmp2, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( tmp3, p_dst, 0, 0 );
-        __lasx_xvstelm_d( tmp3, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-    }
-
-    for( i_loop_height = i_h4w; i_loop_height--; )
-    {
-        in0 = __lasx_xvld( p_src0, 0 );
-        p_src0 += i_src0_stride;
-        in1 = __lasx_xvld( p_src1, 0 );
-        p_src1 += i_src1_stride;
-
-        tmp0 = __lasx_xvilvl_b( in1, in0 );
-
-        __lasx_xvstelm_d( tmp0, p_dst, 0, 0 );
-        __lasx_xvstelm_d( tmp0, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static void plane_copy_deinterleave_lasx( uint8_t *p_dst0,
-                                          intptr_t i_dst_stride0,
-                                          uint8_t *p_dst1,
-                                          intptr_t i_dst_stride1,
-                                          uint8_t *p_src, intptr_t i_src_stride,
-                                          int32_t i_width, int32_t i_height )
-{
-    core_plane_copy_deinterleave_lasx( p_src, i_src_stride,
-                                       p_dst0, i_dst_stride0,
-                                       p_dst1, i_dst_stride1,
-                                       i_width, i_height );
-}
-
-static void load_deinterleave_chroma_fenc_lasx( uint8_t *p_dst, uint8_t *p_src,
-                                                intptr_t i_src_stride,
-                                                int32_t i_height )
-{
-    core_plane_copy_deinterleave_lasx( p_src, i_src_stride, p_dst, FENC_STRIDE,
-                                       ( p_dst + ( FENC_STRIDE / 2 ) ),
-                                       FENC_STRIDE, 8, i_height );
-}
-
-static void plane_copy_interleave_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
-                                       uint8_t *p_src0, intptr_t i_src_stride0,
-                                       uint8_t *p_src1, intptr_t i_src_stride1,
-                                       int32_t i_width, int32_t i_height )
-{
-    core_plane_copy_interleave_lasx( p_src0, i_src_stride0,
-                                     p_src1, i_src_stride1,
-                                     p_dst, i_dst_stride,
-                                     i_width, i_height );
-}
-
-static void load_deinterleave_chroma_fdec_lasx( uint8_t *p_dst, uint8_t *p_src,
-                                                intptr_t i_src_stride,
-                                                int32_t i_height )
-{
-    core_plane_copy_deinterleave_lasx( p_src, i_src_stride, p_dst, FDEC_STRIDE,
-                                       ( p_dst + ( FDEC_STRIDE / 2 ) ),
-                                       FDEC_STRIDE, 8, i_height );
-}
-
-static void store_interleave_chroma_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
-                                          uint8_t *p_src0, uint8_t *p_src1,
-                                          int32_t i_height )
-{
-    core_store_interleave_chroma_lasx( p_src0, FDEC_STRIDE, p_src1, FDEC_STRIDE,
-                                       p_dst, i_dst_stride, i_height );
-}
-
-static void memzero_aligned_lasx( void *p_dst, size_t n )
-{
-    uint32_t i_tot32 = n >> 5;
-    uint32_t i_remain = n - ( i_tot32 << 5 );
-    int8_t i_cnt;
-    __m256i zero = __lasx_xvldi( 0 );
-
-    for ( i_cnt = i_tot32; i_cnt--; )
-    {
-        __lasx_xvst( zero, p_dst, 0 );
-        p_dst += 32;
-    }
-
-    if( i_remain )
-    {
-        memset( p_dst, 0, i_remain );
-    }
-}
-
-static void prefetch_ref_lasx( uint8_t *pix, intptr_t stride, int32_t parity )
-{
-    int32_t tmp = 0;
-    uint8_t *pix_tmp = pix, *pix_tmp2 = pix;
-
-    __asm__ volatile(
-    "addi.d    %[parity],    %[parity],      -1                   \n\t"
-    "addi.d    %[pix],       %[pix],         64                   \n\t"
-    "and       %[parity],    %[parity],      %[stride]            \n\t"
-    "slli.d    %[tmp],       %[parity],      3                    \n\t"
-    "add.d     %[pix_tmp],   %[pix],         %[tmp]               \n\t"
-    "slli.d    %[tmp],       %[stride],      1                    \n\t"
-    "add.d     %[parity],    %[stride],      %[tmp]               \n\t"
-    "preld     0,            %[pix_tmp],     0                    \n\t"
-    "add.d     %[pix_tmp2],  %[pix_tmp],     %[stride]            \n\t"
-    "preld     0,            %[pix_tmp2],    0                    \n\t"
-    "add.d     %[pix_tmp2],  %[pix_tmp2],    %[stride]            \n\t"
-    "preld     0,            %[pix_tmp2],    0                    \n\t"
-    "add.d     %[pix_tmp],   %[pix_tmp],     %[parity]            \n\t"
-    "preld     0,            %[pix_tmp],     0                    \n\t"
-    "add.d     %[pix],       %[pix_tmp2],    %[tmp]               \n\t"
-    "preld     0,            %[pix],         0                    \n\t"
-    "add.d     %[pix_tmp],   %[pix],         %[stride]            \n\t"
-    "preld     0,            %[pix_tmp],     0                    \n\t"
-    "add.d     %[pix_tmp],   %[pix_tmp],     %[stride]            \n\t"
-    "preld     0,            %[pix_tmp],     0                    \n\t"
-    "add.d     %[pix],       %[pix],         %[parity]            \n\t"
-    "preld     0,            %[pix],         0                    \n\t"
-     : [tmp]"+&r"(tmp), [pix_tmp]"+&r"(pix_tmp),
-       [pix_tmp2]"+&r"(pix_tmp2), [pix]"+&r"(pix),
-       [parity]"+&r"(parity)
-     : [stride]"r"(stride)
-     :
-    );
-}
-
-static void prefetch_fenc_422_lasx( uint8_t *pix_y, intptr_t stride_y,
-                                    uint8_t *pix_uv, intptr_t stride_uv,
-                                    int32_t mb_x )
-{
-    int64_t num1 = 0;
-    int64_t num2 = 0;
-    uint8_t *y_tmp = pix_y, *uv_tmp = pix_uv;
-
-    __asm__ volatile(
-    "andi      %[num1],      %[mb_x],         3                  \n\t"
-    "mul.d     %[num1],      %[num1],         %[stride_y]        \n\t"
-    "andi      %[mb_x],      %[mb_x],         6                  \n\t"
-    "mul.d     %[num2],      %[mb_x],         %[stride_uv]       \n\t"
-    "addi.d    %[pix_y],     %[pix_y],        64                 \n\t"
-    "addi.d    %[pix_uv],    %[pix_uv],       64                 \n\t"
-    "slli.d    %[num1],      %[num1],         2                  \n\t"
-    "add.d     %[pix_y],     %[pix_y],        %[num1]            \n\t"
-    "preld     0,            %[pix_y],        0                  \n\t"
-    "add.d     %[y_tmp],     %[pix_y],        %[stride_y]        \n\t"
-    "preld     0,            %[y_tmp],        0                  \n\t"
-    "add.d     %[pix_y],     %[y_tmp],        %[stride_y]        \n\t"
-    "preld     0,            %[pix_y],        0                  \n\t"
-    "slli.d    %[num2],      %[num2],         2                  \n\t"
-    "add.d     %[pix_y],     %[pix_y],        %[stride_y]        \n\t"
-    "preld     0,            %[pix_y],        0                  \n\t"
-    "add.d     %[pix_uv],    %[pix_uv],       %[num2]            \n\t"
-    "preld     0,            %[pix_uv],       0                  \n\t"
-    "add.d     %[uv_tmp],    %[pix_uv],       %[stride_uv]       \n\t"
-    "preld     0,            %[uv_tmp],       0                  \n\t"
-    "add.d     %[pix_uv],    %[uv_tmp],       %[stride_uv]       \n\t"
-    "preld     0,            %[pix_uv],       0                  \n\t"
-    "add.d     %[pix_uv],    %[pix_uv],       %[stride_uv]       \n\t"
-    "preld     0,            %[pix_uv],       0                  \n\t"
-     : [y_tmp]"+&r"(y_tmp),
-       [uv_tmp]"+&r"(uv_tmp),
-       [num2]"+&r"(num2),
-       [num1]"+&r"(num1),
-       [mb_x]"+&r"(mb_x),
-       [pix_y]"+&r"(pix_y),
-       [pix_uv]"+&r"(pix_uv)
-     : [stride_y]"r"(stride_y), [stride_uv]"r"(stride_uv)
-     :
-    );
-}
-
-static void prefetch_fenc_420_lasx( uint8_t *pix_y, intptr_t stride_y,
-                                    uint8_t *pix_uv, intptr_t stride_uv,
-                                    int32_t mb_x )
-{
-    int64_t num1 = 0;
-    int64_t num2 = 0;
-    uint8_t *y_tmp = pix_y;
-
-    __asm__ volatile(
-    "andi      %[num1],      %[mb_x],         3                  \n\t"
-    "mul.d     %[num1],      %[num1],         %[stride_y]        \n\t"
-    "andi      %[mb_x],      %[mb_x],         6                  \n\t"
-    "mul.d     %[num2],      %[mb_x],         %[stride_uv]       \n\t"
-    "addi.d    %[pix_y],     %[pix_y],        64                 \n\t"
-    "addi.d    %[pix_uv],    %[pix_uv],       64                 \n\t"
-    "slli.d    %[num1],      %[num1],         2                  \n\t"
-    "add.d     %[pix_y],     %[pix_y],        %[num1]            \n\t"
-    "preld     0,            %[pix_y],        0                  \n\t"
-    "add.d     %[y_tmp],     %[pix_y],        %[stride_y]        \n\t"
-    "preld     0,            %[y_tmp],        0                  \n\t"
-    "add.d     %[pix_y],     %[y_tmp],        %[stride_y]        \n\t"
-    "preld     0,            %[pix_y],        0                  \n\t"
-    "slli.d    %[num2],      %[num2],         2                  \n\t"
-    "add.d     %[pix_y],     %[pix_y],        %[stride_y]        \n\t"
-    "preld     0,            %[pix_y],        0                  \n\t"
-    "add.d     %[pix_uv],    %[pix_uv],       %[num2]            \n\t"
-    "preld     0,            %[pix_uv],       0                  \n\t"
-    "add.d     %[pix_uv],    %[pix_uv],       %[stride_uv]       \n\t"
-    "preld     0,            %[pix_uv],       0                  \n\t"
-     : [y_tmp]"+&r"(y_tmp),
-       [num2]"+&r"(num2),
-       [num1]"+&r"(num1),
-       [mb_x]"+&r"(mb_x),
-       [pix_y]"+&r"(pix_y),
-       [pix_uv]"+&r"(pix_uv)
-     : [stride_y]"r"(stride_y), [stride_uv]"r"(stride_uv)
-     :
-    );
-}
+PLANE_INTERLEAVE(lasx)
+PLANE_COPY_YUYV(32, lasx)
 
 #define x264_mc_chroma_lasx x264_template(mc_chroma_lasx)
 void x264_mc_chroma_lasx( uint8_t *p_dst_u, uint8_t *p_dst_v,
@@ -1941,27 +202,27 @@ void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
         pf->offsetsub = mc_weight_wtab_lasx;
         pf->weight_cache = weight_cache_lasx;
 
-        pf->copy_16x16_unaligned = mc_copy_w16_lasx;
-        pf->copy[PIXEL_16x16] = mc_copy_w16_lasx;
-        pf->copy[PIXEL_8x8] = mc_copy_w8_lasx;
-        pf->copy[PIXEL_4x4] = mc_copy_w4_lasx;
+        pf->copy_16x16_unaligned = x264_mc_copy_w16_lasx;
+        pf->copy[PIXEL_16x16] = x264_mc_copy_w16_lasx;
+        pf->copy[PIXEL_8x8] = x264_mc_copy_w8_lasx;
+        pf->copy[PIXEL_4x4] = x264_mc_copy_w4_lasx;
 
-        pf->store_interleave_chroma = store_interleave_chroma_lasx;
-        pf->load_deinterleave_chroma_fenc = load_deinterleave_chroma_fenc_lasx;
-        pf->load_deinterleave_chroma_fdec = load_deinterleave_chroma_fdec_lasx;
+        pf->store_interleave_chroma = x264_store_interleave_chroma_lasx;
+        pf->load_deinterleave_chroma_fenc = x264_load_deinterleave_chroma_fenc_lasx;
+        pf->load_deinterleave_chroma_fdec = x264_load_deinterleave_chroma_fdec_lasx;
 
         pf->plane_copy_interleave = plane_copy_interleave_lasx;
-        pf->plane_copy_deinterleave = plane_copy_deinterleave_lasx;
-        pf->plane_copy_deinterleave_yuyv = plane_copy_deinterleave_lasx;
+        pf->plane_copy_deinterleave = x264_plane_copy_deinterleave_lasx;
+        pf->plane_copy_deinterleave_yuyv = plane_copy_deinterleave_yuyv_lasx;
 
         pf->hpel_filter = x264_hpel_filter_lasx;
         pf->memcpy_aligned = x264_memcpy_aligned_lasx;
-        pf->memzero_aligned = memzero_aligned_lasx;
-        pf->frame_init_lowres_core = frame_init_lowres_core_lasx;
+        pf->memzero_aligned = x264_memzero_aligned_lasx;
+        pf->frame_init_lowres_core = x264_frame_init_lowres_core_lasx;
 
-        pf->prefetch_fenc_420 = prefetch_fenc_420_lasx;
-        pf->prefetch_fenc_422 = prefetch_fenc_422_lasx;
-        pf->prefetch_ref  = prefetch_ref_lasx;
+        pf->prefetch_fenc_420 = x264_prefetch_fenc_420_lasx;
+        pf->prefetch_fenc_422 = x264_prefetch_fenc_422_lasx;
+        pf->prefetch_ref  = x264_prefetch_ref_lasx;
     }
 #endif // !HIGH_BIT_DEPTH
 }
diff --git a/common/loongarch/mc.h b/common/loongarch/mc.h
index ed91585b..fc582944 100644
--- a/common/loongarch/mc.h
+++ b/common/loongarch/mc.h
@@ -49,6 +49,15 @@ void x264_pixel_avg_4x4_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t
 #define x264_pixel_avg_4x2_lasx x264_template(pixel_avg_4x2_lasx)
 void x264_pixel_avg_4x2_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
 
+#define x264_pixel_avg2_w4_lasx x264_template(pixel_avg2_w4_lasx)
+void x264_pixel_avg2_w4_lasx ( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int );
+#define x264_pixel_avg2_w8_lasx x264_template(pixel_avg2_w8_lasx)
+void x264_pixel_avg2_w8_lasx ( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int );
+#define x264_pixel_avg2_w16_lasx x264_template(pixel_avg2_w16_lasx)
+void x264_pixel_avg2_w16_lasx ( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int );
+#define x264_pixel_avg2_w20_lasx x264_template(pixel_avg2_w20_lasx)
+void x264_pixel_avg2_w20_lasx ( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int );
+
 #define x264_mc_weight_w20_lasx x264_template(mc_weight_w20_lasx)
 void x264_mc_weight_w20_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
 #define x264_mc_weight_w20_noden_lasx x264_template(mc_weight_w20_noden_lasx)
@@ -66,7 +75,49 @@ void x264_mc_weight_w4_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_we
 #define x264_mc_weight_w4_noden_lasx x264_template(mc_weight_w4_noden_lasx)
 void x264_mc_weight_w4_noden_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
 
+#define x264_mc_copy_w16_lasx x264_template(mc_copy_w16_lasx)
+void x264_mc_copy_w16_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_mc_copy_w8_lasx x264_template(mc_copy_w8_lasx)
+void x264_mc_copy_w8_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_mc_copy_w4_lasx x264_template(mc_copy_w4_lasx)
+void x264_mc_copy_w4_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+
+#define x264_store_interleave_chroma_lasx x264_template(store_interleave_chroma_lasx)
+void x264_store_interleave_chroma_lasx( pixel *dst, intptr_t i_dst, pixel *srcu, pixel *srcv, int height );
+#define x264_load_deinterleave_chroma_fenc_lasx x264_template(load_deinterleave_chroma_fenc_lasx)
+void x264_load_deinterleave_chroma_fenc_lasx( pixel *dst, pixel *src, intptr_t i_src, int height );
+#define x264_load_deinterleave_chroma_fdec_lasx x264_template(load_deinterleave_chroma_fdec_lasx)
+void x264_load_deinterleave_chroma_fdec_lasx( pixel *dst, pixel *src, intptr_t i_src, int height );
+
+#define x264_plane_copy_interleave_core_lasx x264_template(plane_copy_interleave_core_lasx)
+void x264_plane_copy_interleave_core_lasx( pixel *dst,  intptr_t i_dst,
+                                           pixel *srcu, intptr_t i_srcu,
+                                           pixel *srcv, intptr_t i_srcv, int w, int h );
+#define x264_plane_copy_deinterleave_lasx x264_template(plane_copy_deinterleave_lasx)
+void x264_plane_copy_deinterleave_lasx( pixel *dstu, intptr_t i_dstu,
+                                        pixel *dstv, intptr_t i_dstv,
+                                        pixel *src,  intptr_t i_src, int w, int h );
+
+#define x264_prefetch_fenc_420_lasx x264_template(prefetch_fenc_420_lasx)
+void x264_prefetch_fenc_420_lasx( uint8_t *pix_y, intptr_t stride_y,
+                                  uint8_t *pix_uv, intptr_t stride_uv,
+                                  int32_t mb_x );
+#define x264_prefetch_fenc_422_lasx x264_template(prefetch_fenc_422_lasx)
+void x264_prefetch_fenc_422_lasx( uint8_t *pix_y, intptr_t stride_y,
+                                  uint8_t *pix_uv, intptr_t stride_uv,
+                                  int32_t mb_x );
+#define x264_prefetch_ref_lasx x264_template(prefetch_ref_lasx)
+void x264_prefetch_ref_lasx( uint8_t *pix, intptr_t stride, int32_t parity );
+
+#define x264_memcpy_aligned_lasx x264_template(memcpy_aligned_lasx)
+void *x264_memcpy_aligned_lasx( void *dst, const void *src, size_t n );
+#define x264_memzero_aligned_lasx x264_template(memzero_aligned_lasx)
+void x264_memzero_aligned_lasx( void *p_dst, size_t n );
+
 #define x264_hpel_filter_lasx x264_template(hpel_filter_lasx)
 void x264_hpel_filter_lasx( pixel *, pixel *, pixel *, pixel *, intptr_t, int, int, int16_t * );
+#define x264_frame_init_lowres_core_lasx x264_template(frame_init_lowres_core_lasx)
+void x264_frame_init_lowres_core_lasx( uint8_t *, uint8_t *, uint8_t *, uint8_t *, uint8_t *,
+                                       intptr_t, intptr_t, int, int );
 
 #endif
diff --git a/common/loongarch/pixel-a.S b/common/loongarch/pixel-a.S
index 776bfe07..b22fbf41 100644
--- a/common/loongarch/pixel-a.S
+++ b/common/loongarch/pixel-a.S
@@ -185,6 +185,111 @@ endfunc
 HADAMARD_AC_WXH_LASX 16, 8
 HADAMARD_AC_WXH_LASX 16, 16
 
+/* uint64_t hadamard_ac_8x8_lasx(uint8_t *p_pix,
+ *                               int32_t i_stride)
+ */
+function hadamard_ac_8x8_lasx
+/* Load intermediate variable */
+    slli.d          t0,    a1,   1
+    add.d           t1,    a1,   t0
+    slli.d          t2,    a1,   2
+
+    LSX_LOADX_4     a0,    a1,   t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d           a0,    a0,   t2
+    LSX_LOADX_4     a0,    a1,   t0,  t1,  vr4,  vr5,  vr6,  vr7
+
+    vilvl.d         vr8,   vr1,  vr0
+    vilvl.d         vr9,   vr3,  vr2
+    vilvl.d         vr10,  vr5,  vr4
+    vilvl.d         vr11,  vr7,  vr6
+    xvpermi.q       xr8,   xr10, 0x02
+    xvpermi.q       xr9,   xr11, 0x02
+    xvpickev.b      xr12,  xr9,  xr8
+    xvpickod.b      xr13,  xr9,  xr8
+    xvaddwev.h.bu   xr8,   xr12, xr13
+    xvaddwod.h.bu   xr9,   xr12, xr13
+    xvsubwev.h.bu   xr10,  xr12, xr13
+    xvsubwod.h.bu   xr11,  xr12, xr13
+    xvadd.h         xr12,  xr8,  xr9
+    xvadd.h         xr13,  xr10, xr11
+    xvsub.h         xr14,  xr8,  xr9
+    xvsub.h         xr15,  xr10, xr11
+
+    xvilvl.h        xr8,   xr13, xr12
+    xvilvh.h        xr9,   xr13, xr12
+    xvilvl.h        xr10,  xr15, xr14
+    xvilvh.h        xr11,  xr15, xr14
+    xvilvl.w        xr12,  xr10, xr8
+    xvilvh.w        xr13,  xr10, xr8
+    xvilvl.w        xr14,  xr11, xr9
+    xvilvh.w        xr15,  xr11, xr9
+    xvadd.h         xr8,   xr12, xr13
+    xvadd.h         xr9,   xr14, xr15
+    xvsub.h         xr10,  xr12, xr13
+    xvsub.h         xr11,  xr14, xr15
+    xvadd.h         xr12,  xr8,  xr9
+    xvadd.h         xr13,  xr10, xr11
+    xvsub.h         xr14,  xr8,  xr9
+    xvsub.h         xr15,  xr10, xr11
+
+    vpickve2gr.hu   t3,    vr12,  0
+    vpickve2gr.hu   t4,    vr12,  4
+    xvor.v          xr16,  xr12,  xr12
+    xvpermi.q       xr16,  xr16,  0x31
+    vpickve2gr.hu   t5,    vr16,  0
+    vpickve2gr.hu   t6,    vr16,  4
+    add.d           t3,    t3,    t4
+    add.d           t5,    t5,    t6
+    add.d           t3,    t3,    t5
+
+    xvadda.h        xr16,  xr12,  xr13
+    xvadda.h        xr18,  xr14,  xr15
+    xvadd.h         xr16,  xr16,  xr18
+    xvpermi.d       xr17,  xr16,  0x4e
+    xvadd.h         xr18,  xr16,  xr17
+    xvhaddw.wu.hu   xr18,  xr18,  xr18
+    xvhaddw.du.wu   xr18,  xr18,  xr18
+    xvhaddw.qu.du   xr18,  xr18,  xr18
+    xvpickve2gr.wu  t4,    xr18,  0
+
+    xvpackev.h      xr8,   xr13,  xr12
+    xvpackev.h      xr9,   xr15,  xr14
+    xvpackod.h      xr10,  xr13,  xr12
+    xvpackod.h      xr11,  xr15,  xr14
+    xvilvl.d        xr12,  xr9,   xr8
+    xvilvh.d        xr13,  xr9,   xr8
+    xvilvl.d        xr14,  xr11,  xr10
+    xvilvh.d        xr15,  xr11,  xr10
+    xvor.v          xr16,  xr12,  xr12
+    xvor.v          xr17,  xr13,  xr13
+    xvpermi.q       xr12,  xr14,  0x02
+    xvpermi.q       xr13,  xr14,  0x12
+    xvpermi.q       xr16,  xr15,  0x03
+    xvpermi.q       xr17,  xr15,  0x13
+
+    xvadd.h         xr8,   xr12,  xr13
+    xvsub.h         xr9,   xr12,  xr13
+    xvadd.h         xr10,  xr16,  xr17
+    xvsub.h         xr11,  xr16,  xr17
+    xvadd.h         xr12,  xr8,   xr10
+    xvadd.h         xr13,  xr9,   xr11
+    xvsub.h         xr14,  xr8,   xr10
+    xvsub.h         xr15,  xr9,   xr11
+    xvadda.h        xr16,  xr12,  xr13
+    xvadda.h        xr17,  xr14,  xr15
+    xvadd.h         xr18,  xr16,  xr17
+    xvpermi.d       xr19,  xr18,  0x4e
+    xvadd.d         xr19,  xr18,  xr19
+    xvhaddw.wu.hu   xr19,  xr19,  xr19
+    xvhaddw.du.wu   xr19,  xr19,  xr19
+    xvhaddw.qu.du   xr19,  xr19,  xr19
+    xvpickve2gr.wu  t5,    xr19,  0
+
+    sub.d           t4,    t4,    t3
+    sub.d           t5,    t5,    t3
+    slli.d          t5,    t5,    32
+    add.d           a0,    t5,    t4
+endfunc
 
 /* int x264_pixel_satd_16x16_lasx(pixel *pix1, intptr_t i_pix1,
  *                                pixel *pix2, intptr_t i_pix2)
@@ -719,6 +824,121 @@ function pixel_satd_8x4_lasx
     srli.d          a0,    t4,   1
 endfunc
 
+/* int x264_pixel_satd_4x16_lasx(pixel *pix1, intptr_t i_pix1,
+ *                               pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_satd_4x16_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    add.d           t4,    a1,   t2
+    add.d           t5,    a3,   t3
+    // Load data from pix1 and pix2
+    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
+    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+    vilvl.w         vr1,   vr2,  vr1
+    vilvl.w         vr3,   vr4,  vr3
+    vilvl.d         vr9,   vr3,  vr1
+    vilvl.w         vr5,   vr6,  vr5
+    vilvl.w         vr7,   vr8,  vr7
+    vilvl.d         vr10,  vr7,  vr5
+
+    slli.d          t0,    a1,   2
+    slli.d          t1,    a3,   2
+    // Load data from pix1 and pix2
+    add.d           a0,    a0,   t0
+    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
+    add.d           a2,    a2,   t1
+    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+    vilvl.w         vr1,   vr2,  vr1
+    vilvl.w         vr3,   vr4,  vr3
+    vilvl.d         vr1,   vr3,  vr1
+    vilvl.w         vr5,   vr6,  vr5
+    vilvl.w         vr7,   vr8,  vr7
+    vilvl.d         vr5,   vr7,  vr5
+    xvpermi.q       xr1,   xr9,  0x20
+    xvpermi.q       xr5,   xr10, 0x20
+
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* b0 + b1 */
+    xvsub.h         xr12,  xr9,  xr10  /* b0 - b1 */
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadda.h        xr9,   xr9,  xr10
+    xvhaddw.wu.hu   xr9,   xr9,  xr9
+    xvhaddw.du.wu   xr9,   xr9,  xr9
+    xvhaddw.qu.du   xr9,   xr9,  xr9
+    xvpickve2gr.wu  t6,    xr9,  0
+    xvpickve2gr.wu  t7,    xr9,  4
+    add.d           t7,    t6,   t7
+
+    // Load data from pix1 and pix2
+    add.d           a0,    a0,   t0
+    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
+    add.d           a2,    a2,   t1
+    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+    vilvl.w         vr1,   vr2,  vr1
+    vilvl.w         vr3,   vr4,  vr3
+    vilvl.d         vr9,   vr3,  vr1
+    vilvl.w         vr5,   vr6,  vr5
+    vilvl.w         vr7,   vr8,  vr7
+    vilvl.d         vr10,  vr7,  vr5
+
+    // Load data from pix1 and pix2
+    add.d           a0,    a0,   t0
+    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
+    add.d           a2,    a2,   t1
+    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+    vilvl.w         vr1,   vr2,  vr1
+    vilvl.w         vr3,   vr4,  vr3
+    vilvl.d         vr1,   vr3,  vr1
+    vilvl.w         vr5,   vr6,  vr5
+    vilvl.w         vr7,   vr8,  vr7
+    vilvl.d         vr5,   vr7,  vr5
+    xvpermi.q       xr1,   xr9,  0x20
+    xvpermi.q       xr5,   xr10, 0x20
+
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* b0 + b1 */
+    xvsub.h         xr12,  xr9,  xr10  /* b0 - b1 */
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadda.h        xr9,   xr9,  xr10
+    xvhaddw.wu.hu   xr9,   xr9,  xr9
+    xvhaddw.du.wu   xr9,   xr9,  xr9
+    xvhaddw.qu.du   xr9,   xr9,  xr9
+    xvpickve2gr.wu  t6,    xr9,  0
+    xvpickve2gr.wu  t5,    xr9,  4
+    add.d           t6,    t5,   t6
+    add.d           t7,    t6,   t7
+    srli.d          a0,    t7,   1
+endfunc
+
 /* int x264_pixel_satd_4x8_lasx(pixel *pix1, intptr_t i_pix1,
  *                              pixel *pix2, intptr_t i_pix2)
  */
@@ -964,10 +1184,87 @@ function pixel_ssd_16x16_lasx
 endfunc
 
 /*
- * int pixel_ssd_8x8_lasx(const Pixel *pix1, intptr_t stride_pix1,
- *                        const Pixel *pix2, intptr_t stride_pix2)
+ * int pixel_ssd_16x8_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                         const Pixel *pix2, intptr_t stride_pix2)
  */
-function pixel_ssd_8x8_lasx
+function pixel_ssd_16x8_lasx
+    slli.d         t0,     a1,    1
+    add.d          t1,     a1,    t0
+    add.d          t2,     a1,    t1
+    slli.d         t3,     a3,    1
+    add.d          t4,     a3,    t3
+    add.d          t5,     a3,    t4
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr2,    xr2
+    vext2xv.hu.bu  xr3,    xr3
+    vext2xv.hu.bu  xr4,    xr4
+    vext2xv.hu.bu  xr5,    xr5
+    vext2xv.hu.bu  xr6,    xr6
+    vext2xv.hu.bu  xr7,    xr7
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+    vext2xv.hu.bu  xr10,   xr10
+    vext2xv.hu.bu  xr11,   xr11
+    vext2xv.hu.bu  xr12,   xr12
+    vext2xv.hu.bu  xr13,   xr13
+    vext2xv.hu.bu  xr14,   xr14
+    vext2xv.hu.bu  xr15,   xr15
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvsub.h        xr2,    xr2,   xr10
+    xvsub.h        xr3,    xr3,   xr11
+    xvsub.h        xr4,    xr4,   xr12
+    xvsub.h        xr5,    xr5,   xr13
+    xvsub.h        xr6,    xr6,   xr14
+    xvsub.h        xr7,    xr7,   xr15
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvmul.h        xr2,    xr2,   xr2
+    xvmul.h        xr3,    xr3,   xr3
+    xvmul.h        xr4,    xr4,   xr4
+    xvmul.h        xr5,    xr5,   xr5
+    xvmul.h        xr6,    xr6,   xr6
+    xvmul.h        xr7,    xr7,   xr7
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr2,    xr2,   xr2
+    xvhaddw.wu.hu  xr3,    xr3,   xr3
+    xvhaddw.wu.hu  xr4,    xr4,   xr4
+    xvhaddw.wu.hu  xr5,    xr5,   xr5
+    xvhaddw.wu.hu  xr6,    xr6,   xr6
+    xvhaddw.wu.hu  xr7,    xr7,   xr7
+    xvadd.w        xr0,    xr0,   xr1
+    xvadd.w        xr2,    xr2,   xr3
+    xvadd.w        xr4,    xr4,   xr5
+    xvadd.w        xr6,    xr6,   xr7
+    xvadd.w        xr0,    xr0,   xr2
+    xvadd.w        xr4,    xr4,   xr6
+    xvadd.w        xr0,    xr0,   xr4
+
+    // Calculate the sum
+    xvhaddw.d.w    xr0,    xr0,   xr0
+    xvhaddw.q.d    xr0,    xr0,   xr0
+    xvpickve2gr.w  t2,     xr0,   0
+    xvpickve2gr.w  t3,     xr0,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+/*
+ * int pixel_ssd_8x16_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                         const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_ssd_8x16_lasx
     slli.d         t0,     a1,    1
     add.d          t1,     a1,    t0
     add.d          t2,     a1,    t1
@@ -1000,6 +1297,50 @@ function pixel_ssd_8x8_lasx
     vext2xv.hu.bu  xr10,   xr10
     vext2xv.hu.bu  xr11,   xr11
 
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvsub.h        xr2,    xr2,   xr10
+    xvsub.h        xr3,    xr3,   xr11
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvmul.h        xr2,    xr2,   xr2
+    xvmul.h        xr3,    xr3,   xr3
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr2,    xr2,   xr2
+    xvhaddw.wu.hu  xr3,    xr3,   xr3
+    xvadd.w        xr0,    xr0,   xr1
+    xvadd.w        xr2,    xr2,   xr3
+    xvadd.w        xr16,   xr0,   xr2
+
+    // Load data from pix1 and pix2
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+
+    vilvl.d        vr0,    vr4,   vr0
+    vilvl.d        vr1,    vr5,   vr1
+    vilvl.d        vr2,    vr6,   vr2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr8,    vr12,  vr8
+    vilvl.d        vr9,    vr13,  vr9
+    vilvl.d        vr10,   vr14,  vr10
+    vilvl.d        vr11,   vr15,  vr11
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr2,    xr2
+    vext2xv.hu.bu  xr3,    xr3
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+    vext2xv.hu.bu  xr10,   xr10
+    vext2xv.hu.bu  xr11,   xr11
+
     // Calculate the square of the difference
     xvsub.h        xr0,    xr0,   xr8
     xvsub.h        xr1,    xr1,   xr9
@@ -1016,6 +1357,7 @@ function pixel_ssd_8x8_lasx
     xvadd.w        xr0,    xr0,   xr1
     xvadd.w        xr2,    xr2,   xr3
     xvadd.w        xr0,    xr0,   xr2
+    xvadd.w        xr0,    xr0,   xr16
 
     // Calculate the sum
     xvhaddw.d.w    xr0,    xr0,   xr0
@@ -1026,20 +1368,269 @@ function pixel_ssd_8x8_lasx
 endfunc
 
 /*
- * int pixel_ssd_4x4_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ * int pixel_ssd_8x8_lasx(const Pixel *pix1, intptr_t stride_pix1,
  *                        const Pixel *pix2, intptr_t stride_pix2)
  */
-function pixel_ssd_4x4_lasx
+function pixel_ssd_8x8_lasx
     slli.d         t0,     a1,    1
-    slli.d         t1,     a3,    1
-    add.d          t2,     t0,    a1
-    add.d          t3,     t1,    a3
+    add.d          t1,     a1,    t0
+    add.d          t2,     a1,    t1
+    slli.d         t3,     a3,    1
+    add.d          t4,     a3,    t3
+    add.d          t5,     a3,    t4
 
     // Load data from pix1 and pix2
-    LSX_LOADX_4    a0,     a1,    t0,  t2,  vr4, vr6, vr8, vr10
-    LSX_LOADX_4    a2,     a3,    t1,  t3,  vr5, vr7, vr9, vr11
-    vilvl.w        vr4,    vr6,   vr4
-    vilvl.w        vr5,    vr7,   vr5
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+
+    vilvl.d        vr0,    vr4,   vr0
+    vilvl.d        vr1,    vr5,   vr1
+    vilvl.d        vr2,    vr6,   vr2
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr8,    vr12,  vr8
+    vilvl.d        vr9,    vr13,  vr9
+    vilvl.d        vr10,   vr14,  vr10
+    vilvl.d        vr11,   vr15,  vr11
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr2,    xr2
+    vext2xv.hu.bu  xr3,    xr3
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+    vext2xv.hu.bu  xr10,   xr10
+    vext2xv.hu.bu  xr11,   xr11
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvsub.h        xr2,    xr2,   xr10
+    xvsub.h        xr3,    xr3,   xr11
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvmul.h        xr2,    xr2,   xr2
+    xvmul.h        xr3,    xr3,   xr3
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr2,    xr2,   xr2
+    xvhaddw.wu.hu  xr3,    xr3,   xr3
+    xvadd.w        xr0,    xr0,   xr1
+    xvadd.w        xr2,    xr2,   xr3
+    xvadd.w        xr0,    xr0,   xr2
+
+    // Calculate the sum
+    xvhaddw.d.w    xr0,    xr0,   xr0
+    xvhaddw.q.d    xr0,    xr0,   xr0
+    xvpickve2gr.w  t2,     xr0,   0
+    xvpickve2gr.w  t3,     xr0,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+/*
+ * int pixel_ssd_8x4_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                        const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_ssd_8x4_lasx
+    slli.d         t0,     a1,    1
+    add.d          t1,     a1,    t0
+    add.d          t2,     a1,    t1
+    slli.d         t3,     a3,    1
+    add.d          t4,     a3,    t3
+    add.d          t5,     a3,    t4
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr4,  vr5,  vr6,  vr7
+
+    vilvl.d        vr0,    vr2,   vr0
+    vilvl.d        vr1,    vr3,   vr1
+    vilvl.d        vr4,    vr6,   vr4
+    vilvl.d        vr5,    vr7,   vr5
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr4,    xr4
+    vext2xv.hu.bu  xr5,    xr5
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr4
+    xvsub.h        xr1,    xr1,   xr5
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvadd.w        xr0,    xr0,   xr1
+
+    // Calculate the sum
+    xvhaddw.d.w    xr0,    xr0,   xr0
+    xvhaddw.q.d    xr0,    xr0,   xr0
+    xvpickve2gr.w  t2,     xr0,   0
+    xvpickve2gr.w  t3,     xr0,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+/*
+ * int pixel_ssd_4x16_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                         const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_ssd_4x16_lasx
+    slli.d         t0,     a1,    1
+    add.d          t1,     a1,    t0
+    add.d          t2,     a1,    t1
+    slli.d         t3,     a3,    1
+    add.d          t4,     a3,    t3
+    add.d          t5,     a3,    t4
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+
+    vilvl.w        vr0,    vr4,   vr0
+    vilvl.w        vr1,    vr5,   vr1
+    vilvl.w        vr2,    vr6,   vr2
+    vilvl.w        vr3,    vr7,   vr3
+    vilvl.w        vr8,    vr12,  vr8
+    vilvl.w        vr9,    vr13,  vr9
+    vilvl.w        vr10,   vr14,  vr10
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.d        vr0,    vr2,   vr0
+    vilvl.d        vr1,    vr3,   vr1
+    vilvl.d        vr8,    vr10,  vr8
+    vilvl.d        vr9,    vr11,  vr9
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvadd.w        xr16,   xr0,   xr1
+
+    // Load data from pix1 and pix2
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+
+    vilvl.w        vr0,    vr4,   vr0
+    vilvl.w        vr1,    vr5,   vr1
+    vilvl.w        vr2,    vr6,   vr2
+    vilvl.w        vr3,    vr7,   vr3
+    vilvl.w        vr8,    vr12,  vr8
+    vilvl.w        vr9,    vr13,  vr9
+    vilvl.w        vr10,   vr14,  vr10
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.d        vr0,    vr2,   vr0
+    vilvl.d        vr1,    vr3,   vr1
+    vilvl.d        vr8,    vr10,  vr8
+    vilvl.d        vr9,    vr11,  vr9
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvadd.w        xr0,    xr0,   xr1
+    xvadd.w        xr0,    xr0,   xr16
+
+    // Calculate the sum
+    xvhaddw.d.w    xr0,    xr0,   xr0
+    xvhaddw.q.d    xr0,    xr0,   xr0
+    xvpickve2gr.w  t2,     xr0,   0
+    xvpickve2gr.w  t3,     xr0,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+/*
+ * int pixel_ssd_4x8_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                        const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_ssd_4x8_lasx
+    slli.d         t0,     a1,    1
+    add.d          t1,     a1,    t0
+    add.d          t2,     a1,    t1
+    slli.d         t3,     a3,    1
+    add.d          t4,     a3,    t3
+    add.d          t5,     a3,    t4
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr0,  vr1,  vr2,  vr3
+    add.d          a0,     a0,    t2
+    LSX_LOADX_4    a0,     a1,    t0,  t1,  vr4,  vr5,  vr6,  vr7
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr8,  vr9,  vr10, vr11
+    add.d          a2,     a2,    t5
+    LSX_LOADX_4    a2,     a3,    t3,  t4,  vr12, vr13, vr14, vr15
+
+    vilvl.w        vr0,    vr4,   vr0
+    vilvl.w        vr1,    vr5,   vr1
+    vilvl.w        vr2,    vr6,   vr2
+    vilvl.w        vr3,    vr7,   vr3
+    vilvl.w        vr8,    vr12,  vr8
+    vilvl.w        vr9,    vr13,  vr9
+    vilvl.w        vr10,   vr14,  vr10
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.d        vr0,    vr2,   vr0
+    vilvl.d        vr1,    vr3,   vr1
+    vilvl.d        vr8,    vr10,  vr8
+    vilvl.d        vr9,    vr11,  vr9
+    vext2xv.hu.bu  xr0,    xr0
+    vext2xv.hu.bu  xr1,    xr1
+    vext2xv.hu.bu  xr8,    xr8
+    vext2xv.hu.bu  xr9,    xr9
+
+    // Calculate the square of the difference
+    xvsub.h        xr0,    xr0,   xr8
+    xvsub.h        xr1,    xr1,   xr9
+    xvmul.h        xr0,    xr0,   xr0
+    xvmul.h        xr1,    xr1,   xr1
+    xvhaddw.wu.hu  xr0,    xr0,   xr0
+    xvhaddw.wu.hu  xr1,    xr1,   xr1
+    xvadd.w        xr0,    xr0,   xr1
+
+    // Calculate the sum
+    xvhaddw.d.w    xr0,    xr0,   xr0
+    xvhaddw.q.d    xr0,    xr0,   xr0
+    xvpickve2gr.w  t2,     xr0,   0
+    xvpickve2gr.w  t3,     xr0,   4
+    add.d          a0,     t2,    t3
+endfunc
+
+/*
+ * int pixel_ssd_4x4_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                        const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_ssd_4x4_lasx
+    slli.d         t0,     a1,    1
+    slli.d         t1,     a3,    1
+    add.d          t2,     t0,    a1
+    add.d          t3,     t1,    a3
+
+    // Load data from pix1 and pix2
+    LSX_LOADX_4    a0,     a1,    t0,  t2,  vr4, vr6, vr8, vr10
+    LSX_LOADX_4    a2,     a3,    t1,  t3,  vr5, vr7, vr9, vr11
+    vilvl.w        vr4,    vr6,   vr4
+    vilvl.w        vr5,    vr7,   vr5
     vilvl.w        vr8,    vr10,  vr8
     vilvl.w        vr9,    vr11,  vr9
     vilvl.d        vr4,    vr8,   vr4
@@ -1059,4 +1650,748 @@ function pixel_ssd_4x4_lasx
     add.d          a0,     t2,    t3
 endfunc
 
+/*
+ * int pixel_sa8d_16x16_lasx(const Pixel *pix1, intptr_t i_pix1,
+ *                           const Pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_sa8d_16x16_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    add.d           t4,    a1,   t2
+    add.d           t5,    a3,   t3
+    slli.d          t6,    a1,   2
+    slli.d          t7,    a3,   2
+    slli.d          t0,    a1,   3
+    slli.d          t1,    a3,   3
+
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr15,  xr11, xr13
+    xvsub.h         xr16,  xr11, xr13
+
+    add.d           a0,    a0,   t6
+    add.d           a2,    a2,   t7
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr9,   xr11, xr13
+    xvsub.h         xr10,  xr11, xr13
+    xvadd.h         xr17,  xr15, xr9
+    xvadd.h         xr18,  xr16, xr10
+    xvsub.h         xr19,  xr15, xr9
+    xvsub.h         xr20,  xr16, xr10
+    xvadda.h        xr17,  xr17, xr18
+    xvadda.h        xr19,  xr19, xr20
+    xvadd.h         xr21,  xr17, xr19
+
+    add.d           a0,    a0,   t6
+    add.d           a2,    a2,   t7
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr15,  xr11, xr13
+    xvsub.h         xr16,  xr11, xr13
+
+    add.d           a0,    a0,   t6
+    add.d           a2,    a2,   t7
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr9,   xr11, xr13
+    xvsub.h         xr10,  xr11, xr13
+    xvadd.h         xr17,  xr15, xr9
+    xvadd.h         xr18,  xr16, xr10
+    xvsub.h         xr19,  xr15, xr9
+    xvsub.h         xr20,  xr16, xr10
+    xvadda.h        xr17,  xr17, xr18
+    xvadda.h        xr19,  xr19, xr20
+    xvadd.h         xr22,  xr17, xr19
+
+    sub.d           a0,    a0,   t6
+    sub.d           a2,    a2,   t7
+    addi.d          a0,    a0,   8
+    addi.d          a2,    a2,   8
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr15,  xr11, xr13
+    xvsub.h         xr16,  xr11, xr13
+
+    add.d           a0,    a0,   t6
+    add.d           a2,    a2,   t7
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr9,   xr11, xr13
+    xvsub.h         xr10,  xr11, xr13
+    xvadd.h         xr17,  xr15, xr9
+    xvadd.h         xr18,  xr16, xr10
+    xvsub.h         xr19,  xr15, xr9
+    xvsub.h         xr20,  xr16, xr10
+    xvadda.h        xr17,  xr17, xr18
+    xvadda.h        xr19,  xr19, xr20
+    xvadd.h         xr23,  xr17, xr19
+
+    sub.d           a0,    a0,   t0
+    sub.d           a2,    a2,   t1
+    sub.d           a0,    a0,   t6
+    sub.d           a2,    a2,   t7
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr15,  xr11, xr13
+    xvsub.h         xr16,  xr11, xr13
+
+    add.d           a0,    a0,   t6
+    add.d           a2,    a2,   t7
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr12, 0x13
+    xvadd.h         xr9,   xr11, xr13
+    xvsub.h         xr10,  xr11, xr13
+    xvadd.h         xr17,  xr15, xr9
+    xvadd.h         xr18,  xr16, xr10
+    xvsub.h         xr19,  xr15, xr9
+    xvsub.h         xr20,  xr16, xr10
+    xvadda.h        xr17,  xr17, xr18
+    xvadda.h        xr19,  xr19, xr20
+    xvadd.h         xr24,  xr17, xr19
+
+    xvadd.h         xr21,  xr21, xr22
+    xvadd.h         xr23,  xr23, xr24
+    xvhaddw.wu.hu   xr21,  xr21, xr21
+    xvhaddw.wu.hu   xr23,  xr23, xr23
+    xvadd.w         xr21,  xr21, xr23
+    xvhaddw.du.wu   xr21,  xr21, xr21
+    xvhaddw.qu.du   xr21,  xr21, xr21
+    xvpickve2gr.du  t4,    xr21, 0
+    xvpickve2gr.du  t5,    xr21, 2
+    add.d           t4,    t4,   t5
+    addi.d          t4,    t4,   2
+    srli.d          a0,    t4,   2
+endfunc
+
+/*
+ * int pixel_sa8d_8x8_lasx(const Pixel *pix1, intptr_t i_pix1,
+ *                         const Pixel *pix2, intptr_t i_pix2)
+ */
+function pixel_sa8d_8x8_lasx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    add.d           t4,    a1,   t2
+    add.d           t5,    a3,   t3
+    slli.d          t6,    a1,   2
+    slli.d          t7,    a3,   2
+
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvor.v          xr14,  xr12, xr12
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr14, 0x13
+    xvadd.h         xr15,  xr11, xr13
+    xvsub.h         xr16,  xr11, xr13
+
+    add.d           a0,    a0,   t6
+    add.d           a2,    a2,   t7
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDD_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    vilvl.d         vr1,   vr2,  vr1
+    vilvl.d         vr3,   vr4,  vr3
+    vilvl.d         vr5,   vr6,  vr5
+    vilvl.d         vr7,   vr8,  vr7
+    xvpermi.q       xr1,   xr3,  0x02
+    xvpermi.q       xr5,   xr7,  0x02
+    xvsubwev.h.bu   xr9,   xr1,  xr5
+    xvsubwod.h.bu   xr10,  xr1,  xr5
+    xvadd.h         xr11,  xr9,  xr10  /* a0 + a1 */
+    xvsub.h         xr12,  xr9,  xr10  /* a0 - a1 */
+    xvpackev.h      xr9,   xr12, xr11
+    xvpackod.h      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.w      xr9,   xr12, xr11
+    xvpackod.w      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10
+    xvsub.h         xr12,  xr9,  xr10
+    xvpackev.d      xr9,   xr12, xr11
+    xvpackod.d      xr10,  xr12, xr11
+    xvadd.h         xr11,  xr9,  xr10  /* HADAMARD4 */
+    xvsub.h         xr12,  xr9,  xr10
+    xvor.v          xr13,  xr11, xr11
+    xvor.v          xr14,  xr12, xr12
+    xvpermi.q       xr11,  xr12, 0x02
+    xvpermi.q       xr13,  xr14, 0x13
+    xvadd.h         xr9,   xr11, xr13
+    xvsub.h         xr10,  xr11, xr13
+
+    xvadd.h         xr17,  xr15, xr9
+    xvadd.h         xr18,  xr16, xr10
+    xvsub.h         xr19,  xr15, xr9
+    xvsub.h         xr20,  xr16, xr10
+    xvadda.h        xr17,  xr17, xr18
+    xvadda.h        xr19,  xr19, xr20
+    xvadd.h         xr17,  xr17, xr19
+    xvhaddw.wu.hu   xr17,  xr17, xr17
+    xvhaddw.du.wu   xr17,  xr17, xr17
+    xvhaddw.qu.du   xr17,  xr17, xr17
+    xvpickve2gr.wu  t4,    xr17, 0
+    xvpickve2gr.wu  t5,    xr17, 4
+    add.d           t4,    t4,   t5
+    addi.d          t4,    t4,   2
+    srli.d          a0,    t4,   2
+endfunc
+
+/*
+ * uint64_t pixel_var_16x16( pixel *pix, intptr_t i_stride )
+ */
+function pixel_var_16x16_lasx
+    slli.d          t0,    a1,   1
+    add.d           t1,    t0,   a1
+    xvxor.v         xr15,  xr15, xr15
+    xvxor.v         xr16,  xr16, xr16
+    xvxor.v         xr17,  xr17, xr17
+    addi.d          t3,    zero, 16
+.var_h16_0:
+    vld             vr0,   a0,   0
+    vldx            vr1,   a0,   a1
+    vldx            vr2,   a0,   t0
+    vldx            vr3,   a0,   t1
+    xvpermi.q       xr1,   xr0,  0x20
+    xvhaddw.hu.bu   xr4,   xr1,  xr1
+    xvhaddw.wu.hu   xr4,   xr4,  xr4
+    xvilvl.b        xr5,   xr15, xr1
+    xvilvh.b        xr6,   xr15, xr1
+    xvdp2add.w.h    xr16,  xr5,  xr5
+    xvdp2add.w.h    xr16,  xr6,  xr6
+
+    xvpermi.q       xr2,   xr3,  0x20
+    xvhaddw.hu.bu   xr7,   xr2,  xr2
+    xvhaddw.wu.hu   xr7,   xr7,  xr7
+    xvilvl.b        xr5,   xr15, xr2
+    xvilvh.b        xr6,   xr15, xr2
+    xvdp2add.w.h    xr16,  xr5,  xr5
+    xvdp2add.w.h    xr16,  xr6,  xr6
+
+    xvadd.w         xr17,  xr4,  xr17
+    xvadd.w         xr17,  xr7,  xr17
+    alsl.d          a0,    a1,   a0,    2
+    addi.d          t3,    t3,   -4
+    blt             zero,  t3,   .var_h16_0
+
+    xvhaddw.du.wu   xr17,  xr17,  xr17
+    xvhaddw.qu.du   xr17,  xr17,  xr17
+    xvpickve2gr.wu  t4,    xr17,  0
+    xvpickve2gr.wu  t5,    xr17,  4
+    add.w           t4,    t4,    t5
+
+    xvhaddw.d.w     xr16,  xr16,  xr16
+    xvhaddw.q.d     xr16,  xr16,  xr16
+    xvpickve2gr.d   t5,    xr16,  0
+    xvpickve2gr.d   t6,    xr16,  2
+    add.d           t5,    t5,    t6
+    slli.d          t5,    t5,    32
+    add.d           a0,    t4,    t5
+endfunc
+
+/*
+ * uint64_t pixel_var_8x16( uint8_t *p_pix, intptr_t i_stride )
+ */
+function pixel_var_8x16_lasx
+    slli.d          t0,    a1,   1
+    add.d           t1,    t0,   a1
+    xvxor.v         xr15,  xr15, xr15
+    xvxor.v         xr16,  xr16, xr16
+    xvxor.v         xr17,  xr17, xr17
+    addi.d          t6,    zero, 16
+.var_h16_1:
+    fld.d           f0,    a0,   0
+    fldx.d          f1,    a0,   a1
+    fldx.d          f2,    a0,   t0
+    fldx.d          f3,    a0,   t1
+    alsl.d          a0,    a1,   a0,   2
+    fld.d           f4,    a0,   0
+    fldx.d          f5,    a0,   a1
+    fldx.d          f6,    a0,   t0
+    fldx.d          f7,    a0,   t1
+
+    vilvl.d         vr0,   vr1,  vr0
+    vilvl.d         vr2,   vr3,  vr2
+    xvpermi.q       xr2,   xr0,  0x20
+    xvhaddw.hu.bu   xr3,   xr2,  xr2
+    xvhaddw.wu.hu   xr3,   xr3,  xr3
+    xvilvl.b        xr8,   xr15, xr2
+    xvilvh.b        xr9,   xr15, xr2
+    xvdp2add.w.h    xr16,  xr8,  xr8
+    xvdp2add.w.h    xr16,  xr9,  xr9
+
+    vilvl.d         vr0,   vr5,  vr4
+    vilvl.d         vr2,   vr7,  vr6
+    xvpermi.q       xr2,   xr0,  0x20
+    xvhaddw.hu.bu   xr6,   xr2,  xr2
+    xvhaddw.wu.hu   xr6,   xr6,  xr6
+    xvilvl.b        xr8,   xr15, xr2
+    xvilvh.b        xr9,   xr15, xr2
+    xvdp2add.w.h    xr16,  xr8,  xr8
+    xvdp2add.w.h    xr16,  xr9,  xr9
+
+    xvadd.w         xr17,  xr17, xr3
+    xvadd.w         xr17,  xr17, xr6
+    alsl.d          a0,    a1,   a0,   2
+    addi.d          t6,    t6,   -8
+    blt             zero,  t6,   .var_h16_1
+
+    xvhaddw.du.wu   xr17,  xr17,  xr17
+    xvhaddw.qu.du   xr17,  xr17,  xr17
+    xvpickve2gr.wu  t4,    xr17,  0
+    xvpickve2gr.wu  t5,    xr17,  4
+    add.w           t4,    t4,    t5
+
+    xvhaddw.d.w     xr16,  xr16,  xr16
+    xvhaddw.q.d     xr16,  xr16,  xr16
+    xvpickve2gr.d   t5,    xr16,  0
+    xvpickve2gr.d   t6,    xr16,  2
+    add.d           t5,    t5,    t6
+    slli.d          t5,    t5,    32
+    add.d           a0,    t4,    t5
+endfunc
+
+/*
+ * uint64_t pixel_var_8x8( uint8_t *p_pix, intptr_t i_stride )
+ */
+function pixel_var_8x8_lasx
+    slli.d          t0,    a1,   1
+    add.d           t1,    t0,   a1
+    xvxor.v         xr15,  xr15, xr15
+    xvxor.v         xr16,  xr16, xr16
+    xvxor.v         xr17,  xr17, xr17
+
+    fld.d           f0,    a0,   0
+    fldx.d          f1,    a0,   a1
+    fldx.d          f2,    a0,   t0
+    fldx.d          f3,    a0,   t1
+    alsl.d          a0,    a1,   a0,   2
+    fld.d           f4,    a0,   0
+    fldx.d          f5,    a0,   a1
+    fldx.d          f6,    a0,   t0
+    fldx.d          f7,    a0,   t1
+
+    vilvl.d         vr0,   vr1,  vr0
+    vilvl.d         vr2,   vr3,  vr2
+    xvpermi.q       xr2,   xr0,  0x20
+    xvhaddw.hu.bu   xr3,   xr2,  xr2
+    xvhaddw.wu.hu   xr3,   xr3,  xr3
+    xvilvl.b        xr8,   xr15, xr2
+    xvilvh.b        xr9,   xr15, xr2
+    xvdp2add.w.h    xr16,  xr8,  xr8
+    xvdp2add.w.h    xr16,  xr9,  xr9
+
+    vilvl.d         vr0,   vr5,  vr4
+    vilvl.d         vr2,   vr7,  vr6
+    xvpermi.q       xr2,   xr0,  0x20
+    xvhaddw.hu.bu   xr6,   xr2,  xr2
+    xvhaddw.wu.hu   xr6,   xr6,  xr6
+    xvilvl.b        xr8,   xr15, xr2
+    xvilvh.b        xr9,   xr15, xr2
+    xvdp2add.w.h    xr16,  xr8,  xr8
+    xvdp2add.w.h    xr16,  xr9,  xr9
+
+    xvadd.w         xr17,  xr17, xr3
+    xvadd.w         xr17,  xr17, xr6
+
+    xvhaddw.du.wu   xr17,  xr17,  xr17
+    xvhaddw.qu.du   xr17,  xr17,  xr17
+    xvpickve2gr.wu  t4,    xr17,  0
+    xvpickve2gr.wu  t5,    xr17,  4
+    add.w           t4,    t4,    t5
+
+    xvhaddw.d.w     xr16,  xr16,  xr16
+    xvhaddw.q.d     xr16,  xr16,  xr16
+    xvpickve2gr.d   t5,    xr16,  0
+    xvpickve2gr.d   t6,    xr16,  2
+    add.d           t5,    t5,    t6
+    slli.d          t5,    t5,    32
+    add.d           a0,    t4,    t5
+endfunc
+
+.macro sse_diff_8width_lasx in0, in1
+    fld.d           f0,    \in0,  0
+    fld.d           f1,    \in0,  FENC_STRIDE
+    fld.d           f2,    \in0,  FENC_STRIDE * 2
+    fld.d           f3,    \in0,  FENC_STRIDE * 3
+    fld.d           f4,    \in1,  0
+    fld.d           f5,    \in1,  FDEC_STRIDE
+    fld.d           f6,    \in1,  FDEC_STRIDE * 2
+    fld.d           f7,    \in1,  FDEC_STRIDE * 3
+
+    vilvl.d         vr0,   vr1,   vr0
+    vilvl.d         vr1,   vr3,   vr2
+    vilvl.d         vr4,   vr5,   vr4
+    vilvl.d         vr5,   vr7,   vr6
+    xvpermi.q       xr1,   xr0,   0x20
+    xvpermi.q       xr5,   xr4,   0x20
+
+    xvilvl.b        xr2,   xr5,   xr1
+    xvilvh.b        xr6,   xr5,   xr1
+    xvhsubw.hu.bu   xr3,   xr2,   xr2
+    xvhsubw.hu.bu   xr4,   xr6,   xr6
+    xvdp2add.w.h    xr8,   xr3,   xr3
+    xvdp2add.w.h    xr8,   xr4,   xr4
+    xvadd.h         xr9,   xr9,   xr3
+    xvadd.h         xr9,   xr9,   xr4
+.endm
+
+/*
+ * int32_t x264_pixel_var2_8x16_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+ *                                    int32_t ssd[2] )
+ */
+function pixel_var2_8x16_lasx
+    add.d           t0,    a0,    zero
+    add.d           t1,    a1,    zero
+    xvxor.v         xr8,   xr8,   xr8
+    xvxor.v         xr9,   xr9,   xr9
+
+    sse_diff_8width_lasx a0, a1
+    addi.d          a0,    a0,    FENC_STRIDE * 4
+    addi.d          a1,    a1,    FDEC_STRIDE * 4
+    sse_diff_8width_lasx a0, a1
+    addi.d          a0,    a0,    FENC_STRIDE * 4
+    addi.d          a1,    a1,    FDEC_STRIDE * 4
+    sse_diff_8width_lasx a0, a1
+    addi.d          a0,    a0,    FENC_STRIDE * 4
+    addi.d          a1,    a1,    FDEC_STRIDE * 4
+    sse_diff_8width_lasx a0, a1
+
+    xvhaddw.w.h     xr9,   xr9,   xr9
+    xvhaddw.d.w     xr9,   xr9,   xr9
+    xvhaddw.q.d     xr9,   xr9,   xr9
+    xvpickve2gr.wu  t2,    xr9,   0
+    xvpickve2gr.wu  t3,    xr9,   4
+    add.w           t2,    t2,    t3
+    xvhaddw.d.w     xr8,   xr8,   xr8
+    xvhaddw.q.d     xr8,   xr8,   xr8
+    xvpickve2gr.wu  t3,    xr8,   0
+    xvpickve2gr.wu  t4,    xr8,   4
+    add.w           t3,    t4,    t3
+    st.w            t3,    a2,    0
+    mul.w           t2,    t2,    t2
+    srai.w          t2,    t2,    7
+    sub.w           t3,    t3,    t2
+
+    xvxor.v         xr8,   xr8,   xr8
+    xvxor.v         xr9,   xr9,   xr9
+    addi.d          a0,    t0,    FENC_STRIDE / 2
+    addi.d          a1,    t1,    FDEC_STRIDE / 2
+    sse_diff_8width_lasx a0, a1
+    addi.d          a0,    a0,    FENC_STRIDE * 4
+    addi.d          a1,    a1,    FDEC_STRIDE * 4
+    sse_diff_8width_lasx a0, a1
+    addi.d          a0,    a0,    FENC_STRIDE * 4
+    addi.d          a1,    a1,    FDEC_STRIDE * 4
+    sse_diff_8width_lasx a0, a1
+    addi.d          a0,    a0,    FENC_STRIDE * 4
+    addi.d          a1,    a1,    FDEC_STRIDE * 4
+    sse_diff_8width_lasx a0, a1
+
+    xvhaddw.w.h     xr9,   xr9,   xr9
+    xvhaddw.d.w     xr9,   xr9,   xr9
+    xvhaddw.q.d     xr9,   xr9,   xr9
+    xvpickve2gr.wu  t4,    xr9,   0
+    xvpickve2gr.wu  t5,    xr9,   4
+    add.w           t4,    t4,    t5
+    xvhaddw.d.w     xr8,   xr8,   xr8
+    xvhaddw.q.d     xr8,   xr8,   xr8
+    xvpickve2gr.wu  t5,    xr8,   0
+    xvpickve2gr.wu  t6,    xr8,   4
+    add.w           t5,    t6,    t5
+    st.w            t5,    a2,    4
+    mul.w           t4,    t4,    t4
+    srai.w          t4,    t4,    7
+    sub.w           t5,    t5,    t4
+    add.w           a0,    t3,    t5
+endfunc
+
+/*
+ * int32_t x264_pixel_var2_8x8_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+ *                                   int32_t ssd[2] )
+ */
+function pixel_var2_8x8_lasx
+    add.d           t0,    a0,    zero
+    add.d           t1,    a1,    zero
+    xvxor.v         xr8,   xr8,   xr8
+    xvxor.v         xr9,   xr9,   xr9
+
+    sse_diff_8width_lasx a0, a1
+    addi.d          a0,    a0,    FENC_STRIDE * 4
+    addi.d          a1,    a1,    FDEC_STRIDE * 4
+    sse_diff_8width_lasx a0, a1
+
+    xvhaddw.w.h     xr9,   xr9,   xr9
+    xvhaddw.d.w     xr9,   xr9,   xr9
+    xvhaddw.q.d     xr9,   xr9,   xr9
+    xvpickve2gr.wu  t2,    xr9,   0
+    xvpickve2gr.wu  t3,    xr9,   4
+    add.w           t2,    t2,    t3
+    xvhaddw.d.w     xr8,   xr8,   xr8
+    xvhaddw.q.d     xr8,   xr8,   xr8
+    xvpickve2gr.wu  t3,    xr8,   0
+    xvpickve2gr.wu  t4,    xr8,   4
+    add.w           t3,    t4,    t3
+    st.w            t3,    a2,    0
+    mul.w           t2,    t2,    t2
+    srai.w          t2,    t2,    6
+    sub.w           t3,    t3,    t2
+
+    xvxor.v        xr8,    xr8,   xr8
+    xvxor.v        xr9,    xr9,   xr9
+    addi.d         a0,     t0,    FENC_STRIDE / 2
+    addi.d         a1,     t1,    FDEC_STRIDE / 2
+    sse_diff_8width_lasx a0, a1
+    addi.d         a0,     a0,    FENC_STRIDE * 4
+    addi.d         a1,     a1,    FDEC_STRIDE * 4
+    sse_diff_8width_lasx a0, a1
+
+    xvhaddw.w.h    xr9,   xr9,   xr9
+    xvhaddw.d.w    xr9,   xr9,   xr9
+    xvhaddw.q.d    xr9,   xr9,   xr9
+    xvpickve2gr.wu t4,    xr9,   0
+    xvpickve2gr.wu t5,    xr9,   4
+    add.w          t4,    t4,    t5
+    xvhaddw.d.w    xr8,   xr8,   xr8
+    xvhaddw.q.d    xr8,   xr8,   xr8
+    xvpickve2gr.wu t5,    xr8,   0
+    xvpickve2gr.wu t6,    xr8,   4
+    add.w          t5,    t6,    t5
+    st.w           t5,    a2,    4
+    mul.w          t4,    t4,    t4
+    srai.w         t4,    t4,    6
+    sub.w          t5,    t5,    t4
+    add.w          a0,    t3,    t5
+endfunc
+
 #endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/pixel-c.c b/common/loongarch/pixel-c.c
index a8bf5f21..89f29670 100644
--- a/common/loongarch/pixel-c.c
+++ b/common/loongarch/pixel-c.c
@@ -25,469 +25,16 @@
  *****************************************************************************/
 
 #include "common/common.h"
-#include "loongson_intrinsics.h"
 #include "pixel.h"
 #include "predict.h"
 
 #if !HIGH_BIT_DEPTH
 
-#define LASX_LOAD_4(p_src, _stride, _stride2, _stride3, _src0, _src1, _src2, _src3)      \
-{                                                                                        \
-    _src0 = __lasx_xvld(p_src, 0);                                                       \
-    _src1 = __lasx_xvldx(p_src, _stride);                                                \
-    _src2 = __lasx_xvldx(p_src, _stride2);                                               \
-    _src3 = __lasx_xvldx(p_src, _stride3);                                               \
-}
-
-static inline int32_t pixel_satd_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                              uint8_t *p_ref, int32_t i_ref_stride,
-                                              uint8_t i_height )
-{
-    int32_t cnt;
-    uint32_t u_sum, sum1, sum2;
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i diff0, diff1, diff2, diff3;
-    __m256i tmp0, tmp1;
-    __m256i sum = __lasx_xvldi(0);
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-
-    for( cnt = i_height >> 3; cnt--; )
-    {
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src0, src1, src2, src3 );
-        p_src += i_src_stride_x4;
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     diff0, diff1, diff2, diff3 );
-        p_src += i_src_stride_x4;
-        src0 = __lasx_xvilvl_w(src1, src0);
-        src1 = __lasx_xvilvl_w(src3, src2);
-        src2 = __lasx_xvilvl_w(diff1, diff0);
-        src3 = __lasx_xvilvl_w(diff3, diff2);
-        src0 = __lasx_xvpermi_q(src0, src2, 0x02);
-        src1 = __lasx_xvpermi_q(src1, src3, 0x02);
-        src0 = __lasx_xvilvl_d(src1, src0);
-
-
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref0, ref1, ref2, ref3 );
-        p_ref += i_ref_stride_x4;
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     diff0, diff1, diff2, diff3 );
-        p_ref += i_ref_stride_x4;
-        ref0 = __lasx_xvilvl_w(ref1, ref0);
-        ref1 = __lasx_xvilvl_w(ref3, ref2);
-        ref2 = __lasx_xvilvl_w(diff1, diff0);
-        ref3 = __lasx_xvilvl_w(diff3, diff2);
-        ref0 = __lasx_xvpermi_q(ref0, ref2, 0x02);
-        ref1 = __lasx_xvpermi_q(ref1, ref3, 0x02);
-        ref0 = __lasx_xvilvl_d(ref1, ref0);
-
-        diff0 = __lasx_xvsubwev_h_bu(src0, ref0);
-        diff1 = __lasx_xvsubwod_h_bu(src0, ref0);
-
-        tmp0 = __lasx_xvadd_h(diff0, diff1);
-        tmp1 = __lasx_xvsub_h(diff0, diff1);
-
-        diff0 = __lasx_xvpackev_h(tmp1, tmp0);
-        diff1 = __lasx_xvpackod_h(tmp1, tmp0);
-
-        tmp0 = __lasx_xvadd_h(diff0, diff1);
-        tmp1 = __lasx_xvsub_h(diff0, diff1);
-
-        diff0 = __lasx_xvpackev_w(tmp1, tmp0);
-        diff1 = __lasx_xvpackod_w(tmp1, tmp0);
-
-        tmp0 = __lasx_xvadd_h(diff0, diff1);
-        tmp1 = __lasx_xvsub_h(diff0, diff1);
-
-        diff0 = __lasx_xvpackev_d(tmp1, tmp0);
-        diff1 = __lasx_xvpackod_d(tmp1, tmp0);
-
-        tmp0 = __lasx_xvadd_h(diff0, diff1);
-        tmp1 = __lasx_xvsub_h(diff0, diff1);
-
-        diff0 = __lasx_xvpackev_d(tmp1, tmp0);
-        diff1 = __lasx_xvpackod_d(tmp1, tmp0);
-
-        diff0 = __lasx_xvadda_h(diff0, diff1);
-        sum = __lasx_xvadd_h(sum, diff0);
-    }
-    sum = __lasx_xvhaddw_wu_hu( sum, sum );
-    sum = __lasx_xvhaddw_du_wu( sum, sum );
-    sum = __lasx_xvhaddw_qu_du( sum, sum );
-    sum1 = __lasx_xvpickve2gr_wu(sum, 0);
-    sum2 = __lasx_xvpickve2gr_wu(sum, 4);
-    u_sum = sum1 + sum2;
-
-    return ( u_sum >> 1 );
-}
-
-static inline int32_t pixel_satd_8width_lasx( uint8_t *p_pix1, int32_t i_stride,
-                                              uint8_t *p_pix2, int32_t i_stride2,
-                                              uint8_t i_height )
-{
-    int32_t sum, i_8 = 8;
-    uint32_t sum1, sum2;
-    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
-
-     __asm__ volatile (
-    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
-    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
-    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
-    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
-    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
-    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
-    "xvldi          $xr16,            0                                            \n\t"
-    "1:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -8                     \n\t"
-    "vld            $vr0,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
-    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
-    "vld            $vr4,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
-    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
-    "vld            $vr8,             %[p_pix2],            0                      \n\t"
-    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
-    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
-    "vld            $vr12,            %[p_pix2],            0                      \n\t"
-    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
-    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
-    "vilvl.d        $vr0,             $vr1,                 $vr0                   \n\t"
-    "vilvl.d        $vr1,             $vr3,                 $vr2                   \n\t"
-    "vilvl.d        $vr2,             $vr5,                 $vr4                   \n\t"
-    "vilvl.d        $vr3,             $vr7,                 $vr6                   \n\t"
-    "xvpermi.q      $xr0,             $xr2,                 2                      \n\t"
-    "xvpermi.q      $xr1,             $xr3,                 2                      \n\t"
-    "vilvl.d        $vr2,             $vr9,                 $vr8                   \n\t"
-    "vilvl.d        $vr3,             $vr11,                $vr10                  \n\t"
-    "vilvl.d        $vr4,             $vr13,                $vr12                  \n\t"
-    "vilvl.d        $vr5,             $vr15,                $vr14                  \n\t"
-    "xvpermi.q      $xr2,             $xr4,                 2                      \n\t"
-    "xvpermi.q      $xr3,             $xr5,                 2                      \n\t"
-    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr2                   \n\t"
-    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr2                   \n\t"
-    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr3                   \n\t"
-    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr3                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
-    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
-    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
-    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvilvl.h       $xr4,             $xr1,                 $xr0                   \n\t"
-    "xvilvh.h       $xr5,             $xr1,                 $xr0                   \n\t"
-    "xvilvl.h       $xr6,             $xr3,                 $xr2                   \n\t"
-    "xvilvh.h       $xr7,             $xr3,                 $xr2                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr0,                 $xr2                   \n\t"
-    "xvadd.h        $xr5,             $xr1,                 $xr3                   \n\t"
-    "xvsub.h        $xr6,             $xr0,                 $xr2                   \n\t"
-    "xvsub.h        $xr7,             $xr1,                 $xr3                   \n\t"
-    "xvadda.h       $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvadda.h       $xr1,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr16,            $xr16,                $xr0                   \n\t"
-    "bge            %[i_height],      %[i_8],               1b                     \n\t"
-    "2:                                                                            \n\t"
-    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
-    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
-    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
-    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
-      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
-      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2),
-      [i_height]"+&r"(i_height)
-    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2), [i_8]"r"(i_8)
-    : "memory"
-    );
-
-    return ( sum >> 1 );
-}
-
-int32_t x264_pixel_satd_4x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                   uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    return pixel_satd_4width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
-}
-
-#define SAD_LOAD                                                              \
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;                   \
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;                   \
-    __m256i diff;                                                             \
-    __m256i sad0 = __lasx_xvldi(0);                                           \
-    __m256i sad1 = __lasx_xvldi(0);                                           \
-    __m256i sad2 = __lasx_xvldi(0);                                           \
-    int32_t i_src_stride_x2 = FENC_STRIDE << 1;                               \
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;                              \
-    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;                  \
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;                 \
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;                           \
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-
-
-#define LOAD_REF_DATA_16W( p_ref, sad)                                        \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
-                 ref0, ref1, ref2, ref3 );                                    \
-    p_ref += i_ref_stride_x4;                                                 \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
-                 ref4, ref5, ref6, ref7 );                                    \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
-    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
-    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );                              \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src1, ref1 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src2, ref2 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-    diff = __lasx_xvabsd_bu( src3, ref3 );                                    \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
-    sad  = __lasx_xvadd_h(sad, diff);                                         \
-
-
-#define ST_REF_DATA(sad)                                  \
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
-    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
-    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
-
-#undef LOAD_REF_DATA_16W
-
-#define LOAD_REF_DATA_8W( p_ref, sad)                                          \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
-                 ref0, ref1, ref2, ref3 );                                     \
-    p_ref += i_ref_stride_x4;                                                  \
-    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
-                 ref4, ref5, ref6, ref7 );                                     \
-    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                      \
-    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
-    ref2 = __lasx_xvilvl_d( ref5, ref4 );                                      \
-    ref3 = __lasx_xvilvl_d( ref7, ref6 );                                      \
-    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                               \
-    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                               \
-    diff = __lasx_xvabsd_bu( src0, ref0 );                                     \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
-    sad  = __lasx_xvadd_h(sad, diff);                                          \
-    diff = __lasx_xvabsd_bu( src1, ref1 );                                     \
-    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
-    sad  = __lasx_xvadd_h(sad, diff);                                          \
-
-#undef SAD_LOAD
-#undef LOAD_REF_DATA_8W
-
-static inline uint32_t sad_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                        uint8_t *p_ref, int32_t i_ref_stride,
-                                        int32_t i_height )
-{
-    int32_t i_ht_cnt;
-    uint32_t result;
-    uint8_t * p_src2;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad = __lasx_xvldi( 0 );
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-    int32_t i_src_stride_x8 = i_src_stride << 3;
-
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src, 0 );
-        src1 = __lasx_xvldx( p_src, i_src_stride );
-        src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-        src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-        p_src2 = p_src + i_src_stride_x4;
-        src4 = __lasx_xvld( p_src2, 0 );
-        src5 = __lasx_xvldx( p_src2, i_src_stride );
-        src6 = __lasx_xvldx( p_src2, i_src_stride_x2 );
-        src7 = __lasx_xvldx( p_src2, i_src_stride_x3 );
-        p_src += i_src_stride_x8;
-        src0 = __lasx_xvilvl_w( src1, src0 );
-        src1 = __lasx_xvilvl_w( src3, src2 );
-        src2 = __lasx_xvilvl_w( src5, src4 );
-        src3 = __lasx_xvilvl_w( src7, src6 );
-        src0 = __lasx_xvilvl_d( src1, src0 );
-        src1 = __lasx_xvilvl_d( src3, src2 );
-        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref0, ref1, ref2, ref3 );
-        p_ref += i_ref_stride_x4;
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref4, ref5, ref6, ref7 );
-        p_ref += i_ref_stride_x4;
-        ref0 = __lasx_xvilvl_w( ref1, ref0 );
-        ref1 = __lasx_xvilvl_w( ref3, ref2 );
-        ref2 = __lasx_xvilvl_w( ref5, ref4 );
-        ref3 = __lasx_xvilvl_w( ref7, ref6 );
-        ref0 = __lasx_xvilvl_d( ref1, ref0 );
-        ref1 = __lasx_xvilvl_d( ref3, ref2 );
-        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-        diff = __lasx_xvabsd_bu( src0, ref0 );
-        diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h( sad, diff );
-    }
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);
-    sad = __lasx_xvhaddw_du_wu(sad, sad);
-    sad = __lasx_xvhaddw_qu_du(sad, sad);
-    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
-
-    return ( result );
-}
-
-int32_t x264_pixel_sad_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                  uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    return sad_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
-}
-
-static inline uint64_t pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix,
-                                                   int32_t i_stride )
-{
-    uint32_t u_sum4 = 0, u_sum8 = 0, u_dc;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i diff0, diff1, diff2, diff3;
-    __m256i sub0, sub1, sub2, sub3;
-    __m256i temp0, temp1, temp2, temp3;
-    int32_t i_stride2 = i_stride << 1;
-    int32_t i_stride3 = i_stride2 + i_stride;
-    int32_t i_stride4 = i_stride2 << 1;
-    v16i16  dc;
-
-    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src0, src1, src2, src3);
-    p_pix += i_stride4;
-    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src4, src5, src6, src7);
-
-    diff0 = __lasx_xvilvl_d(src1, src0);
-    diff1 = __lasx_xvilvl_d(src3, src2);
-    diff2 = __lasx_xvilvl_d(src5, src4);
-    diff3 = __lasx_xvilvl_d(src7, src6);
-    diff0 = __lasx_xvpermi_q(diff0, diff2, 0x02);
-    diff1 = __lasx_xvpermi_q(diff1, diff3, 0x02);
-    diff2 = __lasx_xvpickev_b(diff1, diff0);
-    diff3 = __lasx_xvpickod_b(diff1, diff0);
-    temp0 = __lasx_xvaddwev_h_bu(diff2, diff3);
-    temp1 = __lasx_xvaddwod_h_bu(diff2, diff3);
-    temp2 = __lasx_xvsubwev_h_bu(diff2, diff3);
-    temp3 = __lasx_xvsubwod_h_bu(diff2, diff3);
-
-    diff0 = __lasx_xvadd_h(temp0, temp1);
-    diff1 = __lasx_xvadd_h(temp2, temp3);
-    diff2 = __lasx_xvsub_h(temp0, temp1);
-    diff3 = __lasx_xvsub_h(temp2, temp3);
-
-    temp0 = __lasx_xvilvl_h(diff1, diff0);
-    temp1 = __lasx_xvilvh_h(diff1, diff0);
-    temp2 = __lasx_xvilvl_h(diff3, diff2);
-    temp3 = __lasx_xvilvh_h(diff3, diff2);
-
-    diff0 = __lasx_xvilvl_w(temp2, temp0);
-    diff1 = __lasx_xvilvh_w(temp2, temp0);
-    diff2 = __lasx_xvilvl_w(temp3, temp1);
-    diff3 = __lasx_xvilvh_w(temp3, temp1);
-
-    temp0 = __lasx_xvadd_h(diff0, diff1);
-    temp2 = __lasx_xvadd_h(diff2, diff3);
-    temp1 = __lasx_xvsub_h(diff0, diff1);
-    temp3 = __lasx_xvsub_h(diff2, diff3);
-
-    diff0 = __lasx_xvadd_h(temp0, temp2);
-    diff1 = __lasx_xvadd_h(temp1, temp3);
-    diff2 = __lasx_xvsub_h(temp0, temp2);
-    diff3 = __lasx_xvsub_h(temp1, temp3);
-
-    dc = (v16i16)diff0;
-    u_dc = (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
-
-    sub0 = __lasx_xvadda_h(diff0, diff1);
-    sub1 = __lasx_xvadda_h(diff2, diff3);
-
-    sub0 = __lasx_xvadd_h(sub0, sub1);
-    sub1 = __lasx_xvpermi_d(sub0, 0x4E);
-    sub0 = __lasx_xvadd_h(sub0, sub1);
-    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
-    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
-    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
-    u_sum4 = __lasx_xvpickve2gr_wu(sub0, 0);
-
-    temp0 = __lasx_xvpackev_h(diff1, diff0);
-    temp1 = __lasx_xvpackev_h(diff3, diff2);
-    temp2 = __lasx_xvpackod_h(diff1, diff0);
-    temp3 = __lasx_xvpackod_h(diff3, diff2);
-
-    sub0 = __lasx_xvilvl_d(temp1, temp0);
-    sub1 = __lasx_xvilvh_d(temp1, temp0);
-    sub2 = __lasx_xvilvl_d(temp3, temp2);
-    sub3 = __lasx_xvilvh_d(temp3, temp2);
-
-    diff0 = __lasx_xvpermi_q(sub0, sub2, 0x02);
-    diff1 = __lasx_xvpermi_q(sub1, sub2, 0x12);
-    diff2 = __lasx_xvpermi_q(sub0, sub3, 0x03);
-    diff3 = __lasx_xvpermi_q(sub1, sub3, 0x13);
-
-    temp0 = __lasx_xvadd_h(diff0, diff1);
-    temp1 = __lasx_xvsub_h(diff0, diff1);
-    temp2 = __lasx_xvadd_h(diff2, diff3);
-    temp3 = __lasx_xvsub_h(diff2, diff3);
-
-    diff0 = __lasx_xvadd_h(temp0, temp2);
-    diff1 = __lasx_xvadd_h(temp1, temp3);
-    diff2 = __lasx_xvsub_h(temp0, temp2);
-    diff3 = __lasx_xvsub_h(temp1, temp3);
-
-    sub0 = __lasx_xvadda_h(diff0, diff1);
-    sub1 = __lasx_xvadda_h(diff2, diff3);
-    sub0 = __lasx_xvadd_h(sub0, sub1);
-    sub1 = __lasx_xvpermi_d(sub0, 0x4E);
-    sub0 = __lasx_xvadd_h(sub0, sub1);
-    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
-    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
-    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
-    u_sum8 = __lasx_xvpickve2gr_wu(sub0, 0);
-
-    u_sum4 = u_sum4 - u_dc;
-    u_sum8 = u_sum8 - u_dc;
-
-    return ((uint64_t) u_sum8 << 32) + u_sum4;
-}
-
 uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
 {
     uint64_t u_sum;
 
-    u_sum = pixel_hadamard_ac_8x8_lasx( p_pix, i_stride );
+    u_sum = x264_hadamard_ac_8x8_lasx( p_pix, i_stride );
 
     return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
 }
@@ -496,216 +43,26 @@ uint64_t x264_pixel_hadamard_ac_8x16_lasx( uint8_t *p_pix, intptr_t i_stride )
 {
     uint64_t u_sum;
 
-    u_sum = pixel_hadamard_ac_8x8_lasx( p_pix, i_stride );
-    u_sum += pixel_hadamard_ac_8x8_lasx( p_pix + ( i_stride << 3 ), i_stride );
+    u_sum = x264_hadamard_ac_8x8_lasx( p_pix, i_stride );
+    u_sum += x264_hadamard_ac_8x8_lasx( p_pix + ( i_stride << 3 ), i_stride );
 
     return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
 }
 
-static int32_t sa8d_8x8_lasx( uint8_t *p_src, int32_t i_src_stride,
-                              uint8_t *p_ref, int32_t i_ref_stride )
-{
-    uint32_t u_sum = 0;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-    int32_t i_src_stride_x4 = i_src_stride << 2;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
-    int32_t i_ref_stride_x4 = i_ref_stride << 2;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
-    __m256i temp0, temp1, temp2, temp3;
-    v4u64 out;
-
-    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
-               i_src_stride_x3, src0, src1, src2, src3 );
-    p_src += i_src_stride_x4;
-    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
-               i_src_stride_x3, src4, src5, src6, src7 );
-    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
-               p_ref, i_ref_stride_x3, ref0, ref1, ref2, ref3 );
-    p_ref += i_ref_stride_x4;
-    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
-               p_ref, i_ref_stride_x3, ref4, ref5, ref6, ref7 );
-
-    DUP4_ARG2( __lasx_xvilvl_b, src0, ref0, src1, ref1, src2, ref2, src3, ref3,
-               src0, src1, src2, src3 );
-    DUP4_ARG2( __lasx_xvilvl_b, src4, ref4, src5, ref5, src6, ref6, src7, ref7,
-               src4, src5, src6, src7 );
-    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src0, src0, src1, src1, src2, src2, src3, src3,
-               src0, src1, src2, src3 );
-    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src4, src4, src5, src5, src6, src6, src7, src7,
-               src4, src5, src6, src7 );
-    LASX_TRANSPOSE8x8_H( src0, src1, src2, src3,
-                         src4, src5, src6, src7,
-                         src0, src1, src2, src3,
-                         src4, src5, src6, src7 );
-    LASX_BUTTERFLY_4_H( src0, src2, src3, src1, diff0, diff1, diff4, diff5 );
-    LASX_BUTTERFLY_4_H( src4, src6, src7, src5, diff2, diff3, diff7, diff6 );
-    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
-    LASX_TRANSPOSE8x8_H( diff0, diff1, diff2, diff3,
-                         diff4, diff5, diff6, diff7,
-                         diff0, diff1, diff2, diff3,
-                         diff4, diff5, diff6, diff7 );
-    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
-
-    temp0 = __lasx_xvadd_h( diff0, diff4 );
-    temp1 = __lasx_xvadd_h( diff1, diff5 );
-    temp2 = __lasx_xvadd_h( diff2, diff6 );
-    temp3 = __lasx_xvadd_h( diff3, diff7 );
-
-    diff0 = __lasx_xvabsd_h( diff0, diff4 );
-    diff1 = __lasx_xvabsd_h( diff1, diff5 );
-    diff2 = __lasx_xvabsd_h( diff2, diff6 );
-    diff3 = __lasx_xvabsd_h( diff3, diff7 );
-    diff0 = __lasx_xvadda_h( diff0, temp0 );
-    diff1 = __lasx_xvadda_h( diff1, temp1 );
-    diff2 = __lasx_xvadda_h( diff2, temp2 );
-    diff3 = __lasx_xvadda_h( diff3, temp3 );
-
-    diff0 = __lasx_xvadd_h( diff0, diff1 );
-    diff0 = __lasx_xvadd_h( diff0, diff2 );
-    diff0 = __lasx_xvadd_h( diff0, diff3 );
-
-    diff0 = __lasx_xvhaddw_wu_hu( diff0, diff0 );
-    out = ( v4u64 ) __lasx_xvhaddw_du_wu( diff0, diff0 );
-    u_sum = out[0] + out[1];
-
-    return u_sum;
-}
-
-static int32_t sa8d_8x16_lasx( uint8_t *p_src, int32_t i_src_stride,
-                               uint8_t *p_ref, int32_t i_ref_stride )
-{
-    uint32_t u_sum = 0;
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-    int32_t i_src_stride_x4 = i_src_stride << 2;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
-    int32_t i_ref_stride_x4 = i_ref_stride << 2;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
-    __m256i temp0, temp1, temp2, temp3;
-
-    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
-               i_src_stride_x3, src0, src1, src2, src3 );
-    p_src += i_src_stride_x4;
-    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
-               i_src_stride_x3, src4, src5, src6, src7 );
-    src0 = __lasx_xvpermi_d(src0, 0x50);
-    src1 = __lasx_xvpermi_d(src1, 0x50);
-    src2 = __lasx_xvpermi_d(src2, 0x50);
-    src3 = __lasx_xvpermi_d(src3, 0x50);
-    src4 = __lasx_xvpermi_d(src4, 0x50);
-    src5 = __lasx_xvpermi_d(src5, 0x50);
-    src6 = __lasx_xvpermi_d(src6, 0x50);
-    src7 = __lasx_xvpermi_d(src7, 0x50);
-
-    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
-               p_ref, i_ref_stride_x3, ref0, ref1, ref2, ref3 );
-    p_ref += i_ref_stride_x4;
-    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
-               p_ref, i_ref_stride_x3, ref4, ref5, ref6, ref7 );
-    ref0 = __lasx_xvpermi_d(ref0, 0x50);
-    ref1 = __lasx_xvpermi_d(ref1, 0x50);
-    ref2 = __lasx_xvpermi_d(ref2, 0x50);
-    ref3 = __lasx_xvpermi_d(ref3, 0x50);
-    ref4 = __lasx_xvpermi_d(ref4, 0x50);
-    ref5 = __lasx_xvpermi_d(ref5, 0x50);
-    ref6 = __lasx_xvpermi_d(ref6, 0x50);
-    ref7 = __lasx_xvpermi_d(ref7, 0x50);
-
-    DUP4_ARG2( __lasx_xvilvl_b, src0, ref0, src1, ref1, src2, ref2, src3, ref3,
-               src0, src1, src2, src3 );
-    DUP4_ARG2( __lasx_xvilvl_b, src4, ref4, src5, ref5, src6, ref6, src7, ref7,
-               src4, src5, src6, src7 );
-    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src0, src0, src1, src1, src2, src2, src3, src3,
-               src0, src1, src2, src3 );
-    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src4, src4, src5, src5, src6, src6, src7, src7,
-               src4, src5, src6, src7 );
-    LASX_TRANSPOSE8x8_H( src0, src1, src2, src3,
-                         src4, src5, src6, src7,
-                         src0, src1, src2, src3,
-                         src4, src5, src6, src7 );
-    LASX_BUTTERFLY_4_H( src0, src2, src3, src1, diff0, diff1, diff4, diff5 );
-    LASX_BUTTERFLY_4_H( src4, src6, src7, src5, diff2, diff3, diff7, diff6 );
-    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
-    LASX_TRANSPOSE8x8_H( diff0, diff1, diff2, diff3,
-                         diff4, diff5, diff6, diff7,
-                         diff0, diff1, diff2, diff3,
-                         diff4, diff5, diff6, diff7 );
-    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
-
-    temp0 = __lasx_xvadd_h( diff0, diff4 );
-    temp1 = __lasx_xvadd_h( diff1, diff5 );
-    temp2 = __lasx_xvadd_h( diff2, diff6 );
-    temp3 = __lasx_xvadd_h( diff3, diff7 );
-
-    diff0 = __lasx_xvabsd_h( diff0, diff4 );
-    diff1 = __lasx_xvabsd_h( diff1, diff5 );
-    diff2 = __lasx_xvabsd_h( diff2, diff6 );
-    diff3 = __lasx_xvabsd_h( diff3, diff7 );
-    diff0 = __lasx_xvadda_h( diff0, temp0 );
-    diff1 = __lasx_xvadda_h( diff1, temp1 );
-    diff2 = __lasx_xvadda_h( diff2, temp2 );
-    diff3 = __lasx_xvadda_h( diff3, temp3 );
-
-    diff0 = __lasx_xvadd_h( diff0, diff1 );
-    diff0 = __lasx_xvadd_h( diff0, diff2 );
-    diff0 = __lasx_xvadd_h( diff0, diff3 );
-
-    u_sum = LASX_HADD_UH_U32( diff0 );
-
-    return u_sum;
-}
-
-int32_t x264_pixel_sa8d_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                  uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    int32_t i32Sum = sa8d_8x8_lasx( p_pix1, i_stride, p_pix2, i_stride2 );
-
-    return ( i32Sum + 2 ) >> 2;
-}
-
-int32_t x264_pixel_sa8d_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
-                                    uint8_t *p_pix2, intptr_t i_stride2 )
-{
-    int32_t i32Sum = sa8d_8x16_lasx( p_pix1, i_stride, p_pix2, i_stride2 ) +
-                     sa8d_8x16_lasx( p_pix1 + 8 * i_stride, i_stride,
-                                     p_pix2 + 8 * i_stride2, i_stride2 );
-
-    return ( i32Sum + 2 ) >> 2;
-}
-
 void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
                                   int32_t p_sad_array[3] )
 {
     ALIGNED_ARRAY_16( uint8_t, pix, [8 * FDEC_STRIDE] );
 
-    x264_intra_predict_v_8x8_lasx( pix, p_edge );
+    x264_predict_8x8_v_lsx( pix, p_edge );
     p_sad_array[0] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 
-    x264_intra_predict_h_8x8_lasx( pix, p_edge );
+    x264_predict_8x8_h_lasx( pix, p_edge );
     p_sad_array[1] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 
-    x264_intra_predict_dc_8x8_lasx( pix, p_edge );
+    x264_predict_8x8_dc_lsx( pix, p_edge );
     p_sad_array[2] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 }
@@ -713,15 +70,15 @@ void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
 void x264_intra_satd_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                   int32_t p_sad_array[3] )
 {
-    x264_intra_predict_vert_4x4_lasx( p_dec );
+    x264_predict_4x4_v_lsx( p_dec );
     p_sad_array[0] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 
-    x264_intra_predict_hor_4x4_lasx( p_dec );
+    x264_predict_4x4_h_lsx( p_dec );
     p_sad_array[1] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 
-    x264_intra_predict_dc_4x4_lasx( p_dec );
+    x264_predict_4x4_dc_lsx( p_dec );
     p_sad_array[2] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 }
@@ -729,15 +86,15 @@ void x264_intra_satd_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
 void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                     int32_t p_sad_array[3] )
 {
-    x264_intra_predict_vert_16x16_lasx( p_dec );
+    x264_predict_16x16_v_lsx( p_dec );
     p_sad_array[0] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
                                                  p_enc, FENC_STRIDE );
 
-    x264_intra_predict_hor_16x16_lasx( p_dec );
+    x264_predict_16x16_h_lsx( p_dec );
     p_sad_array[1] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
                                                  p_enc, FENC_STRIDE );
 
-    x264_intra_predict_dc_16x16_lasx( p_dec );
+    x264_predict_16x16_dc_lsx( p_dec );
     p_sad_array[2] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
                                                  p_enc, FENC_STRIDE );
 }
@@ -745,15 +102,15 @@ void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
 void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                    int32_t p_sad_array[3] )
 {
-    x264_intra_predict_dc_4blk_8x8_lasx( p_dec );
+    x264_predict_8x8c_dc_lsx( p_dec );
     p_sad_array[0] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 
-    x264_intra_predict_hor_8x8_lasx( p_dec );
+    x264_predict_8x8c_h_lsx( p_dec );
     p_sad_array[1] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 
-    x264_intra_predict_vert_8x8_lasx( p_dec );
+    x264_predict_8x8c_v_lsx( p_dec );
     p_sad_array[2] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
                                                p_enc, FENC_STRIDE );
 }
@@ -761,15 +118,15 @@ void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
 void x264_intra_sad_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                  int32_t p_sad_array[3] )
 {
-    x264_intra_predict_vert_4x4_lasx( p_dec );
+    x264_predict_4x4_v_lsx( p_dec );
     p_sad_array[0] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 
-    x264_intra_predict_hor_4x4_lasx( p_dec );
+    x264_predict_4x4_h_lsx( p_dec );
     p_sad_array[1] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 
-    x264_intra_predict_dc_4x4_lasx( p_dec );
+    x264_predict_4x4_dc_lsx( p_dec );
     p_sad_array[2] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 }
@@ -777,15 +134,15 @@ void x264_intra_sad_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
 void x264_intra_sad_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                    int32_t p_sad_array[3] )
 {
-    x264_intra_predict_vert_16x16_lasx( p_dec );
+    x264_predict_16x16_v_lsx( p_dec );
     p_sad_array[0] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
                                                 p_enc, FENC_STRIDE );
 
-    x264_intra_predict_hor_16x16_lasx( p_dec );
+    x264_predict_16x16_h_lsx( p_dec );
     p_sad_array[1] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
                                                 p_enc, FENC_STRIDE );
 
-    x264_intra_predict_dc_16x16_lasx( p_dec );
+    x264_predict_16x16_dc_lsx( p_dec );
     p_sad_array[2] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
                                                 p_enc, FENC_STRIDE );
 }
@@ -795,15 +152,15 @@ void x264_intra_sad_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
 {
     ALIGNED_ARRAY_16( uint8_t, pix, [8 * FDEC_STRIDE] );
 
-    x264_intra_predict_v_8x8_lasx( pix, p_edge );
+    x264_predict_8x8_v_lsx( pix, p_edge );
     p_sad_array[0] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 
-    x264_intra_predict_h_8x8_lasx( pix, p_edge );
+    x264_predict_8x8_h_lasx( pix, p_edge );
     p_sad_array[1] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 
-    x264_intra_predict_dc_8x8_lasx( pix, p_edge );
+    x264_predict_8x8_dc_lsx( pix, p_edge );
     p_sad_array[2] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 }
@@ -811,586 +168,17 @@ void x264_intra_sad_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
 void x264_intra_sad_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                   int32_t p_sad_array[3] )
 {
-    x264_intra_predict_dc_4blk_8x8_lasx( p_dec );
+    x264_predict_8x8c_dc_lsx( p_dec );
     p_sad_array[0] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 
-    x264_intra_predict_hor_8x8_lasx( p_dec );
+    x264_predict_8x8c_h_lsx( p_dec );
     p_sad_array[1] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 
-    x264_intra_predict_vert_8x8_lasx( p_dec );
+    x264_predict_8x8c_v_lsx( p_dec );
     p_sad_array[2] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
                                               p_enc, FENC_STRIDE );
 }
 
-#define SSD_LOAD_8(_p_src, _stride, _stride2, _stride3, _stride4,                  \
-                   _src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7)         \
-{                                                                                  \
-    _src0 = __lasx_xvld(_p_src, 0);                                                \
-    _src1 = __lasx_xvldx(_p_src, _stride);                                         \
-    _src2 = __lasx_xvldx(_p_src, _stride2);                                        \
-    _src3 = __lasx_xvldx(_p_src, _stride3);                                        \
-    _p_src += _stride4;                                                            \
-    _src4 = __lasx_xvld(_p_src, 0);                                                \
-    _src5 = __lasx_xvldx(_p_src, _stride);                                         \
-    _src6 = __lasx_xvldx(_p_src, _stride2);                                        \
-    _src7 = __lasx_xvldx(_p_src, _stride3);                                        \
-}
-
-#define SSD_INSERT_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,       \
-                     _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)       \
-{                                                                                  \
-    _src0 = __lasx_xvpermi_q(_src0, _src1, 0x02);                                  \
-    _src2 = __lasx_xvpermi_q(_src2, _src3, 0x02);                                  \
-    _src4 = __lasx_xvpermi_q(_src4, _src5, 0x02);                                  \
-    _src6 = __lasx_xvpermi_q(_src6, _src7, 0x02);                                  \
-                                                                                   \
-    _ref0 = __lasx_xvpermi_q(_ref0, _ref1, 0x02);                                  \
-    _ref2 = __lasx_xvpermi_q(_ref2, _ref3, 0x02);                                  \
-    _ref4 = __lasx_xvpermi_q(_ref4, _ref5, 0x02);                                  \
-    _ref6 = __lasx_xvpermi_q(_ref6, _ref7, 0x02);                                  \
-}
-
-#define SSD_SUB_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,          \
-                  _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)          \
-{                                                                                  \
-    _src1 = __lasx_xvsubwev_h_bu(_src0, _ref0);                                    \
-    _ref1 = __lasx_xvsubwod_h_bu(_src0, _ref0);                                    \
-    _src3 = __lasx_xvsubwev_h_bu(_src2, _ref2);                                    \
-    _ref3 = __lasx_xvsubwod_h_bu(_src2, _ref2);                                    \
-    _src5 = __lasx_xvsubwev_h_bu(_src4, _ref4);                                    \
-    _ref5 = __lasx_xvsubwod_h_bu(_src4, _ref4);                                    \
-    _src7 = __lasx_xvsubwev_h_bu(_src6, _ref6);                                    \
-    _ref7 = __lasx_xvsubwod_h_bu(_src6, _ref6);                                    \
-}
-
-int32_t x264_pixel_ssd_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                  uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t u_ssd;
-    intptr_t src_stride2 = i_src_stride << 1;
-    intptr_t ref_stride2 = i_ref_stride << 1;
-    intptr_t src_stride3 = i_src_stride + src_stride2;
-    intptr_t ref_stride3 = i_ref_stride + ref_stride2;
-    intptr_t src_stride4 = src_stride2 << 1;
-    intptr_t ref_stride4 = ref_stride2 << 1;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-
-    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
-               src0, src1, src2, src3, src4, src5, src6, src7);
-
-    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
-               ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-
-    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
-                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-
-    SSD_SUB_8(src0, src1, src2, src3, src4, src5, src6, src7,
-              ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-
-    src0 = __lasx_xvmulwev_w_h(src1, src1);
-    src0 = __lasx_xvmaddwod_w_h(src0, src1, src1);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref1, ref1);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref1, ref1);
-    src0 = __lasx_xvmaddwev_w_h(src0, src3, src3);
-    src0 = __lasx_xvmaddwod_w_h(src0, src3, src3);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref3, ref3);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref3, ref3);
-    src0 = __lasx_xvmaddwev_w_h(src0, src5, src5);
-    src0 = __lasx_xvmaddwod_w_h(src0, src5, src5);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref5, ref5);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref5, ref5);
-    src0 = __lasx_xvmaddwev_w_h(src0, src7, src7);
-    src0 = __lasx_xvmaddwod_w_h(src0, src7, src7);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref7, ref7);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref7, ref7);
-
-    ref0 = __lasx_xvhaddw_d_w(src0, src0);
-    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
-    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
-
-    return u_ssd;
-}
-
-#undef SSD_LOAD_8
-#undef SSD_INSERT_8
-#undef SSD_SUB_8
-
-#define SSD_LOAD_8(_p_src, _src_stride, _src0, _src1, _src2,                       \
-                   _src3, _src4, _src5, _src6, _src7)                              \
-{                                                                                  \
-    _src0 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src1 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src2 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src3 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src4 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src5 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src6 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src7 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
-}
-
-#define SSD_INSERT_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,       \
-                     _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)       \
-{                                                                                  \
-    _src0 = __lasx_xvilvl_b(_src0, _ref0);                                         \
-    _src1 = __lasx_xvilvl_b(_src1, _ref1);                                         \
-    _src2 = __lasx_xvilvl_b(_src2, _ref2);                                         \
-    _src3 = __lasx_xvilvl_b(_src3, _ref3);                                         \
-    _src4 = __lasx_xvilvl_b(_src4, _ref4);                                         \
-    _src5 = __lasx_xvilvl_b(_src5, _ref5);                                         \
-    _src6 = __lasx_xvilvl_b(_src6, _ref6);                                         \
-    _src7 = __lasx_xvilvl_b(_src7, _ref7);                                         \
-}
-
-int32_t x264_pixel_ssd_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                  uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t u_ssd;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
-
-    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
-               src4, src5, src6, src7);
-    p_src += i_src_stride;
-    SSD_LOAD_8(p_src, i_src_stride, src8, src9, src10, src11,
-               src12, src13, src14, src15);
-    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
-               ref4, ref5, ref6, ref7);
-    p_ref += i_ref_stride;
-    SSD_LOAD_8(p_ref, i_ref_stride, ref8, ref9, ref10, ref11,
-               ref12, ref13, ref14, ref15);
-
-    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
-                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
-                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
-
-    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
-    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
-    src4 = __lasx_xvpermi_q(src4, src5, 0x02);
-    src6 = __lasx_xvpermi_q(src6, src7, 0x02);
-    src8 = __lasx_xvpermi_q(src8, src9, 0x02);
-    src10 = __lasx_xvpermi_q(src10, src11, 0x02);
-    src12 = __lasx_xvpermi_q(src12, src13, 0x02);
-    src14 = __lasx_xvpermi_q(src14, src15, 0x02);
-    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
-    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
-    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
-    ref6 = __lasx_xvhsubw_hu_bu(src6, src6);
-    ref8 = __lasx_xvhsubw_hu_bu(src8, src8);
-    ref10 = __lasx_xvhsubw_hu_bu(src10, src10);
-    ref12 = __lasx_xvhsubw_hu_bu(src12, src12);
-    ref14 = __lasx_xvhsubw_hu_bu(src14, src14);
-    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref6, ref6);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref6, ref6);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref8, ref8);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref8, ref8);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref10, ref10);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref10, ref10);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref12, ref12);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref12, ref12);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref14, ref14);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref14, ref14);
-    ref0 = __lasx_xvhaddw_d_w(src0, src0);
-    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
-    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
-
-    return u_ssd;
-}
-
-int32_t x264_pixel_ssd_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t u_ssd;
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-
-    src0 = __lasx_xvldrepl_d( p_src, 0 );
-    p_src += i_src_stride;
-    src1 = __lasx_xvldrepl_d( p_src, 0 );
-    p_src += i_src_stride;
-    src2 = __lasx_xvldrepl_d( p_src, 0 );
-    p_src += i_src_stride;
-    src3 = __lasx_xvldrepl_d( p_src, 0 );
-
-    ref0 = __lasx_xvldrepl_d( p_ref, 0 );
-    p_ref += i_ref_stride;
-    ref1 = __lasx_xvldrepl_d( p_ref, 0 );
-    p_ref += i_ref_stride;
-    ref2 = __lasx_xvldrepl_d( p_ref, 0 );
-    p_ref += i_ref_stride;
-    ref3 = __lasx_xvldrepl_d( p_ref, 0 );
-
-    src0 = __lasx_xvilvl_b(src0, ref0);
-    src1 = __lasx_xvilvl_b(src1, ref1);
-    src2 = __lasx_xvilvl_b(src2, ref2);
-    src3 = __lasx_xvilvl_b(src3, ref3);
-    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
-    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
-    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
-    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
-    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
-    ref0 = __lasx_xvhaddw_d_w(src0, src0);
-    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
-    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
-
-    return u_ssd;
-}
-
-#undef SSD_LOAD_8
-
-#define SSD_LOAD_8(_p_src, _src_stride, _src0, _src1, _src2,                       \
-                   _src3, _src4, _src5, _src6, _src7)                              \
-{                                                                                  \
-    _src0 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src1 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src2 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src3 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src4 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src5 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src6 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
-    _p_src += _src_stride;                                                         \
-    _src7 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
-}
-
-int32_t x264_pixel_ssd_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                  uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t u_ssd;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
-
-    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
-               src4, src5, src6, src7);
-    p_src += i_src_stride;
-    SSD_LOAD_8(p_src, i_src_stride, src8, src9, src10, src11,
-               src12, src13, src14, src15);
-    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
-               ref4, ref5, ref6, ref7);
-    p_ref += i_ref_stride;
-    SSD_LOAD_8(p_ref, i_ref_stride, ref8, ref9, ref10, ref11,
-               ref12, ref13, ref14, ref15);
-
-    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
-                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
-                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
-
-    src0 = __lasx_xvilvl_d(src1, src0);
-    src2 = __lasx_xvilvl_d(src3, src2);
-    src4 = __lasx_xvilvl_d(src5, src4);
-    src6 = __lasx_xvilvl_d(src7, src6);
-    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
-    src4 = __lasx_xvpermi_q(src4, src6, 0x02);
-
-    src1 = __lasx_xvilvl_d(src9, src8);
-    src3 = __lasx_xvilvl_d(src11, src10);
-    src5 = __lasx_xvilvl_d(src13, src12);
-    src7 = __lasx_xvilvl_d(src15, src14);
-    src1 = __lasx_xvpermi_q(src1, src3, 0x02);
-    src5 = __lasx_xvpermi_q(src5, src7, 0x02);
-
-    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
-    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
-    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
-    ref0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
-    src4 = __lasx_xvmaddwev_w_h(ref0, ref4, ref4);
-    ref4 = __lasx_xvmaddwod_w_h(src4, ref4, ref4);
-
-    ref1 = __lasx_xvhsubw_hu_bu(src1, src1);
-    ref5 = __lasx_xvhsubw_hu_bu(src5, src5);
-    src1 = __lasx_xvmaddwev_w_h(ref4, ref1, ref1);
-    src1 = __lasx_xvmaddwod_w_h(src1, ref1, ref1);
-    src1 = __lasx_xvmaddwev_w_h(src1, ref5, ref5);
-    src1 = __lasx_xvmaddwod_w_h(src1, ref5, ref5);
-    ref4 = __lasx_xvhaddw_d_w(src1, src1);
-    ref4 = __lasx_xvhaddw_q_d(ref4, ref4);
-    u_ssd = __lasx_xvpickve2gr_w(ref4, 0) + __lasx_xvpickve2gr_w(ref4, 4);
-
-    return u_ssd;
-}
-
-int32_t x264_pixel_ssd_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    uint32_t u_ssd;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-
-    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
-               src4, src5, src6, src7);
-    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
-               ref4, ref5, ref6, ref7);
-
-    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
-                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
-
-    src0 = __lasx_xvilvl_d(src1, src0);
-    src2 = __lasx_xvilvl_d(src3, src2);
-    src4 = __lasx_xvilvl_d(src5, src4);
-    src6 = __lasx_xvilvl_d(src7, src6);
-    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
-    src4 = __lasx_xvpermi_q(src4, src6, 0x02);
-
-    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
-    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
-    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
-    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
-    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
-    ref4 = __lasx_xvhaddw_d_w(src0, src0);
-    ref4 = __lasx_xvhaddw_q_d(ref4, ref4);
-    u_ssd = __lasx_xvpickve2gr_w(ref4, 0) + __lasx_xvpickve2gr_w(ref4, 4);
-
-    return u_ssd;
-}
-
-#undef SSD_LOAD_8
-#undef SSD_INSERT_8
-
-#define LASX_CALC_MSE_AVG_B( src, ref, var, sub )                          \
-{                                                                          \
-    __m256i src_l0_m, src_l1_m;                                            \
-    __m256i res_l0_m, res_l1_m;                                            \
-                                                                           \
-    src_l1_m = __lasx_xvilvl_b( src, ref );                                \
-    src_l0_m = __lasx_xvilvh_b( src, ref );                                \
-    DUP2_ARG2( __lasx_xvhsubw_hu_bu, src_l0_m, src_l0_m, src_l1_m,         \
-               src_l1_m, res_l0_m, res_l1_m );                             \
-    DUP2_ARG3( __lasx_xvdp2add_w_h, var, res_l0_m, res_l0_m,               \
-               var, res_l1_m, res_l1_m, var, var );                        \
-                                                                           \
-    res_l0_m = __lasx_xvadd_h( res_l0_m, res_l1_m );                       \
-    sub = __lasx_xvadd_h( sub, res_l0_m );                                 \
-}
-
-#define VARIANCE_WxH( sse, diff, shift )                                \
-    ( ( sse ) - ( ( ( uint32_t )( diff ) * ( diff ) ) >> ( shift ) ) )
-
-static inline uint32_t sse_diff_8width_lasx( uint8_t *p_src,
-                                             int32_t i_src_stride,
-                                             uint8_t *p_ref,
-                                             int32_t i_ref_stride,
-                                             int32_t i_height,
-                                             int32_t *p_diff )
-{
-    int32_t i_ht_cnt;
-    uint32_t u_sse;
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i avg = __lasx_xvldi( 0 );
-    __m256i var = __lasx_xvldi( 0 );
-
-    for( i_ht_cnt = ( i_height >> 2 ); i_ht_cnt--; )
-    {
-        src0 = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-        src1 = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-        src2 = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-        src3 = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-
-        ref0 = __lasx_xvldrepl_d( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref1 = __lasx_xvldrepl_d( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref2 = __lasx_xvldrepl_d( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref3 = __lasx_xvldrepl_d( p_ref, 0 );
-        p_ref += i_ref_stride;
-
-        DUP4_ARG2( __lasx_xvpickev_d, src1, src0, src3, src2, ref1, ref0, ref3, ref2,
-                   src0, src1, ref0, ref1 );
-        src0 = __lasx_xvpermi_q( src1, src0, 0x20 );
-        ref0 = __lasx_xvpermi_q( ref1, ref0, 0x20 );
-        LASX_CALC_MSE_AVG_B( src0, ref0, var, avg );
-    }
-
-    avg = __lasx_xvhaddw_w_h( avg, avg );
-    *p_diff = LASX_HADD_SW_S32( avg );
-    u_sse = LASX_HADD_SW_S32( var );
-
-    return u_sse;
-}
-
-static uint64_t avc_pixel_var16width_lasx( uint8_t *p_pix, int32_t i_stride,
-                                           uint8_t i_height )
-{
-    uint32_t u_sum = 0, u_sqr_out = 0, u_cnt;
-    int32_t i_stride_x2 = i_stride << 1;
-    int32_t i_stride_x3 = i_stride_x2 + i_stride;
-    int32_t i_stride_x4 = i_stride << 2;
-    int32_t i_stride_x5 = i_stride_x4 + i_stride;
-    int32_t i_stride_x6 = i_stride_x4 + i_stride_x2;
-    int32_t i_stride_x7 = i_stride_x4 + i_stride_x3;
-    __m256i pix0, pix1, pix2, pix3, pix4, pix5, pix6, pix7;
-    __m256i zero = __lasx_xvldi( 0 );
-    __m256i add, pix_h, pix_l;
-    __m256i sqr = __lasx_xvldi( 0 );
-
-#define LASX_PIXEL_VAR_16W( src0, src1 )                       \
-    src0 = __lasx_xvpermi_q( src1, src0, 0x20 );               \
-    add = __lasx_xvhaddw_hu_bu( src0, src0 );                  \
-    u_sum += LASX_HADD_UH_U32( add );                          \
-    pix_h =__lasx_xvilvl_b( zero, src0 );                      \
-    pix_l =__lasx_xvilvh_b( zero, src0 );                      \
-    DUP2_ARG3( __lasx_xvdp2add_w_h, sqr, pix_h, pix_h,         \
-               sqr, pix_l, pix_l, sqr, sqr );
-
-    for( u_cnt = ( i_height >> 3 ); u_cnt--; )
-    {
-        DUP4_ARG2( __lasx_xvldx, p_pix, 0, p_pix, i_stride, p_pix, i_stride_x2, p_pix,
-                   i_stride_x3, pix0, pix1, pix2, pix3 );
-        DUP4_ARG2( __lasx_xvldx, p_pix, i_stride_x4, p_pix, i_stride_x5, p_pix,
-                   i_stride_x6, p_pix, i_stride_x7, pix4, pix5, pix6, pix7 );
-        p_pix += ( i_stride << 3 );
-
-        LASX_PIXEL_VAR_16W( pix0, pix1 );
-        LASX_PIXEL_VAR_16W( pix2, pix3 );
-        LASX_PIXEL_VAR_16W( pix4, pix5 );
-        LASX_PIXEL_VAR_16W( pix6, pix7 );
-    }
-
-    u_sqr_out = LASX_HADD_SW_S32( sqr );
-
-#undef LASX_PIXEL_VAR_16W
-
-    return ( u_sum + ( ( uint64_t ) u_sqr_out << 32 ) );
-}
-
-static uint64_t avc_pixel_var8width_lasx( uint8_t *p_pix, int32_t i_stride,
-                                          uint8_t i_height )
-{
-    uint32_t u_sum = 0, u_sqr_out = 0, u_cnt;
-    __m256i pix0, pix1, pix2, pix3, pix4, pix5, pix6, pix7;
-    __m256i zero = __lasx_xvldi( 0 );
-    __m256i add, pix_h, pix_l;
-    __m256i sqr = __lasx_xvldi( 0 );
-
-#define LASX_PIXEL_VAR_8W( src0, src1, src2, src3 )            \
-    src0 = __lasx_xvpickev_d( src1, src0 );                    \
-    src1 = __lasx_xvpickev_d( src3, src2 );                    \
-    src0 = __lasx_xvpermi_q( src1, src0, 0x20 );               \
-    add = __lasx_xvhaddw_hu_bu( src0, src0 );                  \
-    u_sum += LASX_HADD_UH_U32( add );                          \
-    pix_h = __lasx_xvilvl_b( zero, src0 );                     \
-    pix_l = __lasx_xvilvh_b( zero, src0 );                     \
-    DUP2_ARG3( __lasx_xvdp2add_w_h, sqr, pix_h, pix_h,         \
-               sqr, pix_l, pix_l, sqr, sqr );
-
-    for( u_cnt = ( i_height >> 3 ); u_cnt--; )
-    {
-        pix0 = __lasx_xvldrepl_d( p_pix, 0 );
-        p_pix += i_stride;
-        pix1 = __lasx_xvldrepl_d( p_pix, 0 );
-        p_pix += i_stride;
-        pix2 = __lasx_xvldrepl_d( p_pix, 0 );
-        p_pix += i_stride;
-        pix3 = __lasx_xvldrepl_d( p_pix, 0 );
-        p_pix += i_stride;
-        pix4 = __lasx_xvldrepl_d( p_pix, 0 );
-        p_pix += i_stride;
-        pix5 = __lasx_xvldrepl_d( p_pix, 0 );
-        p_pix += i_stride;
-        pix6 = __lasx_xvldrepl_d( p_pix, 0 );
-        p_pix += i_stride;
-        pix7 = __lasx_xvldrepl_d( p_pix, 0 );
-        p_pix += i_stride;
-
-        LASX_PIXEL_VAR_8W( pix0, pix1, pix2, pix3 );
-        LASX_PIXEL_VAR_8W( pix4, pix5, pix6, pix7 );
-    }
-
-    u_sqr_out = LASX_HADD_SW_S32( sqr );
-
-#undef LASX_PIXEL_VAR_8W
-
-    return ( u_sum + ( ( uint64_t ) u_sqr_out << 32 ) );
-}
-
-uint64_t x264_pixel_var_16x16_lasx( uint8_t *p_pix, intptr_t i_stride )
-{
-    return avc_pixel_var16width_lasx( p_pix, i_stride, 16 );
-}
-
-uint64_t x264_pixel_var_8x16_lasx( uint8_t *p_pix, intptr_t i_stride )
-{
-    return avc_pixel_var8width_lasx( p_pix, i_stride, 16 );
-}
-
-uint64_t x264_pixel_var_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
-{
-    return avc_pixel_var8width_lasx( p_pix, i_stride, 8 );
-}
-
-int32_t x264_pixel_var2_8x16_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
-                                   int32_t ssd[2] )
-{
-    int32_t i_var = 0, i_diff_u = 0, i_sqr_u = 0;
-    int32_t i_diff_v = 0, i_sqr_v = 0;
-
-    i_sqr_u = sse_diff_8width_lasx( p_pix1, FENC_STRIDE,
-                                    p_pix2, FDEC_STRIDE, 16, &i_diff_u );
-    i_sqr_v = sse_diff_8width_lasx( p_pix1 + (FENC_STRIDE >> 1),
-                                    FENC_STRIDE,
-                                    p_pix2 + (FDEC_STRIDE >> 1),
-                                    FDEC_STRIDE, 16, &i_diff_v );
-    i_var = VARIANCE_WxH( i_sqr_u, i_diff_u, 7 ) +
-            VARIANCE_WxH( i_sqr_v, i_diff_v, 7 );
-    ssd[0] = i_sqr_u;
-    ssd[1] = i_sqr_v;
-
-    return i_var;
-}
-
-int32_t x264_pixel_var2_8x8_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
-                                  int32_t ssd[2] )
-{
-    int32_t i_var = 0, i_diff_u = 0, i_sqr_u = 0;
-    int32_t i_diff_v = 0, i_sqr_v = 0;
-
-    i_sqr_u = sse_diff_8width_lasx( p_pix1, FENC_STRIDE,
-                                    p_pix2, FDEC_STRIDE, 8, &i_diff_u );
-    i_sqr_v = sse_diff_8width_lasx( p_pix1 + (FENC_STRIDE >> 1),
-                                    FENC_STRIDE,
-                                    p_pix2 + (FDEC_STRIDE >> 1),
-                                    FDEC_STRIDE, 8, &i_diff_v );
-    i_var = VARIANCE_WxH( i_sqr_u, i_diff_u, 6 ) +
-            VARIANCE_WxH( i_sqr_v, i_diff_v, 6 );
-    ssd[0] = i_sqr_u;
-    ssd[1] = i_sqr_v;
-
-    return i_var;
-}
-
 #endif
diff --git a/common/loongarch/pixel.h b/common/loongarch/pixel.h
index 067d5c76..958400d3 100644
--- a/common/loongarch/pixel.h
+++ b/common/loongarch/pixel.h
@@ -149,6 +149,8 @@ int32_t x264_pixel_sad_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
 int32_t x264_pixel_sad_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride );
 
+#define x264_hadamard_ac_8x8_lasx x264_template(hadamard_ac_8x8_lasx)
+uint64_t x264_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride );
 #define x264_pixel_hadamard_ac_8x8_lasx x264_template(pixel_hadamard_ac_8x8_lasx)
 uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride );
 #define x264_pixel_hadamard_ac_8x16_lasx x264_template(pixel_hadamard_ac_8x16_lasx)
diff --git a/common/loongarch/predict-a.S b/common/loongarch/predict-a.S
index a3f21a8d..938df142 100644
--- a/common/loongarch/predict-a.S
+++ b/common/loongarch/predict-a.S
@@ -283,44 +283,36 @@ endfunc
 /* void x264_predict_8x8c_v_lsx( pixel *src )
  */
 function predict_8x8c_v_lsx
-    fld.d       f0,    a0,    -FDEC_STRIDE
-    fst.d       f0,    a0,    0
-    fst.d       f0,    a0,    FDEC_STRIDE
-    fst.d       f0,    a0,    FDEC_STRIDE * 2
-    fst.d       f0,    a0,    FDEC_STRIDE * 3
-    fst.d       f0,    a0,    FDEC_STRIDE * 4
-    fst.d       f0,    a0,    FDEC_STRIDE * 5
-    fst.d       f0,    a0,    FDEC_STRIDE * 6
-    fst.d       f0,    a0,    FDEC_STRIDE * 7
+    fld.d         f0,    a0,    -FDEC_STRIDE
+    fst.d         f0,    a0,    0
+    fst.d         f0,    a0,    FDEC_STRIDE
+    fst.d         f0,    a0,    FDEC_STRIDE * 2
+    fst.d         f0,    a0,    FDEC_STRIDE * 3
+    fst.d         f0,    a0,    FDEC_STRIDE * 4
+    fst.d         f0,    a0,    FDEC_STRIDE * 5
+    fst.d         f0,    a0,    FDEC_STRIDE * 6
+    fst.d         f0,    a0,    FDEC_STRIDE * 7
 endfunc
 
 /* void x264_predict_8x8c_h_lsx( pixel *src )
  */
 function predict_8x8c_h_lsx
-    ld.bu           t0,    a0,    -1
-    ld.bu           t1,    a0,    FDEC_STRIDE - 1
-    ld.bu           t2,    a0,    FDEC_STRIDE * 2 - 1
-    ld.bu           t3,    a0,    FDEC_STRIDE * 3 - 1
-    ld.bu           t4,    a0,    FDEC_STRIDE * 4 - 1
-    ld.bu           t5,    a0,    FDEC_STRIDE * 5 - 1
-    ld.bu           t6,    a0,    FDEC_STRIDE * 6 - 1
-    ld.bu           t7,    a0,    FDEC_STRIDE * 7 - 1
-    vreplgr2vr.b    vr0,   t0
-    vreplgr2vr.b    vr1,   t1
-    vreplgr2vr.b    vr2,   t2
-    vreplgr2vr.b    vr3,   t3
-    vreplgr2vr.b    vr4,   t4
-    vreplgr2vr.b    vr5,   t5
-    vreplgr2vr.b    vr6,   t6
-    vreplgr2vr.b    vr7,   t7
-    fst.d           f0,    a0,    0
-    fst.d           f1,    a0,    FDEC_STRIDE
-    fst.d           f2,    a0,    FDEC_STRIDE * 2
-    fst.d           f3,    a0,    FDEC_STRIDE * 3
-    fst.d           f4,    a0,    FDEC_STRIDE * 4
-    fst.d           f5,    a0,    FDEC_STRIDE * 5
-    fst.d           f6,    a0,    FDEC_STRIDE * 6
-    fst.d           f7,    a0,    FDEC_STRIDE * 7
+    vldrepl.b     vr0,   a0,    -1
+    vldrepl.b     vr1,   a0,    FDEC_STRIDE - 1
+    vldrepl.b     vr2,   a0,    FDEC_STRIDE * 2 - 1
+    vldrepl.b     vr3,   a0,    FDEC_STRIDE * 3 - 1
+    vldrepl.b     vr4,   a0,    FDEC_STRIDE * 4 - 1
+    vldrepl.b     vr5,   a0,    FDEC_STRIDE * 5 - 1
+    vldrepl.b     vr6,   a0,    FDEC_STRIDE * 6 - 1
+    vldrepl.b     vr7,   a0,    FDEC_STRIDE * 7 - 1
+    fst.d         f0,    a0,    0
+    fst.d         f1,    a0,    FDEC_STRIDE
+    fst.d         f2,    a0,    FDEC_STRIDE * 2
+    fst.d         f3,    a0,    FDEC_STRIDE * 3
+    fst.d         f4,    a0,    FDEC_STRIDE * 4
+    fst.d         f5,    a0,    FDEC_STRIDE * 5
+    fst.d         f6,    a0,    FDEC_STRIDE * 6
+    fst.d         f7,    a0,    FDEC_STRIDE * 7
 endfunc
 
 /* void x264_predict_8x8c_dc_lsx( pixel *src )
@@ -1101,5 +1093,4 @@ function predict_16x16_p_lasx
     add.w         t5,    t5,    t1
 .endr
 endfunc
-
 #endif /* !HIGH_BIT_DEPT H */
diff --git a/common/loongarch/predict-c.c b/common/loongarch/predict-c.c
index c90d8117..fc12f3d8 100644
--- a/common/loongarch/predict-c.c
+++ b/common/loongarch/predict-c.c
@@ -25,562 +25,71 @@
  *****************************************************************************/
 
 #include "common/common.h"
-#include "loongson_intrinsics.h"
 #include "predict.h"
 
-#if !HIGH_BIT_DEPTH
-
-static inline void intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src,
-                                                   int32_t i_stride )
-{
-    uint32_t u_mask = 0x01010101;
-    int32_t i_stride_x4 = i_stride << 2;
-    uint8_t *p_src1, *p_src2;
-    __m256i sum, mask;
-    v8u32 out;
-
-    sum = __lasx_xvldx( p_src, -i_stride );
-    sum = __lasx_xvhaddw_hu_bu( sum, sum );
-    out = ( v8u32 ) __lasx_xvhaddw_wu_hu( sum, sum );
-    mask = __lasx_xvreplgr2vr_w( u_mask );
-
-    p_src1 = p_src - 1;
-    p_src2 = p_src1 + ( i_stride << 2 );
-    out[0] += p_src1[0];
-    out[2] = p_src2[0];
-
-    p_src1 += i_stride;
-    p_src2 += i_stride;
-    out[0] += p_src1[0];
-    out[2] += p_src2[0];
-
-    p_src1 += i_stride;
-    p_src2 += i_stride;
-    out[0] += p_src1[0];
-    out[2] += p_src2[0];
-
-    p_src1 += i_stride;
-    p_src2 += i_stride;
-    out[0] += p_src1[0];
-    out[2] += p_src2[0];
-
-    out[0] = ( out[0] + 4 ) >> 3;
-    out[3] = ( out[1] + out[2] + 4 ) >> 3;
-    out[1] = ( out[1] + 2 ) >> 2;
-    out[2] = ( out[2] + 2 ) >> 2;
-
-    out = ( v8u32 ) __lasx_xvmul_w( ( __m256i ) out, mask );
-
-    __lasx_xvstelm_d( out, p_src, 0, 0 );
-    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
-    p_src += i_stride;
-
-    __lasx_xvstelm_d( out, p_src, 0, 0 );
-    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
-    p_src += i_stride;
-
-    __lasx_xvstelm_d( out, p_src, 0, 0 );
-    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
-    p_src += i_stride;
-
-    __lasx_xvstelm_d( out, p_src, 0, 0 );
-    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
-    p_src += i_stride;
-}
-
-static inline void intra_predict_dc_4x4_lasx( uint8_t *p_src_top,
-                                              uint8_t *p_src_left,
-                                              int32_t i_src_stride_left,
-                                              uint8_t *p_dst,
-                                              int32_t i_dst_stride,
-                                              uint8_t is_above,
-                                              uint8_t is_left )
+void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] )
 {
-    uint32_t u_row;
-    uint32_t u_addition = 0;
-    int32_t i_dst_stride_x2 =  i_dst_stride << 1;
-    int32_t i_dst_stride_x3 =  i_dst_stride_x2 + i_dst_stride;
-    __m256i src, store;
-    v8u32 sum;
-
-    if( is_left && is_above )
-    {
-        src  = __lasx_xvld( p_src_top, 0 );
-        src = __lasx_xvhaddw_hu_bu( src, src );
-        sum = ( v8u32 ) __lasx_xvhaddw_wu_hu( src, src );
-        u_addition = sum[0];
-
-        for( u_row = 0; u_row < 4; u_row++ )
-        {
-            u_addition += p_src_left[u_row * i_src_stride_left];
-        }
-
-        u_addition = ( u_addition + 4 ) >> 3;
-        store = __lasx_xvreplgr2vr_b( u_addition );
-    }
-    else if( is_left )
-    {
-        for( u_row = 0; u_row < 4; u_row++ )
-        {
-            u_addition += p_src_left[u_row * i_src_stride_left];
-        }
-
-        u_addition = ( u_addition + 2 ) >> 2;
-        store = __lasx_xvreplgr2vr_b( u_addition );
-    }
-    else if( is_above )
-    {
-        src  = __lasx_xvld( p_src_top, 0 );
-        src = __lasx_xvhaddw_hu_bu( src, src );
-        src = __lasx_xvhaddw_wu_hu( src, src );
-        src = __lasx_xvsrari_w( src, 2 );
-
-        store = __lasx_xvrepl128vei_b( src, 0 );
-    }
-    else
+    if( cpu&X264_CPU_LASX )
     {
-        u_addition = 128;
-
-        store = __lasx_xvreplgr2vr_b( u_addition );
+#if !HIGH_BIT_DEPTH
+        pf[I_PRED_16x16_V ]     = x264_predict_16x16_v_lsx;
+        pf[I_PRED_16x16_H ]     = x264_predict_16x16_h_lsx;
+        pf[I_PRED_16x16_DC]     = x264_predict_16x16_dc_lsx;
+        pf[I_PRED_16x16_P ]     = x264_predict_16x16_p_lasx;
+        pf[I_PRED_16x16_DC_LEFT]= x264_predict_16x16_dc_left_lsx;
+        pf[I_PRED_16x16_DC_TOP ]= x264_predict_16x16_dc_top_lsx;
+        pf[I_PRED_16x16_DC_128 ]= x264_predict_16x16_dc_128_lsx;
+#endif
     }
-
-    __lasx_xvstelm_w( store, p_dst, 0, 0 );
-    __lasx_xvstelm_w( store, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_w( store, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_w( store, p_dst + i_dst_stride_x3, 0, 0 );
 }
 
-static inline void intra_predict_dc_8x8_lasx( uint8_t *p_src_top,
-                                              uint8_t *p_src_left,
-                                              uint8_t *p_dst,
-                                              int32_t i_dst_stride )
+void x264_predict_8x8c_init_lasx( int cpu, x264_predict_t pf[7] )
 {
-    __m256i src0, src1, store;
-    int32_t i_dst_stride_x2 =  i_dst_stride << 1;
-    int32_t i_dst_stride_x3 =  i_dst_stride_x2 + i_dst_stride;
-
-    src0 = __lasx_xvldrepl_d( p_src_top, 0 );
-    src1 = __lasx_xvldrepl_d( p_src_left, 0 );
-    src0 = __lasx_xvpickev_d( src1, src0 );
-
-    src0 = __lasx_xvhaddw_hu_bu( src0, src0 );
-    src0 = __lasx_xvhaddw_wu_hu( src0, src0 );
-    src0 = __lasx_xvhaddw_du_wu( src0, src0 );
-    src0 = __lasx_xvpickev_w( src0, src0 );
-    src0 = __lasx_xvhaddw_du_wu( src0, src0 );
-    src0 = __lasx_xvsrari_w( src0, 4 );
-    store = __lasx_xvrepl128vei_b( src0, 0 );
-
-    __lasx_xvstelm_d( store, p_dst, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
-    p_dst += ( i_dst_stride  << 2);
-    __lasx_xvstelm_d( store, p_dst, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
-}
-
-static inline void intra_predict_dc_16x16_lasx( uint8_t *p_src_top,
-                                                uint8_t *p_src_left,
-                                                int32_t i_src_stride_left,
-                                                uint8_t *p_dst,
-                                                int32_t i_dst_stride,
-                                                uint8_t is_above,
-                                                uint8_t is_left )
-{
-    uint32_t u_row;
-    int32_t i_index = 0;
-    uint32_t u_addition = 0;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
-    __m256i src, store;
-    v4u64 sum;
-
-    if( is_left && is_above )
-    {
-        src  = __lasx_xvld( p_src_top, 0 );
-        src = __lasx_xvhaddw_hu_bu( src, src );
-        src = __lasx_xvhaddw_wu_hu( src, src );
-        src = __lasx_xvhaddw_du_wu( src, src );
-        src = __lasx_xvpickev_w( src, src );
-        sum = ( v4u64 ) __lasx_xvhaddw_du_wu( src, src );
-        u_addition = sum[0];
-
-        for( u_row = 0; u_row < 4; u_row++ )
-        {
-            u_addition += p_src_left[i_index];
-            i_index += i_src_stride_left;
-            u_addition += p_src_left[i_index];
-            i_index += i_src_stride_left;
-            u_addition += p_src_left[i_index];
-            i_index += i_src_stride_left;
-            u_addition += p_src_left[i_index];
-            i_index += i_src_stride_left;
-        }
-
-        u_addition = ( u_addition + 16 ) >> 5;
-        store = __lasx_xvreplgr2vr_b( u_addition );
-    }
-    else if( is_left )
-    {
-        for( u_row = 0; u_row < 4; u_row++ )
-        {
-            u_addition += p_src_left[i_index];
-            i_index += i_src_stride_left;
-            u_addition += p_src_left[i_index];
-            i_index += i_src_stride_left;
-            u_addition += p_src_left[i_index];
-            i_index += i_src_stride_left;
-            u_addition += p_src_left[i_index];
-            i_index += i_src_stride_left;
-        }
-
-        u_addition = ( u_addition + 8 ) >> 4;
-        store = __lasx_xvreplgr2vr_b( u_addition );
-    }
-    else if( is_above )
-    {
-        src  = __lasx_xvld( p_src_top, 0 );
-        src = __lasx_xvhaddw_hu_bu( src, src );
-        src = __lasx_xvhaddw_wu_hu( src, src );
-        src = __lasx_xvhaddw_du_wu( src, src );
-        src = __lasx_xvpickev_w( src, src );
-        src = __lasx_xvhaddw_du_wu( src, src );
-        src = __lasx_xvsrari_d( src, 4 );
-
-        store = __lasx_xvrepl128vei_b( src, 0 );
-    }
-    else
+    if( cpu&X264_CPU_LASX )
     {
-        u_addition = 128;
-
-        store = __lasx_xvreplgr2vr_b( u_addition );
+#if !HIGH_BIT_DEPTH
+        pf[I_PRED_CHROMA_P]      = x264_predict_8x8c_p_lsx;
+        pf[I_PRED_CHROMA_V]      = x264_predict_8x8c_v_lsx;
+        pf[I_PRED_CHROMA_H]      = x264_predict_8x8c_h_lsx;
+        pf[I_PRED_CHROMA_DC]     = x264_predict_8x8c_dc_lsx;
+        pf[I_PRED_CHROMA_DC_128] = x264_predict_8x8c_dc_128_lsx;
+        pf[I_PRED_CHROMA_DC_TOP] = x264_predict_8x8c_dc_top_lsx;
+        pf[I_PRED_CHROMA_DC_LEFT]= x264_predict_8x8c_dc_left_lsx;
+#endif
     }
-
-    __lasx_xvstelm_d( store, p_dst, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
-    p_dst += ( i_dst_stride  << 2);
-    __lasx_xvstelm_d( store, p_dst, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
-    p_dst += ( i_dst_stride  << 2);
-    __lasx_xvstelm_d( store, p_dst, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
-    p_dst += ( i_dst_stride  << 2);
-    __lasx_xvstelm_d( store, p_dst, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
-    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
 }
 
-static inline void intra_predict_horiz_16x16_lasx( uint8_t *p_src,
-                                                   int32_t i_src_stride,
-                                                   uint8_t *p_dst,
-                                                   int32_t i_dst_stride )
+void x264_predict_8x8_init_lasx( int cpu, x264_predict8x8_t pf[12], x264_predict_8x8_filter_t *predict_filter )
 {
-    uint32_t u_row;
-    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
-    __m256i src0, src1, src2, src3;
-
-    for( u_row = 4; u_row--; )
+    if( cpu&X264_CPU_LASX )
     {
-        u_inp0 = p_src[0];
-        p_src += i_src_stride;
-        u_inp1 = p_src[0];
-        p_src += i_src_stride;
-        u_inp2 = p_src[0];
-        p_src += i_src_stride;
-        u_inp3 = p_src[0];
-        p_src += i_src_stride;
-
-        src0 = __lasx_xvreplgr2vr_b( u_inp0 );
-        src1 = __lasx_xvreplgr2vr_b( u_inp1 );
-        src2 = __lasx_xvreplgr2vr_b( u_inp2 );
-        src3 = __lasx_xvreplgr2vr_b( u_inp3 );
-
-        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src0, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( src1, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src1, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( src2, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src2, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_d( src3, p_dst, 0, 0 );
-        __lasx_xvstelm_d( src3, p_dst, 8, 1 );
-        p_dst += i_dst_stride;
+#if !HIGH_BIT_DEPTH
+        pf[I_PRED_8x8_V]      = x264_predict_8x8_v_lsx;
+        pf[I_PRED_8x8_H]      = x264_predict_8x8_h_lasx;
+        pf[I_PRED_8x8_DC]     = x264_predict_8x8_dc_lsx;
+        pf[I_PRED_8x8_DDL]    = x264_predict_8x8_ddl_lasx;
+        pf[I_PRED_8x8_DDR]    = x264_predict_8x8_ddr_lasx;
+        pf[I_PRED_8x8_VR]     = x264_predict_8x8_vr_lasx;
+        pf[I_PRED_8x8_VL]     = x264_predict_8x8_vl_lasx;
+        pf[I_PRED_8x8_DC_LEFT]= x264_predict_8x8_dc_left_lsx;
+        pf[I_PRED_8x8_DC_TOP] = x264_predict_8x8_dc_top_lsx;
+        pf[I_PRED_8x8_DC_128] = x264_predict_8x8_dc_128_lsx;
+#endif
     }
 }
 
-static inline void intra_predict_horiz_8x8_lasx( uint8_t *p_src,
-                                                 int32_t i_src_stride,
-                                                 uint8_t *p_dst,
-                                                 int32_t i_dst_stride )
+void x264_predict_4x4_init_lasx( int cpu, x264_predict_t pf[12] )
 {
-    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
-    __m256i src0, src1, src2, src3;
-
-    u_inp0 = p_src[0];
-    p_src += i_src_stride;
-    u_inp1 = p_src[0];
-    p_src += i_src_stride;
-    u_inp2 = p_src[0];
-    p_src += i_src_stride;
-    u_inp3 = p_src[0];
-    p_src += i_src_stride;
-
-    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
-    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
-    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
-    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
-
-    __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_d( src1, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_d( src2, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_d( src3, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-
-    u_inp0 = p_src[0];
-    p_src += i_src_stride;
-    u_inp1 = p_src[0];
-    p_src += i_src_stride;
-    u_inp2 = p_src[0];
-    p_src += i_src_stride;
-    u_inp3 = p_src[0];
-    p_src += i_src_stride;
-
-    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
-    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
-    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
-    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
-
-    __lasx_xvstelm_d( src0, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_d( src1, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_d( src2, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_d( src3, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-}
-
-static inline void intra_predict_horiz_4x4_lasx( uint8_t *p_src,
-                                                 int32_t i_src_stride,
-                                                 uint8_t *p_dst,
-                                                 int32_t i_dst_stride )
-{
-    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
-    __m256i src0, src1, src2, src3;
-
-    u_inp0 = p_src[0];
-    p_src += i_src_stride;
-    u_inp1 = p_src[0];
-    p_src += i_src_stride;
-    u_inp2 = p_src[0];
-    p_src += i_src_stride;
-    u_inp3 = p_src[0];
-    p_src += i_src_stride;
-
-    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
-    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
-    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
-    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
-
-    __lasx_xvstelm_w( src0, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_w( src1, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_w( src2, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-    __lasx_xvstelm_w( src3, p_dst, 0, 0 );
-    p_dst += i_dst_stride;
-}
-
-static inline void intra_predict_vert_16x16_lasx( uint8_t *p_src,
-                                                  uint8_t *p_dst,
-                                                  int32_t i_dst_stride )
-{
-    __m256i src;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
-    src  = __lasx_xvld( p_src, 0 );
-
-    __lasx_xvstelm_d( src, p_dst, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
-    p_dst += ( i_dst_stride  << 2);
-    __lasx_xvstelm_d( src, p_dst, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
-    p_dst += ( i_dst_stride  << 2);
-    __lasx_xvstelm_d( src, p_dst, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
-    p_dst += ( i_dst_stride  << 2);
-    __lasx_xvstelm_d( src, p_dst, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
-    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
-}
-
-static inline void intra_predict_vert_8x8_lasx( uint8_t *p_src,
-                                                uint8_t *p_dst,
-                                                int32_t i_dst_stride )
-{
-    __m256i out;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
-
-    out = __lasx_xvldrepl_d( p_src, 0 );
-
-    __lasx_xvstelm_d( out, p_dst, 0, 0 );
-    __lasx_xvstelm_d( out, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x3, 0, 0 );
-    p_dst += ( i_dst_stride << 2 );
-    __lasx_xvstelm_d( out, p_dst, 0, 0 );
-    __lasx_xvstelm_d( out, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x3, 0, 0 );
-}
-
-static inline void intra_predict_vert_4x4_lasx( uint8_t *p_src,
-                                                uint8_t *p_dst,
-                                                int32_t i_dst_stride )
-{
-    __m256i out;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
-
-    out = __lasx_xvldrepl_w( p_src, 0 );
-
-    __lasx_xvstelm_w( out, p_dst, 0, 0 );
-    __lasx_xvstelm_w( out, p_dst + i_dst_stride, 0, 0 );
-    __lasx_xvstelm_w( out, p_dst + i_dst_stride_x2, 0, 0 );
-    __lasx_xvstelm_w( out, p_dst + i_dst_stride_x3, 0, 0 );
-}
-
-void x264_intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src )
-{
-    intra_predict_dc_4blk_8x8_lasx( p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_hor_8x8_lasx( uint8_t *p_src )
-{
-    intra_predict_horiz_8x8_lasx( ( p_src - 1 ), FDEC_STRIDE,
-                                  p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_vert_8x8_lasx( uint8_t *p_src )
-{
-    intra_predict_vert_8x8_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_dc_4x4_lasx( uint8_t *p_src )
-{
-    intra_predict_dc_4x4_lasx( ( p_src - FDEC_STRIDE ), ( p_src - 1 ),
-                               FDEC_STRIDE, p_src, FDEC_STRIDE, 1, 1 );
-}
-
-void x264_intra_predict_hor_4x4_lasx( uint8_t *p_src )
-{
-    intra_predict_horiz_4x4_lasx( ( p_src - 1 ), FDEC_STRIDE,
-                                  p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_vert_4x4_lasx( uint8_t *p_src )
-{
-    intra_predict_vert_4x4_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_hor_16x16_lasx( uint8_t *p_src )
-{
-    intra_predict_horiz_16x16_lasx( ( p_src - 1 ), FDEC_STRIDE,
-                                    p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_vert_16x16_lasx( uint8_t *p_src )
-{
-    intra_predict_vert_16x16_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_dc_16x16_lasx( uint8_t *p_src )
-{
-    intra_predict_dc_16x16_lasx( ( p_src - FDEC_STRIDE ), ( p_src - 1 ),
-                                 FDEC_STRIDE, p_src, FDEC_STRIDE, 1, 1 );
-}
-
-void x264_intra_predict_dc_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
-{
-    intra_predict_dc_8x8_lasx( ( pu_xyz + 16 ), ( pu_xyz + 7 ),
-                               p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_h_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
-{
-    intra_predict_horiz_8x8_lasx( ( pu_xyz + 14 ), -1, p_src, FDEC_STRIDE );
-}
-
-void x264_intra_predict_v_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
-{
-    intra_predict_vert_8x8_lasx( ( pu_xyz + 16 ), p_src, FDEC_STRIDE );
-}
-
-void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] )
-{
-    if ( cpu&X264_CPU_LASX ) {
+    if( cpu&X264_CPU_LASX )
+    {
 #if !HIGH_BIT_DEPTH
-        pf[I_PRED_16x16_V]    = x264_intra_predict_vert_16x16_lasx;
-        pf[I_PRED_16x16_H]    = x264_intra_predict_hor_16x16_lasx;
-        pf[I_PRED_16x16_DC]   = x264_intra_predict_dc_16x16_lasx;
+        pf[I_PRED_4x4_V]      = x264_predict_4x4_v_lsx;
+        pf[I_PRED_4x4_H]      = x264_predict_4x4_h_lsx;
+        pf[I_PRED_4x4_DC]     = x264_predict_4x4_dc_lsx;
+        pf[I_PRED_4x4_DDL]    = x264_predict_4x4_ddl_lsx;
+        pf[I_PRED_4x4_DC_LEFT]= x264_predict_4x4_dc_left_lsx;
+        pf[I_PRED_4x4_DC_TOP] = x264_predict_4x4_dc_top_lsx;
+        pf[I_PRED_4x4_DC_128] = x264_predict_4x4_dc_128_lsx;
 #endif
     }
 }
-
-#endif
diff --git a/common/loongarch/predict.h b/common/loongarch/predict.h
index dc7e9a6a..d4657f4c 100644
--- a/common/loongarch/predict.h
+++ b/common/loongarch/predict.h
@@ -27,37 +27,6 @@
 #ifndef X264_LOONGARCH_PREDICT_H
 #define X264_LOONGARCH_PREDICT_H
 
-#define x264_intra_predict_dc_16x16_lasx x264_template(intra_predict_dc_16x16_lasx)
-void x264_intra_predict_dc_16x16_lasx( uint8_t *p_src );
-#define x264_intra_predict_hor_16x16_lasx x264_template(intra_predict_hor_16x16_lasx)
-void x264_intra_predict_hor_16x16_lasx( uint8_t *p_src );
-#define x264_intra_predict_vert_16x16_lasx x264_template(intra_predict_vert_16x16_lasx)
-void x264_intra_predict_vert_16x16_lasx( uint8_t *p_src );
-
-#define x264_intra_predict_dc_4blk_8x8_lasx x264_template(intra_predict_dc_4blk_8x8_lasx)
-void x264_intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src );
-#define x264_intra_predict_hor_8x8_lasx x264_template(intra_predict_hor_8x8_lasx)
-void x264_intra_predict_hor_8x8_lasx( uint8_t *p_src );
-#define x264_intra_predict_vert_8x8_lasx x264_template(intra_predict_vert_8x8_lasx)
-void x264_intra_predict_vert_8x8_lasx( uint8_t *p_src );
-
-#define x264_intra_predict_dc_4x4_lasx x264_template(intra_predict_dc_4x4_lasx)
-void x264_intra_predict_dc_4x4_lasx( uint8_t *p_src );
-#define x264_intra_predict_hor_4x4_lasx x264_template(intra_predict_hor_4x4_lasx)
-void x264_intra_predict_hor_4x4_lasx( uint8_t *p_src );
-#define x264_intra_predict_vert_4x4_lasx x264_template(intra_predict_vert_4x4_lasx)
-void x264_intra_predict_vert_4x4_lasx( uint8_t *p_src );
-
-#define x264_intra_predict_dc_8x8_lasx x264_template(intra_predict_dc_8x8_lasx)
-void x264_intra_predict_dc_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
-#define x264_intra_predict_h_8x8_lasx x264_template(intra_predict_h_8x8_lasx)
-void x264_intra_predict_h_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
-#define x264_intra_predict_v_8x8_lasx x264_template(intra_predict_v_8x8_lasx)
-void x264_intra_predict_v_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
-
-#define x264_predict_16x16_init_lasx x264_template(predict_16x16_init_lasx)
-void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] );
-
 #define x264_predict_8x8c_p_lsx x264_template(predict_8x8c_p_lsx)
 void x264_predict_8x8c_p_lsx(uint8_t *p_src);
 
@@ -70,26 +39,26 @@ void x264_predict_8x8c_h_lsx(uint8_t *p_src);
 #define x264_predict_8x8c_dc_lsx x264_template(predict_8x8c_dc_lsx)
 void x264_predict_8x8c_dc_lsx(pixel *src);
 
-#define predict_8x8c_dc_128_lsx x264_template(predict_8x8c_dc_128_lsx)
-void predict_8x8c_dc_128_lsx(pixel *src);
+#define x264_predict_8x8c_dc_128_lsx x264_template(predict_8x8c_dc_128_lsx)
+void x264_predict_8x8c_dc_128_lsx(pixel *src);
 
-#define predict_8x8c_dc_top_lsx x264_template(predict_8x8c_dc_top_lsx)
-void predict_8x8c_dc_top_lsx(pixel *src);
+#define x264_predict_8x8c_dc_top_lsx x264_template(predict_8x8c_dc_top_lsx)
+void x264_predict_8x8c_dc_top_lsx(pixel *src);
 
-#define predict_8x8c_dc_left_lsx x264_template(predict_8x8c_dc_left_lsx)
-void predict_8x8c_dc_left_lsx(pixel *src);
+#define x264_predict_8x8c_dc_left_lsx x264_template(predict_8x8c_dc_left_lsx)
+void x264_predict_8x8c_dc_left_lsx(pixel *src);
 
 #define x264_predict_16x16_dc_lsx x264_template(predict_16x16_dc_lsx)
 void x264_predict_16x16_dc_lsx( pixel *src );
 
-#define predict_16x16_dc_left_lsx x264_template(predict_16x16_dc_left_lsx)
-void predict_16x16_dc_left_lsx( pixel *src );
+#define x264_predict_16x16_dc_left_lsx x264_template(predict_16x16_dc_left_lsx)
+void x264_predict_16x16_dc_left_lsx( pixel *src );
 
-#define predict_16x16_dc_top_lsx x264_template(predict_16x16_dc_top_lsx)
-void predict_16x16_dc_top_lsx( pixel *src );
+#define x264_predict_16x16_dc_top_lsx x264_template(predict_16x16_dc_top_lsx)
+void x264_predict_16x16_dc_top_lsx( pixel *src );
 
-#define predict_16x16_dc_128_lsx x264_template(predict_16x16_dc_128_lsx)
-void predict_16x16_dc_128_lsx( pixel *src );
+#define x264_predict_16x16_dc_128_lsx x264_template(predict_16x16_dc_128_lsx)
+void x264_predict_16x16_dc_128_lsx( pixel *src );
 
 #define x264_predict_16x16_h_lsx x264_template(predict_16x16_h_lsx)
 void x264_predict_16x16_h_lsx( pixel *src );
@@ -151,4 +120,13 @@ void x264_predict_4x4_dc_left_lsx( pixel *p_src );
 #define x264_predict_4x4_dc_128_lsx x264_template(predict_4x4_dc_128_lsx)
 void x264_predict_4x4_dc_128_lsx( pixel *p_src );
 
+#define x264_predict_4x4_init_lasx x264_template(predict_4x4_init_lasx)
+void x264_predict_4x4_init_lasx( int cpu, x264_predict_t pf[12] );
+#define x264_predict_8x8_init_lasx x264_template(predict_8x8_init_lasx)
+void x264_predict_8x8_init_lasx( int cpu, x264_predict8x8_t pf[12], x264_predict_8x8_filter_t *predict_filter );
+#define x264_predict_8x8c_init_lasx x264_template(predict_8x8c_init_lasx)
+void x264_predict_8x8c_init_lasx( int cpu, x264_predict_t pf[7] );
+#define x264_predict_16x16_init_lasx x264_template(predict_16x16_init_lasx)
+void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] );
+
 #endif
diff --git a/common/loongarch/sad-a.S b/common/loongarch/sad-a.S
index f91715d2..8207f6d6 100644
--- a/common/loongarch/sad-a.S
+++ b/common/loongarch/sad-a.S
@@ -2012,6 +2012,86 @@ function pixel_sad_8x4_lasx
     add.d           a0,    t2,   t3
 endfunc
 
+/*
+ * int32_t pixel_sad_4x16_lasx(const Pixel *pix1, intptr_t stride_pix1,
+ *                             const Pixel *pix2, intptr_t stride_pix2)
+ */
+function pixel_sad_4x16_lasx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+    slli.d          t5,    a1,   2
+    slli.d          t6,    a3,   2
+
+    // Load data from pix1 and pix2
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3,  f4,  f5,  f6
+    add.d           a0,    a0,   t5
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f7,  f8,  f9,  f10
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f11, f12, f13, f14
+    add.d           a2,    a2,   t6
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f15, f16, f17, f18
+
+    vilvl.w         vr3,   vr7,  vr3
+    vilvl.w         vr4,   vr8,  vr4
+    vilvl.w         vr5,   vr9,  vr5
+    vilvl.w         vr6,   vr10, vr6
+    vilvl.w         vr11,  vr15, vr11
+    vilvl.w         vr12,  vr16, vr12
+    vilvl.w         vr13,  vr17, vr13
+    vilvl.w         vr14,  vr18, vr14
+
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr11,  vr13, vr11
+    vilvl.d         vr12,  vr14, vr12
+    xvpermi.q       xr3,   xr4,  0x02
+    xvpermi.q       xr11,  xr12, 0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr3,   xr3,  xr11
+    xvhaddw.hu.bu   xr0,   xr3,  xr3
+
+    // Load data from pix1 and pix2
+    add.d           a0,    a0,   t5
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3,  f4,  f5,  f6
+    add.d           a0,    a0,   t5
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f7,  f8,  f9,  f10
+    add.d           a2,    a2,   t6
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f11, f12, f13, f14
+    add.d           a2,    a2,   t6
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f15, f16, f17, f18
+
+    vilvl.w         vr3,   vr7,  vr3
+    vilvl.w         vr4,   vr8,  vr4
+    vilvl.w         vr5,   vr9,  vr5
+    vilvl.w         vr6,   vr10, vr6
+    vilvl.w         vr11,  vr15, vr11
+    vilvl.w         vr12,  vr16, vr12
+    vilvl.w         vr13,  vr17, vr13
+    vilvl.w         vr14,  vr18, vr14
+
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr11,  vr13, vr11
+    vilvl.d         vr12,  vr14, vr12
+    xvpermi.q       xr3,   xr4,  0x02
+    xvpermi.q       xr11,  xr12, 0x02
+
+    // Calculate the absolute value of the difference
+    xvabsd.bu       xr3,   xr3,  xr11
+    xvhaddw.hu.bu   xr3,   xr3,  xr3
+    xvadd.h         xr0,   xr0,  xr3
+
+    // Calculate the sum
+    xvhaddw.w.h     xr0,   xr0,  xr0
+    xvhaddw.d.w     xr0,   xr0,  xr0
+    xvhaddw.q.d     xr0,   xr0,  xr0
+    xvpickve2gr.wu  t2,    xr0,  0
+    xvpickve2gr.wu  t3,    xr0,  4
+    add.d           a0,    t2,   t3
+endfunc
+
 /* int32_t x264_pixel_sad_4x8_lasx(uint8_t *p_src, intptr_t i_src_stride,
  *                                 uint8_t *p_ref, intptr_t i_ref_stride)
  */
diff --git a/common/predict.c b/common/predict.c
index d9fb024c..8238f939 100644
--- a/common/predict.c
+++ b/common/predict.c
@@ -925,18 +925,6 @@ void x264_predict_16x16_init( int cpu, x264_predict_t pf[7] )
         pf[I_PRED_16x16_DC_128 ]= x264_intra_predict_dc_128_16x16_msa;
     }
 #endif
-#if HAVE_LASX
-    if( cpu&X264_CPU_LASX )
-    {
-        pf[I_PRED_16x16_V ]     = x264_predict_16x16_v_lsx;
-        pf[I_PRED_16x16_H ]     = x264_predict_16x16_h_lsx;
-        pf[I_PRED_16x16_DC]     = x264_predict_16x16_dc_lsx;
-        pf[I_PRED_16x16_P ]     = x264_predict_16x16_p_lasx;
-        pf[I_PRED_16x16_DC_LEFT]= predict_16x16_dc_left_lsx;
-        pf[I_PRED_16x16_DC_TOP ]= predict_16x16_dc_top_lsx;
-        pf[I_PRED_16x16_DC_128 ]= predict_16x16_dc_128_lsx;
-    }
-#endif
 #endif
 
 #if ARCH_LOONGARCH64
@@ -980,19 +968,8 @@ void x264_predict_8x8c_init( int cpu, x264_predict_t pf[7] )
 #endif
 #endif
 
-#if !HIGH_BIT_DEPTH
 #if HAVE_LASX
-    if( cpu&X264_CPU_LASX )
-    {
-        pf[I_PRED_CHROMA_P]      = x264_predict_8x8c_p_lsx;
-        pf[I_PRED_CHROMA_V]      = x264_predict_8x8c_v_lsx;
-        pf[I_PRED_CHROMA_H]      = x264_predict_8x8c_h_lsx;
-        pf[I_PRED_CHROMA_DC]     = x264_predict_8x8c_dc_lsx;
-        pf[I_PRED_CHROMA_DC_128] = predict_8x8c_dc_128_lsx;
-        pf[I_PRED_CHROMA_DC_TOP] = predict_8x8c_dc_top_lsx;
-        pf[I_PRED_CHROMA_DC_LEFT]= predict_8x8c_dc_left_lsx;
-    }
-#endif
+    x264_predict_8x8c_init_lasx( cpu, pf );
 #endif
 }
 
@@ -1056,22 +1033,8 @@ void x264_predict_8x8_init( int cpu, x264_predict8x8_t pf[12], x264_predict_8x8_
 #endif
 #endif
 
-#if !HIGH_BIT_DEPTH
 #if HAVE_LASX
-    if( cpu&X264_CPU_LASX )
-    {
-        pf[I_PRED_8x8_V]      = x264_predict_8x8_v_lsx;
-        pf[I_PRED_8x8_H]      = x264_predict_8x8_h_lasx;
-        pf[I_PRED_8x8_DC]     = x264_predict_8x8_dc_lsx;
-        pf[I_PRED_8x8_DDL]    = x264_predict_8x8_ddl_lasx;
-        pf[I_PRED_8x8_DDR]    = x264_predict_8x8_ddr_lasx;
-        pf[I_PRED_8x8_VR]     = x264_predict_8x8_vr_lasx;
-        pf[I_PRED_8x8_VL]     = x264_predict_8x8_vl_lasx;
-        pf[I_PRED_8x8_DC_LEFT]= x264_predict_8x8_dc_left_lsx;
-        pf[I_PRED_8x8_DC_TOP] = x264_predict_8x8_dc_top_lsx;
-        pf[I_PRED_8x8_DC_128] = x264_predict_8x8_dc_128_lsx;
-    }
-#endif
+    x264_predict_8x8_init_lasx( cpu, pf, predict_filter );
 #endif
 }
 
@@ -1102,19 +1065,8 @@ void x264_predict_4x4_init( int cpu, x264_predict_t pf[12] )
     x264_predict_4x4_init_aarch64( cpu, pf );
 #endif
 
-#if !HIGH_BIT_DEPTH
 #if HAVE_LASX
-    if( cpu&X264_CPU_LASX )
-    {
-        pf[I_PRED_4x4_V]      = x264_predict_4x4_v_lsx;
-        pf[I_PRED_4x4_H]      = x264_predict_4x4_h_lsx;
-        pf[I_PRED_4x4_DC]     = x264_predict_4x4_dc_lsx;
-        pf[I_PRED_4x4_DDL]    = x264_predict_4x4_ddl_lsx;
-        pf[I_PRED_4x4_DC_LEFT]= x264_predict_4x4_dc_left_lsx;
-        pf[I_PRED_4x4_DC_TOP] = x264_predict_4x4_dc_top_lsx;
-        pf[I_PRED_4x4_DC_128] = x264_predict_4x4_dc_128_lsx;
-    }
-#endif
+    x264_predict_4x4_init_lasx( cpu, pf );
 #endif
 }
 
