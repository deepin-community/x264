From 22b659b1d89220e25a7df750c2dafdf830a82460 Mon Sep 17 00:00:00 2001
From: Hao Chen <chenhao@loongson.cn>
Date: Fri, 13 Aug 2021 17:11:35 +0800
Subject: [PATCH] [loongarch]: Add loongarch SIMD optimization.

Add loongarch SIMD support and LASX optimization functions.
This patch contains patchs frome ec9efde5 to ad278944.

Change-Id: I918d9084674a35b35ad52fb015631bd448f79e6f
Signed-off-by: Hao Chen <chenhao@loongson.cn>
---
 Makefile                               |   21 +
 common/cpu.c                           |   33 +
 common/dct.c                           |   17 +
 common/deblock.c                       |   13 +
 common/loongarch/dct-c.c               |  446 +++
 common/loongarch/dct.h                 |   53 +
 common/loongarch/deblock-c.c           |  890 ++++++
 common/loongarch/deblock.h             |   41 +
 common/loongarch/generic_macros_lasx.h | 3792 ++++++++++++++++++++++
 common/loongarch/mc-c.c                | 4089 ++++++++++++++++++++++++
 common/loongarch/mc.h                  |   33 +
 common/loongarch/pixel-c.c             | 2608 +++++++++++++++
 common/loongarch/pixel.h               |  232 ++
 common/loongarch/predict-c.c           |  484 +++
 common/loongarch/predict.h             |   58 +
 common/loongarch/quant-c.c             |  273 ++
 common/loongarch/quant.h               |   42 +
 common/mc.c                            |    6 +
 common/pixel.c                         |   34 +-
 common/quant.c                         |   14 +
 config.guess                           |  911 +++---
 config.sub                             |  386 +--
 configure                              |   28 +-
 x264.h                                 |    4 +
 24 files changed, 13830 insertions(+), 678 deletions(-)
 create mode 100644 common/loongarch/dct-c.c
 create mode 100644 common/loongarch/dct.h
 create mode 100644 common/loongarch/deblock-c.c
 create mode 100644 common/loongarch/deblock.h
 create mode 100644 common/loongarch/generic_macros_lasx.h
 create mode 100644 common/loongarch/mc-c.c
 create mode 100644 common/loongarch/mc.h
 create mode 100644 common/loongarch/pixel-c.c
 create mode 100644 common/loongarch/pixel.h
 create mode 100644 common/loongarch/predict-c.c
 create mode 100644 common/loongarch/predict.h
 create mode 100644 common/loongarch/quant-c.c
 create mode 100644 common/loongarch/quant.h

diff --git a/Makefile b/Makefile
index e4aa9006..d8baf659 100644
--- a/Makefile
+++ b/Makefile
@@ -198,6 +198,18 @@ SRCS_X += common/mips/dct-c.c \
 endif
 endif
 
+# LOONGARCH optimization
+ifeq ($(SYS_ARCH),LOONGARCH)
+ifneq ($(findstring HAVE_LASX 1, $(CONFIG)),)
+SRCS_X += common/loongarch/pixel-c.c \
+          common/loongarch/predict-c.c \
+          common/loongarch/quant-c.c \
+          common/loongarch/dct-c.c \
+          common/loongarch/mc-c.c \
+          common/loongarch/deblock-c.c
+endif
+endif
+
 endif
 
 ifneq ($(HAVE_GETOPT_LONG),1)
@@ -291,6 +303,15 @@ common/mips/%-c-8.o: common/mips/%-c.c
 common/mips/%-c-10.o: common/mips/%-c.c
 	$(CC) $(CFLAGS) $(MSA_CFLAGS) -c -o $@ $< -DHIGH_BIT_DEPTH=1 -DBIT_DEPTH=10
 
+common/loongarch/%-c.o: common/loongarch/%-c.c
+	$(CC) $(CFLAGS) $(LSX_CFLAGS) $(LASX_CFLAGS) -c -o $@ $<
+
+common/loongarch/%-c-8.o: common/loongarch/%-c.c
+	$(CC) $(CFLAGS) $(LSX_CFLAGS) $(LASX_CFLAGS) -c -o $@ $< -DHIGH_BIT_DEPTH=0 -DBIT_DEPTH=8
+
+common/loongarch/%-c-10.o: common/loongarch/%-c.c
+	$(CC) $(CFLAGS) $(LSX_CFLAGS) $(LASX_CFLAGS) -c -o $@ $< -DHIGH_BIT_DEPTH=1 -DBIT_DEPTH=10
+
 %.o: %.asm common/x86/x86inc.asm common/x86/x86util.asm
 	$(AS) $(ASFLAGS) -o $@ $<
 	-@ $(if $(STRIP), $(STRIP) -x $@) # delete local/anonymous symbols, so they don't show up in oprofile
diff --git a/common/cpu.c b/common/cpu.c
index 321415f8..af3511b3 100644
--- a/common/cpu.c
+++ b/common/cpu.c
@@ -93,6 +93,9 @@ const x264_cpu_name_t x264_cpu_names[] =
     {"NEON",            X264_CPU_NEON},
 #elif ARCH_MIPS
     {"MSA",             X264_CPU_MSA},
+#elif ARCH_LOONGARCH
+    {"LSX",             X264_CPU_LSX},
+    {"LASX",            X264_CPU_LASX},
 #endif
     {"", 0},
 };
@@ -442,6 +445,36 @@ uint32_t x264_cpu_detect( void )
      return flags;
 }
 
+#elif ARCH_LOONGARCH
+
+#define LOONGARCH_CFG2    0x02
+#define LOONGARCH_LSX     ( 1 << 6 )
+#define LOONGARCH_LASX    ( 1 << 7 )
+
+uint32_t x264_cpu_detect( void )
+{
+    uint32_t flags = 0;
+    uint32_t cfg = LOONGARCH_CFG2;
+    uint32_t reg;
+
+    __asm__ volatile(
+        "cpucfg %0, %1 \n\t"
+        : "+&r"(reg)
+        : "r"(cfg)
+    );
+
+#if HAVE_LSX
+    if ( reg & LOONGARCH_LSX )
+        flags |= X264_CPU_LSX;
+#endif
+#if HAVE_LASX
+    if ( reg & LOONGARCH_LASX )
+        flags |= X264_CPU_LASX;
+#endif
+
+    return flags;
+}
+
 #else
 
 uint32_t x264_cpu_detect( void )
diff --git a/common/dct.c b/common/dct.c
index b428a4df..14417700 100644
--- a/common/dct.c
+++ b/common/dct.c
@@ -41,6 +41,9 @@
 #if ARCH_MIPS
 #   include "mips/dct.h"
 #endif
+#if ARCH_LOONGARCH
+#   include "loongarch/dct.h"
+#endif
 
 static void dct4x4dc( dctcoef d[16] )
 {
@@ -728,6 +731,20 @@ void x264_dct_init( int cpu, x264_dct_function_t *dctf )
     }
 #endif
 
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        dctf->sub4x4_dct       = x264_sub4x4_dct_lasx;
+        dctf->sub8x8_dct       = x264_sub8x8_dct_lasx;
+        dctf->sub16x16_dct     = x264_sub16x16_dct_lasx;
+        dctf->add4x4_idct      = x264_add4x4_idct_lasx;
+        dctf->add8x8_idct      = x264_add8x8_idct_lasx;
+        dctf->add16x16_idct    = x264_add16x16_idct_lasx;
+        dctf->sub8x8_dct8      = x264_sub8x8_dct8_lasx;
+        dctf->sub16x16_dct8    = x264_sub16x16_dct8_lasx;
+    }
+#endif
+
 #endif // HIGH_BIT_DEPTH
 }
 
diff --git a/common/deblock.c b/common/deblock.c
index 7ec58535..a7985730 100644
--- a/common/deblock.c
+++ b/common/deblock.c
@@ -666,6 +666,9 @@ void x264_macroblock_deblock( x264_t *h )
 #if HAVE_MSA
 #include "mips/deblock.h"
 #endif
+#if HAVE_LASX
+#include "loongarch/deblock.h"
+#endif
 
 void x264_deblock_init( int cpu, x264_deblock_function_t *pf, int b_mbaff )
 {
@@ -802,6 +805,16 @@ void x264_deblock_init( int cpu, x264_deblock_function_t *pf, int b_mbaff )
         pf->deblock_strength = x264_deblock_strength_msa;
     }
 #endif
+
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf->deblock_luma[1] = x264_deblock_v_luma_lasx;
+        pf->deblock_luma[0] = x264_deblock_h_luma_lasx;
+        pf->deblock_strength = x264_deblock_strength_lasx;
+    }
+#endif
+
 #endif // !HIGH_BIT_DEPTH
 
     /* These functions are equivalent, so don't duplicate them. */
diff --git a/common/loongarch/dct-c.c b/common/loongarch/dct-c.c
new file mode 100644
index 00000000..1a454442
--- /dev/null
+++ b/common/loongarch/dct-c.c
@@ -0,0 +1,446 @@
+/*****************************************************************************
+ * dct-c.c: loongarch transform and zigzag
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "generic_macros_lasx.h"
+#include "dct.h"
+
+#if !HIGH_BIT_DEPTH
+
+#define LASX_LD4x4( p_src, out0, out1, out2, out3 )     \
+{                                                       \
+    out0 = LASX_LD( p_src );                            \
+    out1 = __lasx_xvpermi_d( out0, 0x55 );              \
+    out2 = __lasx_xvpermi_d( out0, 0xAA );              \
+    out3 = __lasx_xvpermi_d( out0, 0xFF );              \
+}
+
+#define LASX_ITRANS_H( in0, in1, in2, in3, out0, out1, out2, out3 )         \
+{                                                                           \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
+                                                                            \
+    tmp0_m = __lasx_xvadd_h( in0, in2 );                                    \
+    tmp1_m = __lasx_xvsub_h( in0, in2 );                                    \
+    tmp2_m = __lasx_xvsrai_h( in1, 1 );                                     \
+    tmp2_m = __lasx_xvsub_h( tmp2_m, in3 );                                 \
+    tmp3_m = __lasx_xvsrai_h( in3, 1 );                                     \
+    tmp3_m = __lasx_xvadd_h( in1, tmp3_m );                                 \
+                                                                            \
+    LASX_BUTTERFLY_4( v16i16, tmp0_m, tmp1_m, tmp2_m, tmp3_m,               \
+                      out0, out1, out2, out3 );                             \
+}
+
+#define LASX_ADDBLK_ST4x4_128SV( in0, in1, in2, in3, p_dst, stride )        \
+{                                                                           \
+    uint32_t src0_m, src1_m, src2_m, src3_m;                                \
+    uint8_t *p_dst0;                                                        \
+    __m256i inp0_m, inp1_m, res0_m, res1_m;                                 \
+    __m256i dst0_m = __lasx_xvldi( 0 );                                     \
+    __m256i dst1_m = __lasx_xvldi( 0 );                                     \
+    __m256i zero_m = __lasx_xvldi( 0 );                                     \
+                                                                            \
+    LASX_ILVL_D_2_128SV( in1, in0, in3, in2, inp0_m, inp1_m )               \
+    src0_m = *( uint32_t* )( p_dst );                                       \
+    p_dst0 = p_dst + stride;                                                \
+    src1_m = *( uint32_t* )( p_dst0 );                                      \
+    p_dst0 += stride;                                                       \
+    src2_m = *( uint32_t* )( p_dst0 );                                      \
+    p_dst0 += stride;                                                       \
+    src3_m = *( uint32_t* )( p_dst0 );                                      \
+    dst0_m = __lasx_xvinsgr2vr_w( dst0_m, src0_m, 0 );                      \
+    dst0_m = __lasx_xvinsgr2vr_w( dst0_m, src1_m, 1 );                      \
+    dst1_m = __lasx_xvinsgr2vr_w( dst1_m, src2_m, 0 );                      \
+    dst1_m = __lasx_xvinsgr2vr_w( dst1_m, src3_m, 1 );                      \
+    LASX_ILVL_B_2_128SV( zero_m, dst0_m, zero_m, dst1_m, res0_m, res1_m );  \
+    res0_m = __lasx_xvadd_h( res0_m, inp0_m );                              \
+    res1_m = __lasx_xvadd_h( res1_m, inp1_m );                              \
+    LASX_CLIP_H_0_255_2( res0_m, res1_m, res0_m, res1_m );                  \
+    LASX_PCKEV_B_2_128SV( res0_m, res0_m, res1_m, res1_m, dst0_m, dst1_m ); \
+                                                                            \
+    LASX_ST_W( dst0_m, 0, p_dst );                                          \
+    p_dst0 = p_dst + stride;                                                \
+    LASX_ST_W( dst0_m, 1, p_dst0 );                                         \
+    p_dst0 += stride;                                                       \
+    LASX_ST_W( dst1_m, 0, p_dst0 );                                         \
+    p_dst0 += stride;                                                       \
+    LASX_ST_W( dst1_m, 1, p_dst0 );                                         \
+}
+
+static void avc_sub4x4_dct_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                 uint8_t *p_ref, int32_t i_dst_stride,
+                                 int16_t *p_dst )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i inp0, inp1, tmp;
+    __m256i diff0, diff1, diff2, diff3;
+    __m256i temp0, temp1, temp2, temp3;
+
+    src0 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src1 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src2 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src3 = __lasx_xvldrepl_w( p_src, 0 );
+    src0 = __lasx_xvpackev_w( src1, src0 );
+    src1 = __lasx_xvpackev_w( src3, src2 );
+    src0 = __lasx_xvpackev_d( src1, src0 );
+
+    ref0 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_dst_stride;
+    ref1 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_dst_stride;
+    ref2 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_dst_stride;
+    ref3 = __lasx_xvldrepl_w( p_ref, 0 );
+    ref0 = __lasx_xvpackev_w( ref1, ref0 );
+    ref1 = __lasx_xvpackev_w( ref3, ref2 );
+    ref0 = __lasx_xvpackev_d( ref1, ref0 );
+
+    LASX_ILVLH_B_128SV( src0, ref0, inp1, inp0 );
+    LASX_HSUB_UB_2( inp0, inp1, diff0, diff2 );
+    LASX_ILVH_D_2_128SV( diff0, diff0, diff2, diff2, diff1, diff3 );
+
+    LASX_BUTTERFLY_4( v16i16, diff0, diff1, diff2, diff3,
+                      temp0, temp1, temp2, temp3 );
+
+    diff0 = __lasx_xvadd_h( temp0, temp1);
+    tmp = __lasx_xvslli_h( temp3, 1);
+    diff1 = __lasx_xvadd_h( tmp, temp2);
+    diff2 = __lasx_xvsub_h( temp0, temp1 );
+    tmp = __lasx_xvslli_h( temp2, 1);
+    diff3 = __lasx_xvsub_h( temp3, tmp );
+
+    LASX_TRANSPOSE4x4_H_128SV( diff0, diff1, diff2, diff3,
+                               temp0, temp1, temp2, temp3 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp2, temp3,
+                      diff0, diff1, diff2, diff3 );
+
+    temp0 = __lasx_xvadd_h( diff0, diff1);
+    tmp = __lasx_xvslli_h( diff3, 1);
+    temp1 = __lasx_xvadd_h( tmp, diff2);
+    temp2 = __lasx_xvsub_h( diff0, diff1 );
+    tmp = __lasx_xvslli_h( diff2, 1);
+    temp3 = __lasx_xvsub_h( diff3, tmp );
+
+    LASX_ILVL_D_2_128SV( temp1, temp0, temp3, temp2, inp0, inp1 );
+    inp0 = __lasx_xvpermi_q(inp1, inp0, 0x20);
+    LASX_ST( inp0, p_dst );
+}
+
+void x264_sub4x4_dct_lasx( int16_t p_dst[16], uint8_t *p_src,
+                           uint8_t *p_ref )
+{
+    avc_sub4x4_dct_lasx( p_src, FENC_STRIDE, p_ref, FDEC_STRIDE, p_dst );
+}
+
+void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src,
+                           uint8_t *p_ref )
+{
+    avc_sub4x4_dct_lasx( &p_src[0], FENC_STRIDE,
+                         &p_ref[0], FDEC_STRIDE, p_dst[0] );
+    avc_sub4x4_dct_lasx( &p_src[4], FENC_STRIDE, &p_ref[4],
+                         FDEC_STRIDE, p_dst[1] );
+    avc_sub4x4_dct_lasx( &p_src[4 * FENC_STRIDE + 0],
+                         FENC_STRIDE, &p_ref[4 * FDEC_STRIDE + 0],
+                         FDEC_STRIDE, p_dst[2] );
+    avc_sub4x4_dct_lasx( &p_src[4 * FENC_STRIDE + 4],
+                         FENC_STRIDE, &p_ref[4 * FDEC_STRIDE + 4],
+                         FDEC_STRIDE, p_dst[3] );
+}
+
+void x264_sub16x16_dct_lasx( int16_t p_dst[16][16],
+                             uint8_t *p_src,
+                             uint8_t *p_ref )
+{
+    x264_sub8x8_dct_lasx( &p_dst[ 0], &p_src[0], &p_ref[0] );
+    x264_sub8x8_dct_lasx( &p_dst[ 4], &p_src[8], &p_ref[8] );
+    x264_sub8x8_dct_lasx( &p_dst[ 8], &p_src[8 * FENC_STRIDE + 0],
+                          &p_ref[8*FDEC_STRIDE+0] );
+    x264_sub8x8_dct_lasx( &p_dst[12], &p_src[8 * FENC_STRIDE + 8],
+                          &p_ref[8*FDEC_STRIDE+8] );
+}
+
+static void avc_idct4x4_addblk_lasx( uint8_t *p_dst, int16_t *p_src,
+                                     int32_t i_dst_stride )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i hres0, hres1, hres2, hres3;
+    __m256i vres0, vres1, vres2, vres3;
+
+    LASX_LD4x4( p_src, src0, src1, src2, src3 );
+    LASX_ITRANS_H( src0, src1, src2, src3, hres0, hres1, hres2, hres3 );
+    LASX_TRANSPOSE4x4_H_128SV( hres0, hres1, hres2, hres3,
+                               hres0, hres1, hres2, hres3 );
+    LASX_ITRANS_H( hres0, hres1, hres2, hres3, vres0, vres1, vres2, vres3 );
+    LASX_SRARI_H_4( vres0, vres1, vres2, vres3,
+                    vres0, vres1, vres2, vres3, 6 );
+    LASX_ADDBLK_ST4x4_128SV( vres0, vres1, vres2, vres3, p_dst, i_dst_stride );
+}
+
+void x264_add4x4_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16] )
+{
+    avc_idct4x4_addblk_lasx( p_dst, pi_dct, FDEC_STRIDE );
+}
+
+void x264_add8x8_idct_lasx( uint8_t *p_dst, int16_t pi_dct[4][16] )
+{
+    avc_idct4x4_addblk_lasx( &p_dst[0], &pi_dct[0][0], FDEC_STRIDE );
+    avc_idct4x4_addblk_lasx( &p_dst[4], &pi_dct[1][0], FDEC_STRIDE );
+    avc_idct4x4_addblk_lasx( &p_dst[4 * FDEC_STRIDE + 0],
+                            &pi_dct[2][0], FDEC_STRIDE );
+    avc_idct4x4_addblk_lasx( &p_dst[4 * FDEC_STRIDE + 4],
+                            &pi_dct[3][0], FDEC_STRIDE );
+}
+
+void x264_add16x16_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16][16] )
+{
+    x264_add8x8_idct_lasx( &p_dst[0], &pi_dct[0] );
+    x264_add8x8_idct_lasx( &p_dst[8], &pi_dct[4] );
+    x264_add8x8_idct_lasx( &p_dst[8 * FDEC_STRIDE + 0], &pi_dct[8] );
+    x264_add8x8_idct_lasx( &p_dst[8 * FDEC_STRIDE + 8], &pi_dct[12] );
+}
+
+/****************************************************************************
+ * 8x8 transform:
+ ****************************************************************************/
+
+void x264_sub8x8_dct8_lasx( int16_t pi_dct[64], uint8_t *p_pix1,
+                            uint8_t *p_pix2 )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m256i temp, temp1;
+    __m256i zero = {0};
+    __m256i s07, s16, s25, s34, d07, d16, d25, d34;
+    __m256i a0, a1, a2, a3, a4, a5, a6, a7;
+
+#define LOAD_PIX_DATA_2(data1, data2)             \
+    LASX_LD_2( p_pix1, FENC_STRIDE, src0, src1 ); \
+    LASX_LD_2( p_pix2, FDEC_STRIDE, src2, src3 ); \
+    LASX_ILVL_B_4_128SV( zero, src0, zero, src1,  \
+                         zero, src2, zero, src3,  \
+                         src0, src1, src2, src3 );\
+    data1 = __lasx_xvsub_h( src0, src2 );         \
+    data2 = __lasx_xvsub_h( src1, src3 );         \
+    p_pix1 += ( FENC_STRIDE << 1 );               \
+    p_pix2 += ( FDEC_STRIDE << 1 );
+
+    LOAD_PIX_DATA_2(tmp0, tmp1);
+    LOAD_PIX_DATA_2(tmp2, tmp3);
+    LOAD_PIX_DATA_2(tmp4, tmp5);
+    LOAD_PIX_DATA_2(tmp6, tmp7);
+
+#undef LOAD_PIX_DATA_2
+
+#define LASX_DCT8_1D                      \
+    s07 = __lasx_xvadd_h( tmp0, tmp7 );   \
+    s16 = __lasx_xvadd_h( tmp1, tmp6 );   \
+    s25 = __lasx_xvadd_h( tmp2, tmp5 );   \
+    s34 = __lasx_xvadd_h( tmp3, tmp4 );   \
+    a0 = __lasx_xvadd_h( s07, s34 );      \
+    a1 = __lasx_xvadd_h( s16, s25 );      \
+    a2 = __lasx_xvsub_h( s07, s34 );      \
+    a3 = __lasx_xvsub_h( s16, s25 );      \
+                                          \
+    d07 = __lasx_xvsub_h( tmp0, tmp7 );   \
+    d16 = __lasx_xvsub_h( tmp1, tmp6 );   \
+    d25 = __lasx_xvsub_h( tmp2, tmp5 );   \
+    d34 = __lasx_xvsub_h( tmp3, tmp4 );   \
+                                          \
+    temp = __lasx_xvsrai_h( d07, 1 );     \
+    temp = __lasx_xvadd_h( temp, d16 );   \
+    temp = __lasx_xvadd_h( temp, d25 );   \
+    a4 = __lasx_xvadd_h( temp, d07 );     \
+                                          \
+    temp = __lasx_xvsrai_h( d25, 1 );     \
+    temp1 = __lasx_xvsub_h( d07, d34 );   \
+    temp1 = __lasx_xvsub_h( temp1, d25 ); \
+    a5 = __lasx_xvsub_h( temp1, temp );   \
+                                          \
+    temp = __lasx_xvsrai_h( d16, 1 );     \
+    temp1 = __lasx_xvadd_h( d07, d34 );   \
+    temp1 = __lasx_xvsub_h( temp1, d16 ); \
+    a6 = __lasx_xvsub_h( temp1, temp );   \
+                                          \
+    temp = __lasx_xvsrai_h( d34, 1 );     \
+    temp1 = __lasx_xvsub_h( d16, d25 );   \
+    temp1 = __lasx_xvadd_h( temp1, d34 ); \
+    a7 = __lasx_xvadd_h( temp1, temp );   \
+                                          \
+    tmp0 = __lasx_xvadd_h( a0, a1 );      \
+    temp = __lasx_xvsrai_h( a7, 2 );      \
+    tmp1 = __lasx_xvadd_h( a4, temp );    \
+    temp = __lasx_xvsrai_h( a3, 1 );      \
+    tmp2 = __lasx_xvadd_h( a2, temp );    \
+    temp = __lasx_xvsrai_h( a6, 2 );      \
+    tmp3 = __lasx_xvadd_h( a5, temp );    \
+    tmp4 = __lasx_xvsub_h( a0, a1 );      \
+    temp = __lasx_xvsrai_h( a5, 2 );      \
+    tmp5 = __lasx_xvsub_h( a6, temp );    \
+    temp = __lasx_xvsrai_h( a2, 1 );      \
+    tmp6 = __lasx_xvsub_h( temp, a3 );    \
+    temp = __lasx_xvsrai_h( a4, 2 );      \
+    tmp7 = __lasx_xvsub_h( temp, a7 );
+
+    LASX_DCT8_1D;
+    LASX_TRANSPOSE8x8_H_128SV( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                               tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    LASX_DCT8_1D;
+
+#undef LASX_DCT8_1D
+
+    LASX_ST_D_2( tmp0, 0, 1, &pi_dct[0], 4 );
+    LASX_ST_D_2( tmp1, 0, 1, &pi_dct[8], 4 );
+    LASX_ST_D_2( tmp2, 0, 1, &pi_dct[16], 4 );
+    LASX_ST_D_2( tmp3, 0, 1, &pi_dct[24], 4 );
+    LASX_ST_D_2( tmp4, 0, 1, &pi_dct[32], 4 );
+    LASX_ST_D_2( tmp5, 0, 1, &pi_dct[40], 4 );
+    LASX_ST_D_2( tmp6, 0, 1, &pi_dct[48], 4 );
+    LASX_ST_D_2( tmp7, 0, 1, &pi_dct[56], 4 );
+}
+
+static void x264_sub8x8_dct8_ext_lasx( int16_t pi_dct1[64],
+                                       uint8_t *p_pix1, uint8_t *p_pix2,
+                                       int16_t pi_dct2[64] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m256i temp, temp1;
+    __m256i zero = {0};
+    __m256i s07, s16, s25, s34, d07, d16, d25, d34;
+    __m256i a0, a1, a2, a3, a4, a5, a6, a7;
+
+#define LOAD_PIX_DATA_2_EXT(data1, data2)         \
+    LASX_LD_2( p_pix1, FENC_STRIDE, src0, src1 ); \
+    LASX_LD_2( p_pix2, FDEC_STRIDE, src2, src3 ); \
+    src0 = __lasx_xvpermi_d( src0, 0x50 );        \
+    src1 = __lasx_xvpermi_d( src1, 0x50 );        \
+    src2 = __lasx_xvpermi_d( src2, 0x50 );        \
+    src3 = __lasx_xvpermi_d( src3, 0x50 );        \
+                                                  \
+    LASX_ILVL_B_4_128SV( zero, src0, zero, src1,  \
+                         zero, src2, zero, src3,  \
+                         src0, src1, src2, src3 );\
+    data1 = __lasx_xvsub_h( src0, src2 );         \
+    data2 = __lasx_xvsub_h( src1, src3 );         \
+    p_pix1 += ( FENC_STRIDE << 1 );               \
+    p_pix2 += ( FDEC_STRIDE << 1 );
+
+    LOAD_PIX_DATA_2_EXT(tmp0, tmp1);
+    LOAD_PIX_DATA_2_EXT(tmp2, tmp3);
+    LOAD_PIX_DATA_2_EXT(tmp4, tmp5);
+    LOAD_PIX_DATA_2_EXT(tmp6, tmp7);
+
+#undef LOAD_PIX_DATA_2_EXT
+
+#define LASX_DCT8_1D_EXT                  \
+    s07 = __lasx_xvadd_h( tmp0, tmp7 );   \
+    s16 = __lasx_xvadd_h( tmp1, tmp6 );   \
+    s25 = __lasx_xvadd_h( tmp2, tmp5 );   \
+    s34 = __lasx_xvadd_h( tmp3, tmp4 );   \
+    a0 = __lasx_xvadd_h( s07, s34 );      \
+    a1 = __lasx_xvadd_h( s16, s25 );      \
+    a2 = __lasx_xvsub_h( s07, s34 );      \
+    a3 = __lasx_xvsub_h( s16, s25 );      \
+                                          \
+    d07 = __lasx_xvsub_h( tmp0, tmp7 );   \
+    d16 = __lasx_xvsub_h( tmp1, tmp6 );   \
+    d25 = __lasx_xvsub_h( tmp2, tmp5 );   \
+    d34 = __lasx_xvsub_h( tmp3, tmp4 );   \
+                                          \
+    temp = __lasx_xvsrai_h( d07, 1 );     \
+    temp = __lasx_xvadd_h( temp, d16 );   \
+    temp = __lasx_xvadd_h( temp, d25 );   \
+    a4 = __lasx_xvadd_h( temp, d07 );     \
+                                          \
+    temp = __lasx_xvsrai_h( d25, 1 );     \
+    temp1 = __lasx_xvsub_h( d07, d34 );   \
+    temp1 = __lasx_xvsub_h( temp1, d25 ); \
+    a5 = __lasx_xvsub_h( temp1, temp );   \
+                                          \
+    temp = __lasx_xvsrai_h( d16, 1 );     \
+    temp1 = __lasx_xvadd_h( d07, d34 );   \
+    temp1 = __lasx_xvsub_h( temp1, d16 ); \
+    a6 = __lasx_xvsub_h( temp1, temp );   \
+                                          \
+    temp = __lasx_xvsrai_h( d34, 1 );     \
+    temp1 = __lasx_xvsub_h( d16, d25 );   \
+    temp1 = __lasx_xvadd_h( temp1, d34 ); \
+    a7 = __lasx_xvadd_h( temp1, temp );   \
+                                          \
+    tmp0 = __lasx_xvadd_h( a0, a1 );      \
+    temp = __lasx_xvsrai_h( a7, 2 );      \
+    tmp1 = __lasx_xvadd_h( a4, temp );    \
+    temp = __lasx_xvsrai_h( a3, 1 );      \
+    tmp2 = __lasx_xvadd_h( a2, temp );    \
+    temp = __lasx_xvsrai_h( a6, 2 );      \
+    tmp3 = __lasx_xvadd_h( a5, temp );    \
+    tmp4 = __lasx_xvsub_h( a0, a1 );      \
+    temp = __lasx_xvsrai_h( a5, 2 );      \
+    tmp5 = __lasx_xvsub_h( a6, temp );    \
+    temp = __lasx_xvsrai_h( a2, 1 );      \
+    tmp6 = __lasx_xvsub_h( temp, a3 );    \
+    temp = __lasx_xvsrai_h( a4, 2 );      \
+    tmp7 = __lasx_xvsub_h( temp, a7 );
+
+    LASX_DCT8_1D_EXT;
+    LASX_TRANSPOSE8x8_H_128SV( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                               tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    LASX_DCT8_1D_EXT;
+
+#undef LASX_DCT8_1D_EXT
+
+    LASX_ST_D_2( tmp0, 0, 1, &pi_dct1[0], 4 );
+    LASX_ST_D_2( tmp1, 0, 1, &pi_dct1[8], 4 );
+    LASX_ST_D_2( tmp2, 0, 1, &pi_dct1[16], 4 );
+    LASX_ST_D_2( tmp3, 0, 1, &pi_dct1[24], 4 );
+    LASX_ST_D_2( tmp4, 0, 1, &pi_dct1[32], 4 );
+    LASX_ST_D_2( tmp5, 0, 1, &pi_dct1[40], 4 );
+    LASX_ST_D_2( tmp6, 0, 1, &pi_dct1[48], 4 );
+    LASX_ST_D_2( tmp7, 0, 1, &pi_dct1[56], 4 );
+
+    LASX_ST_D_2( tmp0, 2, 3, &pi_dct2[0], 4 );
+    LASX_ST_D_2( tmp1, 2, 3, &pi_dct2[8], 4 );
+    LASX_ST_D_2( tmp2, 2, 3, &pi_dct2[16], 4 );
+    LASX_ST_D_2( tmp3, 2, 3, &pi_dct2[24], 4 );
+    LASX_ST_D_2( tmp4, 2, 3, &pi_dct2[32], 4 );
+    LASX_ST_D_2( tmp5, 2, 3, &pi_dct2[40], 4 );
+    LASX_ST_D_2( tmp6, 2, 3, &pi_dct2[48], 4 );
+    LASX_ST_D_2( tmp7, 2, 3, &pi_dct2[56], 4 );
+}
+
+void x264_sub16x16_dct8_lasx( int16_t pi_dct[4][64], uint8_t *p_pix1,
+                              uint8_t *p_pix2 )
+{
+    x264_sub8x8_dct8_ext_lasx( pi_dct[0], &p_pix1[0], &p_pix2[0],
+                               pi_dct[1] );
+    x264_sub8x8_dct8_ext_lasx( pi_dct[2], &p_pix1[8 * FENC_STRIDE + 0],
+                               &p_pix2[8*FDEC_STRIDE+0], pi_dct[3] );
+}
+
+#endif
diff --git a/common/loongarch/dct.h b/common/loongarch/dct.h
new file mode 100644
index 00000000..9b1d1003
--- /dev/null
+++ b/common/loongarch/dct.h
@@ -0,0 +1,53 @@
+/*****************************************************************************
+ * dct.h: loongarch transform and zigzag
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_DCT_H
+#define X264_LOONGARCH_DCT_H
+
+#define x264_sub4x4_dct_lasx x264_template(sub4x4_dct_lasx)
+void x264_sub4x4_dct_lasx( int16_t p_dst[16], uint8_t *p_src, uint8_t *p_ref );
+#define x264_sub8x8_dct_lasx x264_template(sub8x8_dct_lasx)
+void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src,
+                           uint8_t *p_ref );
+#define x264_sub16x16_dct_lasx x264_template(sub16x16_dct_lasx)
+void x264_sub16x16_dct_lasx( int16_t p_dst[16][16], uint8_t *p_src,
+                             uint8_t *p_ref );
+
+#define x264_sub8x8_dct8_lasx x264_template(sub8x8_dct8_lasx)
+void x264_sub8x8_dct8_lasx( int16_t pi_dct[64], uint8_t *p_pix1,
+                            uint8_t *p_pix2 );
+#define x264_sub16x16_dct8_lasx x264_template(sub16x16_dct8_lasx)
+void x264_sub16x16_dct8_lasx( int16_t pi_dct[4][64], uint8_t *p_pix1,
+                              uint8_t *p_pix2 );
+
+#define x264_add4x4_idct_lasx x264_template(add4x4_idct_lasx)
+void x264_add4x4_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16] );
+#define x264_add8x8_idct_lasx x264_template(add8x8_idct_lasx)
+void x264_add8x8_idct_lasx( uint8_t *p_dst, int16_t pi_dct[4][16] );
+#define x264_add16x16_idct_lasx x264_template(add16x16_idct_lasx)
+void x264_add16x16_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16][16] );
+
+#endif
diff --git a/common/loongarch/deblock-c.c b/common/loongarch/deblock-c.c
new file mode 100644
index 00000000..1338cd24
--- /dev/null
+++ b/common/loongarch/deblock-c.c
@@ -0,0 +1,890 @@
+/*****************************************************************************
+ * deblock-c.c: loongarch deblocking
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "generic_macros_lasx.h"
+#include "deblock.h"
+
+#if !HIGH_BIT_DEPTH
+
+#define LASX_LPF_P1_OR_Q1( p0_or_q0_org_in, q0_or_p0_org_in,         \
+                           p1_or_q1_org_in, p2_or_q2_org_in,         \
+                           negate_tc_in, tc_in, p1_or_q1_out )       \
+{                                                                    \
+    __m256i clip0, temp;                                             \
+                                                                     \
+    clip0 = __lasx_xvavgr_hu( p0_or_q0_org_in, q0_or_p0_org_in );    \
+    temp = __lasx_xvslli_h( p1_or_q1_org_in, 1 );                    \
+    clip0 = __lasx_xvsub_h( clip0, temp );                           \
+    clip0 = __lasx_xvavg_h( p2_or_q2_org_in, clip0 );                \
+    LASX_CLIP_H( clip0, negate_tc_in, tc_in );                       \
+    p1_or_q1_out = __lasx_xvadd_h( p1_or_q1_org_in, clip0 );         \
+}
+
+#define LASX_LPF_P0Q0( q0_or_p0_org_in, p0_or_q0_org_in,             \
+                       p1_or_q1_org_in, q1_or_p1_org_in,             \
+                       negate_threshold_in, threshold_in,            \
+                       p0_or_q0_out, q0_or_p0_out )                  \
+{                                                                    \
+    __m256i q0_sub_p0, p1_sub_q1, delta;                             \
+                                                                     \
+    q0_sub_p0 = __lasx_xvsub_h( q0_or_p0_org_in, p0_or_q0_org_in );  \
+    p1_sub_q1 = __lasx_xvsub_h( p1_or_q1_org_in, q1_or_p1_org_in );  \
+    q0_sub_p0 = __lasx_xvslli_h( q0_sub_p0, 2 );                     \
+    p1_sub_q1 = __lasx_xvaddi_hu( p1_sub_q1, 4 );                    \
+    delta = __lasx_xvadd_h( q0_sub_p0, p1_sub_q1 );                  \
+    delta = __lasx_xvsrai_h( delta, 3 );                             \
+                                                                     \
+    LASX_CLIP_H( delta, negate_threshold_in, threshold_in );         \
+                                                                     \
+    p0_or_q0_out = __lasx_xvadd_h( p0_or_q0_org_in, delta );         \
+    q0_or_p0_out = __lasx_xvsub_h( q0_or_p0_org_in, delta );         \
+                                                                     \
+    LASX_CLIP_H_0_255_2( p0_or_q0_out, q0_or_p0_out,                 \
+                         p0_or_q0_out, q0_or_p0_out );               \
+}
+
+void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
+                               int32_t i_alpha, int32_t i_beta, int8_t *p_tc0 )
+{
+    uint8_t *p_src;
+    __m256i beta, bs, tc;
+    __m256i zero = __lasx_xvldi( 0 );
+
+    tc = LASX_LD( p_tc0 );
+    tc = __lasx_xvilvl_b( tc, tc );
+    tc = __lasx_xvilvl_h( tc, tc );
+
+    beta = __lasx_xvsle_w( zero, tc );
+    bs = __lasx_xvandi_b( beta, 0x01 );
+
+    if( !__lasx_xbz_v( bs ) )
+    {
+        __m256i is_less_than, is_less_than_beta, is_bs_greater_than0;
+        __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+        __m256i p3_org, p2_org, p1_org, p0_org, q0_org, q1_org, q2_org, q3_org;
+        __m256i p2_org_l, p1_org_l, p0_org_l, q0_org_l, q1_org_l, q2_org_l;
+        __m256i p2_org_h, p1_org_h, p0_org_h, q0_org_h, q1_org_h, q2_org_h;
+        __m256i tc_l, tc_h;
+        __m256i mask_l = { 0, 2, 0, 2 };
+        __m256i mask_h = { 3, 0, 3, 0 };
+
+        is_bs_greater_than0 = __lasx_xvslt_bu( zero, bs );
+
+        {
+            p_src = p_pix - 4;
+
+            LASX_LD_8( p_src, i_stride,
+                       src0, src1, src2, src3, src4, src5, src6, src7 );
+            p_src += ( i_stride << 3 );
+            LASX_LD_8( p_src, i_stride,
+                       p2_org_l, p1_org_l, p0_org_l, q0_org_l,
+                       q1_org_l, q2_org_l, p2_org_h, p1_org_h );
+
+            LASX_TRANSPOSE16x8_B_128SV( src0, src1, src2, src3,
+                                        src4, src5, src6, src7,
+                                        p2_org_l, p1_org_l, p0_org_l, q0_org_l,
+                                        q1_org_l, q2_org_l, p2_org_h, p1_org_h,
+                                        p3_org, p2_org, p1_org, p0_org,
+                                        q0_org, q1_org, q2_org, q3_org );
+        }
+        {
+            src0 = __lasx_xvabsd_bu( p0_org, q0_org );
+            src1 = __lasx_xvabsd_bu( p1_org, p0_org );
+            src2 = __lasx_xvabsd_bu( q1_org, q0_org );
+
+            src3 = __lasx_xvreplgr2vr_b( i_alpha );
+            beta = __lasx_xvreplgr2vr_b( i_beta );
+
+            src4 = __lasx_xvslt_bu( src0, src3 );
+            is_less_than_beta = __lasx_xvslt_bu( src1, beta );
+            is_less_than = __lasx_xvand_v( is_less_than_beta, src4 );
+            is_less_than_beta = __lasx_xvslt_bu( src2, beta );
+            is_less_than = __lasx_xvand_v( is_less_than_beta,
+                                           is_less_than );
+            is_less_than = __lasx_xvand_v( is_less_than,
+                                           is_bs_greater_than0 );
+        }
+        if( !__lasx_xbz_v( is_less_than ) )
+        {
+            __m256i negate_tc, sign_negate_tc;
+            __m256i negate_tc_l, i16_negatetc_h;
+
+            negate_tc = __lasx_xvsub_b( zero, tc );
+            sign_negate_tc = __lasx_xvslti_b( negate_tc, 0 );
+
+            LASX_ILVLH_B_128SV( sign_negate_tc, negate_tc, i16_negatetc_h,
+                                negate_tc_l );
+
+            LASX_ILVLH_B_128SV( zero, tc, tc_h, tc_l );
+            LASX_ILVLH_B_128SV( zero, p1_org, p1_org_h, p1_org_l );
+            LASX_ILVLH_B_128SV( zero, p0_org, p0_org_h, p0_org_l );
+            LASX_ILVLH_B_128SV( zero, q0_org, q0_org_h, q0_org_l );
+
+            {
+                __m256i p2_asub_p0;
+                __m256i is_less_than_beta_l, is_less_than_beta_h;
+
+                p2_asub_p0 = __lasx_xvabsd_bu( p2_org, p0_org );
+                is_less_than_beta = __lasx_xvslt_bu( p2_asub_p0, beta );
+                is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
+                                                    is_less_than );
+
+                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_l ) )
+                {
+                    p2_org_l = __lasx_xvilvl_b( zero, p2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, p1_org_l, p2_org_l,
+                                       negate_tc_l, tc_l, src2 );
+                }
+
+                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_h ) )
+                {
+                    p2_org_h = __lasx_xvilvh_b( zero, p2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, p1_org_h, p2_org_h,
+                                       i16_negatetc_h, tc_h, src6 );
+                }
+            }
+
+            if( !__lasx_xbz_v( is_less_than_beta ) )
+            {
+                src6 = __lasx_xvpickev_b( src6, src2 );
+                LASX_BMNZ( p1_org, src6, is_less_than_beta, p1_org );
+
+                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
+                tc = __lasx_xvadd_b( tc, is_less_than_beta );
+            }
+
+            {
+                __m256i u8_q2asub_q0;
+                __m256i is_less_than_beta_h, is_less_than_beta_l;
+
+                u8_q2asub_q0 = __lasx_xvabsd_bu( q2_org, q0_org );
+                is_less_than_beta = __lasx_xvslt_bu( u8_q2asub_q0, beta );
+                is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
+                                                    is_less_than );
+
+                q1_org_l = __lasx_xvilvl_b( zero, q1_org );
+
+                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_l ) )
+                {
+                    q2_org_l = __lasx_xvilvl_b( zero, q2_org );
+                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, q1_org_l, q2_org_l,
+                                       negate_tc_l, tc_l, src3 );
+                }
+
+                q1_org_h = __lasx_xvilvh_b( zero, q1_org );
+
+                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_h ) )
+                {
+                    q2_org_h = __lasx_xvilvh_b( zero, q2_org );
+                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, q1_org_h, q2_org_h,
+                                       i16_negatetc_h, tc_h, src7 );
+                }
+            }
+
+            if( !__lasx_xbz_v( is_less_than_beta ) )
+            {
+                src7 = __lasx_xvpickev_b( src7, src3 );
+                LASX_BMNZ( q1_org, src7, is_less_than_beta, q1_org );
+
+                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
+                tc = __lasx_xvadd_b( tc, is_less_than_beta );
+            }
+
+            {
+                __m256i threshold_l, negate_thresh_l;
+                __m256i threshold_h, negate_thresh_h;
+                __m256i negate_thresh, sign_negate_thresh;
+
+                negate_thresh = __lasx_xvsub_b( zero, tc );
+                sign_negate_thresh = __lasx_xvslti_b( negate_thresh, 0 );
+
+                LASX_ILVL_B_2_128SV( zero, tc, sign_negate_thresh, negate_thresh,
+                                     threshold_l, negate_thresh_l );
+
+                LASX_LPF_P0Q0( q0_org_l, p0_org_l, p1_org_l, q1_org_l,
+                               negate_thresh_l, threshold_l, src0, src1 );
+
+                threshold_h = __lasx_xvilvh_b( zero, tc );
+                negate_thresh_h = __lasx_xvilvh_b( sign_negate_thresh,
+                                                   negate_thresh );
+
+                LASX_LPF_P0Q0( q0_org_h, p0_org_h, p1_org_h, q1_org_h,
+                               negate_thresh_h, threshold_h, src4, src5 );
+            }
+
+            src4 = __lasx_xvpickev_b( src4, src0 );
+            src5 = __lasx_xvpickev_b( src5, src1 );
+
+            LASX_BMNZ( p0_org, src4, is_less_than, p0_org );
+            LASX_BMNZ( q0_org, src5, is_less_than, q0_org );
+        }
+        {
+            p_src = p_pix - 3;
+
+            LASX_ILVLH_B_128SV( p1_org, p2_org, src2, src0 );
+            LASX_ILVLH_B_128SV( q0_org, p0_org, src3, src1 );
+            LASX_ILVLH_B_128SV( q2_org, q1_org, src5, src4 );
+
+            LASX_ILVLH_H_128SV( src1, src0, src7, src6 );
+            LASX_ILVLH_H_128SV( src3, src2, src1, src0 );
+
+            LASX_ST_W( src6, 0, p_src );
+            LASX_ST_H( src4, 0, ( p_src + 4 ) );
+            p_src += i_stride;
+            LASX_ST_W( src6, 1, p_src );
+            LASX_ST_H( src4, 1, ( p_src + 4 ) );
+
+            p_src += i_stride;
+            LASX_ST_W( src6, 2, p_src );
+            LASX_ST_H( src4, 2, ( p_src + 4 ) );
+            p_src += i_stride;
+            LASX_ST_W( src6, 3, p_src );
+            LASX_ST_H( src4, 3, ( p_src + 4 ) );
+
+            p_src += i_stride;
+            LASX_ST_W( src7, 0, p_src );
+            LASX_ST_H( src4, 4, ( p_src + 4 ) );
+            p_src += i_stride;
+            LASX_ST_W( src7, 1, p_src );
+            LASX_ST_H( src4, 5, ( p_src + 4 ) );
+
+            p_src += i_stride;
+            LASX_ST_W( src7, 2, p_src );
+            LASX_ST_H( src4, 6, ( p_src + 4 ) );
+            p_src += i_stride;
+            LASX_ST_W( src7, 3, p_src );
+            LASX_ST_H( src4, 7, ( p_src + 4 ) );
+
+            p_src += i_stride;
+            LASX_ST_W( src0, 0, p_src );
+            LASX_ST_H( src5, 0, ( p_src + 4 ) );
+            p_src += i_stride;
+            LASX_ST_W( src0, 1, p_src );
+            LASX_ST_H( src5, 1, ( p_src + 4 ) );
+
+            p_src += i_stride;
+            LASX_ST_W( src0, 2, p_src );
+            LASX_ST_H( src5, 2, ( p_src + 4 ) );
+            p_src += i_stride;
+            LASX_ST_W( src0, 3, p_src );
+            LASX_ST_H( src5, 3, ( p_src + 4 ) );
+
+            p_src += i_stride;
+            LASX_ST_W( src1, 0, p_src );
+            LASX_ST_H( src5, 4, ( p_src + 4 ) );
+            p_src += i_stride;
+            LASX_ST_W( src1, 1, p_src );
+            LASX_ST_H( src5, 5, ( p_src + 4 ) );
+
+            p_src += i_stride;
+            LASX_ST_W( src1, 2, p_src );
+            LASX_ST_H( src5, 6, ( p_src + 4 ) );
+            p_src += i_stride;
+            LASX_ST_W( src1, 3, p_src );
+            LASX_ST_H( src5, 7, ( p_src + 4 ) );
+        }
+    }
+}
+
+void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
+                               int32_t i_alpha, int32_t i_beta, int8_t *p_tc0 )
+{
+    __m256i bs, tc, beta;
+    __m256i zero = __lasx_xvldi( 0 );
+
+    tc = LASX_LD( p_tc0 );
+    tc = __lasx_xvilvl_b( tc, tc );
+    tc = __lasx_xvilvl_h( tc, tc );
+
+    beta = __lasx_xvsle_w( zero, tc );
+    bs = __lasx_xvandi_b( beta, 0x01 );
+
+    if( !__lasx_xbz_v( bs ) )
+    {
+        __m256i p2_asub_p0, u8_q2asub_q0;
+        __m256i alpha, is_less_than, is_less_than_beta;
+        __m256i src0, src1, src2, src3, src6, src4, src5, src7;
+        __m256i p2_org, p1_org, p0_org, q0_org, q1_org, q2_org;
+        __m256i p2_org_l, p1_org_l, p0_org_l, q0_org_l, q1_org_l, q2_org_l;
+        __m256i p2_org_h, p1_org_h, p0_org_h, q0_org_h, q1_org_h, q2_org_h;
+        __m256i mask_l = { 0, 2, 0, 2 };
+        __m256i mask_h = { 3, 0, 3, 0 };
+
+        alpha = __lasx_xvreplgr2vr_b( i_alpha );
+        beta = __lasx_xvreplgr2vr_b( i_beta );
+
+        p2_org = LASX_LD( p_pix - ( 3 * i_stride ) );
+        LASX_LD_4( p_pix - ( i_stride << 1 ), i_stride,
+                   p1_org, p0_org, q0_org, q1_org );
+        {
+            src5 = __lasx_xvslt_bu( zero, bs );
+            src0 = __lasx_xvabsd_bu( p0_org, q0_org );
+            src1 = __lasx_xvabsd_bu( p1_org, p0_org );
+            src2 = __lasx_xvabsd_bu( q1_org, q0_org );
+
+            src4 = __lasx_xvslt_bu( src0, alpha );
+            is_less_than_beta = __lasx_xvslt_bu( src1, beta );
+            is_less_than = __lasx_xvand_v( is_less_than_beta,
+                                           src4 );
+            is_less_than_beta = __lasx_xvslt_bu( src2, beta );
+            is_less_than = __lasx_xvand_v( is_less_than_beta,
+                                           is_less_than );
+            is_less_than = __lasx_xvand_v( is_less_than, src5 );
+        }
+
+        if( !__lasx_xbz_v( is_less_than ) )
+        {
+            __m256i sign_negate_tc, negate_tc;
+            __m256i negate_tc_l, i16_negatetc_h, tc_h, tc_l;
+
+            q2_org = LASX_LD( p_pix + ( i_stride << 1 ) );
+            negate_tc = __lasx_xvsub_b( zero, tc );
+            sign_negate_tc = __lasx_xvslti_b( negate_tc, 0 );
+
+            LASX_ILVLH_B_128SV( sign_negate_tc, negate_tc,
+                                i16_negatetc_h, negate_tc_l );
+
+            LASX_ILVLH_B_128SV( zero, tc, tc_h, tc_l );
+            LASX_ILVLH_B_128SV( zero, p1_org, p1_org_h, p1_org_l );
+            LASX_ILVLH_B_128SV( zero, p0_org, p0_org_h, p0_org_l );
+            LASX_ILVLH_B_128SV( zero, q0_org, q0_org_h, q0_org_l );
+
+            p2_asub_p0 = __lasx_xvabsd_bu( p2_org, p0_org );
+            is_less_than_beta = __lasx_xvslt_bu( p2_asub_p0, beta );
+            is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
+                                                is_less_than );
+            {
+                __m256i is_less_than_beta_l, is_less_than_beta_h;
+
+                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_l ) )
+                {
+                    p2_org_l = __lasx_xvilvl_b( zero, p2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, p1_org_l, p2_org_l,
+                                       negate_tc_l, tc_l, src2 );
+                }
+
+                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_h ) )
+                {
+                    p2_org_h = __lasx_xvilvh_b( zero, p2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, p1_org_h, p2_org_h,
+                                       i16_negatetc_h, tc_h, src6 );
+                }
+            }
+            if( !__lasx_xbz_v( is_less_than_beta ) )
+            {
+                src6 = __lasx_xvpickev_b( src6, src2 );
+                LASX_BMNZ( p1_org, src6, is_less_than_beta, p1_org );
+                LASX_ST_Q( p1_org, 0, p_pix - ( i_stride << 1 ) );
+
+                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
+                tc = __lasx_xvadd_b( tc, is_less_than_beta );
+            }
+
+            u8_q2asub_q0 = __lasx_xvabsd_bu( q2_org, q0_org );
+            is_less_than_beta = __lasx_xvslt_bu( u8_q2asub_q0, beta );
+            is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
+                                                is_less_than );
+
+            {
+                __m256i is_less_than_beta_l, is_less_than_beta_h;
+                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
+                                                       zero );
+
+                q1_org_l = __lasx_xvilvl_b( zero, q1_org );
+                if( !__lasx_xbz_v( is_less_than_beta_l ) )
+                {
+                    q2_org_l = __lasx_xvilvl_b( zero, q2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, q1_org_l, q2_org_l,
+                                       negate_tc_l, tc_l, src3 );
+                }
+                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
+                                                       zero );
+
+                q1_org_h = __lasx_xvilvh_b( zero, q1_org );
+                if( !__lasx_xbz_v( is_less_than_beta_h ) )
+                {
+                    q2_org_h = __lasx_xvilvh_b( zero, q2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, q1_org_h, q2_org_h,
+                                       i16_negatetc_h, tc_h, src7 );
+                }
+            }
+            if( !__lasx_xbz_v( is_less_than_beta ) )
+            {
+                src7 = __lasx_xvpickev_b( src7, src3 );
+                LASX_BMNZ( q1_org, src7, is_less_than_beta, q1_org );
+                LASX_ST_Q( q1_org, 0, p_pix + i_stride );
+
+                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
+                tc = __lasx_xvadd_b( tc, is_less_than_beta );
+            }
+            {
+                __m256i negate_thresh, sign_negate_thresh;
+                __m256i threshold_l, threshold_h;
+                __m256i negate_thresh_h, negate_thresh_l;
+
+                negate_thresh = __lasx_xvsub_b( zero, tc );
+                sign_negate_thresh = __lasx_xvslti_b( negate_thresh, 0 );
+
+                LASX_ILVL_B_2_128SV( zero, tc, sign_negate_thresh, negate_thresh,
+                                     threshold_l, negate_thresh_l );
+                LASX_LPF_P0Q0( q0_org_l, p0_org_l, p1_org_l, q1_org_l,
+                               negate_thresh_l, threshold_l, src0, src1 );
+
+                threshold_h = __lasx_xvilvh_b( zero, tc );
+                negate_thresh_h = __lasx_xvilvh_b( sign_negate_thresh,
+                                                   negate_thresh );
+                LASX_LPF_P0Q0( q0_org_h, p0_org_h, p1_org_h, q1_org_h,
+                               negate_thresh_h, threshold_h, src4, src5 );
+            }
+
+            src4 = __lasx_xvpickev_b( src4, src0 );
+            src5 = __lasx_xvpickev_b( src5, src1 );
+
+            LASX_BMNZ( p0_org, src4, is_less_than, p0_org );
+            LASX_BMNZ( q0_org, src5, is_less_than, q0_org );
+
+            LASX_ST_Q( p0_org, 0, ( p_pix - i_stride ) );
+            LASX_ST_Q( q0_org, 0, p_pix );
+        }
+    }
+}
+
+static void avc_deblock_strength_lasx( uint8_t *nnz,
+                                       int8_t pi_lef[2][X264_SCAN8_LUMA_SIZE],
+                                       int16_t pi_mv[2][X264_SCAN8_LUMA_SIZE][2],
+                                       uint8_t pu_bs[2][8][4],
+                                       int32_t i_mvy_himit )
+{
+    __m256i nnz0, nnz1, nnz2, nnz3, nnz4;
+    __m256i nnz_mask, ref_mask, mask, one, two, dst = { 0 };
+    __m256i ref0, ref1, ref2, ref3, ref4;
+    __m256i temp_vec0, temp_vec1, temp_vec2;
+    __m256i mv0, mv1, mv2, mv3, mv4, mv5, mv6, mv7, mv8, mv9, mv_a, mv_b;
+    __m256i four, mvy_himit_vec, sub0, sub1;
+    int8_t* p_lef = pi_lef[0];
+    int16_t* p_mv = pi_mv[0][0];
+
+    nnz0 = LASX_LD( nnz + 4 );
+    nnz2 = LASX_LD( nnz + 20 );
+    nnz4 = LASX_LD( nnz + 36 );
+
+    ref0 = LASX_LD( p_lef + 4 );
+    ref2 = LASX_LD( p_lef + 20 );
+    ref4 = LASX_LD( p_lef + 36 );
+
+    mv0 = LASX_LD( p_mv + 8 );
+    mv1 = LASX_LD( p_mv + 24 );
+    mv2 = LASX_LD( p_mv + 40 );
+    mv3 = LASX_LD( p_mv + 56 );
+    mv4 = LASX_LD( p_mv + 72 );
+
+    mvy_himit_vec = __lasx_xvreplgr2vr_h( i_mvy_himit );
+    four = __lasx_xvreplgr2vr_h( 4 );
+    mask = __lasx_xvldi( 0 );
+    one = __lasx_xvldi( 1 );
+    two = __lasx_xvldi( 2 );
+
+    mv5 = __lasx_xvpickod_h( mv0, mv0 );
+    mv6 = __lasx_xvpickod_h( mv1, mv1 );
+    mv_a = __lasx_xvpickev_h( mv0, mv0 );
+    mv_b = __lasx_xvpickev_h( mv1, mv1 );
+    nnz1 = __lasx_xvrepl128vei_w( nnz0, 2 );
+    ref1 = __lasx_xvrepl128vei_w( ref0, 2 );
+    nnz_mask = __lasx_xvor_v( nnz0, nnz1 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    LASX_BMNZ( two, mask, nnz_mask, two );
+
+    ref_mask = __lasx_xvseq_b( ref0, ref1 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    LASX_BMNZ( dst, one, ref_mask, dst );
+    LASX_BMNZ( two, dst, nnz_mask, dst );
+
+    LASX_ST_W( dst, 0, pu_bs[1][0] );
+
+    dst = __lasx_xvldi( 0 );
+    two = __lasx_xvldi( 2 );
+
+    mv5 = __lasx_xvpickod_h( mv1, mv1 );
+    mv6 = __lasx_xvpickod_h( mv2, mv2 );
+    mv_a = __lasx_xvpickev_h( mv1, mv1 );
+    mv_b = __lasx_xvpickev_h( mv2, mv2 );
+
+    nnz_mask = __lasx_xvor_v( nnz2, nnz1 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    LASX_BMNZ( two, mask, nnz_mask, two );
+
+    ref_mask = __lasx_xvseq_b( ref1, ref2 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    LASX_BMNZ( dst, one, ref_mask, dst );
+    LASX_BMNZ( two, dst, nnz_mask, dst );
+
+    LASX_ST_W( dst, 0, pu_bs[1][1] );
+
+    dst = __lasx_xvldi( 0 );
+    two = __lasx_xvldi( 2 );
+
+    mv5 = __lasx_xvpickod_h( mv2, mv2 );
+    mv6 = __lasx_xvpickod_h( mv3, mv3 );
+    mv_a = __lasx_xvpickev_h( mv2, mv2 );
+    mv_b = __lasx_xvpickev_h( mv3, mv3 );
+
+    nnz3 = __lasx_xvrepl128vei_w( nnz2, 2 );
+    ref3 = __lasx_xvrepl128vei_w( ref2, 2 );
+
+    nnz_mask = __lasx_xvor_v( nnz3, nnz2 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    LASX_BMNZ( two, mask, nnz_mask, two );
+
+    ref_mask = __lasx_xvseq_b( ref2, ref3 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    LASX_BMNZ( dst, one, ref_mask, dst );
+    LASX_BMNZ( two, dst, nnz_mask, dst );
+
+    LASX_ST_W( dst, 0, pu_bs[1][2] );
+
+    dst = __lasx_xvldi( 0 );
+    two = __lasx_xvldi( 2 );
+
+    mv5 = __lasx_xvpickod_h( mv3, mv3 );
+    mv6 = __lasx_xvpickod_h( mv4, mv4 );
+    mv_a = __lasx_xvpickev_h( mv3, mv3 );
+    mv_b = __lasx_xvpickev_h( mv4, mv4 );
+
+    nnz_mask = __lasx_xvor_v( nnz4, nnz3 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    LASX_BMNZ( two, mask, nnz_mask, two );
+
+    ref_mask = __lasx_xvseq_b( ref3, ref4 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    LASX_BMNZ( dst, one, ref_mask, dst );
+    LASX_BMNZ( two, dst, nnz_mask, dst );
+
+    LASX_ST_W( dst, 0, pu_bs[1][3] );
+
+    nnz0 = LASX_LD( nnz + 8 );
+    nnz2 = LASX_LD( nnz + 24 );
+
+    ref0 = LASX_LD( p_lef + 8 );
+    ref2 = LASX_LD( p_lef + 24 );
+
+    mv0 = LASX_LD( p_mv + 16 );
+    mv1 = LASX_LD( p_mv + 24 );
+    mv2 = LASX_LD( p_mv + 32 );
+    mv3 = LASX_LD( p_mv + 40 );
+    mv4 = LASX_LD( p_mv + 48 );
+    mv7 = LASX_LD( p_mv + 56 );
+    mv8 = LASX_LD( p_mv + 64 );
+    mv9 = LASX_LD( p_mv + 72 );
+
+    nnz1 = __lasx_xvrepl128vei_d( nnz0, 1 );
+    nnz3 = __lasx_xvrepl128vei_d( nnz2, 1 );
+
+    LASX_ILVL_B_2_128SV( nnz2, nnz0, nnz3, nnz1, temp_vec0, temp_vec1 );
+
+    LASX_ILVLH_B_128SV( temp_vec1, temp_vec0, nnz1, temp_vec2 );
+
+    nnz0 = __lasx_xvrepl128vei_w( temp_vec2, 3 );
+    nnz2 = __lasx_xvrepl128vei_w( nnz1, 1 );
+    nnz3 = __lasx_xvrepl128vei_w( nnz1, 2 );
+    nnz4 = __lasx_xvrepl128vei_w( nnz1, 3 );
+
+    ref1 = __lasx_xvrepl128vei_d( ref0, 1 );
+    ref3 = __lasx_xvrepl128vei_d( ref2, 1 );
+
+    LASX_ILVL_B_2_128SV( ref2, ref0, ref3, ref1, temp_vec0, temp_vec1 );
+
+    LASX_ILVLH_B_128SV( temp_vec1, temp_vec0, ref1, temp_vec2 );
+
+    ref0 = __lasx_xvrepl128vei_w( temp_vec2, 3 );
+
+    ref2 = __lasx_xvrepl128vei_w( ref1, 1 );
+    ref3 = __lasx_xvrepl128vei_w( ref1, 2 );
+    ref4 = __lasx_xvrepl128vei_w( ref1, 3 );
+
+    LASX_TRANSPOSE8X4_H_128SV( mv0, mv2, mv4, mv8, mv5, mv5, mv5, mv0 );
+    LASX_TRANSPOSE8X4_H_128SV( mv1, mv3, mv7, mv9, mv1, mv2, mv3, mv4 );
+
+    mvy_himit_vec = __lasx_xvreplgr2vr_h( i_mvy_himit );
+    four = __lasx_xvreplgr2vr_h( 4 );
+    mask = __lasx_xvldi( 0 );
+    one = __lasx_xvldi( 1 );
+    two = __lasx_xvldi( 2 );
+    dst = __lasx_xvldi( 0 );
+
+    mv5 = __lasx_xvrepl128vei_d( mv0, 1 );
+    mv6 = __lasx_xvrepl128vei_d( mv1, 1 );
+
+    nnz_mask = __lasx_xvor_v( nnz0, nnz1 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    LASX_BMNZ( two, mask, nnz_mask, two );
+
+    ref_mask = __lasx_xvseq_b( ref0, ref1 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv1, mv0 );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    LASX_BMNZ( dst, one, ref_mask, dst );
+    LASX_BMNZ( two, dst, nnz_mask, dst );
+
+    LASX_ST_W( dst, 0, pu_bs[0][0] );
+
+    two = __lasx_xvldi( 2 );
+    dst = __lasx_xvldi( 0 );
+
+    mv5 = __lasx_xvrepl128vei_d( mv1, 1 );
+    mv6 = __lasx_xvrepl128vei_d( mv2, 1 );
+
+    nnz_mask = __lasx_xvor_v( nnz1, nnz2 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    LASX_BMNZ( two, mask, nnz_mask, two );
+
+    ref_mask = __lasx_xvseq_b( ref1, ref2 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv2, mv1 );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    LASX_BMNZ( dst, one, ref_mask, dst );
+    LASX_BMNZ( two, dst, nnz_mask, dst );
+
+    LASX_ST_W( dst, 0, pu_bs[0][1] );
+
+    two = __lasx_xvldi( 2 );
+    dst = __lasx_xvldi( 0 );
+
+    mv5 = __lasx_xvrepl128vei_d( mv2, 1 );
+    mv6 = __lasx_xvrepl128vei_d( mv3, 1 );
+
+    nnz_mask = __lasx_xvor_v( nnz2, nnz3 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    LASX_BMNZ( two, mask, nnz_mask, two );
+
+    ref_mask = __lasx_xvseq_b( ref2, ref3 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv3, mv2 );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    LASX_BMNZ( dst, one, ref_mask, dst );
+    LASX_BMNZ( two, dst, nnz_mask, dst );
+
+    LASX_ST_W( dst, 0, pu_bs[0][2] );
+
+    two = __lasx_xvldi( 2 );
+    dst = __lasx_xvldi( 0 );
+
+    mv5 = __lasx_xvrepl128vei_d( mv3, 1 );
+    mv6 = __lasx_xvrepl128vei_d( mv4, 1 );
+
+    nnz_mask = __lasx_xvor_v( nnz3, nnz4 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    LASX_BMNZ( two, mask, nnz_mask, two );
+
+    ref_mask = __lasx_xvseq_b( ref3, ref4 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv4, mv3 );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    LASX_BMNZ( dst, one, ref_mask, dst );
+    LASX_BMNZ( two, dst, nnz_mask, dst );
+
+    LASX_ST_W( dst, 0, pu_bs[0][3] );
+}
+
+void x264_deblock_strength_lasx( uint8_t u_nnz[X264_SCAN8_SIZE],
+                                 int8_t pi_lef[2][X264_SCAN8_LUMA_SIZE],
+                                 int16_t pi_mv[2][X264_SCAN8_LUMA_SIZE][2],
+                                 uint8_t pu_bs[2][8][4], int32_t i_mvy_himit,
+                                 int32_t i_bframe )
+{
+    int32_t i_edge, i, loc, locn;
+    int8_t* p_lef0 = pi_lef[0];
+    int8_t* p_lef1 = pi_lef[1];
+    uint8_t (*p_bs0)[4] = pu_bs[0];
+    uint8_t (*p_bs1)[4] = pu_bs[1];
+    int16_t (*p_mv0)[2] = pi_mv[0];
+    int16_t (*p_mv1)[2] = pi_mv[1];
+
+    if( i_bframe )
+    {
+        for( i_edge = 0; i_edge < 4; i_edge++ )
+        {
+            loc = X264_SCAN8_0 + i_edge;
+            for( i = 0; i < 4; i++, loc += 8 )
+            {
+                locn = loc - 1;
+                if( u_nnz[loc] || u_nnz[locn] )
+                {
+                    p_bs0[i_edge][i] = 2;
+                }
+                else if( p_lef0[loc] != p_lef0[locn] ||
+                         abs( p_mv0[loc][0] - p_mv0[locn][0] ) >= 4 ||
+                         abs( p_mv0[loc][1] - p_mv0[locn][1] ) >= i_mvy_himit ||
+                         ( p_lef1[loc] != p_lef1[locn] ||
+                           abs( p_mv1[loc][0] - p_mv1[locn][0] ) >= 4 ||
+                           abs( p_mv1[loc][1] - p_mv1[locn][1] ) >= i_mvy_himit )
+                       )
+                {
+                    p_bs0[i_edge][i] = 1;
+                }
+                else
+                {
+                    p_bs0[i_edge][i] = 0;
+                }
+            }
+        }
+
+        for( i_edge = 0; i_edge < 4; i_edge++ )
+        {
+            loc = X264_SCAN8_0 + ( i_edge << 3 );
+            for( i = 0; i < 4; i++, loc++ )
+            {
+                locn = loc - 8;
+                if( u_nnz[loc] || u_nnz[locn] )
+                {
+                    p_bs1[i_edge][i] = 2;
+                }
+                else if( p_lef0[loc] != p_lef0[locn] ||
+                         abs( p_mv0[loc][0] - p_mv0[locn][0] ) >= 4 ||
+                         abs( p_mv0[loc][1] - p_mv0[locn][1] ) >= i_mvy_himit ||
+                         ( p_lef1[loc] != p_lef1[locn] ||
+                           abs( p_mv1[loc][0] - p_mv1[locn][0] ) >= 4 ||
+                           abs( p_mv1[loc][1] - p_mv1[locn][1] ) >= i_mvy_himit )
+                       )
+                {
+                    p_bs1[i_edge][i] = 1;
+                }
+                else
+                {
+                    p_bs1[i_edge][i] = 0;
+                }
+            }
+        }
+    }
+    else
+    {
+        avc_deblock_strength_lasx( u_nnz, pi_lef, pi_mv, pu_bs, i_mvy_himit );
+    }
+}
+
+#endif
diff --git a/common/loongarch/deblock.h b/common/loongarch/deblock.h
new file mode 100644
index 00000000..4af3a4e8
--- /dev/null
+++ b/common/loongarch/deblock.h
@@ -0,0 +1,41 @@
+/*****************************************************************************
+ * deblock.h: loongarch deblocking
+ *****************************************************************************
+ * Copyright (C) 2017-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_DEBLOCK_H
+#define X264_LOONGARCH_DEBLOCK_H
+
+#if !HIGH_BIT_DEPTH
+#define x264_deblock_v_luma_lasx x264_template(deblock_v_luma_lasx)
+void x264_deblock_v_luma_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta, int8_t *tc0 );
+#define x264_deblock_h_luma_lasx x264_template(deblock_h_luma_lasx)
+void x264_deblock_h_luma_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta, int8_t *tc0 );
+#define x264_deblock_strength_lasx x264_template(deblock_strength_lasx)
+void x264_deblock_strength_lasx( uint8_t nnz[X264_SCAN8_SIZE], int8_t ref[2][X264_SCAN8_LUMA_SIZE],
+                                 int16_t mv[2][X264_SCAN8_LUMA_SIZE][2], uint8_t bs[2][8][4], int mvy_limit,
+                                 int bframe );
+#endif
+
+#endif
diff --git a/common/loongarch/generic_macros_lasx.h b/common/loongarch/generic_macros_lasx.h
new file mode 100644
index 00000000..de45bead
--- /dev/null
+++ b/common/loongarch/generic_macros_lasx.h
@@ -0,0 +1,3792 @@
+/*****************************************************************************
+ * generic_macros_lasx.h: loongarch macros
+ *****************************************************************************
+ * Copyright (C) 2020 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: Shiyou Yin   <yinshiyou-hf@loongson.cn>
+ *          Xiwei Gu     <guxiwei-hf@loongson.cn>
+ *          Jin Bo       <jinbo@loongson.cn>
+ *          Hao Chen     <chenhao@loongson.cn>
+ *          Lu Wang      <wanglu@loongson.cn>
+ *          Peng Zhou    <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef GENERIC_MACROS_LASX_H
+#define GENERIC_MACROS_LASX_H
+
+#include <stdint.h>
+#include <lasxintrin.h>
+
+/**
+ * MAJOR version: Macro usage changes.
+ * MINOR version: Add new macros, or bug fix.
+ * MICRO version: Comment changes or implementation changes
+ */
+#define LSOM_LASX_VERSION_MAJOR 3
+#define LSOM_LASX_VERSION_MINOR 1
+#define LSOM_LASX_VERSION_MICRO 0
+
+/* Description : Load 256-bit vector data with stride
+ * Arguments   : Inputs  - psrc    (source pointer to load from)
+ *                       - stride
+ *               Outputs - out0, out1, ~
+ * Details     : Load 256-bit data in 'out0' from (psrc)
+ *               Load 256-bit data in 'out1' from (psrc + stride)
+ */
+#define LASX_LD(psrc) *((__m256i *)(psrc))
+
+#define LASX_LD_2(psrc, stride, out0, out1)                                 \
+{                                                                           \
+    out0 = LASX_LD(psrc);                                                   \
+    out1 = LASX_LD((psrc) + stride);                                        \
+}
+
+#define LASX_LD_4(psrc, stride, out0, out1, out2, out3)                     \
+{                                                                           \
+    LASX_LD_2((psrc), stride, out0, out1);                                  \
+    LASX_LD_2((psrc) + 2 * stride , stride, out2, out3);                    \
+}
+
+#define LASX_LD_8(psrc, stride, out0, out1, out2, out3, out4, out5,         \
+                  out6, out7)                                               \
+{                                                                           \
+    LASX_LD_4((psrc), stride, out0, out1, out2, out3);                      \
+    LASX_LD_4((psrc) + 4 * stride, stride, out4, out5, out6, out7);         \
+}
+
+/* Description : Store 256-bit vector data with stride
+ * Arguments   : Inputs  - in0, in1, ~
+ *                       - pdst    (destination pointer to store to)
+ *                       - stride
+ * Details     : Store 256-bit data from 'in0' to (pdst)
+ *               Store 256-bit data from 'in1' to (pdst + stride)
+ */
+#define LASX_ST(in, pdst) *((__m256i *)(pdst)) = (in)
+
+#define LASX_ST_2(in0, in1, pdst, stride)                                   \
+{                                                                           \
+    LASX_ST(in0, (pdst));                                                   \
+    LASX_ST(in1, (pdst) + stride);                                          \
+}
+
+#define LASX_ST_4(in0, in1, in2, in3, pdst, stride)                         \
+{                                                                           \
+    LASX_ST_2(in0, in1, (pdst), stride);                                    \
+    LASX_ST_2(in2, in3, (pdst) + 2 * stride, stride);                       \
+}
+
+#define LASX_ST_8(in0, in1, in2, in3, in4, in5, in6, in7, pdst, stride)     \
+{                                                                           \
+    LASX_ST_4(in0, in1, in2, in3, (pdst), stride);                          \
+    LASX_ST_4(in4, in5, in6, in7, (pdst) + 4 * stride, stride);             \
+}
+
+/* Description : Store half word elements of vector with stride
+ * Arguments   : Inputs  - in   source vector
+ *                       - idx, idx0, idx1,  ~
+ *                       - pdst    (destination pointer to store to)
+ *                       - stride
+ * Details     : Store half word 'idx0' from 'in' to (pdst)
+ *               Store half word 'idx1' from 'in' to (pdst + stride)
+ *               Similar for other elements
+ * Example     : LASX_ST_H(in, idx, pdst)
+ *          in : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
+ *        idx0 : 0x01
+ *        out0 : 2
+ */
+#define LASX_ST_H(in, idx, pdst)                                          \
+{                                                                         \
+    __lasx_xvstelm_h(in, pdst, 0, idx);                                   \
+}
+
+#define LASX_ST_H_2(in, idx0, idx1, pdst, stride)                         \
+{                                                                         \
+    LASX_ST_H(in, idx0, (pdst));                                          \
+    LASX_ST_H(in, idx1, (pdst) + stride);                                 \
+}
+
+#define LASX_ST_H_4(in, idx0, idx1, idx2, idx3, pdst, stride)             \
+{                                                                         \
+    LASX_ST_H_2(in, idx0, idx1, (pdst), stride);                          \
+    LASX_ST_H_2(in, idx2, idx3, (pdst) + 2 * stride, stride);             \
+}
+
+
+/* Description : Store word elements of vector with stride
+ * Arguments   : Inputs  - in   source vector
+ *                       - idx, idx0, idx1,  ~
+ *                       - pdst    (destination pointer to store to)
+ *                       - stride
+ * Details     : Store word 'idx0' from 'in' to (pdst)
+ *               Store word 'idx1' from 'in' to (pdst + stride)
+ *               Similar for other elements
+ * Example     : LASX_ST_W(in, idx, pdst)
+ *          in : 1, 2, 3, 4, 5, 6, 7, 8
+ *        idx0 : 0x01
+ *        out0 : 2
+ */
+#define LASX_ST_W(in, idx, pdst)                                          \
+{                                                                         \
+    __lasx_xvstelm_w(in, pdst, 0, idx);                                   \
+}
+
+#define LASX_ST_W_2(in, idx0, idx1, pdst, stride)                         \
+{                                                                         \
+    LASX_ST_W(in, idx0, (pdst));                                          \
+    LASX_ST_W(in, idx1, (pdst) + stride);                                 \
+}
+
+#define LASX_ST_W_4(in, idx0, idx1, idx2, idx3, pdst, stride)             \
+{                                                                         \
+    LASX_ST_W_2(in, idx0, idx1, (pdst), stride);                          \
+    LASX_ST_W_2(in, idx2, idx3, (pdst) + 2 * stride, stride);             \
+}
+
+#define LASX_ST_W_8(in, idx0, idx1, idx2, idx3, idx4, idx5, idx6, idx7,   \
+                    pdst, stride)                                         \
+{                                                                         \
+    LASX_ST_W_4(in, idx0, idx1, idx2, idx3, (pdst), stride);              \
+    LASX_ST_W_4(in, idx4, idx5, idx6, idx7, (pdst) + 4 * stride, stride); \
+}
+
+/* Description : Store double word elements of vector with stride
+ * Arguments   : Inputs  - in   source vector
+ *                       - idx, idx0, idx1, ~
+ *                       - pdst    (destination pointer to store to)
+ *                       - stride
+ * Details     : Store double word 'idx0' from 'in' to (pdst)
+ *               Store double word 'idx1' from 'in' to (pdst + stride)
+ *               Similar for other elements
+ * Example     : See LASX_ST_W(in, idx, pdst)
+ */
+#define LASX_ST_D(in, idx, pdst)                                         \
+{                                                                        \
+    __lasx_xvstelm_d(in, pdst, 0, idx);                                  \
+}
+
+#define LASX_ST_D_2(in, idx0, idx1, pdst, stride)                        \
+{                                                                        \
+    LASX_ST_D(in, idx0, (pdst));                                         \
+    LASX_ST_D(in, idx1, (pdst) + stride);                                \
+}
+
+#define LASX_ST_D_4(in, idx0, idx1, idx2, idx3, pdst, stride)            \
+{                                                                        \
+    LASX_ST_D_2(in, idx0, idx1, (pdst), stride);                         \
+    LASX_ST_D_2(in, idx2, idx3, (pdst) + 2 * stride, stride);            \
+}
+
+/* Description : Store quad word elements of vector with stride
+ * Arguments   : Inputs  - in   source vector
+ *                       - idx, idx0, idx1, ~
+ *                       - pdst    (destination pointer to store to)
+ *                       - stride
+ * Details     : Store quad word 'idx0' from 'in' to (pdst)
+ *               Store quad word 'idx1' from 'in' to (pdst + stride)
+ *               Similar for other elements
+ * Example     : See LASX_ST_W(in, idx, pdst)
+ */
+#define LASX_ST_Q(in, idx, pdst)                                         \
+{                                                                        \
+    LASX_ST_D(in, (idx << 1), pdst);                                     \
+    LASX_ST_D(in, (( idx << 1) + 1), (char*)(pdst) + 8);                 \
+}
+
+#define LASX_ST_Q_2(in, idx0, idx1, pdst, stride)                        \
+{                                                                        \
+    LASX_ST_Q(in, idx0, (pdst));                                         \
+    LASX_ST_Q(in, idx1, (pdst) + stride);                                \
+}
+
+#define LASX_ST_Q_4(in, idx0, idx1, idx2, idx3, pdst, stride)            \
+{                                                                        \
+    LASX_ST_Q_2(in, idx0, idx1, (pdst), stride);                         \
+    LASX_ST_Q_2(in, idx2, idx3, (pdst) + 2 * stride, stride);            \
+}
+
+/* Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - unsigned halfword
+ * Details     : Unsigned byte elements from in0 are iniplied with
+ *               unsigned byte elements from in0 producing a result
+ *               twice the size of input i.e. unsigned halfword.
+ *               Then this iniplication results of adjacent odd-even elements
+ *               are added together and stored to the out vector
+ *               (2 unsigned halfword results)
+ * Example     : see LASX_DP2_W_H
+ */
+#define LASX_DP2_H_BU(in0, in1, out0)                   \
+{                                                       \
+    __m256i _tmp0_m ;                                   \
+                                                        \
+    _tmp0_m = __lasx_xvmulwev_h_bu( in0, in1 );         \
+    out0 = __lasx_xvmaddwod_h_bu( _tmp0_m, in0, in1 );  \
+}
+#define LASX_DP2_H_BU_2(in0, in1, in2, in3, out0, out1) \
+{                                                       \
+    LASX_DP2_H_BU(in0, in1, out0);                      \
+    LASX_DP2_H_BU(in2, in3, out1);                      \
+}
+#define LASX_DP2_H_BU_4(in0, in1, in2, in3,             \
+                        in4, in5, in6, in7,             \
+                        out0, out1, out2, out3)         \
+{                                                       \
+    LASX_DP2_H_BU_2(in0, in1, in0, in1, out0, out1);    \
+    LASX_DP2_H_BU_2(in4, in5, in6, in7, out2, out3);    \
+}
+
+/* Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - signed halfword
+ * Details     : Signed byte elements from in0 are iniplied with
+ *               signed byte elements from in0 producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this iniplication results of adjacent odd-even elements
+ *               are added together and stored to the out vector
+ *               (2 signed halfword results)
+ * Example     : see LASX_DP2_W_H
+ */
+#define LASX_DP2_H_B(in0, in1, out0)                      \
+{                                                         \
+    __m256i _tmp0_m ;                                     \
+                                                          \
+    _tmp0_m = __lasx_xvmulwev_h_b( in0, in1 );            \
+    out0 = __lasx_xvmaddwod_h_b( _tmp0_m, in0, in1 );     \
+}
+#define LASX_DP2_H_B_2(in0, in1, in2, in3, out0, out1)    \
+{                                                         \
+    LASX_DP2_H_B(in0, in1, out0);                         \
+    LASX_DP2_H_B(in2, in3, out1);                         \
+}
+#define LASX_DP2_H_B_4(in0, in1, in2, in3,                \
+                       in4, in5, in6, in7,                \
+                       out0, out1, out2, out3)            \
+{                                                         \
+    LASX_DP2_H_B_2(in0, in1, in2, in3, out0, out1);       \
+    LASX_DP2_H_B_2(in4, in5, in6, in7, out2, out3);       \
+}
+
+/* Description : Dot product of half word vector elements
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ *               Return Type - signed word
+ * Details     : Signed half word elements from in* are iniplied with
+ *               signed half word elements from in* producing a result
+ *               twice the size of input i.e. signed word.
+ *               Then this iniplication results of adjacent odd-even elements
+ *               are added together and stored to the out vector.
+ * Example     : LASX_DP2_W_H(in0, in1, out0)
+ *               in0:   1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
+ *               in0:   8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
+ *               out0:  22,38,38,22, 22,38,38,22
+ */
+#define LASX_DP2_W_H(in0, in1, out0)                   \
+{                                                      \
+    __m256i _tmp0_m ;                                  \
+                                                       \
+    _tmp0_m = __lasx_xvmulwev_w_h( in0, in1 );         \
+    out0 = __lasx_xvmaddwod_w_h( _tmp0_m, in0, in1 );  \
+}
+#define LASX_DP2_W_H_2(in0, in1, in2, in3, out0, out1)             \
+{                                                                  \
+    LASX_DP2_W_H(in0, in1, out0);                                  \
+    LASX_DP2_W_H(in2, in3, out1);                                  \
+}
+#define LASX_DP2_W_H_4(in0, in1, in2, in3,                         \
+                       in4, in5, in6, in7, out0, out1, out2, out3) \
+{                                                                  \
+    LASX_DP2_W_H_2(in0, in1, in2, in3, out0, out1);                \
+    LASX_DP2_W_H_2(in4, in5, in6, in7, out2, out3);                \
+}
+#define LASX_DP2_W_H_8(in0, in1, in2, in3, in4, in5, in6, in7,         \
+                       in8, in9, in10, in11, in12, in13, in14, in15,   \
+                       out0, out1, out2, out3, out4, out5, out6, out7) \
+{                                                                      \
+    LASX_DP2_W_H_4(in0, in1, in2, in3, in4, in5, in6, in7,             \
+                   out0, out1, out2, out3);                            \
+    LASX_DP2_W_H_4(in8, in9, in10, in11, in12, in13, in14, in15,       \
+                   out4, out5, out6, out7);                            \
+}
+
+/* Description : Dot product of word vector elements
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ *               Retrun Type - signed double
+ * Details     : Signed word elements from in* are iniplied with
+ *               signed word elements from in* producing a result
+ *               twice the size of input i.e. signed double word.
+ *               Then this iniplication results of adjacent odd-even elements
+ *               are added together and stored to the out vector.
+ * Example     : see LASX_DP2_W_H
+ */
+#define LASX_DP2_D_W(in0, in1, out0)                    \
+{                                                       \
+    __m256i _tmp0_m ;                                   \
+                                                        \
+    _tmp0_m = __lasx_xvmulwev_d_w( in0, in1 );          \
+    out0 = __lasx_xvmaddwod_d_w( _tmp0_m, in0, in1 );   \
+}
+#define LASX_DP2_D_W_2(in0, in1, in2, in3, out0, out1)  \
+{                                                       \
+    LASX_DP2_D_W(in0, in1, out0);                       \
+    LASX_DP2_D_W(in2, in3, out1);                       \
+}
+#define LASX_DP2_D_W_4(in0, in1, in2, in3,                             \
+                       in4, in5, in6, in7, out0, out1, out2, out3)     \
+{                                                                      \
+    LASX_DP2_D_W_2(in0, in1, in2, in3, out0, out1);                    \
+    LASX_DP2_D_W_2(in4, in5, in6, in7, out2, out3);                    \
+}
+#define LASX_DP2_D_W_8(in0, in1, in2, in3, in4, in5, in6, in7,         \
+                       in8, in9, in10, in11, in12, in13, in14, in15,   \
+                       out0, out1, out2, out3, out4, out5, out6, out7) \
+{                                                                      \
+    LASX_DP2_D_W_4(in0, in1, in2, in3, in4, in5, in6, in7,             \
+                   out0, out1, out2, out3);                            \
+    LASX_DP2_D_W_4(in8, in9, in10, in11, in12, in13, in14, in15,       \
+                   out4, out5, out6, out7);                            \
+}
+
+/* Description : Dot product of halfword vector elements
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - as per RTYPE
+ * Details     : Unsigned halfword elements from 'in0' are iniplied with
+ *               halfword elements from 'in0' producing a result
+ *               twice the size of input i.e. unsigned word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and written to the 'out0' vector
+ */
+#define LASX_DP2_W_HU_H(in0, in1, out0)                   \
+{                                                         \
+    __m256i _tmp0_m;                                      \
+                                                          \
+    _tmp0_m = __lasx_xvmulwev_w_hu_h( in0, in1 );         \
+    out0 = __lasx_xvmaddwod_w_hu_h( _tmp0_m, in0, in1 );  \
+}
+
+#define LASX_DP2_W_HU_H_2(in0, in1, in2, in3, out0, out1) \
+{                                                         \
+    LASX_DP2_W_HU_H(in0, in1, out0);                      \
+    LASX_DP2_W_HU_H(in2, in3, out1);                      \
+}
+
+#define LASX_DP2_W_HU_H_4(in0, in1, in2, in3,             \
+                          in4, in5, in6, in7,             \
+                          out0, out1, out2, out3)         \
+{                                                         \
+    LASX_DP2_W_HU_H_2(in0, in1, in2, in3, out0, out1);    \
+    LASX_DP2_W_HU_H_2(in4, in5, in6, in7, out2, out3);    \
+}
+
+/* Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in0 are iniplied with
+ *               signed byte elements from in0 producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this iniplication results of adjacent odd-even elements
+ *               are added to the out vector
+ *               (2 signed halfword results)
+ * Example     : LASX_DP2ADD_H_B(in0, in1, in2, out0)
+ *               in0:  1,2,3,4, 1,2,3,4, 1,2,3,4, 1,2,3,4,
+ *               in1:  1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
+ *                     1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *               in2:  8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
+ *                     8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *               out0: 23,40,41,26, 23,40,41,26, 23,40,41,26, 23,40,41,26
+ */
+#define LASX_DP2ADD_H_B(in0, in1, in2, out0)                 \
+{                                                            \
+    __m256i _tmp0_m;                                         \
+                                                             \
+    _tmp0_m = __lasx_xvmaddwev_h_b( in0, in1, in2 );         \
+    out0 = __lasx_xvmaddwod_h_b( _tmp0_m, in1, in2 );        \
+}
+#define LASX_DP2ADD_H_B_2(in0, in1, in2, in3, in4, in5, out0, out1)  \
+{                                                                    \
+    LASX_DP2ADD_H_B(in0, in1, in2, out0);                            \
+    LASX_DP2ADD_H_B(in3, in4, in5, out1);                            \
+}
+#define LASX_DP2ADD_H_B_4(in0, in1, in2, in3, in4, in5,                \
+                          in6, in7, in8, in9, in10, in11,              \
+                          out0, out1, out2, out3)                      \
+{                                                                      \
+    LASX_DP2ADD_H_B_2(in0, in1, in2, in3, in4, in5, out0, out1);       \
+    LASX_DP2ADD_H_B_2(in6, in7, in8, in9, in10, in11, out2, out3);     \
+}
+
+/* Description : Dot product of halfword vector elements
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - as per RTYPE
+ * Details     : Signed halfword elements from 'in0' are iniplied with
+ *               signed halfword elements from 'in0' producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and written to the 'out0' vector
+ */
+#define LASX_DP2ADD_W_H(in0, in1, in2, out0)                 \
+{                                                            \
+    __m256i _tmp0_m;                                         \
+                                                             \
+    _tmp0_m = __lasx_xvmaddwev_w_h( in0, in1, in2 );         \
+    out0 = __lasx_xvmaddwod_w_h( _tmp0_m, in1, in2 );        \
+}
+#define LASX_DP2ADD_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1 ) \
+{                                                                    \
+    LASX_DP2ADD_W_H(in0, in1, in2, out0);                            \
+    LASX_DP2ADD_W_H(in3, in4, in5, out1);                            \
+}
+#define LASX_DP2ADD_W_H_4(in0, in1, in2, in3, in4, in5,              \
+                          in6, in7, in8, in9, in10, in11,            \
+                          out0, out1, out2, out3)                    \
+{                                                                    \
+    LASX_DP2ADD_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1);     \
+    LASX_DP2ADD_W_H_2(in6, in7, in8, in9, in10, in11, out2, out3);   \
+}
+
+/* Description : Dot product of halfword vector elements
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - as per RTYPE
+ * Details     : Unsigned halfword elements from 'in0' are iniplied with
+ *               unsigned halfword elements from 'in0' producing a result
+ *               twice the size of input i.e. unsigned word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and written to the 'out0' vector
+ */
+#define LASX_DP2ADD_W_HU(in0, in1, in2, out0)          \
+{                                                      \
+    __m256i _tmp0_m;                                   \
+                                                       \
+    _tmp0_m = __lasx_xvmaddwev_w_hu( in0, in1, in2 );  \
+    out0 = __lasx_xvmaddwod_w_hu( _tmp0_m, in1, in2 ); \
+}
+#define LASX_DP2ADD_W_HU_2(in0, in1, in2, in3, in4, in5, out0, out1) \
+{                                                                    \
+    LASX_DP2ADD_W_HU(in0, in1, in2, out0);                           \
+    LASX_DP2ADD_W_HU(in3, in4, in5, out1);                           \
+}
+#define LASX_DP2ADD_W_HU_4(in0, in1, in2, in3, in4, in5,             \
+                           in6, in7, in8, in9, in10, in11,           \
+                           out0, out1, out2, out3)                   \
+{                                                                    \
+    LASX_DP2ADD_W_HU_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
+    LASX_DP2ADD_W_HU_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
+}
+
+/* Description : Dot product of halfword vector elements
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - as per RTYPE
+ * Details     : Unsigned halfword elements from 'in0' are iniplied with
+ *               halfword elements from 'in0' producing a result
+ *               twice the size of input i.e. unsigned word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and written to the 'out0' vector
+ */
+#define LASX_DP2ADD_W_HU_H(in0, in1, in2, out0)           \
+{                                                         \
+    __m256i _tmp0_m;                                      \
+                                                          \
+    _tmp0_m = __lasx_xvmaddwev_w_hu_h( in0, in1, in2 );   \
+    out0 = __lasx_xvmaddwod_w_hu_h( _tmp0_m, in1, in2 );  \
+}
+
+#define LASX_DP2ADD_W_HU_H_2(in0, in1, in2, in3, in4, in5, out0, out1) \
+{                                                                      \
+    LASX_DP2ADD_W_HU_H(in0, in1, in2, out0);                           \
+    LASX_DP2ADD_W_HU_H(in3, in4, in5, out1);                           \
+}
+
+#define LASX_DP2ADD_W_HU_H_4(in0, in1, in2, in3, in4, in5,             \
+                             in6, in7, in8, in9, in10, in11,           \
+                             out0, out1, out2, out3)                   \
+{                                                                      \
+    LASX_DP2ADD_W_HU_H_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
+    LASX_DP2ADD_W_HU_H_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
+}
+
+/* Description : Vector Unsigned Dot Product and Subtract.
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - as per RTYPE
+ * Details     : Unsigned byte elements from 'in0' are iniplied with
+ *               unsigned byte elements from 'in0' producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtract from double width elements,
+ *               then written to the 'out0' vector.
+ */
+#define LASX_DP2SUB_H_BU(in0, in1, in2, out0)             \
+{                                                         \
+    __m256i _tmp0_m;                                      \
+                                                          \
+    _tmp0_m = __lasx_xvmulwev_h_bu( in1, in2 );           \
+    _tmp0_m = __lasx_xvmaddwod_h_bu( _tmp0_m, in1, in2 ); \
+    out0 = __lasx_xvsub_h( in0, _tmp0_m );                \
+}
+
+#define LASX_DP2SUB_H_BU_2(in0, in1, in2, in3, in4, in5, out0, out1) \
+{                                                                    \
+    LASX_DP2SUB_H_BU(in0, in1, in2, out0);                           \
+    LASX_DP2SUB_H_BU(in0, in1, in2, out0);                           \
+}
+
+#define LASX_DP2SUB_H_BU_4(in0, in1, in2, in3, in4, in5,             \
+                           in6, in7, in8, in9, in10, in11,           \
+                           out0, out1, out2, out3)                   \
+{                                                                    \
+    LASX_DP2SUB_H_BU_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
+    LASX_DP2SUB_H_BU_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
+}
+
+/* Description : Vector Signed Dot Product and Subtract.
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - as per RTYPE
+ * Details     : Signed halfword elements from 'in0' are iniplied with
+ *               signed halfword elements from 'in0' producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtract from double width elements,
+ *               then written to the 'out0' vector.
+ */
+#define LASX_DP2SUB_W_H(in0, in1, in2, out0)             \
+{                                                        \
+    __m256i _tmp0_m;                                     \
+                                                         \
+    _tmp0_m = __lasx_xvmulwev_w_h( in1, in2 );           \
+    _tmp0_m = __lasx_xvmaddwod_w_h( _tmp0_m, in1, in2 ); \
+    out0 = __lasx_xvsub_w( in0, _tmp0_m );               \
+}
+
+#define LASX_DP2SUB_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1) \
+{                                                                   \
+    LASX_DP2SUB_W_H(in0, in1, in2, out0);                           \
+    LASX_DP2SUB_W_H(in3, in4, in5, out1);                           \
+}
+
+#define LASX_DP2SUB_W_H_4(in0, in1, in2, in3, in4, in5,             \
+                          in6, in7, in8, in9, in10, in11,           \
+                          out0, out1, out2, out3)                   \
+{                                                                   \
+    LASX_DP2SUB_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
+    LASX_DP2SUB_W_H_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
+}
+
+/* Description : Dot product of half word vector elements
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ *               Return Type - signed word
+ * Details     : Signed half word elements from in* are iniplied with
+ *               signed half word elements from in* producing a result
+ *               twice the size of input i.e. signed word.
+ *               Then this iniplication results of four adjacent elements
+ *               are added together and stored to the out vector.
+ * Example     : LASX_DP2_W_H(in0, in0, out0)
+ *               in0:   3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1,
+ *               in0:   -2,1,1,0, 1,0,0,0, 0,0,1,0, 1,0,0,1,
+ *               out0:  -2,0,1,1,
+ */
+#define LASX_DP4_D_H(in0, in1, out0)                         \
+{                                                            \
+    __m256i _tmp0_m ;                                        \
+                                                             \
+    _tmp0_m = __lasx_xvmulwev_w_h( in0, in1 );               \
+    _tmp0_m = __lasx_xvmaddwod_w_h( _tmp0_m, in0, in1 );     \
+    out0  = __lasx_xvhaddw_d_w( _tmp0_m, _tmp0_m );          \
+}
+#define LASX_DP4_D_H_2(in0, in1, in2, in3, out0, out1)       \
+{                                                            \
+    LASX_DP4_D_H(in0, in1, out0);                            \
+    LASX_DP4_D_H(in2, in3, out1);                            \
+}
+#define LASX_DP4_D_H_4(in0, in1, in2, in3,                            \
+                       in4, in5, in6, in7, out0, out1, out2, out3)    \
+{                                                                     \
+    LASX_DP4_D_H_2(in0, in1, in2, in3, out0, out1);                   \
+    LASX_DP4_D_H_2(in4, in5, in6, in7, out2, out3);                   \
+}
+
+/* Description : The high half of the vector elements are expanded and
+ *               added after being doubled
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in0 vector and the in1 vector are added after the
+ *               higher half of the two-fold sign extension ( signed byte
+ *               to signed half word ) and stored to the out vector.
+ * Example     : see LASX_ADDWL_W_H_128SV
+ */
+#define LASX_ADDWH_H_B_128SV(in0, in1, out0)                                  \
+{                                                                             \
+    __m256i _tmp0_m, _tmp1_m;                                                 \
+                                                                              \
+    _tmp0_m = __lasx_xvilvh_b( in0, in0 );                                    \
+    _tmp1_m = __lasx_xvilvh_b( in1, in1 );                                    \
+    out0 = __lasx_xvaddwev_h_b( _tmp0_m, _tmp1_m );                           \
+}
+#define LASX_ADDWH_H_B_2_128SV(in0, in1, in2, in3, out0, out1)                \
+{                                                                             \
+    LASX_ADDWH_H_B_128SV(in0, in1, out0);                                     \
+    LASX_ADDWH_H_B_128SV(in2, in3, out1);                                     \
+}
+#define LASX_ADDWH_H_B_4_128SV(in0, in1, in2, in3,                            \
+                               in4, in5, in6, in7, out0, out1, out2, out3)    \
+{                                                                             \
+    LASX_ADDWH_H_B_2_128SV(in0, in1, in2, in3, out0, out1);                   \
+    LASX_ADDWH_H_B_2_128SV(in4, in5, in6, in7, out2, out3);                   \
+}
+
+/* Description : The high half of the vector elements are expanded and
+ *               added after being doubled
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in0 vector and the in1 vector are added after the
+ *               higher half of the two-fold sign extension ( signed half word
+ *               to signed word ) and stored to the out vector.
+ * Example     : see LASX_ADDWL_W_H_128SV
+ */
+#define LASX_ADDWH_W_H_128SV(in0, in1, out0)                                  \
+{                                                                             \
+    __m256i _tmp0_m, _tmp1_m;                                                 \
+                                                                              \
+    _tmp0_m = __lasx_xvilvh_h( in0, in0 );                                    \
+    _tmp1_m = __lasx_xvilvh_h( in1, in1 );                                    \
+    out0 = __lasx_xvaddwev_w_h( _tmp0_m, _tmp1_m );                           \
+}
+#define LASX_ADDWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1)                \
+{                                                                             \
+    LASX_ADDWH_W_H_128SV(in0, in1, out0);                                     \
+    LASX_ADDWH_W_H_128SV(in2, in3, out1);                                     \
+}
+#define LASX_ADDWH_W_H_4_128SV(in0, in1, in2, in3,                            \
+                               in4, in5, in6, in7, out0, out1, out2, out3)    \
+{                                                                             \
+    LASX_ADDWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                   \
+    LASX_ADDWH_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                   \
+}
+
+/* Description : The low half of the vector elements are expanded and
+ *               added after being doubled
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in0 vector and the in1 vector are added after the
+ *               lower half of the two-fold sign extension ( signed byte
+ *               to signed half word ) and stored to the out vector.
+ * Example     : see LASX_ADDWL_W_H_128SV
+ */
+#define LASX_ADDWL_H_B_128SV(in0, in1, out0)                                  \
+{                                                                             \
+    __m256i _tmp0_m, _tmp1_m;                                                 \
+                                                                              \
+    _tmp0_m = __lasx_xvsllwil_h_b( in0, 0 );                                  \
+    _tmp1_m = __lasx_xvsllwil_h_b( in1, 0 );                                  \
+    out0 = __lasx_xvadd_h( _tmp0_m, _tmp1_m );                                \
+}
+#define LASX_ADDWL_H_B_2_128SV(in0, in1, in2, in3, out0, out1)                \
+{                                                                             \
+    LASX_ADDWL_H_B_128SV(in0, in1, out0);                                     \
+    LASX_ADDWL_H_B_128SV(in2, in3, out1);                                     \
+}
+#define LASX_ADDWL_H_B_4_128SV(in0, in1, in2, in3,                            \
+                               in4, in5, in6, in7, out0, out1, out2, out3)    \
+{                                                                             \
+    LASX_ADDWL_H_B_2_128SV(in0, in1, in2, in3, out0, out1);                   \
+    LASX_ADDWL_H_B_2_128SV(in4, in5, in6, in7, out2, out3);                   \
+}
+
+/* Description : The low half of the vector elements are expanded and
+ *               added after being doubled
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in0 vector and the in1 vector are added after the
+ *               lower half of the two-fold sign extension ( signed half word
+ *               to signed word ) and stored to the out vector.
+ * Example     : LASX_ADDWL_W_H_128SV(in0, in1, out0)
+ *               in0   3,0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1,
+ *               in1   2,-1,1,2, 1,0,0,0, 1,0,1,0, 1,0,0,1,
+ *               out0  5,-1,4,2, 1,0,2,-1,
+ */
+#define LASX_ADDWL_W_H_128SV(in0, in1, out0)                                  \
+{                                                                             \
+    __m256i _tmp0_m;                                                          \
+                                                                              \
+    _tmp0_m = __lasx_xvilvl_h(in1, in0);                                      \
+    out0 = __lasx_xvhaddw_w_h( _tmp0_m, _tmp0_m );                            \
+}
+#define LASX_ADDWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1)                \
+{                                                                             \
+    LASX_ADDWL_W_H_128SV(in0, in1, out0);                                     \
+    LASX_ADDWL_W_H_128SV(in2, in3, out1);                                     \
+}
+#define LASX_ADDWL_W_H_4_128SV(in0, in1, in2, in3,                            \
+                               in4, in5, in6, in7, out0, out1, out2, out3)    \
+{                                                                             \
+    LASX_ADDWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                   \
+    LASX_ADDWL_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                   \
+}
+
+/* Description : The low half of the vector elements are expanded and
+ *               added after being doubled
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in0 vector and the in1 vector are added after the
+ *               lower half of the two-fold zero extension ( unsigned byte
+ *               to unsigned half word ) and stored to the out vector.
+ */
+#define LASX_ADDWL_H_BU_128SV(in0, in1, out0)                                 \
+{                                                                             \
+    __m256i _tmp0_m;                                                          \
+                                                                              \
+    _tmp0_m = __lasx_xvilvl_b(in1, in0);                                      \
+    out0 = __lasx_xvhaddw_hu_bu( _tmp0_m, _tmp0_m );                          \
+}
+#define LASX_ADDWL_H_BU_2_128SV(in0, in1, in2, in3, out0, out1)               \
+{                                                                             \
+    LASX_ADDWL_H_BU_128SV(in0, in1, out0);                                    \
+    LASX_ADDWL_H_BU_128SV(in2, in3, out1);                                    \
+}
+#define LASX_ADDWL_H_BU_4_128SV(in0, in1, in2, in3,                           \
+                                in4, in5, in6, in7, out0, out1, out2, out3)   \
+{                                                                             \
+    LASX_ADDWL_H_BU_2_128SV(in0, in1, in2, in3, out0, out1);                  \
+    LASX_ADDWL_H_BU_2_128SV(in4, in5, in6, in7, out2, out3);                  \
+}
+
+/* Description : The low half of the vector elements are expanded and
+ *               added after being doubled
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : In1 vector plus in0 vector after double zero extension
+ *               ( unsigned byte to half word ),add and stored to the out vector.
+ * Example     : reference to LASX_ADDW_W_W_H_128SV(in0, in1, out0)
+ */
+#define LASX_ADDW_H_H_BU_128SV(in0, in1, out0)                                \
+{                                                                             \
+    __m256i _tmp1_m;                                                          \
+                                                                              \
+    _tmp1_m = __lasx_xvsllwil_hu_bu( in1, 0 );                                \
+    out0 = __lasx_xvadd_h( in0, _tmp1_m );                                    \
+}
+#define LASX_ADDW_H_H_BU_2_128SV(in0, in1, in2, in3, out0, out1)              \
+{                                                                             \
+    LASX_ADDW_H_H_BU_128SV(in0, in1, out0);                                   \
+    LASX_ADDW_H_H_BU_128SV(in2, in3, out1);                                   \
+}
+#define LASX_ADDW_H_H_BU_4_128SV(in0, in1, in2, in3,                          \
+                                 in4, in5, in6, in7, out0, out1, out2, out3)  \
+{                                                                             \
+    LASX_ADDW_H_H_BU_2_128SV(in0, in1, in2, in3, out0, out1);                 \
+    LASX_ADDW_H_H_BU_2_128SV(in4, in5, in6, in7, out2, out3);                 \
+}
+
+/* Description : The low half of the vector elements are expanded and
+ *               added after being doubled
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : In1 vector plus in0 vector after double sign extension
+ *               ( signed half word to word ),add and stored to the out vector.
+ * Example     : LASX_ADDW_W_W_H_128SV(in0, in1, out0)
+ *               in0   0,1,0,0, -1,0,0,1,
+ *               in1   2,-1,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1,
+ *               out0  2,0,1,2, -1,0,1,1,
+ */
+#define LASX_ADDW_W_W_H_128SV(in0, in1, out0)                                 \
+{                                                                             \
+    __m256i _tmp1_m;                                                          \
+                                                                              \
+    _tmp1_m = __lasx_xvsllwil_w_h( in1, 0 );                                  \
+    out0 = __lasx_xvadd_w( in0, _tmp1_m );                                    \
+}
+#define LASX_ADDW_W_W_H_2_128SV(in0, in1, in2, in3, out0, out1)               \
+{                                                                             \
+    LASX_ADDW_W_W_H_128SV(in0, in1, out0);                                    \
+    LASX_ADDW_W_W_H_128SV(in2, in3, out1);                                    \
+}
+#define LASX_ADDW_W_W_H_4_128SV(in0, in1, in2, in3,                           \
+                                in4, in5, in6, in7, out0, out1, out2, out3)   \
+{                                                                             \
+    LASX_ADDW_W_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                  \
+    LASX_ADDW_W_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                  \
+}
+
+/* Description : Multiplication and addition calculation after expansion
+ *               of the lower half of the vector
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in1 vector and the in0 vector are multiplied after
+ *               the lower half of the two-fold sign extension ( signed
+ *               half word to signed word ) , and the result is added to
+ *               the vector in0, the stored to the out vector.
+ * Example     : LASX_MADDWL_W_H_128SV(in0, in1, in2, out0)
+ *               in0   1,2,3,4, 5,6,7 8
+ *               in1   1,2,3,4, 1,2,3,4, 5,6,7,8, 5,6,7,8
+ *               in2   200,300,400,500, 2000,3000,4000,5000,
+ *                     -200,-300,-400,-500, -2000,-3000,-4000,-5000
+ *               out0  5,-1,4,2, 1,0,2,-1,
+ */
+#define LASX_MADDWL_W_H_128SV(in0, in1, in2, out0)                            \
+{                                                                             \
+    __m256i _tmp0_m, _tmp1_m;                                                 \
+                                                                              \
+    _tmp0_m = __lasx_xvsllwil_w_h( in1, 0 );                                  \
+    _tmp1_m = __lasx_xvsllwil_w_h( in2, 0 );                                  \
+    _tmp0_m = __lasx_xvmul_w( _tmp0_m, _tmp1_m );                             \
+    out0 = __lasx_xvadd_w( _tmp0_m, in0 );                                    \
+}
+#define LASX_MADDWL_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1)     \
+{                                                                             \
+    LASX_MADDWL_W_H_128SV(in0, in1, in2, out0);                               \
+    LASX_MADDWL_W_H_128SV(in3, in4, in5, out1);                               \
+}
+#define LASX_MADDWL_W_H_4_128SV(in0, in1, in2, in3, in4, in5,                \
+                                in6, in7, in8, in9, in10, in11,              \
+                                out0, out1, out2, out3)                      \
+{                                                                            \
+    LASX_MADDWL_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1);       \
+    LASX_MADDWL_W_H_2_128SV(in6, in7, in8, in9, in10, in11, out2, out3);     \
+}
+
+/* Description : Multiplication and addition calculation after expansion
+ *               of the higher half of the vector
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in1 vector and the in0 vector are multiplied after
+ *               the higher half of the two-fold sign extension ( signed
+ *               half word to signed word ) , and the result is added to
+ *               the vector in0, the stored to the out vector.
+ * Example     : see LASX_MADDWL_W_H_128SV
+ */
+#define LASX_MADDWH_W_H_128SV(in0, in1, in2, out0)                            \
+{                                                                             \
+    __m256i _tmp0_m, _tmp1_m;                                                 \
+                                                                              \
+    _tmp0_m = __lasx_xvilvh_h( in1, in1 );                                    \
+    _tmp1_m = __lasx_xvilvh_h( in2, in2 );                                    \
+    _tmp0_m = __lasx_xvmulwev_w_h( _tmp0_m, _tmp1_m );                        \
+    out0 = __lasx_xvadd_w( _tmp0_m, in0 );                                    \
+}
+#define LASX_MADDWH_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1)     \
+{                                                                             \
+    LASX_MADDWH_W_H_128SV(in0, in1, in2, out0);                               \
+    LASX_MADDWH_W_H_128SV(in3, in4, in5, out1);                               \
+}
+#define LASX_MADDWH_W_H_4_128SV(in0, in1, in2, in3, in4, in5,                \
+                                in6, in7, in8, in9, in10, in11,              \
+                                out0, out1, out2, out3)                      \
+{                                                                            \
+    LASX_MADDWH_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1);       \
+    LASX_MADDWH_W_H_2_128SV(in6, in7, in8, in9, in10, in11, out2, out3);     \
+}
+
+/* Description : Multiplication calculation after expansion
+ *               of the lower half of the vector
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in1 vector and the in0 vector are multiplied after
+ *               the lower half of the two-fold sign extension ( signed
+ *               half word to signed word ) , the stored to the out vector.
+ * Example     : LASX_MULWL_W_H_128SV(in0, in1, out0)
+ *               in0   3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1,
+ *               in1   2,-1,1,2, 1,0,0,0,  0,0,1,0, 1,0,0,1,
+ *               out0  6,1,3,0, 0,0,1,0,
+ */
+#define LASX_MULWL_W_H_128SV(in0, in1, out0)                    \
+{                                                               \
+    __m256i _tmp0_m, _tmp1_m;                                   \
+                                                                \
+    _tmp0_m = __lasx_xvsllwil_w_h( in0, 0 );                    \
+    _tmp1_m = __lasx_xvsllwil_w_h( in1, 0 );                    \
+    out0 = __lasx_xvmul_w( _tmp0_m, _tmp1_m );                  \
+}
+#define LASX_MULWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1)  \
+{                                                               \
+    LASX_MULWL_W_H_128SV(in0, in1, out0);                       \
+    LASX_MULWL_W_H_128SV(in2, in3, out1);                       \
+}
+#define LASX_MULWL_W_H_4_128SV(in0, in1, in2, in3,              \
+                               in4, in5, in6, in7,              \
+                               out0, out1, out2, out3)          \
+{                                                               \
+    LASX_MULWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1);     \
+    LASX_MULWL_W_H_2_128SV(in4, in5, in6, in7, out2, out3);     \
+}
+
+/* Description : Multiplication calculation after expansion
+ *               of the lower half of the vector
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in1 vector and the in0 vector are multiplied after
+ *               the lower half of the two-fold sign extension ( signed
+ *               half word to signed word ) , the stored to the out vector.
+ * Example     : see LASX_MULWL_W_H_128SV
+ */
+#define LASX_MULWH_W_H_128SV(in0, in1, out0)                    \
+{                                                               \
+    __m256i _tmp0_m, _tmp1_m;                                   \
+                                                                \
+    _tmp0_m = __lasx_xvilvh_h( in0, in0 );                      \
+    _tmp1_m = __lasx_xvilvh_h( in1, in1 );                      \
+    out0 = __lasx_xvmulwev_w_h( _tmp0_m, _tmp1_m );             \
+}
+#define LASX_MULWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1)  \
+{                                                               \
+    LASX_MULWH_W_H_128SV(in0, in1, out0);                       \
+    LASX_MULWH_W_H_128SV(in2, in3, out1);                       \
+}
+#define LASX_MULWH_W_H_4_128SV(in0, in1, in2, in3,              \
+                               in4, in5, in6, in7,              \
+                               out0, out1, out2, out3)          \
+{                                                               \
+    LASX_MULWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1);     \
+    LASX_MULWH_W_H_2_128SV(in4, in5, in6, in7, out2, out3);     \
+}
+
+/* Description : The low half of the vector elements are expanded and
+ *               added after being doubled
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0,  out1,  ~
+ * Details     : The in1 vector add the in0 vector after the
+ *               lower half of the two-fold zero extension ( unsigned byte
+ *               to unsigned half word ) and stored to the out vector.
+ */
+#define LASX_SADDW_HU_HU_BU_128SV(in0, in1, out0)                    \
+{                                                                    \
+    __m256i _tmp1_m;                                                 \
+    __m256i _zero_m = { 0 };                                         \
+                                                                     \
+    _tmp1_m = __lasx_xvilvl_b( _zero_m, in1 );                       \
+    out0 = __lasx_xvsadd_hu( in0, _tmp1_m );                         \
+}
+#define LASX_SADDW_HU_HU_BU_2_128SV(in0, in1, in2, in3, out0, out1)  \
+{                                                                    \
+    LASX_SADDW_HU_HU_BU_128SV(in0, in1, out0);                       \
+    LASX_SADDW_HU_HU_BU_128SV(in2, in3, out1);                       \
+}
+#define LASX_SADDW_HU_HU_BU_4_128SV(in0, in1, in2, in3,              \
+                                    in4, in5, in6, in7,              \
+                                    out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_SADDW_HU_HU_BU_2_128SV(in0, in1, in2, in3, out0, out1);     \
+    LASX_SADDW_HU_HU_BU_2_128SV(in4, in5, in6, in7, out2, out3);     \
+}
+
+/* Description : Low 8-bit vector elements unsigned extension to halfword
+ * Arguments   : Inputs  - in0,  in1,  ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low 8-bit elements from in0 unsigned extension to halfword,
+ *               written to output vector out0. Similar for in1.
+ * Example     : See LASX_UNPCK_L_W_H(in0, out0)
+ */
+#define LASX_UNPCK_L_HU_BU(in0, out0)                                          \
+{                                                                              \
+    out0 = __lasx_vext2xv_hu_bu(in0);                                          \
+}
+
+#define LASX_UNPCK_L_HU_BU_2(in0, in1, out0, out1)                             \
+{                                                                              \
+    LASX_UNPCK_L_HU_BU(in0, out0);                                             \
+    LASX_UNPCK_L_HU_BU(in1, out1);                                             \
+}
+
+#define LASX_UNPCK_L_HU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3)       \
+{                                                                              \
+    LASX_UNPCK_L_HU_BU_2(in0, in1, out0, out1);                                \
+    LASX_UNPCK_L_HU_BU_2(in2, in3, out2, out3);                                \
+}
+
+#define LASX_UNPCK_L_HU_BU_8(in0, in1, in2, in3, in4, in5, in6, in7,           \
+                             out0, out1, out2, out3, out4, out5, out6, out7)   \
+{                                                                              \
+    LASX_UNPCK_L_HU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3);          \
+    LASX_UNPCK_L_HU_BU_4(in4, in5, in6, in7, out4, out5, out6, out7);          \
+}
+
+/* Description : Low 8-bit vector elements unsigned extension to word
+ * Arguments   : Inputs  - in0,  in1,  ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low 8-bit elements from in0 unsigned extension to word,
+ *               written to output vector out0. Similar for in1.
+ * Example     : See LASX_UNPCK_L_W_H(in0, out0)
+ */
+#define LASX_UNPCK_L_WU_BU(in0, out0)                                         \
+{                                                                             \
+    out0 = __lasx_vext2xv_wu_bu(in0);                                         \
+}
+
+#define LASX_UNPCK_L_WU_BU_2(in0, in1, out0, out1)                            \
+{                                                                             \
+    LASX_UNPCK_L_WU_BU(in0, out0);                                            \
+    LASX_UNPCK_L_WU_BU(in1, out1);                                            \
+}
+
+#define LASX_UNPCK_L_WU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3)      \
+{                                                                             \
+    LASX_UNPCK_L_WU_BU_2(in0, in1, out0, out1);                               \
+    LASX_UNPCK_L_WU_BU_2(in2, in3, out2, out3);                               \
+}
+
+#define LASX_UNPCK_L_WU_BU_8(in0, in1, in2, in3, in4, in5, in6, in7,          \
+                             out0, out1, out2, out3, out4, out5, out6, out7)  \
+{                                                                             \
+    LASX_UNPCK_L_WU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3);         \
+    LASX_UNPCK_L_WU_BU_4(in4, in5, in6, in7, out4, out5, out6, out7);         \
+}
+
+/* Description : Low 8-bit vector elements signed extension to halfword
+ * Arguments   : Inputs  - in0,  in1,  ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low 8-bit elements from in0 signed extension to halfword,
+ *               written to output vector out0. Similar for in1.
+ * Example     : See LASX_UNPCK_L_W_H(in0, out0)
+ */
+#define LASX_UNPCK_L_H_B(in0, out0)                                          \
+{                                                                            \
+    out0 = __lasx_vext2xv_h_b(in0);                                          \
+}
+
+#define LASX_UNPCK_L_H_B_2(in0, in1, out0, out1)                             \
+{                                                                            \
+    LASX_UNPCK_L_H_B(in0, out0);                                             \
+    LASX_UNPCK_L_H_B(in1, out1);                                             \
+}
+
+#define LASX_UNPCK_L_H_B_4(in0, in1, in2, in3, out0, out1, out2, out3)       \
+{                                                                            \
+    LASX_UNPCK_L_H_B_2(in0, in1, out0, out1);                                \
+    LASX_UNPCK_L_H_B_2(in2, in3, out2, out3);                                \
+}
+
+/* Description : Low halfword vector elements signed extension to word
+ * Arguments   : Inputs  - in0,  in1,  ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low halfword elements from in0 signed extension to
+ *               word, written to output vector out0. Similar for in1.
+ *               Similar for other pairs.
+ * Example     : LASX_UNPCK_L_W_H(in0, out0)
+ *         in0 : 3, 0, 3, 0,  0, 0, 0, -1,  0, 0, 1, 1,  0, 0, 0, 1
+ *        out0 : 3, 0, 3, 0,  0, 0, 0, -1
+ */
+#define LASX_UNPCK_L_W_H(in0, out0)                                         \
+{                                                                           \
+    out0 = __lasx_vext2xv_w_h(in0);                                         \
+}
+
+#define LASX_UNPCK_L_W_H_2(in0, in1, out0, out1)                            \
+{                                                                           \
+    LASX_UNPCK_L_W_H(in0, out0);                                            \
+    LASX_UNPCK_L_W_H(in1, out1);                                            \
+}
+
+#define LASX_UNPCK_L_W_H_4(in0, in1, in2, in3, out0, out1, out2, out3)      \
+{                                                                           \
+    LASX_UNPCK_L_W_H_2(in0, in1, out0, out1);                               \
+    LASX_UNPCK_L_W_H_2(in2, in3, out2, out3);                               \
+}
+
+#define LASX_UNPCK_L_W_H_8(in0, in1, in2, in3, in4, in5, in6, in7,          \
+                           out0, out1, out2, out3, out4, out5, out6, out7)  \
+{                                                                           \
+    LASX_UNPCK_L_W_H_4(in0, in1, in2, in3, out0, out1, out2, out3);         \
+    LASX_UNPCK_L_W_H_4(in4, in5, in6, in7, out4, out5, out6, out7);         \
+}
+
+/* Description : Interleave odd byte elements from vectors.
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out, out0, ~
+ * Details     : Odd byte elements of in_h and odd byte
+ *               elements of in_l are interleaved and copied to out.
+ * Example     : See LASX_ILVOD_W(in_h, in_l, out)
+ */
+#define LASX_ILVOD_B(in_h, in_l, out)                                            \
+{                                                                                \
+    out = __lasx_xvpackod_b((in_h, in1_l);                                       \
+}
+
+#define LASX_ILVOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                   \
+{                                                                                \
+    LASX_ILVOD_B(in0_h, in0_l, out0);                                            \
+    LASX_ILVOD_B(in1_h, in1_l, out1);                                            \
+}
+
+#define LASX_ILVOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,   \
+                       out0, out1, out2, out3)                                   \
+{                                                                                \
+    LASX_ILVOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                      \
+    LASX_ILVOD_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                      \
+}
+
+/* Description : Interleave odd half word elements from vectors.
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out, out0, ~
+ * Details     : Odd half word elements of in_h and odd half word
+ *               elements of in_l are interleaved and copied to out.
+ * Example     : See LASX_ILVOD_W(in_h, in_l, out)
+ */
+#define LASX_ILVOD_H(in_h, in_l, out)                                           \
+{                                                                               \
+    out = __lasx_xvpackod_h(in_h, in_l);                                        \
+}
+
+#define LASX_ILVOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
+{                                                                               \
+    LASX_ILVOD_H(in0_h, in0_l, out0);                                           \
+    LASX_ILVOD_H(in1_h, in1_l, out1);                                           \
+}
+
+#define LASX_ILVOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                       out0, out1, out2, out3)                                  \
+{                                                                               \
+    LASX_ILVOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
+    LASX_ILVOD_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
+}
+
+/* Description : Interleave odd word elements from vectors.
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out, out0, ~
+ * Details     : Odd word elements of in_h and odd word
+ *               elements of in_l are interleaved and copied to out.
+ * Example     : See LASX_ILVOD_W(in_h, in_l, out)
+ *        in_h : 1, 2, 3, 4,   5, 6, 7, 8
+ *        in_l : 1, 0, 3, 1,   1, 2, 3, 4
+ *         out : 0, 2, 1, 4,   2, 6, 4, 8
+ */
+#define LASX_ILVOD_W(in_h, in_l, out)                                           \
+{                                                                               \
+    out = __lasx_xvpackod_w(in_h, in_l);                                        \
+}
+
+#define LASX_ILVOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
+{                                                                               \
+    LASX_ILVOD_W(in0_h, in0_l, out0);                                           \
+    LASX_ILVOD_W(in1_h, in1_l, out1);                                           \
+}
+
+#define LASX_ILVOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                       out0, out1, out2, out3)                                  \
+{                                                                               \
+    LASX_ILVOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
+    LASX_ILVOD_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
+}
+
+/* Description : Interleave odd double word elements from vectors.
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out, out0, ~
+ * Details     : Odd double word elements of in_h and odd double word
+ *               elements of in_l are interleaved and copied to out.
+ * Example     : LASX_ILVOD_W(in_h, in_l, out)
+ */
+#define LASX_ILVOD_D(in_h, in_l, out)                                           \
+{                                                                               \
+    out = __lasx_xvpackod_d(in_h, in_l);                                        \
+}
+
+#define LASX_ILVOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
+{                                                                               \
+    LASX_ILVOD_D(in0_h, in0_l, out0);                                           \
+    LASX_ILVOD_D(in1_h, in1_l, out1);                                           \
+}
+
+#define LASX_ILVOD_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                       out0, out1, out2, out3)                                  \
+{                                                                               \
+    LASX_ILVOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
+    LASX_ILVOD_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
+}
+
+/* Description : Interleave right half of byte elements from vectors
+ * Arguments   : Inputs  - in0_h,  in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low half of byte elements of in_l and high half of byte
+ *               elements of in_h are interleaved and copied to out0.
+ *               Similar for other pairs
+ * Example     : See LASX_ILVL_W(in_h, in_l, out0)
+ */
+#define LASX_ILVL_B(in_h, in_l, out0)                                      \
+{                                                                          \
+    __m256i tmp0, tmp1;                                                    \
+    tmp0 = __lasx_xvilvl_b(in_h, in_l);                                    \
+    tmp1 = __lasx_xvilvh_b(in_h, in_l);                                    \
+    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
+}
+
+#define LASX_ILVL_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
+{                                                                    \
+    LASX_ILVL_B(in0_h, in0_l, out0)                                  \
+    LASX_ILVL_B(in1_h, in1_l, out1)                                  \
+}
+
+#define LASX_ILVL_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                      in3_h, in3_l, out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_ILVL_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
+    LASX_ILVL_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
+}
+
+#define LASX_ILVL_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                      out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                              \
+    LASX_ILVL_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                  out0, out1, out2, out3);                                     \
+    LASX_ILVL_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                  out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave low half of byte elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h,  in0_l,  ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low half of byte elements of in_l and low half of byte
+ *               elements of in_h are interleaved and copied to out0.
+ *               Similar for other pairs
+ * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
+ */
+#define LASX_ILVL_B_128SV(in_h, in_l, out0)                                   \
+{                                                                             \
+    out0 = __lasx_xvilvl_b(in_h, in_l);                                       \
+}
+
+#define LASX_ILVL_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
+{                                                                             \
+    LASX_ILVL_B_128SV(in0_h, in0_l, out0);                                    \
+    LASX_ILVL_B_128SV(in1_h, in1_l, out1);                                    \
+}
+
+#define LASX_ILVL_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
+                            in3_h, in3_l, out0, out1, out2, out3)             \
+{                                                                             \
+    LASX_ILVL_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
+    LASX_ILVL_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
+}
+
+#define LASX_ILVL_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                                    \
+    LASX_ILVL_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                        out0, out1, out2, out3);                                     \
+    LASX_ILVL_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                        out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave low half of half word elements from vectors
+ * Arguments   : Inputs  - in0_h,  in0_l,  ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low half of half word elements of in_l and right half of
+ *               half word elements of in_h are interleaved and copied to
+ *               out0. Similar for other pairs.
+ * Example     : See LASX_ILVL_W(in_h, in_l, out0)
+ */
+#define LASX_ILVL_H(in_h, in_l, out0)                                      \
+{                                                                          \
+    __m256i tmp0, tmp1;                                                    \
+    tmp0 = __lasx_xvilvl_h(in_h, in_l);                                    \
+    tmp1 = __lasx_xvilvh_h(in_h, in_l);                                    \
+    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
+}
+
+#define LASX_ILVL_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
+{                                                                    \
+    LASX_ILVL_H(in0_h, in0_l, out0)                                  \
+    LASX_ILVL_H(in1_h, in1_l, out1)                                  \
+}
+
+#define LASX_ILVL_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                      in3_h, in3_l, out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_ILVL_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
+    LASX_ILVL_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
+}
+
+#define LASX_ILVL_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                      out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                              \
+    LASX_ILVL_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                  out0, out1, out2, out3);                                     \
+    LASX_ILVL_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                  out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave low half of half word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h,  in0_l,  ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low half of half word elements of in_l and low half of half
+ *               word elements of in_h are interleaved and copied to
+ *               out0. Similar for other pairs.
+ * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
+ */
+#define LASX_ILVL_H_128SV(in_h, in_l, out0)                                   \
+{                                                                             \
+    out0 = __lasx_xvilvl_h(in_h, in_l);                                       \
+}
+
+#define LASX_ILVL_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
+{                                                                             \
+    LASX_ILVL_H_128SV(in0_h, in0_l, out0);                                    \
+    LASX_ILVL_H_128SV(in1_h, in1_l, out1);                                    \
+}
+
+#define LASX_ILVL_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
+                            in3_h, in3_l, out0, out1, out2, out3)             \
+{                                                                             \
+    LASX_ILVL_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
+    LASX_ILVL_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
+}
+
+#define LASX_ILVL_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                                    \
+    LASX_ILVL_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                        out0, out1, out2, out3);                                     \
+    LASX_ILVL_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                        out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave low half of word elements from vectors
+ * Arguments   : Inputs  - in0_h, in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low half of halfword elements of in_l and low half of word
+ *               elements of in_h are interleaved and copied to out0.
+ *               Similar for other pairs
+ * Example     : LASX_ILVL_W(in_h, in_l, out0)
+ *        in_h : 0, 1, 0, 1,  0, 1, 0, 1
+ *        in_l : 1, 2, 3, 4,  5, 6, 7, 8
+ *        out0 : 1, 0, 2, 1,  3, 0, 4, 1
+ */
+#define LASX_ILVL_W(in_h, in_l, out0)                                      \
+{                                                                          \
+    __m256i tmp0, tmp1;                                                    \
+    tmp0 = __lasx_xvilvl_w(in_h, in_l);                                    \
+    tmp1 = __lasx_xvilvh_w(in_h, in_l);                                    \
+    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
+}
+
+#define LASX_ILVL_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
+{                                                                    \
+    LASX_ILVL_W(in0_h, in0_l, out0)                                  \
+    LASX_ILVL_W(in1_h, in1_l, out1)                                  \
+}
+
+#define LASX_ILVL_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                      in3_h, in3_l, out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_ILVL_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
+    LASX_ILVL_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
+}
+
+#define LASX_ILVL_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                      out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                              \
+    LASX_ILVL_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                  out0, out1, out2, out3);                                     \
+    LASX_ILVL_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                  out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave low half of word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h, in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low half of halfword elements of in_l and low half of word
+ *               elements of in_h are interleaved and copied to out0.
+ *               Similar for other pairs
+ * Example     : LASX_ILVL_W_128SV(in_h, in_l, out0)
+ *        in_h : 0, 1, 0, 1, 0, 1, 0, 1
+ *        in_l : 1, 2, 3, 4, 5, 6, 7, 8
+ *        out0 : 1, 0, 2, 1, 5, 0, 6, 1
+ */
+#define LASX_ILVL_W_128SV(in_h, in_l, out0)                             \
+{                                                                       \
+    out0 = __lasx_xvilvl_w(in_h, in_l);                                 \
+}
+
+#define LASX_ILVL_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
+{                                                                             \
+    LASX_ILVL_W_128SV(in0_h, in0_l, out0);                                    \
+    LASX_ILVL_W_128SV(in1_h, in1_l, out1);                                    \
+}
+
+#define LASX_ILVL_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
+                            in3_h, in3_l, out0, out1, out2, out3)             \
+{                                                                             \
+    LASX_ILVL_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
+    LASX_ILVL_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
+}
+
+#define LASX_ILVL_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                                    \
+    LASX_ILVL_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                        out0, out1, out2, out3);                                     \
+    LASX_ILVL_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                        out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave low half of double word elements from vectors
+ * Arguments   : Inputs  - in0_h,  in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Low half of double word elements of in_l and low half of
+ *               double word elements of in_h are interleaved and copied to out0.
+ *               Similar for other pairs
+ * Example     : See LASX_ILVL_W(in_h, in_l, out0)
+ */
+#define LASX_ILVL_D(in_h, in_l, out0)                                   \
+{                                                                       \
+    __m256i tmp0, tmp1;                                                 \
+    tmp0 = __lasx_xvilvl_d(in_h, in_l);                                 \
+    tmp1 = __lasx_xvilvh_d(in_h, in_l);                                 \
+    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
+}
+
+#define LASX_ILVL_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
+{                                                                    \
+    LASX_ILVL_D(in0_h, in0_l, out0)                                  \
+    LASX_ILVL_D(in1_h, in1_l, out1)                                  \
+}
+
+#define LASX_ILVL_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                      in3_h, in3_l, out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_ILVL_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
+    LASX_ILVL_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
+}
+
+#define LASX_ILVL_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                      out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                              \
+    LASX_ILVL_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                  out0, out1, out2, out3);                                     \
+    LASX_ILVL_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                  out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave right half of double word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h,  in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Right half of double word elements of in_l and right half of
+ *               double word elements of in_h are interleaved and copied to out0.
+ *               Similar for other pairs.
+ * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
+ */
+#define LASX_ILVL_D_128SV(in_h, in_l, out0)                              \
+{                                                                        \
+    out0 = __lasx_xvilvl_d(in_h, in_l);                                  \
+}
+
+#define LASX_ILVL_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
+{                                                                             \
+    LASX_ILVL_D_128SV(in0_h, in0_l, out0);                                    \
+    LASX_ILVL_D_128SV(in1_h, in1_l, out1);                                    \
+}
+
+#define LASX_ILVL_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
+                            in3_h, in3_l, out0, out1, out2, out3)             \
+{                                                                             \
+    LASX_ILVL_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
+    LASX_ILVL_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
+}
+
+#define LASX_ILVL_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                                    \
+    LASX_ILVL_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                        out0, out1, out2, out3);                                     \
+    LASX_ILVL_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                        out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave high half of byte elements from vectors
+ * Arguments   : Inputs  - in0_h,  in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : High half of byte elements of in_l and high half of
+ *               byte
+ *               elements of in_h are interleaved and copied to out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVH_W(in_h, in_l, out0)
+ */
+#define LASX_ILVH_B(in_h, in_l, out0)                            \
+{                                                                \
+    __m256i tmp0, tmp1;                                          \
+    tmp0 = __lasx_xvilvl_b(in_h, in_l);                          \
+    tmp1 = __lasx_xvilvh_b(in_h, in_l);                          \
+    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
+}
+
+#define LASX_ILVH_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
+{                                                                    \
+    LASX_ILVH_B(in0_h, in0_l, out0)                                  \
+    LASX_ILVH_B(in1_h, in1_l, out1)                                  \
+}
+
+#define LASX_ILVH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                      in3_h, in3_l, out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_ILVH_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
+    LASX_ILVH_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
+}
+
+#define LASX_ILVH_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                      out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                              \
+    LASX_ILVH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                  out0, out1, out2, out3);                                     \
+    LASX_ILVH_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                  out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave high half of byte elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h,  in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : High half of  byte elements  of  in_l and high half
+ *               of byte elements of in_h are interleaved and copied
+ *               to out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
+ */
+#define LASX_ILVH_B_128SV(in_h, in_l, out0)                     \
+{                                                               \
+    out0 = __lasx_xvilvh_b(in_h, in_l);                         \
+}
+
+#define LASX_ILVH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
+{                                                                             \
+    LASX_ILVH_B_128SV(in0_h, in0_l, out0);                                    \
+    LASX_ILVH_B_128SV(in1_h, in1_l, out1);                                    \
+}
+
+#define LASX_ILVH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
+                            in3_h, in3_l, out0, out1, out2, out3)             \
+{                                                                             \
+    LASX_ILVH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
+    LASX_ILVH_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
+}
+
+#define LASX_ILVH_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                                    \
+    LASX_ILVH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                        out0, out1, out2, out3);                                     \
+    LASX_ILVH_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                        out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave high half of half word elements from vectors
+ * Arguments   : Inputs  - in0_h,  in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : High half of half word elements of in_l and high half of
+ *               half word
+ *               elements of in_h are interleaved and copied to out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVH_W(in_h, in_l, out0)
+ */
+#define LASX_ILVH_H(in_h, in_l, out0)                           \
+{                                                                \
+    __m256i tmp0, tmp1;                                          \
+    tmp0 = __lasx_xvilvl_h(in_h, in_l);                          \
+    tmp1 = __lasx_xvilvh_h(in_h, in_l);                          \
+    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
+}
+
+#define LASX_ILVH_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
+{                                                                    \
+    LASX_ILVH_H(in0_h, in0_l, out0)                                  \
+    LASX_ILVH_H(in1_h, in1_l, out1)                                  \
+}
+
+#define LASX_ILVH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                      in3_h, in3_l, out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_ILVH_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
+    LASX_ILVH_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
+}
+
+#define LASX_ILVH_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                      out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                              \
+    LASX_ILVH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                  out0, out1, out2, out3);                                     \
+    LASX_ILVH_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                  out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave high half of half word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h,  in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : High half of  half word elements  of  in_l and high half
+ *               of half word elements of in_h are interleaved and copied
+ *               to out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
+ */
+#define LASX_ILVH_H_128SV(in_h, in_l, out0)                     \
+{                                                               \
+    out0 = __lasx_xvilvh_h(in_h, in_l);                         \
+}
+
+#define LASX_ILVH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
+{                                                                             \
+    LASX_ILVH_H_128SV(in0_h, in0_l, out0);                                    \
+    LASX_ILVH_H_128SV(in1_h, in1_l, out1);                                    \
+}
+
+#define LASX_ILVH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
+                            in3_h, in3_l, out0, out1, out2, out3)             \
+{                                                                             \
+    LASX_ILVH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
+    LASX_ILVH_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
+}
+
+#define LASX_ILVH_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                                    \
+    LASX_ILVH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                        out0, out1, out2, out3);                                     \
+    LASX_ILVH_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                        out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave high half of word elements from vectors
+ * Arguments   : Inputs  - in0_h, in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : High half of word elements of in_l and high half of
+ *               word elements of in_h are interleaved and copied to
+ *               out0.
+ *               Similar for other pairs.
+ * Example     : LASX_ILVH_W(in_h, in_l, out0)
+ *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
+ *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
+ *         out0: 5, -5,  6, -6,  7, -7,  8, -8
+ */
+#define LASX_ILVH_W(in_h, in_l, out0)                            \
+{                                                                \
+    __m256i tmp0, tmp1;                                          \
+    tmp0 = __lasx_xvilvl_w(in_h, in_l);                          \
+    tmp1 = __lasx_xvilvh_w(in_h, in_l);                          \
+    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
+}
+
+#define LASX_ILVH_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
+{                                                                    \
+    LASX_ILVH_W(in0_h, in0_l, out0)                                  \
+    LASX_ILVH_W(in1_h, in1_l, out1)                                  \
+}
+
+#define LASX_ILVH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                      in3_h, in3_l, out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_ILVH_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
+    LASX_ILVH_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
+}
+
+#define LASX_ILVH_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                      out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                              \
+    LASX_ILVH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                  out0, out1, out2, out3);                                     \
+    LASX_ILVH_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                  out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave high half of word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h, in0_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : High half of word elements of every 128-bit of in_l
+ *               and high half of word elements of every 128-bit of
+ *               in_h are interleaved and copied to out0.
+ *               Similar for other pairs.
+ * Example     : LASX_ILVH_W_128SV(in_h, in_l, out0)
+ *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
+ *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
+ *         out0: 3, -3,  4, -4,  7, -7,  8, -8*
+ */
+#define LASX_ILVH_W_128SV(in_h, in_l, out0)                        \
+{                                                                  \
+    out0 = __lasx_xvilvh_w(in_h, in_l);                            \
+}
+
+#define LASX_ILVH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
+{                                                                             \
+    LASX_ILVH_W_128SV(in0_h, in0_l, out0);                                    \
+    LASX_ILVH_W_128SV(in1_h, in1_l, out1);                                    \
+}
+
+#define LASX_ILVH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
+                            in3_h, in3_l, out0, out1, out2, out3)             \
+{                                                                             \
+    LASX_ILVH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
+    LASX_ILVH_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
+}
+
+#define LASX_ILVH_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                                    \
+    LASX_ILVH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                        out0, out1, out2, out3);                                     \
+    LASX_ILVH_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                        out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave high half of double word elements from vectors
+ * Arguments   : Inputs  - in_h,  in_l,  ~
+ *               Outputs - out0, out1, ~
+ :* Details    : High half of double word elements of in_l and high half of
+ *               double word elements of in_h are interleaved and copied to
+ *               out0.
+ *               Similar for other pairs.
+ * Example    : see LASX_ILVH_W(in_h, in_l, out0)
+ */
+#define LASX_ILVH_D(in_h, in_l, out0)                           \
+{                                                               \
+    __m256i tmp0, tmp1;                                         \
+    tmp0 = __lasx_xvilvl_d(in_h, in_l);                         \
+    tmp1 = __lasx_xvilvh_d(in_h, in_l);                         \
+    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                  \
+}
+
+#define LASX_ILVH_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
+{                                                                    \
+    LASX_ILVH_D(in0_h, in0_l, out0)                                  \
+    LASX_ILVH_D(in1_h, in1_l, out1)                                  \
+}
+
+#define LASX_ILVH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                      in3_h, in3_l, out0, out1, out2, out3)          \
+{                                                                    \
+    LASX_ILVH_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
+    LASX_ILVH_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
+}
+
+#define LASX_ILVH_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                      out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                              \
+    LASX_ILVH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                  out0, out1, out2, out3);                                     \
+    LASX_ILVH_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                  out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave high half of double word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0,  in1,  ~
+ *               Outputs - out0, out1, ~
+ * Details     : High half of double word elements of every 128-bit in_l and
+ *               high half of double word elements of every 128-bit in_h are
+ *               interleaved and copied to out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
+ */
+#define LASX_ILVH_D_128SV(in_h, in_l, out0)                             \
+{                                                                       \
+    out0 = __lasx_xvilvh_d(in_h, in_l);                                 \
+}
+
+#define LASX_ILVH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
+{                                                                             \
+    LASX_ILVH_D_128SV(in0_h, in0_l, out0);                                    \
+    LASX_ILVH_D_128SV(in1_h, in1_l, out1);                                    \
+}
+
+#define LASX_ILVH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
+                            in3_h, in3_l, out0, out1, out2, out3)             \
+{                                                                             \
+    LASX_ILVH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
+    LASX_ILVH_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
+}
+
+#define LASX_ILVH_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
+                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7)          \
+{                                                                                    \
+    LASX_ILVH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
+                        out0, out1, out2, out3);                                     \
+    LASX_ILVH_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
+                        out4, out5, out6, out7);                                     \
+}
+
+/* Description : Interleave byte elements from vectors
+ * Arguments   : Inputs  - in_h,  in_l,  ~
+ *               Outputs - out_h, out_l, ~
+ * Details     : Low half of  byte elements  of in_l and low half of byte
+ *               elements  of in_h  are interleaved  and copied  to  out_l.
+ *               High half of byte elements of in_l and high half of byte
+ *               elements of in_h are interleaved and copied to out_h.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVLH_W(in_h, in_l, out_l, out_h)
+ */
+#define LASX_ILVLH_B(in_h, in_l, out_h, out_l)                          \
+{                                                                       \
+    __m256i tmp0, tmp1;                                                 \
+    tmp0  = __lasx_xvilvl_b(in_h, in_l);                                \
+    tmp1  = __lasx_xvilvh_b(in_h, in_l);                                \
+    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                         \
+    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                         \
+}
+
+#define LASX_ILVLH_B_2(in0_h, in0_l, in1_h, in1_l,                      \
+                       out0_h, out0_l, out1_h, out1_l)                  \
+{                                                                       \
+    LASX_ILVLH_B(in0_h, in0_l, out0_h, out0_l);                         \
+    LASX_ILVLH_B(in1_h, in1_l, out1_h, out1_l);                         \
+}
+
+#define LASX_ILVLH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
+{                                                                                       \
+    LASX_ILVLH_B_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
+    LASX_ILVLH_B_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
+}
+
+#define LASX_ILVLH_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
+                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
+                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
+{                                                                                       \
+    LASX_ILVLH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
+                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
+    LASX_ILVLH_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
+                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
+}
+
+/* Description : Interleave byte elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h,  in_l,  ~
+ *               Outputs - out_h, out_l, ~
+ * Details     : Low half of byte elements of in_l and low half of byte elements
+ *               of in_h are interleaved and copied to out_l. High  half  of byte
+ *               elements  of in_h  and high half  of byte elements  of in_l  are
+ *               interleaved and copied to out_h.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
+ */
+#define LASX_ILVLH_B_128SV(in_h, in_l, out_h, out_l)                           \
+{                                                                              \
+    LASX_ILVL_B_128SV(in_h, in_l, out_l);                                      \
+    LASX_ILVH_B_128SV(in_h, in_l, out_h);                                      \
+}
+
+#define LASX_ILVLH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
+{                                                                                         \
+    LASX_ILVLH_B_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
+    LASX_ILVLH_B_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
+}
+
+#define LASX_ILVLH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
+                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
+{                                                                                              \
+    LASX_ILVLH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
+    LASX_ILVLH_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
+}
+
+#define LASX_ILVLH_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
+                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
+                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
+{                                                                                             \
+    LASX_ILVLH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
+                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
+    LASX_ILVLH_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
+                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
+}
+
+/* Description : Interleave half word elements from vectors
+ * Arguments   : Inputs  - in_h,  in_l,  ~
+ *               Outputs - out_h, out_l, ~
+ * Details     : Low half of  half word elements  of in_l and low half of half
+ *               word elements of in_h  are  interleaved  and  copied  to out_l.
+ *               High half of half word elements of in_l and high half of half
+ *               word elements of in_h are interleaved and copied to out_h.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVLH_W(in_h, in_l, out_h, out_l)
+ */
+#define LASX_ILVLH_H(in_h, in_l, out_h, out_l)                           \
+{                                                                        \
+    __m256i tmp0, tmp1;                                                  \
+    tmp0  = __lasx_xvilvl_h(in_h, in_l);                                 \
+    tmp1  = __lasx_xvilvh_h(in_h, in_l);                                 \
+    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
+    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
+}
+
+#define LASX_ILVLH_H_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
+                       out1_h, out1_l)                                   \
+{                                                                        \
+    LASX_ILVLH_H(in0_h, in0_l, out0_h, out0_l);                          \
+    LASX_ILVLH_H(in1_h, in1_l, out1_h, out1_l);                          \
+}
+
+#define LASX_ILVLH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
+{                                                                                       \
+    LASX_ILVLH_H_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
+    LASX_ILVLH_H_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
+}
+
+#define LASX_ILVLH_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
+                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
+                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
+{                                                                                       \
+    LASX_ILVLH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
+                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
+    LASX_ILVLH_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
+                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
+}
+
+/* Description : Interleave half word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h,  in0_l,  ~
+ *               Outputs - out0_h, out0_l, ~
+ * Details     : Low half of half word elements  of every 128-bit of in_l and
+ *               low half of half word elements  of every 128-bit of in_h are
+ *               interleaved and copied to out_l.
+ *               High half of half word elements of every 128-bit of in_l and
+ *               high half of half word elements of every 128-bit of in_h are
+ *               interleaved and copied to out_h.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
+ */
+#define LASX_ILVLH_H_128SV(in_h, in_l, out_h, out_l)                            \
+{                                                                               \
+    LASX_ILVL_H_128SV(in_h, in_l, out_l);                                       \
+    LASX_ILVH_H_128SV(in_h, in_l, out_h);                                       \
+}
+
+#define LASX_ILVLH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
+{                                                                                         \
+    LASX_ILVLH_H_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
+    LASX_ILVLH_H_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
+}
+
+#define LASX_ILVLH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
+                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
+{                                                                                              \
+    LASX_ILVLH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
+    LASX_ILVLH_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
+}
+
+#define LASX_ILVLH_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
+                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
+                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
+{                                                                                             \
+    LASX_ILVLH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
+                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
+    LASX_ILVLH_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
+                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
+}
+
+/* Description : Interleave word elements from vectors
+ * Arguments   : Inputs  - in_h,  in_l,  ~
+ *               Outputs - out_h, out_l, ~
+ * Details     : Low half of  word elements  of in_l and low half of word
+ *               elements of in_h  are  interleaved  and  copied  to out_l.
+ *               High half of word elements of in_l and high half of word
+ *               elements of in_h are interleaved and copied to out_h.
+ *               Similar for other pairs.
+ * Example     : LASX_ILVLH_W(in_h, in_l, out_h, out_l)
+ *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
+ *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
+ *        out_h: 5, -5,  6, -6,  7, -7,  8, -8
+ *        out_l: 1, -1,  2, -2,  3, -3,  4, -4
+ */
+#define LASX_ILVLH_W(in_h, in_l, out_h, out_l)                           \
+{                                                                        \
+    __m256i tmp0, tmp1;                                                  \
+    tmp0  = __lasx_xvilvl_w(in_h, in_l);                                 \
+    tmp1  = __lasx_xvilvh_w(in_h, in_l);                                 \
+    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
+    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
+}
+
+#define LASX_ILVLH_W_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
+                       out1_h, out1_l)                                   \
+{                                                                        \
+    LASX_ILVLH_W(in0_h, in0_l, out0_h, out0_l);                          \
+    LASX_ILVLH_W(in1_h, in1_l, out1_h, out1_l);                          \
+}
+
+#define LASX_ILVLH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
+{                                                                                       \
+    LASX_ILVLH_W_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
+    LASX_ILVLH_W_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
+}
+
+#define LASX_ILVLH_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
+                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
+                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
+{                                                                                       \
+    LASX_ILVLH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
+                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
+    LASX_ILVLH_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
+                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
+}
+
+/* Description : Interleave word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in0_h,  in0_l,  ~
+ *               Outputs - out0_h, out0_l, ~
+ * Details     : Low half of word elements  of every 128-bit of in_l and
+ *               low half of word elements  of every 128-bit of in_h are
+ *               interleaved and copied to out_l.
+ *               High half of word elements of every 128-bit of in_l and
+ *               high half of word elements of every 128-bit of in_h are
+ *               interleaved and copied to out_h.
+ *               Similar for other pairs.
+ * Example     : LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
+ *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
+ *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
+ *        out_h: 3, -3,  4, -4,  7, -7,  8, -8
+ *        out_l: 1, -1,  2, -2,  5, -5,  6, -6
+ */
+#define LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)                            \
+{                                                                               \
+    LASX_ILVL_W_128SV(in_h, in_l, out_l);                                       \
+    LASX_ILVH_W_128SV(in_h, in_l, out_h);                                       \
+}
+
+#define LASX_ILVLH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
+{                                                                                         \
+    LASX_ILVLH_W_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
+    LASX_ILVLH_W_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
+}
+
+#define LASX_ILVLH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
+                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
+{                                                                                              \
+    LASX_ILVLH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
+    LASX_ILVLH_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
+}
+
+#define LASX_ILVLH_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
+                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
+                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
+{                                                                                             \
+    LASX_ILVLH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
+                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
+    LASX_ILVLH_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
+                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
+}
+
+/* Description : Interleave double word elements from vectors
+ * Arguments   : Inputs  - in_h,  in_l,  ~
+ *               Outputs - out_h, out_l, ~
+ * Details     : Low half of double word  elements  of in_l and low half of
+ *               double word elements of in_h are interleaved and copied to
+ *               out_l. High half of double word  elements  of in_l and high
+ *               half of double word  elements  of in_h are interleaved and
+ *               copied to out_h.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVLH_W(in_h, in_l, out_h, out_l)
+ */
+#define LASX_ILVLH_D(in_h, in_l, out_h, out_l)                           \
+{                                                                        \
+    __m256i tmp0, tmp1;                                                  \
+    tmp0  = __lasx_xvilvl_d(in_h, in_l);                                 \
+    tmp1  = __lasx_xvilvh_d(in_h, in_l);                                 \
+    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
+    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
+}
+
+#define LASX_ILVLH_D_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
+                       out1_h, out1_l)                                   \
+{                                                                        \
+    LASX_ILVLH_D(in0_h, in0_l, out0_h, out0_l);                          \
+    LASX_ILVLH_D(in1_h, in1_l, out1_h, out1_l);                          \
+}
+
+#define LASX_ILVLH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
+{                                                                                       \
+    LASX_ILVLH_D_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
+    LASX_ILVLH_D_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
+}
+
+#define LASX_ILVLH_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
+                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
+                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
+{                                                                                       \
+    LASX_ILVLH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
+                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
+    LASX_ILVLH_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
+                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
+}
+
+/* Description : Interleave double word elements from vectors
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h,  in_l,  ~
+ *               Outputs - out_h, out_l, ~
+ * Details     : Low half of double word elements of every 128-bit  of in_l and
+ *               low half of double word elements of every 128-bit  of in_h are
+ *               interleaved and copied to out_l.
+ *               High half of double word elements of every 128-bit of in_l and
+ *               high half of double word elements of every 128-bit of in_h are
+ *               interleaved and copied to out_h.
+ *               Similar for other pairs.
+ * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
+ */
+#define LASX_ILVLH_D_128SV(in_h, in_l, out_h, out_l)                            \
+{                                                                               \
+    LASX_ILVL_D_128SV(in_h, in_l, out_l);                                       \
+    LASX_ILVH_D_128SV(in_h, in_l, out_h);                                       \
+}
+
+#define LASX_ILVLH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
+{                                                                                         \
+    LASX_ILVLH_D_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
+    LASX_ILVLH_D_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
+}
+
+#define LASX_ILVLH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
+                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
+{                                                                                              \
+    LASX_ILVLH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
+    LASX_ILVLH_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
+}
+
+#define LASX_ILVLH_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
+                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
+                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
+                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
+{                                                                                             \
+    LASX_ILVLH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
+                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
+    LASX_ILVLH_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
+                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
+}
+
+/* Description : Immediate number of columns to slide with zero
+ * Arguments   : Inputs  - in0, in1, slide_val, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Byte elements from every 128-bit of in0 vector
+ *               are slide into  out0  by  number  of  elements
+ *               specified by slide_val.
+ * Example     : LASX_SLDI_B_0_128SV(in0, out0, slide_val)
+ *          in0: 1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,
+ *               19,20,21,22,23,24,25,26,27,28,29,30,31,32
+ *         out0: 4, 5,6,7,8,9,10,11,12,13,14,15,16,0,0,0,20,21,
+ *               22,23,24,25,26,27,28,29,30,31,32,0,0,0
+ *    slide_val: 3
+ */
+#define LASX_SLDI_B_0_128SV(in0, out0, slide_val)                   \
+{                                                                   \
+    out0 = __lasx_xvbsrl_v(in0, slide_val);                         \
+}
+
+#define LASX_SLDI_B_2_0_128SV(in0, in1, out0, out1, slide_val)      \
+{                                                                   \
+    LASX_SLDI_B_0_128SV(in0, out0, slide_val);                      \
+    LASX_SLDI_B_0_128SV(in1, out1, slide_val);                      \
+}
+
+#define LASX_SLDI_B_4_0_128SV(in0, in1, in2, in3,                   \
+                              out0, out1, out2, out3, slide_val)    \
+{                                                                   \
+    LASX_SLDI_B_2_0_128SV(in0, in1, out0, out1, slide_val);         \
+    LASX_SLDI_B_2_0_128SV(in2, in3, out2, out3, slide_val);         \
+}
+
+/* Description : Pack even byte elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even byte elements of in_l are copied to the low half of
+ *               out0.  Even byte elements of in_h are copied to the high
+ *               half of out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
+ */
+#define LASX_PCKEV_B(in_h, in_l, out0)                                  \
+{                                                                       \
+    out0 = __lasx_xvpickev_b(in_h, in_l);                               \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                                \
+}
+
+#define LASX_PCKEV_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                       \
+    LASX_PCKEV_B(in0_h, in0_l, out0);                                   \
+    LASX_PCKEV_B(in1_h, in1_l, out1);                                   \
+}
+
+#define LASX_PCKEV_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                       in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                       \
+    LASX_PCKEV_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
+    LASX_PCKEV_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
+}
+
+#define LASX_PCKEV_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    LASX_PCKEV_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
+                   in3_h, in3_l, out0, out1, out2, out3);                      \
+    LASX_PCKEV_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
+                   in7_h, in7_l, out4, out5, out6, out7);                      \
+}
+
+/* Description : Pack even byte elements of vector pairs
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even byte elements of in_l are copied to the low half of
+ *               out0.  Even byte elements of in_h are copied to the high
+ *               half of out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_PCKEV_W_128SV(in_h, in_l, out0)
+ */
+#define LASX_PCKEV_B_128SV(in_h, in_l, out0)                            \
+{                                                                       \
+    out0 = __lasx_xvpickev_b(in_h, in_l);                               \
+}
+
+#define LASX_PCKEV_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)    \
+{                                                                       \
+    LASX_PCKEV_B_128SV(in0_h, in0_l, out0);                             \
+    LASX_PCKEV_B_128SV(in1_h, in1_l, out1);                             \
+}
+
+#define LASX_PCKEV_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
+                             in3_h, in3_l, out0, out1, out2, out3)      \
+{                                                                       \
+    LASX_PCKEV_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);       \
+    LASX_PCKEV_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);       \
+}
+
+#define LASX_PCKEV_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
+                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l,  \
+                             in6_h, in6_l, in7_h, in7_l, out0, out1,    \
+                             out2, out3, out4, out5, out6, out7)        \
+{                                                                       \
+    LASX_PCKEV_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                         in3_h, in3_l, out0, out1, out2, out3);         \
+    LASX_PCKEV_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,      \
+                         in7_h, in7_l, out4, out5, out6, out7);         \
+}
+
+/* Description : Pack even half word elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even half word elements of in_l are copied to the  low
+ *               half of out0.  Even  half  word  elements  of in_h are
+ *               copied to the high half of out0.
+ * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
+ */
+#define LASX_PCKEV_H(in_h, in_l, out0)                                 \
+{                                                                      \
+    out0 = __lasx_xvpickev_h(in_h, in_l);                              \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                               \
+}
+
+#define LASX_PCKEV_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                       \
+    LASX_PCKEV_H(in0_h, in0_l, out0);                                   \
+    LASX_PCKEV_H(in1_h, in1_l, out1);                                   \
+}
+
+#define LASX_PCKEV_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                       in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                       \
+    LASX_PCKEV_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
+    LASX_PCKEV_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
+}
+
+#define LASX_PCKEV_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    LASX_PCKEV_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
+                   in3_h, in3_l, out0, out1, out2, out3);                      \
+    LASX_PCKEV_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
+                   in7_h, in7_l, out4, out5, out6, out7);                      \
+}
+
+/* Description : Pack even half word elements of vector pairs
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even half word elements of in_l are copied to the  low
+ *               half of out0.  Even  half  word  elements  of in_h are
+ *               copied to the high half of out0.
+ * Example     : see LASX_PCKEV_W_128SV(in_h, in_l, out0)
+ */
+#define LASX_PCKEV_H_128SV(in_h, in_l, out0)                            \
+{                                                                       \
+    out0 = __lasx_xvpickev_h(in_h, in_l);                               \
+}
+
+#define LASX_PCKEV_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)    \
+{                                                                       \
+    LASX_PCKEV_H_128SV(in0_h, in0_l, out0);                             \
+    LASX_PCKEV_H_128SV(in1_h, in1_l, out1);                             \
+}
+
+#define LASX_PCKEV_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
+                       in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                       \
+    LASX_PCKEV_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);       \
+    LASX_PCKEV_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);       \
+}
+
+#define LASX_PCKEV_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
+                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l,  \
+                             in6_h, in6_l, in7_h, in7_l, out0, out1,    \
+                             out2, out3, out4, out5, out6, out7)        \
+{                                                                       \
+    LASX_PCKEV_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
+                   in3_h, in3_l, out0, out1, out2, out3);               \
+    LASX_PCKEV_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,      \
+                   in7_h, in7_l, out4, out5, out6, out7);               \
+}
+
+/* Description : Pack even word elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even word  elements  of  in_l are copied to
+ *               the low  half of out0.  Even word elements
+ *               of in_h are copied to the high half of out0.
+ * Example     : LASX_PCKEV_W(in_h, in_l, out0)
+ *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
+ *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
+ *         out0:  1,  3,  5,  7, -1, -3, -5, -7
+ */
+#define LASX_PCKEV_W(in_h, in_l, out0)                    \
+{                                                         \
+    out0 = __lasx_xvpickev_w(in_h, in_l);                 \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                  \
+}
+
+#define LASX_PCKEV_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                       \
+    LASX_PCKEV_W(in0_h, in0_l, out0);                                   \
+    LASX_PCKEV_W(in1_h, in1_l, out1);                                   \
+}
+
+#define LASX_PCKEV_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                       in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                       \
+    LASX_PCKEV_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
+    LASX_PCKEV_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
+}
+
+#define LASX_PCKEV_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    LASX_PCKEV_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
+                   in3_h, in3_l, out0, out1, out2, out3);                      \
+    LASX_PCKEV_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
+                   in7_h, in7_l, out4, out5, out6, out7);                      \
+}
+
+/* Description : Pack even word elements of vector pairs
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even word  elements  of  in_l are copied to
+ *               the low  half of out0.  Even word elements
+ *               of in_h are copied to the high half of out0.
+ * Example     : LASX_PCKEV_W_128SV(in_h, in_l, out0)
+ *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
+ *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
+ *         out0:  1,  3, -1, -3,  5,  7, -5, -7
+ */
+#define LASX_PCKEV_W_128SV(in_h, in_l, out0)                           \
+{                                                                      \
+    out0 = __lasx_xvpickev_w(in_h, in_l);                              \
+}
+
+#define LASX_PCKEV_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)   \
+{                                                                      \
+    LASX_PCKEV_W_128SV(in0_h, in0_l, out0);                            \
+    LASX_PCKEV_W_128SV(in1_h, in1_l, out1);                            \
+}
+
+#define LASX_PCKEV_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, \
+                             in3_h, in3_l, out0, out1, out2, out3)     \
+{                                                                      \
+    LASX_PCKEV_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);      \
+    LASX_PCKEV_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);      \
+}
+
+#define LASX_PCKEV_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, \
+                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l, \
+                             in6_h, in6_l, in7_h, in7_l, out0, out1,   \
+                             out2, out3, out4, out5, out6, out7)       \
+{                                                                      \
+    LASX_PCKEV_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,     \
+                         in3_h, in3_l, out0, out1, out2, out3);        \
+    LASX_PCKEV_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,     \
+                         in7_h, in7_l, out4, out5, out6, out7);        \
+}
+
+/* Description : Pack even half word elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even half word elements of in_l are copied to the  low
+ *               half of out0.  Even  half  word  elements  of in_h are
+ *               copied to the high half of out0.
+ * Example     : See LASX_PCKEV_W(in_h, in_l, out0)
+ */
+#define LASX_PCKEV_D(in_h, in_l, out0)                                        \
+{                                                                             \
+    out0 = __lasx_xvpickev_d(in_h, in_l);                                     \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                                      \
+}
+
+#define LASX_PCKEV_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                \
+{                                                                             \
+    LASX_PCKEV_D(in0_h, in0_l, out0)                                          \
+    LASX_PCKEV_D(in1_h, in1_l, out1)                                          \
+}
+
+#define LASX_PCKEV_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,              \
+                       in3_h, in3_l, out0, out1, out2, out3)                  \
+{                                                                             \
+    LASX_PCKEV_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                    \
+    LASX_PCKEV_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)                    \
+}
+
+/* Description : Pack even half word elements of vector pairs
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even half word elements of in_l are copied to the  low
+ *               half of out0.  Even  half  word  elements  of in_h are
+ *               copied to the high half of out0.
+ * Example     : LASX_PCKEV_D_128SV(in_h, in_l, out0)
+ *        in_h : 1, 2, 3, 4
+ *        in_l : 5, 6, 7, 8
+ *        out0 : 5, 1, 7, 3
+ */
+#define LASX_PCKEV_D_128SV(in_h, in_l, out0)                                  \
+{                                                                             \
+    out0 = __lasx_xvpickev_d(in_h, in_l);                                     \
+}
+
+#define LASX_PCKEV_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                             \
+    LASX_PCKEV_D_128SV(in0_h, in0_l, out0)                                    \
+    LASX_PCKEV_D_128SV(in1_h, in1_l, out1)                                    \
+}
+
+#define LASX_PCKEV_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                             in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                             \
+    LASX_PCKEV_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
+    LASX_PCKEV_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
+}
+
+/* Description : Pack even quad word elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Even quad elements of in_l are copied to the low
+ *               half of out0. Even  quad  elements  of  in_h are
+ *               copied to the high half of out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
+ */
+#define LASX_PCKEV_Q(in_h, in_l, out0)                          \
+{                                                               \
+    out0 = __lasx_xvpermi_q(in_h, in_l, 0x20);                  \
+}
+
+#define LASX_PCKEV_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                       \
+    LASX_PCKEV_Q(in0_h, in0_l, out0);                                   \
+    LASX_PCKEV_Q(in1_h, in1_l, out1);                                   \
+}
+
+#define LASX_PCKEV_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                       in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                       \
+    LASX_PCKEV_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
+    LASX_PCKEV_Q_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
+}
+
+#define LASX_PCKEV_Q_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    LASX_PCKEV_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
+                   in3_h, in3_l, out0, out1, out2, out3);                      \
+    LASX_PCKEV_Q_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
+                   in7_h, in7_l, out4, out5, out6, out7);                      \
+}
+
+/* Description : Pack odd byte elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd byte elements of in_l are copied to the low half of
+ *               out0. Odd byte elements of in_h are copied to the high
+ *               half of out0.
+ *               Similar for other pairs.
+ * Example     : see LASX_PCKOD_W(in_h, in_l, out0)
+ */
+#define LASX_PCKOD_B(in_h, in_l, out0)                                         \
+{                                                                              \
+    out0 = __lasx_xvpickod_b(in_h, in_l);                                      \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
+}
+
+#define LASX_PCKOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
+{                                                                              \
+    LASX_PCKOD_B(in0_h, in0_l, out0);                                          \
+    LASX_PCKOD_B(in1_h, in1_l, out1);                                          \
+}
+
+#define LASX_PCKOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
+                       in3_h, in3_l, out0, out1, out2, out3)                   \
+{                                                                              \
+    LASX_PCKOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
+    LASX_PCKOD_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
+}
+
+#define LASX_PCKOD_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    LASX_PCKOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
+                   in3_h, in3_l, out0, out1, out2, out3);                      \
+    LASX_PCKOD_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
+                   in7_h, in7_l, out4, out5, out6, out7);                      \
+}
+
+/* Description : Pack odd half word elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd half word elements of in_l are copied to the low
+ *               half of out0. Odd half word elements of in_h are copied
+ *               to the high half of out0.
+ * Example     : see LASX_PCKOD_W(in_h, in_l, out0)
+ */
+#define LASX_PCKOD_H(in_h, in_l, out0)                                         \
+{                                                                              \
+    out0 = __lasx_xvpickod_h(in_h, in_l);                                      \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
+}
+
+#define LASX_PCKOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
+{                                                                              \
+    LASX_PCKOD_H(in0_h, in0_l, out0);                                          \
+    LASX_PCKOD_H(in1_h, in1_l, out1);                                          \
+}
+
+#define LASX_PCKOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
+                       in3_h, in3_l, out0, out1, out2, out3)                   \
+{                                                                              \
+    LASX_PCKOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
+    LASX_PCKOD_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
+}
+
+#define LASX_PCKOD_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    LASX_PCKOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
+                   in3_h, in3_l, out0, out1, out2, out3);                      \
+    LASX_PCKOD_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
+                   in7_h, in7_l, out4, out5, out6, out7);                      \
+}
+
+/* Description : Pack odd word elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd word elements of in_l are copied to the low half of out0.
+ *               Odd word elements of in_h are copied to the high half of out0.
+ * Example     : LASX_PCKOD_W(in_h, in_l, out0)
+ *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
+ *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
+ *         out0:  2,  4,  6,  8, -2, -4, -6, -8
+ */
+#define LASX_PCKOD_W(in_h, in_l, out0)                                         \
+{                                                                              \
+    out0 = __lasx_xvpickod_w(in_h, in_l);                                      \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
+}
+
+#define LASX_PCKOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
+{                                                                              \
+    LASX_PCKOD_W(in0_h, in0_l, out0);                                          \
+    LASX_PCKOD_W(in1_h, in1_l, out1);                                          \
+}
+
+#define LASX_PCKOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
+                       in3_h, in3_l, out0, out1, out2, out3)                   \
+{                                                                              \
+    LASX_PCKOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
+    LASX_PCKOD_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
+}
+
+#define LASX_PCKOD_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    LASX_PCKOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
+                   in3_h, in3_l, out0, out1, out2, out3);                      \
+    LASX_PCKOD_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
+                   in7_h, in7_l, out4, out5, out6, out7);                      \
+}
+
+/* Description : Pack odd half word elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd half word elements of in_l are copied to the low
+ *               half of out0. Odd half word elements of in_h are
+ *               copied to the high half of out0.
+ * Example     : See LASX_PCKOD_W(in_h, in_l, out0)
+ */
+#define LASX_PCKOD_D(in_h, in_l, out0)                                        \
+{                                                                             \
+    out0 = __lasx_xvpickod_d(in_h, in_l);                                     \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                                      \
+}
+
+#define LASX_PCKOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                \
+{                                                                             \
+    LASX_PCKOD_D(in0_h, in0_l, out0)                                          \
+    LASX_PCKOD_D(in1_h, in1_l, out1)                                          \
+}
+
+#define LASX_PCKOD_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,              \
+                       in3_h, in3_l, out0, out1, out2, out3)                  \
+{                                                                             \
+    LASX_PCKOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                    \
+    LASX_PCKOD_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)                    \
+}
+
+/* Description : Pack odd quad word elements of vector pairs
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd quad elements of in0_h are copied to the high half of
+ *               out0 & odd quad elements of in0_l are copied to the low
+ *               half of out0.
+ *               Odd quad elements of in1_h are copied to the high half of
+ *               out1 & odd quad elements of in1_l are copied to the low
+ *               half of out1.
+ *               LASX_PCKOD_Q(in_h, in_l, out0)
+ *               in_h:   0,0,0,0, 0,0,0,0, 19,10,11,12, 13,14,15,16
+ *               in_l:   0,0,0,0, 0,0,0,0, 1,2,3,4, 5,6,7,8
+ *               out0:  1,2,3,4, 5,6,7,8, 19,10,11,12, 13,14,15,16
+ */
+#define LASX_PCKOD_Q(in_h, in_l, out0)                                         \
+{                                                                              \
+    out0 = __lasx_xvpermi_q(in_h, in_l, 0x31);                                 \
+}
+
+#define LASX_PCKOD_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
+{                                                                              \
+    LASX_PCKOD_Q(in0_h, in0_l, out0);                                          \
+    LASX_PCKOD_Q(in1_h, in1_l, out1);                                          \
+}
+
+#define LASX_PCKOD_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
+                       in3_h, in3_l, out0, out1, out2, out3)                   \
+{                                                                              \
+    LASX_PCKOD_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
+    LASX_PCKOD_Q_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
+}
+
+#define LASX_PCKOD_Q_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
+                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    LASX_PCKOD_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
+                   in3_h, in3_l, out0, out1, out2, out3);                      \
+    LASX_PCKOD_Q_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
+                   in7_h, in7_l, out4, out5, out6, out7);                      \
+}
+
+/* Description : Pack odd byte elements of vector pairsi
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd byte elements of in_l are copied to the low
+ *               half of out0 of . Odd byte elements of in_h are
+ *               copied to the high half of out0.
+ * Example     : See LASX_PCKOD_D_128SV(in_h, in_l, out0)
+ */
+#define LASX_PCKOD_B_128SV(in_h, in_l, out0)                                  \
+{                                                                             \
+    out0 = __lasx_xvpickod_b(in_h, in_l);                                     \
+}
+
+#define LASX_PCKOD_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                             \
+    LASX_PCKOD_B_128SV(in0_h, in0_l, out0)                                    \
+    LASX_PCKOD_B_128SV(in1_h, in1_l, out1)                                    \
+}
+
+#define LASX_PCKOD_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                             in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                             \
+    LASX_PCKOD_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
+    LASX_PCKOD_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
+}
+
+/* Description : Pack odd half word elements of vector pairsi
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd half word elements of in_l are copied to the low
+ *               half of out0 of . Odd half word elements of in_h are
+ *               copied to the high half of out0.
+ * Example     : See LASX_PCKOD_D_128SV(in_h, in_l, out0)
+ */
+#define LASX_PCKOD_H_128SV(in_h, in_l, out0)                                  \
+{                                                                             \
+    out0 = __lasx_xvpickod_h(in_h, in_l);                                     \
+}
+
+#define LASX_PCKOD_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                             \
+    LASX_PCKOD_H_128SV(in0_h, in0_l, out0)                                    \
+    LASX_PCKOD_H_128SV(in1_h, in1_l, out1)                                    \
+}
+
+#define LASX_PCKOD_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                             in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                             \
+    LASX_PCKOD_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
+    LASX_PCKOD_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
+}
+
+/* Description : Pack odd word elements of vector pairsi
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd word elements of in_l are copied to the low
+ *               half of out0 of . Odd word elements of in_h are
+ *               copied to the high half of out0.
+ * Example     : See LASX_PCKOD_D_128SV(in_h, in_l, out0)
+ */
+#define LASX_PCKOD_W_128SV(in_h, in_l, out0)                                  \
+{                                                                             \
+    out0 = __lasx_xvpickod_w(in_h, in_l);                                     \
+}
+
+#define LASX_PCKOD_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                             \
+    LASX_PCKOD_W_128SV(in0_h, in0_l, out0)                                    \
+    LASX_PCKOD_W_128SV(in1_h, in1_l, out1)                                    \
+}
+
+#define LASX_PCKOD_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                             in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                             \
+    LASX_PCKOD_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
+    LASX_PCKOD_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
+}
+
+/* Description : Pack odd half word elements of vector pairsi
+ *               (128-bit symmetry version)
+ * Arguments   : Inputs  - in_h, in_l, ~
+ *               Outputs - out0, out1, ~
+ * Details     : Odd half word elements of in_l are copied to the low
+ *               half of out0 of . Odd half word elements of in_h are
+ *               copied to the high half of out0.
+ * Example     : LASX_PCKOD_D_128SV(in_h, in_l, out0)
+ *        in_h : 1, 2, 3, 4
+ *        in_l : 5, 6, 7, 8
+ *        out0 : 6, 2, 8, 4
+ */
+#define LASX_PCKOD_D_128SV(in_h, in_l, out0)                                  \
+{                                                                             \
+    out0 = __lasx_xvpickod_d(in_h, in_l);                                     \
+}
+
+#define LASX_PCKOD_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
+{                                                                             \
+    LASX_PCKOD_D_128SV(in0_h, in0_l, out0)                                    \
+    LASX_PCKOD_D_128SV(in1_h, in1_l, out1)                                    \
+}
+
+#define LASX_PCKOD_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
+                             in3_h, in3_l, out0, out1, out2, out3)            \
+{                                                                             \
+    LASX_PCKOD_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
+    LASX_PCKOD_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
+}
+
+/* Description : Transposes 8x8 block with half word elements in vectors.
+ * Arguments   : Inputs  - in0, in1, ~
+ *               Outputs - out0, out1, ~
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LASX_TRANSPOSE8x8_H_128SV
+ *         in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *         in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *         in2 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *         in3 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *         in4 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *         in5 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *         in6 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *         in7 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *
+ *        out0 : 1,8,8,1, 9,1,1,9, 1,8,8,1, 9,1,1,9
+ *        out1 : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ *        out2 : 3,3,3,3, 3,3,3,3, 3,3,3,3, 3,3,3,3
+ *        out3 : 4,4,4,4, 4,4,4,4, 4,4,4,4, 4,4,4,4
+ *        out4 : 5,5,5,5, 5,5,5,5, 5,5,5,5, 5,5,5,5
+ *        out5 : 6,6,6,6, 6,6,6,6, 6,6,6,6, 6,6,6,6
+ *        out6 : 7,7,7,7, 7,7,7,7, 7,7,7,7, 7,7,7,7
+ *        out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
+ */
+#define LASX_TRANSPOSE8x8_H_128SV(in0, in1, in2, in3, in4, in5, in6, in7,           \
+                                  out0, out1, out2, out3, out4, out5, out6, out7)   \
+{                                                                                   \
+    __m256i s0_m, s1_m;                                                             \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                         \
+    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                         \
+                                                                                    \
+    LASX_ILVL_H_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                            \
+    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp1_m, tmp0_m);                                 \
+    LASX_ILVH_H_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                            \
+    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp3_m, tmp2_m);                                 \
+                                                                                    \
+    LASX_ILVL_H_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                            \
+    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp5_m, tmp4_m);                                 \
+    LASX_ILVH_H_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                            \
+    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp7_m, tmp6_m);                                 \
+                                                                                    \
+    LASX_PCKEV_D_4_128SV(tmp0_m, tmp4_m, tmp1_m, tmp5_m, tmp2_m, tmp6_m,            \
+                         tmp3_m, tmp7_m, out0, out2, out4, out6);                   \
+    LASX_PCKOD_D_4_128SV(tmp0_m, tmp4_m, tmp1_m, tmp5_m, tmp2_m, tmp6_m,            \
+                         tmp3_m, tmp7_m, out1, out3, out5, out7);                   \
+}
+
+/* Description : Transposes 8x8 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ */
+#define LASX_TRANSPOSE8x8_W(in0, in1, in2, in3, in4, in5, in6, in7,         \
+                            out0, out1, out2, out3, out4, out5, out6, out7) \
+{                                                                           \
+    __m256i s0_m, s1_m;                                                     \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
+    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                 \
+                                                                            \
+    LASX_ILVL_W_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                    \
+    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp1_m, tmp0_m);                         \
+    LASX_ILVH_W_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                    \
+    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp3_m, tmp2_m);                         \
+                                                                            \
+    LASX_ILVL_W_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                    \
+    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp5_m, tmp4_m);                         \
+    LASX_ILVH_W_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                    \
+    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp7_m, tmp6_m);                         \
+    LASX_PCKEV_Q_4(tmp4_m, tmp0_m, tmp5_m, tmp1_m, tmp6_m, tmp2_m,          \
+                   tmp7_m, tmp3_m, out0, out1, out2, out3);                 \
+    LASX_PCKOD_Q_4(tmp4_m, tmp0_m, tmp5_m, tmp1_m, tmp6_m, tmp2_m,          \
+                   tmp7_m, tmp3_m, out4, out5, out6, out7);                 \
+}
+
+/* Description : Transposes 2x2 block with quad word elements in vectors
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ * Details     :
+ */
+#define LASX_TRANSPOSE2x2_Q(in0, in1, out0, out1) \
+{                                                 \
+    __m256i tmp0;                                 \
+    tmp0 = __lasx_xvpermi_q(in1, in0, 0x02);      \
+    out1 = __lasx_xvpermi_q(in1, in0, 0x13);      \
+    out0 = tmp0;                                  \
+}
+
+/* Description : Transposes 4x4 block with double word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     :
+ */
+#define LASX_TRANSPOSE4x4_D(in0, in1, in2, in3, out0, out1, out2, out3) \
+{                                                                       \
+    __m256i tmp0, tmp1, tmp2, tmp3;                                     \
+    LASX_ILVLH_D_2_128SV(in1, in0, in3, in2, tmp0, tmp1, tmp2, tmp3);   \
+    out0 = __lasx_xvpermi_q(tmp2, tmp0, 0x20);                          \
+    out2 = __lasx_xvpermi_q(tmp2, tmp0, 0x31);                          \
+    out1 = __lasx_xvpermi_q(tmp3, tmp1, 0x20);                          \
+    out3 = __lasx_xvpermi_q(tmp3, tmp1, 0x31);                          \
+}
+
+/* Description : Transpose 4x4 block with half word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ *               Return Type - signed halfword
+ */
+#define LASX_TRANSPOSE4x4_H_128SV(in0, in1, in2, in3, out0, out1, out2, out3) \
+{                                                                             \
+    __m256i s0_m, s1_m;                                                       \
+                                                                              \
+    LASX_ILVL_H_2_128SV(in1, in0, in3, in2, s0_m, s1_m);                      \
+    LASX_ILVLH_W_128SV(s1_m, s0_m, out2, out0);                               \
+    out1 = __lasx_xvilvh_d(out0, out0);                                       \
+    out3 = __lasx_xvilvh_d(out2, out2);                                       \
+}
+
+/* Description : Transposes input 8x8 byte block
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *                         (input 8x8 byte block)
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ *                         (output 8x8 byte block)
+ * Details     :
+ */
+#define LASX_TRANSPOSE8x8_B(in0, in1, in2, in3, in4, in5, in6, in7,         \
+                            out0, out1, out2, out3, out4, out5, out6, out7) \
+{                                                                           \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
+    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                 \
+    LASX_ILVL_B_4_128SV(in2, in0, in3, in1, in6, in4, in7, in5,             \
+                       tmp0_m, tmp1_m, tmp2_m, tmp3_m);                     \
+    LASX_ILVLH_B_128SV(tmp1_m, tmp0_m, tmp5_m, tmp4_m);                     \
+    LASX_ILVLH_B_128SV(tmp3_m, tmp2_m, tmp7_m, tmp6_m);                     \
+    LASX_ILVLH_W_128SV(tmp6_m, tmp4_m, out2, out0);                         \
+    LASX_ILVLH_W_128SV(tmp7_m, tmp5_m, out6, out4);                         \
+    LASX_SLDI_B_2_0_128SV(out0, out2, out1, out3, 8);                       \
+    LASX_SLDI_B_2_0_128SV(out4, out6, out5, out7, 8);                       \
+}
+
+/* Description : Transposes input 16x8 byte block
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7,
+ *                         in8, in9, in10, in11, in12, in13, in14, in15
+ *                         (input 16x8 byte block)
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ *                         (output 8x16 byte block)
+ * Details     :
+ */
+#define LASX_TRANSPOSE16x8_B(in0, in1, in2, in3, in4, in5, in6, in7,              \
+                             in8, in9, in10, in11, in12, in13, in14, in15,        \
+                             out0, out1, out2, out3, out4, out5, out6, out7)      \
+{                                                                                 \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                       \
+    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                       \
+    __m256i t0, t1, t2, t3, t4, t5, t6, t7;                                       \
+    LASX_ILVL_B_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
+                        in10, in8, in11, in9, in14, in12, in15, in13,             \
+                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
+                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
+    LASX_ILVLH_B_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
+    LASX_ILVLH_B_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
+    LASX_ILVLH_W_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
+    LASX_ILVLH_W_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
+    LASX_ILVLH_D_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out1, out0, out3, out2); \
+    LASX_ILVLH_D_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out5, out4, out7, out6); \
+}
+
+/* Description : Transposes input 16x8 byte block
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7,
+ *                         in8, in9, in10, in11, in12, in13, in14, in15
+ *                         (input 16x8 byte block)
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ *                         (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LASX_TRANSPOSE16x8_H
+ *         in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in2 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in3 : 4,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in4 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in5 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in6 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in7 : 8,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in8 : 9,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *         in9 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        in10 : 0,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        in11 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        in12 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        in13 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        in14 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        in15 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *
+ *        out0 : 1,2,3,4,5,6,7,8,9,1,0,2,3,7,5,6
+ *        out1 : 2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
+ *        out2 : 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
+ *        out3 : 4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4
+ *        out4 : 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
+ *        out5 : 6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
+ *        out6 : 7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
+ *        out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
+ */
+#define LASX_TRANSPOSE16x8_H(in0, in1, in2, in3, in4, in5, in6, in7,              \
+                             in8, in9, in10, in11, in12, in13, in14, in15,        \
+                             out0, out1, out2, out3, out4, out5, out6, out7)      \
+{                                                                                 \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                       \
+    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                       \
+    __m256i t0, t1, t2, t3, t4, t5, t6, t7;                                       \
+    LASX_ILVL_H_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
+                        in10, in8, in11, in9, in14, in12, in15, in13,             \
+                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
+                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
+    LASX_ILVLH_H_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
+    LASX_ILVLH_H_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
+    LASX_ILVLH_D_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
+    LASX_ILVLH_D_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
+    LASX_PCKEV_Q_2(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out1);                   \
+    LASX_PCKEV_Q_2(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out2, out3);                   \
+                                                                                  \
+    LASX_ILVH_H_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
+                        in10, in8, in11, in9, in14, in12, in15, in13,             \
+                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
+                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
+    LASX_ILVLH_H_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
+    LASX_ILVLH_H_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
+    LASX_ILVLH_D_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
+    LASX_ILVLH_D_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
+    LASX_PCKEV_Q_2(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out4, out5);                   \
+    LASX_PCKEV_Q_2(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out6, out7);                   \
+}
+
+/* Description : Clips all signed word elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs  - in       (input vector)
+ *               Outputs - out_m    (output vector with clipped elements)
+ *               Return Type - signed word
+ */
+#define LASX_CLIP_W_0_255(in, out_m)        \
+{                                           \
+    out_m = __lasx_xvmaxi_w(in, 0);         \
+    out_m = __lasx_xvsat_wu(out_m, 7);      \
+}
+
+#define LASX_CLIP_W_0_255_2(in0, in1, out0, out1)  \
+{                                                  \
+    LASX_CLIP_W_0_255(in0, out0);                  \
+    LASX_CLIP_W_0_255(in1, out1);                  \
+}
+
+#define LASX_CLIP_W_0_255_4(in0, in1, in2, in3, out0, out1, out2, out3)  \
+{                                                                        \
+    LASX_CLIP_W_0_255_2(in0, in1, out0, out1);                           \
+    LASX_CLIP_W_0_255_2(in2, in3, out2, out3);                           \
+}
+
+/* Description : Clips all signed halfword elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs  - in       (input vector)
+ *               Outputs - out_m    (output vector with clipped elements)
+ *               Return Type - signed halfword
+ */
+#define LASX_CLIP_H_0_255(in, out_m)        \
+{                                           \
+    out_m = __lasx_xvmaxi_h(in, 0);         \
+    out_m = __lasx_xvsat_hu(out_m, 7);      \
+}
+
+#define LASX_CLIP_H_0_255_2(in0, in1, out0, out1)  \
+{                                                  \
+    LASX_CLIP_H_0_255(in0, out0);                  \
+    LASX_CLIP_H_0_255(in1, out1);                  \
+}
+
+#define LASX_CLIP_H_0_255_4(in0, in1, in2, in3, out0, out1, out2, out3)  \
+{                                                                        \
+    LASX_CLIP_H_0_255_2(in0, in1, out0, out1);                           \
+    LASX_CLIP_H_0_255_2(in2, in3, out2, out3);                           \
+}
+
+/* Description : Clips all halfword elements of input vector between min & max
+ *               out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
+ * Arguments   : Inputs  - in    (input vector)
+ *                       - min   (min threshold)
+ *                       - max   (max threshold)
+ *               Outputs - in    (output vector with clipped elements)
+ *               Return Type - signed halfword
+ */
+#define LASX_CLIP_H(in, min, max)    \
+{                                    \
+    in = __lasx_xvmax_h(min, in);    \
+    in = __lasx_xvmin_h(max, in);    \
+}
+
+/* Description : Dot product and addition of 3 signed byte input vectors
+ * Arguments   : Inputs  - in0, in1, in2, coeff0, coeff1, coeff2
+ *               Outputs - out0_m
+ *               Return Type - signed halfword
+ * Details     : Dot product of 'in0' with 'coeff0'
+ *               Dot product of 'in1' with 'coeff1'
+ *               Dot product of 'in2' with 'coeff2'
+ *               Addition of all the 3 vector results
+ *               out0_m = (in0 * coeff0) + (in1 * coeff1) + (in2 * coeff2)
+ */
+#define LASX_DP2ADD_H_B_3(in0, in1, in2, out0_m, coeff0, coeff1, coeff2) \
+{                                                                        \
+    LASX_DP2_H_B(in0, coeff0, out0_m);                                   \
+    LASX_DP2ADD_H_B(out0_m, in1, coeff1, out0_m);                        \
+    LASX_DP2ADD_H_B(out0_m, in2, coeff2, out0_m);                        \
+}
+
+/* Description : Each byte element is logically xor'ed with immediate 128
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - in0, in1 (in-place)
+ * Details     : Each unsigned byte element from input vector 'in0' is
+ *               logically xor'ed with 128 and result is in-place stored in
+ *               'in0' vector
+ *               Each unsigned byte element from input vector 'in1' is
+ *               logically xor'ed with 128 and result is in-place stored in
+ *               'in1' vector
+ *               Similar for other pairs
+ * Example     : LASX_XORI_B_128(in0)
+ *               in0: 9,10,11,12, 13,14,15,16, 121,122,123,124, 125,126,127,128, 17,18,19,20, 21,22,23,24,
+ *               248,249,250,251, 252,253,254,255,
+ *               in0: 137,138,139,140, 141,142,143,144, 249,250,251,252, 253,254,255,0, 145,146,147,148,
+ *               149,150,151,152, 120,121,122,123, 124,125,126,127
+ */
+#define LASX_XORI_B_128(in0)                                 \
+{                                                            \
+    in0 = __lasx_xvxori_b(in0, 128);                         \
+}
+#define LASX_XORI_B_2_128(in0, in1)                          \
+{                                                            \
+    LASX_XORI_B_128(in0);                                    \
+    LASX_XORI_B_128(in1);                                    \
+}
+#define LASX_XORI_B_4_128(in0, in1, in2, in3)                \
+{                                                            \
+    LASX_XORI_B_2_128(in0, in1);                             \
+    LASX_XORI_B_2_128(in2, in3);                             \
+}
+#define LASX_XORI_B_8_128(in0, in1, in2, in3, in4, in5, in6, in7)  \
+{                                                                  \
+    LASX_XORI_B_4_128(in0, in1, in2, in3);                         \
+    LASX_XORI_B_4_128(in4, in5, in6, in7);                         \
+}
+
+/* Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'indx0 < 8' use SPLATI_R_*,
+ *               if 'indx0 >= 8' use SPLATI_L_*
+ * Arguments   : Inputs  - in, idx0, idx1
+ *               Outputs - out0, out1
+ * Details     : 'idx0' element value from 'in' vector is replicated to all
+ *                elements in 'out0' vector
+ *                Valid index range for halfword operation is 0-7
+ */
+#define LASX_SPLATI_L_H(in, idx0, out0)                        \
+{                                                              \
+    in = __lasx_xvpermi_q(in, in, 0x02);                       \
+    out0 = __lasx_xvrepl128vei_h(in, idx0);                    \
+}
+#define LASX_SPLATI_H_H(in, idx0, out0)                        \
+{                                                              \
+    in = __lasx_xvpermi_q(in, in, 0X13);                       \
+    out0 = __lasx_xvrepl128vei_h(in, idx0 - 8);                \
+}
+#define LASX_SPLATI_L_H_2(in, idx0, idx1, out0, out1)          \
+{                                                              \
+    LASX_SPLATI_L_H(in, idx0, out0);                           \
+    out1 = __lasx_xvrepl128vei_h(in, idx1);                    \
+}
+#define LASX_SPLATI_H_H_2(in, idx0, idx1, out0, out1)          \
+{                                                              \
+    LASX_SPLATI_H_H(in, idx0, out0);                           \
+    out1 = __lasx_xvrepl128vei_h(in, idx1 - 8);                \
+}
+#define LASX_SPLATI_L_H_4(in, idx0, idx1, idx2, idx3,          \
+                          out0, out1, out2, out3)              \
+{                                                              \
+    LASX_SPLATI_L_H_2(in, idx0, idx1, out0, out1);             \
+    out2 = __lasx_xvrepl128vei_h(in, idx2);                    \
+    out3 = __lasx_xvrepl128vei_h(in, idx3);                    \
+}
+#define SPLATI_H_H_4(in, idx0, idx1, idx2, idx3,               \
+                     out0, out1, out2, out3)                   \
+{                                                              \
+    LASX_SPLATI_H_H_2(in, idx0, idx1, out0, out1);             \
+    out2 = __lasx_xvrepl128vei_h(in, idx2 - 8);                \
+    out3 = __lasx_xvrepl128vei_h(in, idx3 - 8);                \
+}
+
+/* Description : Pack even elements of input vectors & xor with 128
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out_m
+ * Details     : Signed byte even elements from 'in0' and 'in1' are packed
+ *               together in one vector and the resulted vector is xor'ed with
+ *               128 to shift the range from signed to unsigned byte
+ */
+#define LASX_PICKEV_XORI128_B(in0, in1, out_m)  \
+{                                               \
+    out_m = __lasx_xvpickev_b(in1, in0);        \
+    out_m = __lasx_xvxori_b(out_m, 128);        \
+}
+
+/* Description : Shift right logical all byte elements of vector.
+ * Arguments   : Inputs  - in, shift
+ *               Outputs - in (in place)
+ * Details     : Each element of vector in is shifted right logical by
+ *               number of bits respective element holds in vector shift and
+ *               result is in place written to in.
+ *               Here, shift is a vector passed in.
+ * Example     : See LASX_SRL_W(in, shift)
+     */
+#define LASX_SRL_B(in, shift)                                         \
+{                                                                     \
+    in = __lasx_xvsrl_b(in, shift);                                   \
+}
+
+#define LASX_SRL_B_2(in0, in1, shift)                                 \
+{                                                                     \
+    LASX_SRL_B(in0, shift);                                           \
+    LASX_SRL_B(in1, shift);                                           \
+}
+
+#define LASX_SRL_B_4(in0, in1, in2, in3, shift)                       \
+{                                                                     \
+    LASX_SRL_B_2(in0, in1, shift);                                    \
+    LASX_SRL_B_2(in2, in3, shift);                                    \
+}
+
+/* Description : Shift right logical all halfword elements of vector.
+ * Arguments   : Inputs  - in, shift
+ *               Outputs - in (in place)
+ * Details     : Each element of vector in is shifted right logical by
+ *               number of bits respective element holds in vector shift and
+ *               result is in place written to in.
+ *               Here, shift is a vector passed in.
+ * Example     : See LASX_SRL_W(in, shift)
+ */
+#define LASX_SRL_H(in, shift)                                         \
+{                                                                     \
+    in = __lasx_xvsrl_h(in, shift);                                   \
+}
+
+#define LASX_SRL_H_2(in0, in1, shift)                                 \
+{                                                                     \
+    LASX_SRL_H(in0, shift);                                           \
+    LASX_SRL_H(in1, shift);                                           \
+}
+
+#define LASX_SRL_H_4(in0, in1, in2, in3, shift)                       \
+{                                                                     \
+    LASX_SRL_H_2(in0, in1, shift);                                    \
+    LASX_SRL_H_2(in2, in3, shift);                                    \
+}
+
+/* Description : Shift right logical all word elements of vector.
+ * Arguments   : Inputs  - in, shift
+ *               Outputs - in (in place)
+ * Details     : Each element of vector in is shifted right logical by
+ *               number of bits respective element holds in vector shift and
+ *               result is in place written to in.
+ *               Here, shift is a vector passed in.
+ * Example     : LASX_SRL_W(in, shift)
+ *          in : 1, 3, 2, -4,      0, -2, 25, 0
+ *       shift : 1, 1, 1, 1,       2, 2, 2, 2
+ *  in(output) : 0, 1, 1, 32766,   0, 16383, 6, 0
+ */
+#define LASX_SRL_W(in, shift)                                         \
+{                                                                     \
+    in = __lasx_xvsrl_w(in, shift);                                   \
+}
+
+#define LASX_SRL_W_2(in0, in1, shift)                                 \
+{                                                                     \
+    LASX_SRL_W(in0, shift);                                           \
+    LASX_SRL_W(in1, shift);                                           \
+}
+
+#define LASX_SRL_W_4(in0, in1, in2, in3, shift)                       \
+{                                                                     \
+    LASX_SRL_W_2(in0, in1, shift);                                    \
+    LASX_SRL_W_2(in2, in3, shift);                                    \
+}
+
+/* Description : Shift right logical all double word elements of vector.
+ * Arguments   : Inputs  - in, shift
+ *               Outputs - in (in place)
+ * Details     : Each element of vector in is shifted right logical by
+ *               number of bits respective element holds in vector shift and
+ *               result is in place written to in.
+ *               Here, shift is a vector passed in.
+ * Example     : See LASX_SRL_W(in, shift)
+ */
+#define LASX_SRL_D(in, shift)                                         \
+{                                                                     \
+    in = __lasx_xvsrl_d(in, shift);                                   \
+}
+
+#define LASX_SRL_D_2(in0, in1, shift)                                 \
+{                                                                     \
+    LASX_SRL_D(in0, shift);                                           \
+    LASX_SRL_D(in1, shift);                                           \
+}
+
+#define LASX_SRL_D_4(in0, in1, in2, in3, shift)                       \
+{                                                                     \
+    LASX_SRL_D_2(in0, in1, shift);                                    \
+    LASX_SRL_D_2(in2, in3, shift);                                    \
+}
+
+
+/* Description : Shift right arithmetic rounded (immediate)
+ * Arguments   : Inputs  - in0, in1, shift
+ *               Outputs - in0, in1, (in place)
+ * Details     : Each element of vector 'in0' is shifted right arithmetic by
+ *               value in 'shift'.
+ *               The last discarded bit is added to shifted value for rounding
+ *               and the result is in place written to 'in0'
+ *               Similar for other pairs
+ * Example     : LASX_SRARI_H(in0, out0, shift)
+ *               in0:   1,2,3,4, -5,-6,-7,-8, 19,10,11,12, 13,14,15,16
+ *               shift: 2
+ *               out0:  0,1,1,1, -1,-1,-2,-2, 5,3,3,3, 3,4,4,4
+ */
+#define LASX_SRARI_H(in0, out0, shift)                              \
+{                                                                   \
+    out0 = __lasx_xvsrari_h(in0, shift);                            \
+}
+#define LASX_SRARI_H_2(in0, in1, out0, out1, shift)                 \
+{                                                                   \
+    LASX_SRARI_H(in0, out0, shift);                                 \
+    LASX_SRARI_H(in1, out1, shift);                                 \
+}
+#define LASX_SRARI_H_4(in0, in1, in2, in3, out0, out1, out2, out3, shift) \
+{                                                                         \
+    LASX_SRARI_H_2(in0, in1, out0, out1, shift);                          \
+    LASX_SRARI_H_2(in2, in3, out2, out3, shift);                          \
+}
+
+/* Description : Shift right arithmetic (immediate)
+ * Arguments   : Inputs  - in0, in1, shift
+ *               Outputs - in0, in1, (in place)
+ * Details     : Each element of vector 'in0' is shifted right arithmetic by
+ *               value in 'shift'.
+ *               Similar for other pairs
+ * Example     : see LASX_SRARI_H(in0, out0, shift)
+ */
+#define LASX_SRAI_W(in0, out0, shift)                                    \
+{                                                                        \
+    out0 = __lasx_xvsrai_w(in0, shift);                                  \
+}
+#define LASX_SRAI_W_2(in0, in1, out0, out1, shift)                       \
+{                                                                        \
+    LASX_SRAI_W(in0, out0, shift);                                       \
+    LASX_SRAI_W(in1, out1, shift);                                       \
+}
+#define LASX_SRAI_W_4(in0, in1, in2, in3, out0, out1, out2, out3, shift) \
+{                                                                        \
+    LASX_SRAI_W_2(in0, in1, out0, out1, shift);                          \
+    LASX_SRAI_W_2(in2, in3, out2, out3, shift);                          \
+}
+#define LASX_SRAI_W_8(in0, in1, in2, in3, in4, in5, in6, in7,                 \
+                      out0, out1, out2, out3, out4, out5, out6, out7, shift)  \
+{                                                                             \
+    LASX_SRAI_W_4(in0, in1, in2, in3, out0, out1, out2, out3, shift);         \
+    LASX_SRAI_W_4(in4, in5, in6, in7, out4, out5, out6, out7, shift);         \
+}
+
+/* Description : Saturate the halfword element values to the max
+ *               unsigned value of (sat_val+1 bits)
+ *               The element data width remains unchanged
+ * Arguments   : Inputs  - in0, in1, in2, in3, sat_val
+ *               Outputs - in0, in1, in2, in3 (in place)
+ *               Return Type - unsigned halfword
+ * Details     : Each unsigned halfword element from 'in0' is saturated to the
+ *               value generated with (sat_val+1) bit range
+ *               Results are in placed to original vectors
+ * Example     : LASX_SAT_H(in0, out0, sat_val)
+ *               in0:    1,2,3,4, 5,6,7,8, 19,10,11,12, 13,14,15,16
+ *               sat_val:3
+ *               out0:   1,2,3,4, 5,6,7,7, 7,7,7,7, 7,7,7,7
+ */
+#define LASX_SAT_H(in0, out0, sat_val)                                     \
+{                                                                          \
+    out0 = __lasx_xvsat_h(in0, sat_val);                                   \
+} //some error in xvsat_h built-in function
+#define LASX_SAT_H_2(in0, in1, out0, out1, sat_val)                        \
+{                                                                          \
+    LASX_SAT_H(in0, out0, sat_val);                                        \
+    LASX_SAT_H(in1, out1, sat_val);                                        \
+}
+#define LASX_SAT_H_4(in0, in1, in2, in3, out0, out1, out2, out3, sat_val)  \
+{                                                                          \
+    LASX_SAT_H_2(in0, in1, out0, out1, sat_val);                           \
+    LASX_SAT_H_2(in2, in3, out2, out3, sat_val);                           \
+}
+
+/* Description : Addition of 2 pairs of vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1
+ * Details     : Each halfwords element from 2 pairs vectors is added
+ *               and 2 results are produced
+ * Example     : LASX_ADD_H(in0, in1, out)
+ *               in0:  1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *               in1:  8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *               out:  9,9,9,9, 9,9,9,9, 9,9,9,9, 9,9,9,9
+ */
+#define LASX_ADD_H(in0, in1, out)             \
+{                                             \
+    out = __lasx_xvadd_h(in0, in1);           \
+}
+#define LASX_ADD_H_2(in0, in1, in2, in3, out0, out1) \
+{                                                    \
+    LASX_ADD_H(in0, in1, out0);                      \
+    LASX_ADD_H(in2, in3, out1);                      \
+}
+#define LASX_ADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7, out0, out1, out2, out3)      \
+{                                                                                         \
+    LASX_ADD_H_2(in0, in1, in2, in3, out0, out1);                                         \
+    LASX_ADD_H_2(in4, in5, in6, in7, out2, out3);                                         \
+}
+#define LASX_ADD_H_8(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11, in12, \
+                     in13, in14, in15, out0, out1, out2, out3, out4, out5, out6, out7)   \
+{                                                                                        \
+    LASX_ADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7, out0, out1, out2, out3);        \
+    LASX_ADD_H_4(in8, in9, in10, in11, in12, in13, in14, in15, out4, out5, out6, out7);  \
+}
+
+/* Description : Horizontal subtraction of unsigned byte vector elements
+ * Arguments   : Inputs  - in0, in1
+ *               Outputs - out0, out1
+ *               Return Type - as per RTYPE
+ * Details     : Each unsigned odd byte element from 'in0' is subtracted from
+ *               even unsigned byte element from 'in0' (pairwise) and the
+ *               halfword result is written to 'out0'
+ */
+#define LASX_HSUB_UB_2(in0, in1, out0, out1)   \
+{                                              \
+    out0 = __lasx_xvhsubw_hu_bu(in0, in0);     \
+    out1 = __lasx_xvhsubw_hu_bu(in1, in1);     \
+}
+
+#define LASX_HSUB_UB_4(in0, in1, in2, in3, out0, out1, out2, out3)    \
+{                                                                     \
+    LASX_HSUB_UB_2(in0, in1, out0, out1);                                   \
+    LASX_HSUB_UB_2(in2, in3, out2, out3);                                   \
+}
+
+/* Description : Shuffle byte vector elements as per mask vector
+ * Arguments   : Inputs  - in0, in1, in2, in3, mask0, mask1
+ *               Outputs - out0, out1
+ *               Return Type - as per RTYPE
+ * Details     : Selective byte elements from in0 & in1 are copied to out0 as
+ *               per control vector mask0
+ *               Selective byte elements from in2 & in3 are copied to out1 as
+ *               per control vector mask1
+ * Example     : LASX_SHUF_B_128SV(in0, in1,  mask0, out0)
+ *               in_h :  9,10,11,12, 13,14,15,16, 0,0,0,0, 0,0,0,0,
+ *                      17,18,19,20, 21,22,23,24, 0,0,0,0, 0,0,0,0
+ *               in_l :  1, 2, 3, 4,  5, 6, 7, 8, 0,0,0,0, 0,0,0,0,
+ *                      25,26,27,28, 29,30,31,32, 0,0,0,0, 0,0,0,0
+ *               mask0:  0, 1, 2, 3,  4, 5, 6, 7, 16,17,18,19, 20,21,22,23,
+ *                      16,17,18,19, 20,21,22,23,  0, 1, 2, 3,  4, 5, 6, 7
+ *               out0 :  1, 2, 3, 4,  5, 6, 7, 8,  9,10,11,12, 13,14,15,16,
+ *                      17,18,19,20, 21,22,23,24, 25,26,27,28, 29,30,31,32
+ */
+
+#define LASX_SHUF_B_128SV(in_h, in_l,  mask0, out0)                            \
+{                                                                              \
+    out0 = __lasx_xvshuf_b(in_h, in_l, mask0);                                 \
+}
+#define LASX_SHUF_B_2_128SV(in0_h, in0_l, in1_h, in1_l, mask0, mask1,          \
+                            out0, out1)                                        \
+{                                                                              \
+    LASX_SHUF_B_128SV(in0_h, in0_l,  mask0, out0);                             \
+    LASX_SHUF_B_128SV(in1_h, in1_l,  mask1, out1);                             \
+}
+#define LASX_SHUF_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,          \
+                            in3_h, in3_l, mask0, mask1, mask2, mask3,          \
+                            out0, out1, out2, out3)                            \
+{                                                                              \
+    LASX_SHUF_B_2_128SV(in0_h, in0_l, in1_h, in1_l, mask0, mask1, out0, out1); \
+    LASX_SHUF_B_2_128SV(in2_h, in2_l, in3_h, in3_l, mask2, mask3, out2, out3); \
+}
+
+/* Description : Addition of signed halfword elements and signed saturation
+ * Arguments   : Inputs  - in0, in1, in2, in3 ~
+ *               Outputs - out0, out1 ~
+ * Details     : Signed halfword elements from 'in0' are added to signed
+ *               halfword elements of 'in1'. The result is then signed saturated
+ *               between -32768 to +32767 (as per halfword data type)
+ *               Similar for other pairs
+ * Example     : LASX_SADD_H(in0, in1, out0)
+ *               in0:   1,2,32766,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
+ *               in1:   8,7,30586,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
+ *               out0:  9,9,32767,9, 9,9,9,9, 9,9,9,9, 9,9,9,9,
+ */
+#define LASX_SADD_H(in0, in1, out0)                            \
+{                                                              \
+    out0 = __lasx_xvsadd_h(in0, in1);                          \
+}
+#define LASX_SADD_H_2(in0, in1, in2, in3, out0, out1)          \
+{                                                              \
+    LASX_SADD_H(in0, in1, out0);                               \
+    LASX_SADD_H(in2, in3, out1);                               \
+}
+#define LASX_SADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7,  \
+                      out0, out1, out2, out3)                  \
+{                                                              \
+    LASX_SADD_H_2(in0, in1, in2, in3, out0, out1);             \
+    LASX_SADD_H_2(in4, in5, in6, in7, out2, out3);             \
+}
+
+/* Description : Average with rounding (in0 + in1 + 1) / 2.
+ * Arguments   : Inputs  - in0, in1, in2, in3,
+ *               Outputs - out0, out1
+ * Details     : Each unsigned byte element from 'in0' vector is added with
+ *               each unsigned byte element from 'in1' vector.
+ *               Average with rounding is calculated and written to 'out0'
+ */
+#define LASX_AVER_BU( in0, in1, out0 )   \
+{                                        \
+    out0 = __lasx_xvavgr_bu( in0, in1 ); \
+}
+
+#define LASX_AVER_BU_2( in0, in1, in2, in3, out0, out1 )  \
+{                                                         \
+    LASX_AVER_BU( in0, in1, out0 );                       \
+    LASX_AVER_BU( in2, in3, out1 );                       \
+}
+
+#define LASX_AVER_BU_4( in0, in1, in2, in3, in4, in5, in6, in7,  \
+                        out0, out1, out2, out3 )                 \
+{                                                                \
+    LASX_AVER_BU_2( in0, in1, in2, in3, out0, out1 );            \
+    LASX_AVER_BU_2( in4, in5, in6, in7, out2, out3 );            \
+}
+
+/* Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     : Butterfly operationuu
+ */
+#define LASX_BUTTERFLY_4(RTYPE, in0, in1, in2, in3, out0, out1, out2, out3)  \
+{                                                                            \
+    out0 = (__m256i)( (RTYPE)in0 + (RTYPE)in3 );                             \
+    out1 = (__m256i)( (RTYPE)in1 + (RTYPE)in2 );                             \
+                                                                             \
+    out2 = (__m256i)( (RTYPE)in1 - (RTYPE)in2 );                             \
+    out3 = (__m256i)( (RTYPE)in0 - (RTYPE)in3 );                             \
+}
+
+/* Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - in0 in1 in2 ~
+ *               Outputs - out0 out1 out2 ~
+ * Details     : Butterfly operation
+ */
+#define LASX_BUTTERFLY_8(RTYPE, in0, in1, in2, in3, in4, in5, in6, in7,    \
+                         out0, out1, out2, out3, out4, out5, out6, out7)   \
+{                                                                          \
+    out0 = (__m256i)( (RTYPE)in0 + (RTYPE)in7 );                           \
+    out1 = (__m256i)( (RTYPE)in1 + (RTYPE)in6 );                           \
+    out2 = (__m256i)( (RTYPE)in2 + (RTYPE)in5 );                           \
+    out3 = (__m256i)( (RTYPE)in3 + (RTYPE)in4 );                           \
+                                                                           \
+    out4 = (__m256i)( (RTYPE)in3 - (RTYPE)in4 );                           \
+    out5 = (__m256i)( (RTYPE)in2 - (RTYPE)in5 );                           \
+    out6 = (__m256i)( (RTYPE)in1 - (RTYPE)in6 );                           \
+    out7 = (__m256i)( (RTYPE)in0 - (RTYPE)in7 );                           \
+}
+
+/*
+ ****************************************************************************
+ ***************  Non-generic macro definition ******************************
+ ****************************************************************************
+ */
+
+/* Description : Transpose 8x4 block with half word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ *               Return Type - signed halfword
+ */
+#define LASX_TRANSPOSE8X4_H_128SV( in0, in1, in2, in3,                    \
+                                   out0, out1, out2, out3 )               \
+{                                                                         \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                               \
+                                                                          \
+    LASX_ILVL_H_2_128SV( in1, in0, in3, in2, tmp0_m, tmp1_m );            \
+    LASX_ILVH_H_2_128SV( in1, in0, in3, in2, tmp2_m, tmp3_m );            \
+    LASX_ILVL_W_2_128SV( tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out2 );    \
+    LASX_ILVH_W_2_128SV( tmp1_m, tmp0_m, tmp3_m, tmp2_m, out1, out3 );    \
+}
+
+/* Description : Transpose 16x8 block into 8x16 with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7,
+ *                         in8, in9, in10, in11, in12, in13, in14, in15
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ *               Return Type - unsigned byte
+ */
+#define LASX_TRANSPOSE16x8_B_128SV( in0, in1, in2, in3,                   \
+                                    in4, in5, in6, in7,                   \
+                                    in8, in9, in10, in11,                 \
+                                    in12, in13, in14, in15,               \
+                                    out0, out1, out2, out3,               \
+                                    out4, out5, out6, out7 )              \
+{                                                                         \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                               \
+    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                               \
+                                                                          \
+    out7 = __lasx_xvpackev_d( in8, in0 );                                 \
+    out6 = __lasx_xvpackev_d( in9, in1 );                                 \
+    out5 = __lasx_xvpackev_d( in10, in2 );                                \
+    out4 = __lasx_xvpackev_d( in11, in3 );                                \
+    out3 = __lasx_xvpackev_d( in12, in4 );                                \
+    out2 = __lasx_xvpackev_d( in13, in5 );                                \
+    out1 = __lasx_xvpackev_d( in14, in6 );                                \
+    out0 = __lasx_xvpackev_d( in15, in7 );                                \
+                                                                          \
+    tmp0_m = __lasx_xvpackev_b( out6, out7 );                             \
+    tmp4_m = __lasx_xvpackod_b( out6, out7 );                             \
+    tmp1_m = __lasx_xvpackev_b( out4, out5 );                             \
+    tmp5_m = __lasx_xvpackod_b( out4, out5 );                             \
+    out5 = __lasx_xvpackev_b( out2, out3 );                               \
+    tmp6_m = __lasx_xvpackod_b( out2, out3 );                             \
+    out7 = __lasx_xvpackev_b( out0, out1 );                               \
+    tmp7_m = __lasx_xvpackod_b( out0, out1 );                             \
+                                                                          \
+    tmp2_m = __lasx_xvpackev_h( tmp1_m, tmp0_m );                         \
+    tmp3_m = __lasx_xvpackev_h( out7, out5 );                             \
+    out0 = __lasx_xvpackev_w( tmp3_m, tmp2_m );                           \
+    out4 = __lasx_xvpackod_w( tmp3_m, tmp2_m );                           \
+                                                                          \
+    tmp2_m = __lasx_xvpackod_h( tmp1_m, tmp0_m );                         \
+    tmp3_m = __lasx_xvpackod_h( out7, out5 );                             \
+    out2 = __lasx_xvpackev_w( tmp3_m, tmp2_m );                           \
+    out6 = __lasx_xvpackod_w( tmp3_m, tmp2_m );                           \
+                                                                          \
+    tmp2_m = __lasx_xvpackev_h( tmp5_m, tmp4_m );                         \
+    tmp3_m = __lasx_xvpackev_h( tmp7_m, tmp6_m );                         \
+    out1 = __lasx_xvpackev_w( tmp3_m, tmp2_m );                           \
+    out5 = __lasx_xvpackod_w( tmp3_m, tmp2_m );                           \
+                                                                          \
+    tmp2_m = __lasx_xvpackod_h( tmp5_m, tmp4_m );                         \
+    tmp2_m = __lasx_xvpackod_h( tmp5_m, tmp4_m );                         \
+    tmp3_m = __lasx_xvpackod_h( tmp7_m, tmp6_m );                         \
+    tmp3_m = __lasx_xvpackod_h( tmp7_m, tmp6_m );                         \
+    out3 = __lasx_xvpackev_w( tmp3_m, tmp2_m );                           \
+    out7 = __lasx_xvpackod_w( tmp3_m, tmp2_m );                           \
+}
+
+/* Description : Horizontal addition of 8 signed word elements of input vector
+ * Arguments   : Input  - in       (signed word vector)
+ *               Output - sum_m    (s32 sum)
+ * Details     : 8 signed word elements of 'in' vector are added together and
+ *               the resulting integer sum is returned
+ */
+#define LASX_HADD_SW_S32( in )                               \
+( {                                                          \
+    int32_t s_sum_m;                                         \
+    v4i64  out;                                              \
+                                                             \
+    out = __lasx_xvhaddw_d_w( in, in );                      \
+    s_sum_m = out[0] + out[1] + out[2] + out[3];             \
+    s_sum_m;                                                 \
+} )
+
+/* Description : Horizontal addition of 16 half word elements of input vector
+ * Arguments   : Input  - in       (half word vector)
+ *               Output - sum_m    (i32 sum)
+ * Details     : 16 half word elements of 'in' vector are added together and
+ *               the resulting integer sum is returned
+ */
+#define LASX_HADD_UH_U32( in )                               \
+( {                                                          \
+    uint32_t u_sum_m;                                        \
+    v4u64  out;                                              \
+    __m256i res_m;                                           \
+                                                             \
+    res_m = __lasx_xvhaddw_wu_hu( in, in );                  \
+    out = ( v4u64 )__lasx_xvhaddw_du_wu( res_m, res_m );     \
+    u_sum_m = out[0] + out[1] + out[2] + out[3];             \
+    u_sum_m;                                                 \
+} )
+
+/* Description : Dot product and addition of 3 signed halfword input vectors
+ * Arguments   : Inputs  - in0, in1, in2, coeff0, coeff1, coeff2
+ *               Output - out0_m
+ *               Return Type - signed halfword
+ * Details     : Dot product of 'in0' with 'coeff0'
+ *               Dot product of 'in1' with 'coeff1'
+ *               Dot product of 'in2' with 'coeff2'
+ *               Addition of all the 3 vector results
+ *               out0_m = (in0 * coeff0) + (in1 * coeff1) + (in2 * coeff2)
+ */
+#define LASX_DPADD_SH_3( in0, in1, in2, coeff0, coeff1, coeff2 )     \
+( {                                                                  \
+    __m256i out0_m;                                                  \
+                                                                     \
+    LASX_DP2ADD_H_B_3(in0, in1, in2, out0_m, coeff0, coeff1, coeff2);\
+    out0_m;                                                          \
+} )
+
+/* Description : Sign extend halfword elements from input vector and return
+ *               the result in pair of vectors
+ * Arguments   : Input  - in            (halfword vector)
+ *               Outputs - out0, out1   (sign extended word vectors)
+ *               Return Type - signed word
+ * Details     : Sign bit of halfword elements from input vector 'in' is
+ *               extracted and interleaved right with same vector 'in0' to
+ *               generate 4 signed word elements in 'out0'
+ *               Then interleaved left with same vector 'in0' to
+ *               generate 4 signed word elements in 'out1'
+ */
+#define LASX_UNPCK_SH_128SV( in, out0, out1 )   \
+{                                               \
+    __m256i tmp_m;                              \
+                                                \
+    tmp_m = __lasx_xvslti_h( in, 0 );           \
+    LASX_ILVLH_H_128SV( tmp_m, in, out1, out0 );\
+}
+
+/* Description : Vector Bit Move If Not Zero
+ * Arguments   : Input  - in0, in1, mask      (vector)
+ *               Outputs - out                (vectors)
+ * Details     : Copy to destination vector in0 all bits from source vector in1
+ *               for which the corresponding bits from target vector mask
+ *               are 1 and leaves unchanged all destination bits for which
+ *               the corresponding target bits are 0.
+ *               out = (in1 AND mask) OR (in0 AND NOT mask)
+ */
+#define LASX_BMNZ( in0, in1, mask, out )  \
+{                                         \
+    __m256i tmp0, tmp1;                   \
+                                          \
+    tmp1 = __lasx_xvand_v( in1, mask);    \
+    tmp0 = __lasx_xvandn_v( mask, in0);   \
+    out = __lasx_xvor_v(tmp1, tmp0);      \
+}
+
+#endif /* GENERIC_MACROS_LASX_H */
diff --git a/common/loongarch/mc-c.c b/common/loongarch/mc-c.c
new file mode 100644
index 00000000..ba04c97b
--- /dev/null
+++ b/common/loongarch/mc-c.c
@@ -0,0 +1,4089 @@
+/*****************************************************************************
+ * mc-c.c: loongarch motion compensation
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "generic_macros_lasx.h"
+#include "mc.h"
+
+#if !HIGH_BIT_DEPTH
+
+static const uint8_t pu_luma_mask_arr[16 * 6] =
+{
+    0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12,
+    0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12,
+    1, 4, 2, 5, 3, 6, 4, 7, 5, 8, 6, 9, 7, 10, 8, 11,
+    1, 4, 2, 5, 3, 6, 4, 7, 5, 8, 6, 9, 7, 10, 8, 11,
+    2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10,
+    2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10
+};
+
+static const uint8_t pu_chroma_mask_arr[16 * 2] =
+{
+    0, 2, 2, 4, 4, 6, 6, 8, 16, 18, 18, 20, 20, 22, 22, 24,
+    0, 2, 2, 4, 4, 6, 6, 8, 16, 18, 18, 20, 20, 22, 22, 24
+};
+
+static const uint8_t pu_chroma_mask_arr1[16 * 2] =
+{
+    0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16,
+    0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14, 16
+};
+
+static const uint8_t pu_core_mask_arr[16 * 2] =
+{
+    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
+    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
+};
+
+static void mc_weight_w20_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                const x264_weight_t *weight, int height )
+{
+    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
+
+    i_offset <<= i_denom;
+
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
+    "xvreplgr2vr.b    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "xvld             $xr3,             %[src],               0                      \n\t"
+    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
+    "xvldx            $xr5,             %[src],               %[src_stride2]         \n\t"
+    "xvldx            $xr6,             %[src],               %[src_stride3]         \n\t"
+    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr9,             $xr5,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr10,            $xr6,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr5,             $xr5,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr6,             $xr6,                 $xr0                   \n\t"
+    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
+    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
+    "xvsadd.h         $xr9,             $xr9,                 $xr1                   \n\t"
+    "xvsadd.h         $xr10,            $xr10,                $xr1                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
+    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
+    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
+    "xvmaxi.h         $xr9,             $xr9,                 0                      \n\t"
+    "xvmaxi.h         $xr10,            $xr10,                0                      \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
+    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
+    "xvssrlrn.bu.h    $xr7,             $xr7,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr8,             $xr8,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr9,             $xr9,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr10,            $xr10,                $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr5,             $xr5,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr6,             $xr6,                 $xr2                   \n\t"
+    "xvilvl.b         $xr3,             $xr3,                 $xr7                   \n\t"
+    "xvilvl.b         $xr4,             $xr4,                 $xr8                   \n\t"
+    "xvilvl.b         $xr5,             $xr5,                 $xr9                   \n\t"
+    "xvilvl.b         $xr6,             $xr6,                 $xr10                  \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr4,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr5,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr5,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr6,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr6,             %[dst],               16,          4         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "xvld             $xr3,             %[src],               0                      \n\t"
+    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
+    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
+    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
+    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlrn.bu.h    $xr7,             $xr7,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr8,             $xr8,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvilvl.b         $xr3,             $xr3,                 $xr7                   \n\t"
+    "xvilvl.b         $xr4,             $xr4,                 $xr8                   \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr4,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
+      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+static void mc_weight_w16_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                const x264_weight_t *weight, int height )
+{
+    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset, i_4 = 4;
+    int zero = 0, src_stride2, src_stride3, src_stride4, dst_stride2, dst_stride3, dst_stride4;
+
+    i_offset <<= i_denom;
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "slli.d           %[dst_stride2],   %[dst_stride],        1                      \n\t"
+    "add.d            %[dst_stride3],   %[dst_stride2],       %[dst_stride]          \n\t"
+    "slli.d           %[dst_stride4],   %[dst_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
+    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "vext2xv.hu.bu    $xr5,             $xr5                                         \n\t"
+    "vext2xv.hu.bu    $xr6,             $xr6                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvmul.h          $xr5,             $xr5,                 $xr0                   \n\t"
+    "xvmul.h          $xr6,             $xr6,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
+    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
+    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
+    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr5,             $xr5,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr6,             $xr6,                 $xr2                   \n\t"
+    "xvpermi.d        $xr3,             $xr3,                 8                      \n\n"
+    "xvpermi.d        $xr4,             $xr4,                 8                      \n\n"
+    "xvpermi.d        $xr5,             $xr5,                 8                      \n\n"
+    "xvpermi.d        $xr6,             $xr6,                 8                      \n\n"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
+    "vstx             $vr5,             %[dst],               %[dst_stride2]         \n\t"
+    "vstx             $vr6,             %[dst],               %[dst_stride3]         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride4]         \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvpermi.d        $xr3,             $xr3,                 8                      \n\n"
+    "xvpermi.d        $xr4,             $xr4,                 8                      \n\n"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride2]         \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4), [dst_stride2]"=&r"(dst_stride2),
+      [dst_stride3]"=&r"(dst_stride3), [dst_stride4]"=&r"(dst_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
+      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+static void mc_weight_w8_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                               const x264_weight_t *weight, int height )
+{
+    int i_4 = 4;
+    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, src_stride2, src_stride3, src_stride4;
+
+    i_offset <<= i_denom;
+    i_offset += (1 << ( i_denom -1 ));
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
+    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vilvl.d          $vr4,             $vr6,                 $vr5                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrln.bu.h     $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,            0        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,            2        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            0        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            2        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,           2         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
+      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+static void mc_weight_w4_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                               const x264_weight_t *weight, int height )
+{
+    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, i_4 = 4;
+
+    i_offset <<= i_denom;
+    i_offset += (1 << ( i_denom -1 ));
+
+    __asm__ volatile(
+    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr5,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr6,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vilvl.w          $vr4,             $vr6,                 $vr5                   \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           5         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
+      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+static void mc_weight_w4_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                     const x264_weight_t *weight, int height )
+{
+    int i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, i_4 = 4;
+
+    __asm__ volatile(
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr5,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr6,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vilvl.w          $vr4,             $vr6,                 $vr5                   \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           5         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
+      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+static void mc_weight_w8_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                     const x264_weight_t *weight, int height )
+{
+    int i_4 = 4;
+    int i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, src_stride2, src_stride3, src_stride4;
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
+    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vilvl.d          $vr4,             $vr6,                 $vr5                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            0        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            2        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            1        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            3        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,            0        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,            2        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
+      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+static void mc_weight_w16_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                      const x264_weight_t *weight, int height )
+{
+    int i_4 = 4;
+    int i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, src_stride2, src_stride3, src_stride4, dst_stride2, dst_stride3, dst_stride4;
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "slli.d           %[dst_stride2],   %[dst_stride],        1                      \n\t"
+    "add.d            %[dst_stride3],   %[dst_stride2],       %[dst_stride]          \n\t"
+    "slli.d           %[dst_stride4],   %[dst_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
+    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "vext2xv.hu.bu    $xr5,             $xr5                                         \n\t"
+    "vext2xv.hu.bu    $xr6,             $xr6                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvmul.h          $xr5,             $xr5,                 $xr0                   \n\t"
+    "xvmul.h          $xr6,             $xr6,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
+    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
+    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr6,             $xr5,                 0                      \n\t"
+    "xvpermi.d        $xr3,             $xr4,                 8                      \n\t"
+    "xvpermi.d        $xr4,             $xr4,                 13                     \n\t"
+    "xvpermi.d        $xr5,             $xr6,                 8                      \n\t"
+    "xvpermi.d        $xr6,             $xr6,                 13                     \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
+    "vstx             $vr5,             %[dst],               %[dst_stride2]         \n\t"
+    "vstx             $vr6,             %[dst],               %[dst_stride3]         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride4]         \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvpermi.d        $xr3,             $xr4,                 8                      \n\t"
+    "xvpermi.d        $xr4,             $xr4,                 13                     \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride2]         \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4), [dst_stride2]"=&r"(dst_stride2),
+      [dst_stride3]"=&r"(dst_stride3), [dst_stride4]"=&r"(dst_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
+      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+static void mc_weight_w20_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                      const x264_weight_t *weight, int height )
+{
+    int i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "xvreplgr2vr.b    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "xvld             $xr3,             %[src],               0                      \n\t"
+    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
+    "xvldx            $xr5,             %[src],               %[src_stride2]         \n\t"
+    "xvldx            $xr6,             %[src],               %[src_stride3]         \n\t"
+    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr9,             $xr5,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr10,            $xr6,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr5,             $xr5,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr6,             $xr6,                 $xr0                   \n\t"
+    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
+    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
+    "xvsadd.h         $xr9,             $xr9,                 $xr1                   \n\t"
+    "xvsadd.h         $xr10,            $xr10,                $xr1                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
+    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
+    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
+    "xvmaxi.h         $xr9,             $xr9,                 0                      \n\t"
+    "xvmaxi.h         $xr10,            $xr10,                0                      \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
+    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr8,             $xr7,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr10,            $xr9,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr6,             $xr5,                 0                      \n\t"
+    "xvilvl.b         $xr3,             $xr4,                 $xr8                   \n\t"
+    "xvilvh.b         $xr4,             $xr4,                 $xr8                   \n\t"
+    "xvilvl.b         $xr5,             $xr6,                 $xr10                  \n\t"
+    "xvilvh.b         $xr6,             $xr6,                 $xr10                  \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr4,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr5,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr5,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr6,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr6,             %[dst],               16,          4         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "xvld             $xr3,             %[src],               0                      \n\t"
+    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
+    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
+    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
+    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr8,             $xr7,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvilvl.b         $xr3,             $xr4,                 $xr8                   \n\t"
+    "xvilvh.b         $xr4,             $xr4,                 $xr8                   \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr4,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+#define MC_WEIGHT(func)                                                                                             \
+static void (* mc##func##_wtab_lasx[6])( uint8_t *, intptr_t, uint8_t *, intptr_t, const x264_weight_t *, int ) =   \
+{                                                                                                                   \
+    mc_weight_w4##func##_lasx,                                                                                 \
+    mc_weight_w4##func##_lasx,                                                                                 \
+    mc_weight_w8##func##_lasx,                                                                                 \
+    mc_weight_w16##func##_lasx,                                                                                \
+    mc_weight_w16##func##_lasx,                                                                                \
+    mc_weight_w20##func##_lasx,                                                                                \
+};
+
+#if !HIGH_BIT_DEPTH
+MC_WEIGHT()
+MC_WEIGHT(_noden)
+#endif
+
+static void weight_cache_lasx( x264_t *h, x264_weight_t *w )
+{
+    if ( w->i_denom >= 1)
+    {
+        w->weightfn = mc_wtab_lasx;
+    }
+    else
+        w->weightfn = mc_noden_wtab_lasx;
+}
+
+static weight_fn_t mc_weight_wtab_lasx[6] =
+{
+    mc_weight_w4_lasx,
+    mc_weight_w4_lasx,
+    mc_weight_w8_lasx,
+    mc_weight_w16_lasx,
+    mc_weight_w16_lasx,
+    mc_weight_w20_lasx,
+};
+
+static void avc_biwgt_opscale_4x2_nw_lasx( uint8_t *p_src1,
+                                           int32_t i_src1_stride,
+                                           uint8_t *p_src2,
+                                           int32_t i_src2_stride,
+                                           uint8_t *p_dst, int32_t i_dst_stride,
+                                           int32_t i_log2_denom,
+                                           int32_t i_src1_weight,
+                                           int32_t i_src2_weight )
+{
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2;
+    __m256i denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+    src0 = __lasx_xvldrepl_w( p_src1, 0 );
+    p_src1 += i_src1_stride;
+    src1 = __lasx_xvldrepl_w( p_src1, 0 );
+    src2 = __lasx_xvpackev_w( src1, src0 );
+
+    src0 = __lasx_xvldrepl_w( p_src2, 0 );
+    p_src2 += i_src2_stride;
+    src1 = __lasx_xvldrepl_w( p_src2, 0 );
+    src0 = __lasx_xvpackev_w( src1, src0 );
+
+    LASX_ILVL_B_128SV( src0, src2, src0);
+
+    LASX_DP2_H_BU( src0, wgt, src0 );
+    src0 = __lasx_xvmaxi_h( src0, 0 );
+    src0 = __lasx_xvssrln_bu_h(src0, denom);
+
+    LASX_ST_W_2( src0, 0, 1, p_dst, i_dst_stride );
+}
+
+static void avc_biwgt_opscale_4x4multiple_nw_lasx( uint8_t *p_src1,
+                                                   int32_t i_src1_stride,
+                                                   uint8_t *p_src2,
+                                                   int32_t i_src2_stride,
+                                                   uint8_t *p_dst,
+                                                   int32_t i_dst_stride,
+                                                   int32_t i_height,
+                                                   int32_t i_log2_denom,
+                                                   int32_t i_src1_weight,
+                                                   int32_t i_src2_weight )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3, tmp0;
+    __m256i denom;
+    int32_t i_dst_stride_x4 = i_dst_stride << 2;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        src0 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src2 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src3 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        tmp0 = __lasx_xvpermi_q( src0, src1, 0x02 );
+
+        src0 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src1 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src2 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src3 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x02 );
+
+        LASX_ILVL_B_128SV( src0, tmp0, src0);
+
+        LASX_DP2_H_BU( src0, wgt, src0 );
+        src0 = __lasx_xvmaxi_h( src0, 0 );
+        src0 = __lasx_xvssrln_bu_h(src0, denom);
+
+        LASX_ST_W_4( src0, 0, 1, 4, 5, p_dst, i_dst_stride );
+        p_dst += i_dst_stride_x4;
+    }
+}
+
+static void avc_biwgt_opscale_4width_nw_lasx( uint8_t *p_src1,
+                                              int32_t i_src1_stride,
+                                              uint8_t *p_src2,
+                                              int32_t i_src2_stride,
+                                              uint8_t *p_dst,
+                                              int32_t i_dst_stride,
+                                              int32_t i_height,
+                                              int32_t i_log2_denom,
+                                              int32_t i_src1_weight,
+                                              int32_t i_src2_weight )
+{
+    if( 2 == i_height )
+    {
+        avc_biwgt_opscale_4x2_nw_lasx( p_src1, i_src1_stride,
+                                       p_src2, i_src2_stride,
+                                       p_dst, i_dst_stride,
+                                       i_log2_denom, i_src1_weight,
+                                       i_src2_weight );
+    }
+    else
+    {
+        avc_biwgt_opscale_4x4multiple_nw_lasx( p_src1, i_src1_stride,
+                                               p_src2, i_src2_stride,
+                                               p_dst, i_dst_stride,
+                                               i_height, i_log2_denom,
+                                               i_src1_weight,
+                                               i_src2_weight );
+    }
+}
+
+static void avc_biwgt_opscale_8width_nw_lasx( uint8_t *p_src1,
+                                              int32_t i_src1_stride,
+                                              uint8_t *p_src2,
+                                              int32_t i_src2_stride,
+                                              uint8_t *p_dst,
+                                              int32_t i_dst_stride,
+                                              int32_t i_height,
+                                              int32_t i_log2_denom,
+                                              int32_t i_src1_weight,
+                                              int32_t i_src2_weight )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3;
+    __m256i denom;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+#define BIWGT_OPSCALE_8W_NW                          \
+    src0 = __lasx_xvldrepl_d( p_src1, 0 );           \
+    p_src1 += i_src1_stride;                         \
+    src1 = __lasx_xvldrepl_d( p_src1, 0 );           \
+    p_src1 += i_src1_stride;                         \
+                                                     \
+    src2 = __lasx_xvldrepl_d( p_src2, 0 );           \
+    p_src2 += i_src2_stride;                         \
+    src3 = __lasx_xvldrepl_d( p_src2, 0 );           \
+    p_src2 += i_src2_stride;                         \
+                                                     \
+    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );     \
+    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );     \
+    LASX_ILVL_B_128SV( src1, src0, src0);            \
+                                                     \
+    LASX_DP2_H_BU( src0, wgt, src0 );                \
+    src0 = __lasx_xvmaxi_h( src0, 0 );               \
+    src0 = __lasx_xvssrln_bu_h(src0, denom);         \
+                                                     \
+    LASX_ST_D_2( src0, 0, 2, p_dst, i_dst_stride );  \
+    p_dst += i_dst_stride_x2;
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        BIWGT_OPSCALE_8W_NW;
+        BIWGT_OPSCALE_8W_NW;
+    }
+
+#undef BIWGT_OPSCALE_8W_NW
+
+}
+static void avc_biwgt_opscale_16width_nw_lasx( uint8_t *p_src1,
+                                               int32_t i_src1_stride,
+                                               uint8_t *p_src2,
+                                               int32_t i_src2_stride,
+                                               uint8_t *p_dst,
+                                               int32_t i_dst_stride,
+                                               int32_t i_height,
+                                               int32_t i_log2_denom,
+                                               int32_t i_src1_weight,
+                                               int32_t i_src2_weight )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i denom;
+    int32_t i_src1_stride_x4 = i_src1_stride << 2;
+    int32_t i_src2_stride_x4 = i_src2_stride << 2;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+#define BIWGT_OPSCALE_16W_NW( srcA, srcB )       \
+    srcA = __lasx_xvpermi_d( srcA, 0x50 );       \
+    srcB = __lasx_xvpermi_d( srcB, 0x50 );       \
+    LASX_ILVL_B_128SV( srcB, srcA, srcA);        \
+                                                 \
+    LASX_DP2_H_B( srcA, wgt, srcA );             \
+    srcA = __lasx_xvmaxi_h( srcA, 0 );           \
+    srcA = __lasx_xvssrln_bu_h(srcA, denom);     \
+                                                 \
+    LASX_ST_D( srcA, 0, p_dst );                 \
+    LASX_ST_D( srcA, 2, ( p_dst + 8 ) );         \
+    p_dst += i_dst_stride;
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        LASX_LD_4( p_src1, i_src1_stride, src0, src1, src2, src3 );
+        p_src1 += i_src1_stride_x4;
+
+        LASX_LD_4( p_src2, i_src2_stride, src4, src5, src6, src7 );
+        p_src2 += i_src2_stride_x4;
+
+        BIWGT_OPSCALE_16W_NW( src0, src4 );
+        BIWGT_OPSCALE_16W_NW( src1, src5 );
+        BIWGT_OPSCALE_16W_NW( src2, src6 );
+        BIWGT_OPSCALE_16W_NW( src3, src7 );
+    }
+
+#undef BIWGT_OPSCALE_16W_NW
+
+}
+
+static void avc_biwgt_opscale_4x2_lasx( uint8_t *p_src1,
+                                        int32_t i_src1_stride,
+                                        uint8_t *p_src2,
+                                        int32_t i_src2_stride,
+                                        uint8_t *p_dst, int32_t i_dst_stride,
+                                        int32_t i_log2_denom,
+                                        int32_t i_src1_weight,
+                                        int32_t i_src2_weight,
+                                        int32_t i_offset_in )
+{
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2;
+    __m256i denom, offset;
+
+    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+    offset = __lasx_xvreplgr2vr_h( i_offset_in );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+    src0 = __lasx_xvldrepl_w( p_src1, 0 );
+    p_src1 += i_src1_stride;
+    src1 = __lasx_xvldrepl_w( p_src1, 0 );
+    src2 = __lasx_xvpackev_w( src1, src0 );
+
+    src0 = __lasx_xvldrepl_w( p_src2, 0 );
+    p_src2 += i_src2_stride;
+    src1 = __lasx_xvldrepl_w( p_src2, 0 );
+    src0 = __lasx_xvpackev_w( src1, src0 );
+
+    LASX_ILVL_B_128SV( src0, src2, src0);
+
+    LASX_DP2_H_BU( src0, wgt, src0 );
+    src0 = __lasx_xvsadd_h( src0, offset );
+    src0 = __lasx_xvmaxi_h( src0, 0 );
+    src0 = __lasx_xvssrln_bu_h(src0, denom);
+
+    LASX_ST_W_2( src0, 0, 1, p_dst, i_dst_stride );
+}
+
+static void avc_biwgt_opscale_4x4multiple_lasx( uint8_t *p_src1,
+                                                int32_t i_src1_stride,
+                                                uint8_t *p_src2,
+                                                int32_t i_src2_stride,
+                                                uint8_t *p_dst,
+                                                int32_t i_dst_stride,
+                                                int32_t i_height,
+                                                int32_t i_log2_denom,
+                                                int32_t i_src1_weight,
+                                                int32_t i_src2_weight,
+                                                int32_t i_offset_in )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3, tmp0;
+    __m256i denom, offset;
+    int32_t i_dst_stride_x4 = i_dst_stride << 2;
+
+    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+    offset = __lasx_xvreplgr2vr_h( i_offset_in );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        src0 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src2 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src3 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        tmp0 = __lasx_xvpermi_q( src0, src1, 0x02 );
+
+        src0 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src1 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src2 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src3 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x02 );
+
+        LASX_ILVL_B_128SV( src0, tmp0, src0);
+
+        LASX_DP2_H_BU( src0, wgt, src0 );
+        src0 = __lasx_xvsadd_h( src0, offset );
+        src0 = __lasx_xvmaxi_h( src0, 0 );
+        src0 = __lasx_xvssrln_bu_h(src0, denom);
+
+        LASX_ST_W_4( src0, 0, 1, 4, 5, p_dst, i_dst_stride );
+        p_dst += i_dst_stride_x4;
+    }
+}
+
+static void avc_biwgt_opscale_4width_lasx( uint8_t *p_src1,
+                                           int32_t i_src1_stride,
+                                           uint8_t *p_src2,
+                                           int32_t i_src2_stride,
+                                           uint8_t *p_dst,
+                                           int32_t i_dst_stride,
+                                           int32_t i_height,
+                                           int32_t i_log2_denom,
+                                           int32_t i_src1_weight,
+                                           int32_t i_src2_weight,
+                                           int32_t i_offset_in )
+{
+    if( 2 == i_height )
+    {
+        avc_biwgt_opscale_4x2_lasx( p_src1, i_src1_stride,
+                                    p_src2, i_src2_stride,
+                                    p_dst, i_dst_stride,
+                                    i_log2_denom, i_src1_weight,
+                                    i_src2_weight, i_offset_in );
+    }
+    else
+    {
+        avc_biwgt_opscale_4x4multiple_lasx( p_src1, i_src1_stride,
+                                            p_src2, i_src2_stride,
+                                            p_dst, i_dst_stride,
+                                            i_height, i_log2_denom,
+                                            i_src1_weight,
+                                            i_src2_weight, i_offset_in );
+    }
+}
+
+static void avc_biwgt_opscale_8width_lasx( uint8_t *p_src1,
+                                           int32_t i_src1_stride,
+                                           uint8_t *p_src2,
+                                           int32_t i_src2_stride,
+                                           uint8_t *p_dst,
+                                           int32_t i_dst_stride,
+                                           int32_t i_height,
+                                           int32_t i_log2_denom,
+                                           int32_t i_src1_weight,
+                                           int32_t i_src2_weight,
+                                           int32_t i_offset_in )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3;
+    __m256i denom, offset;
+    int32_t i_dst_stride_x2 = ( i_dst_stride << 1 );
+
+    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+    offset = __lasx_xvreplgr2vr_h( i_offset_in );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+#define BIWGT_OPSCALE_8W                             \
+    src0 = __lasx_xvldrepl_d( p_src1, 0 );           \
+    p_src1 += i_src1_stride;                         \
+    src1 = __lasx_xvldrepl_d( p_src1, 0 );           \
+    p_src1 += i_src1_stride;                         \
+                                                     \
+    src2 = __lasx_xvldrepl_d( p_src2, 0 );           \
+    p_src2 += i_src2_stride;                         \
+    src3 = __lasx_xvldrepl_d( p_src2, 0 );           \
+    p_src2 += i_src2_stride;                         \
+                                                     \
+    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );     \
+    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );     \
+    LASX_ILVL_B_128SV( src1, src0, src0);            \
+                                                     \
+    LASX_DP2_H_BU( src0, wgt, src0 );                \
+    src0 = __lasx_xvsadd_h( src0, offset );          \
+    src0 = __lasx_xvmaxi_h( src0, 0 );               \
+    src0 = __lasx_xvssrln_bu_h(src0, denom);         \
+                                                     \
+    LASX_ST_D_2( src0, 0, 2, p_dst, i_dst_stride );  \
+    p_dst += i_dst_stride_x2;
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        BIWGT_OPSCALE_8W;
+        BIWGT_OPSCALE_8W;
+    }
+
+#undef BIWGT_OPSCALE_8W
+
+}
+
+static void avc_biwgt_opscale_16width_lasx( uint8_t *p_src1,
+                                            int32_t i_src1_stride,
+                                            uint8_t *p_src2,
+                                            int32_t i_src2_stride,
+                                            uint8_t *p_dst,
+                                            int32_t i_dst_stride,
+                                            int32_t i_height,
+                                            int32_t i_log2_denom,
+                                            int32_t i_src1_weight,
+                                            int32_t i_src2_weight,
+                                            int32_t i_offset_in )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i denom, offset;
+    int32_t i_src1_stride_x4 = i_src1_stride << 2;
+    int32_t i_src2_stride_x4 = i_src2_stride << 2;
+
+    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+    offset = __lasx_xvreplgr2vr_h( i_offset_in );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+#define BIWGT_OPSCALE_16W( srcA, srcB )          \
+    srcA = __lasx_xvpermi_d( srcA, 0x50 );       \
+    srcB = __lasx_xvpermi_d( srcB, 0x50 );       \
+    LASX_ILVL_B_128SV( srcB, srcA, srcA);        \
+                                                 \
+    LASX_DP2_H_BU( srcA, wgt, srcA );            \
+    srcA = __lasx_xvsadd_h( srcA, offset );      \
+    srcA = __lasx_xvmaxi_h( srcA, 0 );           \
+    srcA = __lasx_xvssrln_bu_h(srcA, denom);     \
+                                                 \
+    LASX_ST_D( srcA, 0, p_dst );                 \
+    LASX_ST_D( srcA, 2, ( p_dst + 8 ) );         \
+    p_dst += i_dst_stride;
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        LASX_LD_4( p_src1, i_src1_stride, src0, src1, src2, src3 );
+        p_src1 += i_src1_stride_x4;
+
+        LASX_LD_4( p_src2, i_src2_stride, src4, src5, src6, src7 );
+        p_src2 += i_src2_stride_x4;
+
+        BIWGT_OPSCALE_16W( src0, src4 );
+        BIWGT_OPSCALE_16W( src1, src5 );
+        BIWGT_OPSCALE_16W( src2, src6 );
+        BIWGT_OPSCALE_16W( src3, src7 );
+    }
+
+#undef BIWGT_OPSCALE_16W
+
+}
+
+static void avg_src_width4_lasx( uint8_t *p_src1, int32_t i_src1_stride,
+                                 uint8_t *p_src2, int32_t i_src2_stride,
+                                 uint8_t *p_dst, int32_t i_dst_stride,
+                                 int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+    __m256i dst0, dst1;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
+    int32_t i_src2_stride_x2 = i_src2_stride << 1;
+
+    for( i_cnt = ( i_height >> 1 ); i_cnt--; )
+    {
+        LASX_LD_2( p_src1, i_src1_stride, src0, src1);
+        p_src1 += i_src1_stride_x2;
+        LASX_LD_2( p_src2, i_src2_stride, dst0, dst1);
+        p_src2 += i_src2_stride_x2;
+
+        LASX_AVER_BU_2( src0, dst0, src1, dst1, dst0, dst1 );
+        LASX_ST_W( dst0, 0, p_dst );
+        p_dst += i_dst_stride;
+        LASX_ST_W( dst1, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+}
+
+static void avg_src_width8_lasx( uint8_t *p_src1, int32_t i_src1_stride,
+                                 uint8_t *p_src2, int32_t i_src2_stride,
+                                 uint8_t *p_dst, int32_t i_dst_stride,
+                                 int32_t i_height )
+{
+    int32_t i_cnt = i_height >> 2;
+    int32_t i_src1_stride_x2, i_src1_stride_x3, i_src1_stride_x4;
+    int32_t i_src2_stride_x2, i_src2_stride_x3, i_src2_stride_x4;
+
+    __asm__ volatile(
+    "slli.w    %[src1_stride2],  %[src1_stride1],  1                    \n\t"
+    "add.w     %[src1_stride3],  %[src1_stride2],  %[src1_stride1]      \n\t"
+    "slli.w    %[src1_stride4],  %[src1_stride1],  2                    \n\t"
+    "slli.w    %[src2_stride2],  %[src2_stride1],  1                    \n\t"
+    "add.w     %[src2_stride3],  %[src2_stride2],  %[src2_stride1]      \n\t"
+    "slli.w    %[src2_stride4],  %[src2_stride1],  2                    \n\t"
+    "beqz      %[cnt],           2f                                     \n\t"
+    "1:                                                                 \n\t"
+    "addi.w    %[cnt],           %[cnt],           -1                   \n\t"
+    "vld       $vr0,             %[src1],          0                    \n\t"
+    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
+    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
+    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
+    "vld       $vr4,             %[src2],          0                    \n\t"
+    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
+    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
+    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
+    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
+    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
+    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
+    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
+    "vstelm.d  $vr0,             %[dst],           0,              0    \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
+    "vstelm.d  $vr1,             %[dst],           0,              0    \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
+    "vstelm.d  $vr2,             %[dst],           0,              0    \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
+    "vstelm.d  $vr3,             %[dst],           0,              0    \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
+    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
+    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
+    "bnez      %[cnt],           1b                                     \n\t"
+    "2:                                                                 \n\t"
+     : [src1]"+&r"( (uint8_t *) p_src1 ),
+       [src2]"+&r"( (uint8_t *) p_src2 ),
+       [src1_stride2]"=&r"(i_src1_stride_x2),
+       [src1_stride3]"=&r"(i_src1_stride_x3),
+       [src1_stride4]"=&r"(i_src1_stride_x4),
+       [src2_stride2]"=&r"(i_src2_stride_x2),
+       [src2_stride3]"=&r"(i_src2_stride_x3),
+       [src2_stride4]"=&r"(i_src2_stride_x4),
+       [dst]"+&r"( (uint8_t *) p_dst ), [cnt]"+&r"(i_cnt)
+     : [src1_stride1]"r"(i_src1_stride),
+       [src2_stride1]"r"(i_src2_stride),
+       [dst_stride1]"r"(i_dst_stride)
+     : "memory"
+    );
+}
+
+static void avg_src_width16_lasx( uint8_t *p_src1, int32_t i_src1_stride,
+                                  uint8_t *p_src2, int32_t i_src2_stride,
+                                  uint8_t *p_dst, int32_t i_dst_stride,
+                                  int32_t i_height )
+{
+    int32_t i_cnt = i_height >> 3;
+    int32_t i_src1_stride_x2, i_src1_stride_x3, i_src1_stride_x4;
+    int32_t i_src2_stride_x2, i_src2_stride_x3, i_src2_stride_x4;
+    int32_t i_dst_stride_x2, i_dst_stride_x3, i_dst_stride_x4;
+
+    __asm__ volatile(
+    "slli.w    %[src1_stride2],  %[src1_stride1],  1                    \n\t"
+    "add.w     %[src1_stride3],  %[src1_stride2],  %[src1_stride1]      \n\t"
+    "slli.w    %[src1_stride4],  %[src1_stride1],  2                    \n\t"
+    "slli.w    %[src2_stride2],  %[src2_stride1],  1                    \n\t"
+    "add.w     %[src2_stride3],  %[src2_stride2],  %[src2_stride1]      \n\t"
+    "slli.w    %[src2_stride4],  %[src2_stride1],  2                    \n\t"
+    "slli.w    %[dst_stride2],   %[dst_stride1],   1                    \n\t"
+    "add.w     %[dst_stride3],   %[dst_stride2],   %[dst_stride1]       \n\t"
+    "slli.w    %[dst_stride4],   %[dst_stride1],   2                    \n\t"
+    "beqz      %[cnt],           2f                                     \n\t"
+    "1:                                                                 \n\t"
+    "addi.w    %[cnt],           %[cnt],           -1                   \n\t"
+    "vld       $vr0,             %[src1],          0                    \n\t"
+    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
+    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
+    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
+    "vld       $vr4,             %[src2],          0                    \n\t"
+    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
+    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
+    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
+    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
+    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
+    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
+    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
+    "vst       $vr0,             %[dst],           0                    \n\t"
+    "vstx      $vr1,             %[dst],           %[dst_stride1]       \n\t"
+    "vstx      $vr2,             %[dst],           %[dst_stride2]       \n\t"
+    "vstx      $vr3,             %[dst],           %[dst_stride3]       \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride4]       \n\t"
+    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
+    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
+
+    "vld       $vr0,             %[src1],          0                    \n\t"
+    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
+    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
+    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
+    "vld       $vr4,             %[src2],          0                    \n\t"
+    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
+    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
+    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
+    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
+    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
+    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
+    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
+    "vst       $vr0,             %[dst],           0                    \n\t"
+    "vstx      $vr1,             %[dst],           %[dst_stride1]       \n\t"
+    "vstx      $vr2,             %[dst],           %[dst_stride2]       \n\t"
+    "vstx      $vr3,             %[dst],           %[dst_stride3]       \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride4]       \n\t"
+    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
+    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
+
+    "bnez      %[cnt],           1b                                     \n\t"
+    "2:                                                                 \n\t"
+     : [src1]"+&r"( (uint8_t *) p_src1 ),
+       [src2]"+&r"( (uint8_t *) p_src2 ),
+       [src1_stride2]"=&r"(i_src1_stride_x2),
+       [src1_stride3]"=&r"(i_src1_stride_x3),
+       [src1_stride4]"=&r"(i_src1_stride_x4),
+       [src2_stride2]"=&r"(i_src2_stride_x2),
+       [src2_stride3]"=&r"(i_src2_stride_x3),
+       [src2_stride4]"=&r"(i_src2_stride_x4),
+       [dst_stride2]"=&r"(i_dst_stride_x2),
+       [dst_stride3]"=&r"(i_dst_stride_x3),
+       [dst_stride4]"=&r"(i_dst_stride_x4),
+       [dst]"+&r"( (uint8_t *) p_dst ), [cnt]"+&r"(i_cnt)
+     : [src1_stride1]"r"(i_src1_stride),
+       [src2_stride1]"r"(i_src2_stride),
+       [dst_stride1]"r"(i_dst_stride)
+     : "memory"
+    );
+}
+
+static void *x264_memcpy_aligned_lasx(void *dst, const void *src, size_t n)
+{
+    int64_t zero = 0, d;
+
+    __asm__ volatile(
+    "andi      %[d],            %[n],              16                   \n\t"
+    "beqz      %[d],            2f                                      \n\t"
+    "addi.d    %[n],            %[n],              -16                  \n\t"
+    "vld       $vr0,            %[src],            0                    \n\t"
+    "vst       $vr0,            %[dst],            0                    \n\t"
+    "addi.d    %[src],          %[src],            16                   \n\t"
+    "addi.d    %[dst],          %[dst],            16                   \n\t"
+    "2:                                                                 \n\t"
+    "andi      %[d],            %[n],              32                   \n\t"
+    "beqz      %[d],            3f                                      \n\t"
+    "addi.d    %[n],            %[n],              -32                  \n\t"
+    "xvld      $xr0,            %[src],            0                    \n\t"
+    "xvst      $xr0,            %[dst],            0                    \n\t"
+    "addi.d    %[src],          %[src],            32                   \n\t"
+    "addi.d    %[dst],          %[dst],            32                   \n\t"
+    "3:                                                                 \n\t"
+    "beqz      %[n],            5f                                      \n\t"
+    "4:                                                                 \n\t"
+    "addi.d    %[n],            %[n],              -64                  \n\t"
+    "xvld      $xr0,            %[src],            32                   \n\t"
+    "xvld      $xr1,            %[src],            0                    \n\t"
+    "xvst      $xr0,            %[dst],            32                   \n\t"
+    "xvst      $xr1,            %[dst],            0                    \n\t"
+    "addi.d    %[src],          %[src],            64                   \n\t"
+    "addi.d    %[dst],          %[dst],            64                   \n\t"
+    "blt       %[zero],         %[n],              4b                   \n\t"
+    "5:                                                                 \n\t"
+    : [dst]"+&r"((void *) dst), [src]"+&r"((void *) src), [n]"+&r"((int64_t) n),
+      [d]"=&r"(d)
+    : [zero]"r"(zero)
+    : "memory"
+    );
+    return NULL;
+}
+
+static void pixel_avg_16x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                  uint8_t *p_pix2, intptr_t pix2_stride,
+                                  uint8_t *p_pix3, intptr_t pix3_stride,
+                                  int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width16_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                              p_pix1, pix1_stride, 16 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_16width_nw_lasx( p_pix2, pix2_stride,
+                                           p_pix3, pix3_stride,
+                                           p_pix1, pix1_stride,
+                                           16, 5, i_weight,
+                                           ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_16width_lasx( p_pix2, pix2_stride,
+                                        p_pix3, pix3_stride,
+                                        p_pix1, pix1_stride,
+                                        16, 5, i_weight,
+                                        ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_16x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                 uint8_t *p_pix2, intptr_t pix2_stride,
+                                 uint8_t *p_pix3, intptr_t pix3_stride,
+                                 int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width16_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                              p_pix1, pix1_stride, 8 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_16width_nw_lasx( p_pix2, pix2_stride,
+                                           p_pix3, pix3_stride,
+                                           p_pix1, pix1_stride,
+                                           8, 5, i_weight,
+                                           ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_16width_lasx( p_pix2, pix2_stride,
+                                        p_pix3, pix3_stride,
+                                        p_pix1, pix1_stride,
+                                        8, 5, i_weight,
+                                        ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_8x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                 uint8_t *p_pix2, intptr_t pix2_stride,
+                                 uint8_t *p_pix3, intptr_t pix3_stride,
+                                 int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 16 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 16, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 16, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_8x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 8 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 8, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 8, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_8x4_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 4 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 4, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 4, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_4x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                 uint8_t *p_pix2, intptr_t pix2_stride,
+                                 uint8_t *p_pix3, intptr_t pix3_stride,
+                                 int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 16 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 16, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 16, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_4x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 8 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 8, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 8, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_4x4_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 4 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 4, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 4, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_4x2_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 2 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_4x2_nw_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 5, i_weight,
+                                       ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_4x2_lasx( p_pix2, pix2_stride,
+                                    p_pix3, pix3_stride,
+                                    p_pix1, pix1_stride, 5, i_weight,
+                                    ( 64 - i_weight ), 0 );
+    }
+}
+
+static inline void avg_src_width16_no_align_lasx( uint8_t *p_src1,
+                                                  int32_t i_src1_stride,
+                                                  uint8_t *p_src2,
+                                                  int32_t i_src2_stride,
+                                                  uint8_t *p_dst,
+                                                  int32_t i_dst_stride,
+                                                  int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = LASX_LD( p_src1 );
+        p_src1 += i_src1_stride;
+        src1 = LASX_LD( p_src2 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        LASX_ST_D( src0, 0, p_dst );
+        LASX_ST_D( src0, 1, ( p_dst + 8 ) );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void avg_src_width20_no_align_lasx( uint8_t *p_src1,
+                                                  int32_t i_src1_stride,
+                                                  uint8_t *p_src2,
+                                                  int32_t i_src2_stride,
+                                                  uint8_t *p_dst,
+                                                  int32_t i_dst_stride,
+                                                  int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = LASX_LD( p_src1 );
+        p_src1 += i_src1_stride;
+        src1 = LASX_LD( p_src2 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        LASX_ST_D( src0, 0, p_dst );
+        LASX_ST_D( src0, 1, ( p_dst + 8 ) );
+        LASX_ST_W( src0, 4, ( p_dst + 16 ) );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void avg_src_width12_no_align_lasx( uint8_t *p_src1,
+                                                  int32_t i_src1_stride,
+                                                  uint8_t *p_src2,
+                                                  int32_t i_src2_stride,
+                                                  uint8_t *p_dst,
+                                                  int32_t i_dst_stride,
+                                                  int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = LASX_LD( p_src1 );
+        p_src1 += i_src1_stride;
+        src1 = LASX_LD( p_src2 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        LASX_ST_D( src0, 0, p_dst );
+        LASX_ST_W( src0, 2, ( p_dst + 8 ) );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void avg_src_width8_no_align_lasx( uint8_t *p_src1,
+                                                 int32_t i_src1_stride,
+                                                 uint8_t *p_src2,
+                                                 int32_t i_src2_stride,
+                                                 uint8_t *p_dst,
+                                                 int32_t i_dst_stride,
+                                                 int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = LASX_LD( p_src1 );
+        p_src1 += i_src1_stride;
+        src1 = LASX_LD( p_src2 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        LASX_ST_D( src0, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void avg_src_width4_no_align_lasx( uint8_t *p_src1,
+                                                 int32_t i_src1_stride,
+                                                 uint8_t *p_src2,
+                                                 int32_t i_src2_stride,
+                                                 uint8_t *p_dst,
+                                                 int32_t i_dst_stride,
+                                                 int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = LASX_LD( p_src1 );
+        p_src1 += i_src1_stride;
+        src1 = LASX_LD( p_src2 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        LASX_ST_W( src0, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void mc_weight_w16_no_align_lasx( uint8_t *p_dst,
+                                                intptr_t i_dst_stride,
+                                                uint8_t *p_src,
+                                                intptr_t i_src_stride,
+                                                const x264_weight_t *pWeight,
+                                                int32_t i_height )
+{
+    int32_t i_log2_denom = pWeight->i_denom;
+    int32_t i_offset = pWeight->i_offset;
+    int32_t i_weight = pWeight->i_scale;
+    uint8_t u_cnt;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i src;
+    __m256i wgt, denom, offset;
+
+    i_offset <<= ( i_log2_denom );
+
+    if( i_log2_denom )
+    {
+        i_offset += ( 1 << ( i_log2_denom - 1 ) );
+    }
+
+    wgt =  __lasx_xvreplgr2vr_h( i_weight );
+    offset = __lasx_xvreplgr2vr_h( i_offset );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
+
+    for( u_cnt = i_height; u_cnt--; )
+    {
+        src = LASX_LD( p_src);
+        p_src += i_src_stride;
+
+        src = __lasx_xvpermi_d( src, 0x50 );
+        LASX_ILVL_B_128SV( zero, src, src);
+
+        src = __lasx_xvmul_h( src, wgt );
+        src = __lasx_xvsadd_h( src, offset );
+        src = __lasx_xvmaxi_h( src, 0 );
+        src = __lasx_xvssrln_bu_h(src, denom);
+
+        LASX_ST_D( src, 0, p_dst );
+        LASX_ST_D( src, 2, ( p_dst + 8 ) );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void mc_weight_w8_no_align_lasx( uint8_t *p_dst,
+                                               intptr_t i_dst_stride,
+                                               uint8_t *p_src,
+                                               intptr_t i_src_stride,
+                                               const x264_weight_t *pWeight,
+                                               int32_t i_height )
+{
+    int32_t i_log2_denom = pWeight->i_denom;
+    int32_t i_offset = pWeight->i_offset;
+    int32_t i_weight = pWeight->i_scale;
+    uint8_t u_cnt;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i src;
+    __m256i wgt, denom, offset;
+
+    i_offset <<= ( i_log2_denom );
+
+    if( i_log2_denom )
+    {
+        i_offset += ( 1 << ( i_log2_denom - 1 ) );
+    }
+
+    wgt =  __lasx_xvreplgr2vr_h( i_weight );
+    offset = __lasx_xvreplgr2vr_h( i_offset );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
+
+    for( u_cnt = i_height; u_cnt--; )
+    {
+        src = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+
+        LASX_ILVL_B_128SV( zero, src, src);
+
+        src = __lasx_xvmul_h( src, wgt );
+        src = __lasx_xvsadd_h( src, offset );
+        src = __lasx_xvmaxi_h( src, 0 );
+        src = __lasx_xvssrln_bu_h(src, denom);
+
+        LASX_ST_D( src, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void mc_weight_w4_no_align_lasx( uint8_t *p_dst,
+                                               intptr_t i_dst_stride,
+                                               uint8_t *p_src,
+                                               intptr_t i_src_stride,
+                                               const x264_weight_t *pWeight,
+                                               int32_t i_height )
+{
+    int32_t i_log2_denom = pWeight->i_denom;
+    int32_t i_offset = pWeight->i_offset;
+    int32_t i_weight = pWeight->i_scale;
+    uint8_t u_cnt;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i src;
+    __m256i wgt, denom, offset;
+
+    i_offset <<= ( i_log2_denom );
+
+    if( i_log2_denom )
+    {
+        i_offset += ( 1 << ( i_log2_denom - 1 ) );
+    }
+
+    wgt =  __lasx_xvreplgr2vr_h( i_weight );
+    offset = __lasx_xvreplgr2vr_h( i_offset );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
+
+    for( u_cnt = i_height; u_cnt--; )
+    {
+        src = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+
+        LASX_ILVL_B_128SV( zero, src, src);
+
+        src = __lasx_xvmul_h( src, wgt );
+        src = __lasx_xvsadd_h( src, offset );
+        src = __lasx_xvmaxi_h( src, 0 );
+        src = __lasx_xvssrln_bu_h(src, denom);
+
+        LASX_ST_W( src, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void mc_weight_w20_no_align_lasx( uint8_t *p_dst,
+                                                intptr_t i_dst_stride,
+                                                uint8_t *p_src,
+                                                intptr_t i_src_stride,
+                                                const x264_weight_t *pWeight,
+                                                int32_t i_height )
+{
+    mc_weight_w16_no_align_lasx( p_dst, i_dst_stride,
+                                 p_src, i_src_stride,
+                                 pWeight, i_height );
+    mc_weight_w4_no_align_lasx( p_dst + 16, i_dst_stride,
+                                p_src + 16, i_src_stride,
+                                pWeight, i_height );
+}
+
+void x264_pixel_avg2_w4_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+                              intptr_t i_src_stride, uint8_t *src2, int i_height)
+{
+    int64_t zero = 2, i_4 = 4;
+
+    __asm__ volatile(
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
+    "vldrepl.w      $vr0,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr1,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vldrepl.w      $vr2,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr3,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vldrepl.w      $vr4,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr5,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vldrepl.w      $vr6,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr7,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
+    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
+    "vavgr.bu       $vr2,             $vr4,                 $vr5                   \n\t"
+    "vavgr.bu       $vr3,             $vr6,                 $vr7                   \n\t"
+    "vstelm.w       $vr0,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.w       $vr1,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.w       $vr2,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.w       $vr3,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "bge            %[i_height],      %[i_4],               1b                     \n\t"
+    "beqz           %[i_height],      3f                                           \n\t"
+    "2:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
+    "vldrepl.w      $vr0,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr1,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vldrepl.w      $vr2,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr3,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
+    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
+    "vstelm.w       $vr0,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.w       $vr1,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "blt            %[zero],          %[i_height],          2b                     \n\t"
+    "3:                                                                            \n\t"
+    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
+      [dst]"+&r"(dst)
+    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+void x264_pixel_avg2_w8_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+                              intptr_t i_src_stride, uint8_t *src2, int i_height)
+{
+    int64_t zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
+
+    __asm__ volatile(
+    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
+    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
+    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
+    "vld            $vr0,             %[src1],              0                      \n\t"
+    "vld            $vr1,             %[src2],              0                      \n\t"
+    "vldx           $vr2,             %[src1],              %[i_src_stride]        \n\t"
+    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
+    "vldx           $vr4,             %[src1],              %[src_stride2]         \n\t"
+    "vldx           $vr5,             %[src2],              %[src_stride2]         \n\t"
+    "vldx           $vr6,             %[src1],              %[src_stride3]         \n\t"
+    "vldx           $vr7,             %[src2],              %[src_stride3]         \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
+    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
+    "vavgr.bu       $vr2,             $vr4,                 $vr5                   \n\t"
+    "vavgr.bu       $vr3,             $vr6,                 $vr7                   \n\t"
+    "vstelm.d       $vr0,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.d       $vr1,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.d       $vr2,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.d       $vr3,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "bge            %[i_height],      %[i_4],               1b                     \n\t"
+    "beqz           %[i_height],      3f                                           \n\t"
+    "2:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
+    "vld            $vr0,             %[src1],              0                      \n\t"
+    "vld            $vr1,             %[src2],              0                      \n\t"
+    "vldx           $vr2,             %[src1],              %[i_src_stride]        \n\t"
+    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
+    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
+    "vstelm.d       $vr0,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.d       $vr1,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "blt            %[zero],          %[i_height],          2b                     \n\t"
+    "3:                                                                            \n\t"
+    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
+      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2), [src_stride3]"=&r"(src_stride3),
+      [src_stride4]"=&r"(src_stride4)
+    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+void x264_pixel_avg2_w16_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+                               intptr_t i_src_stride, uint8_t *src2, int i_height)
+{
+    int64_t src_stride2, dst_stride2, dst_stride3, src_stride3, src_stride4, dst_stride4;
+    int64_t zero = 0, i_4 = 4;
+
+    __asm__ volatile(
+    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
+    "slli.d         %[dst_stride2],   %[i_dst_stride],      1                      \n\t"
+    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
+    "add.d          %[dst_stride3],   %[dst_stride2],       %[i_dst_stride]        \n\t"
+    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "slli.d         %[dst_stride4],   %[dst_stride2],       1                      \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
+    "vld            $vr0,             %[src1],              0                      \n\t"
+    "vldx           $vr1,             %[src1],              %[i_src_stride]        \n\t"
+    "vldx           $vr2,             %[src1],              %[src_stride2]         \n\t"
+    "vldx           $vr3,             %[src1],              %[src_stride3]         \n\t"
+    "vld            $vr4,             %[src2],              0                      \n\t"
+    "vldx           $vr5,             %[src2],              %[i_src_stride]        \n\t"
+    "vldx           $vr6,             %[src2],              %[src_stride2]         \n\t"
+    "vldx           $vr7,             %[src2],              %[src_stride3]         \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr4                   \n\t"
+    "vavgr.bu       $vr1,             $vr1,                 $vr5                   \n\t"
+    "vavgr.bu       $vr2,             $vr2,                 $vr6                   \n\t"
+    "vavgr.bu       $vr3,             $vr3,                 $vr7                   \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
+    "vst            $vr0,             %[dst],               0                      \n\t"
+    "vstx           $vr1,             %[dst],               %[i_dst_stride]        \n\t"
+    "vstx           $vr2,             %[dst],               %[dst_stride2]         \n\t"
+    "vstx           $vr3,             %[dst],               %[dst_stride3]         \n\t"
+    "add.d          %[dst],           %[dst],               %[dst_stride4]         \n\t"
+    "bge            %[i_height],      %[i_4],               1b                     \n\t"
+    "beqz           %[i_height],      3f                                           \n\t"
+    "2:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
+    "vld            $vr0,             %[src1],              0                      \n\t"
+    "vldx           $vr1,             %[src1],              %[i_src_stride]        \n\t"
+    "vld            $vr2,             %[src2],              0                      \n\t"
+    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr2                   \n\t"
+    "vavgr.bu       $vr1,             $vr1,                 $vr3                   \n\t"
+    "vst            $vr0,             %[dst],               0                      \n\t"
+    "vstx           $vr1,             %[dst],               %[i_dst_stride]        \n\t"
+    "add.d          %[dst],           %[dst],               %[dst_stride2]         \n\t"
+    "blt            %[zero],          %[i_height],          2b                     \n\t"
+    "3:                                                                            \n\t"
+    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
+      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2), [dst_stride2]"=&r"(dst_stride2),
+      [src_stride3]"=&r"(src_stride3), [dst_stride3]"=&r"(dst_stride3),
+      [src_stride4]"=&r"(src_stride4), [dst_stride4]"=&r"(dst_stride4)
+    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+void x264_pixel_avg2_w20_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+                               intptr_t i_src_stride, uint8_t *src2, int i_height)
+{
+    int64_t zero = 0, i_4 = 4;
+    int64_t src_stride2, src_stride3, src_stride4;
+
+    __asm__ volatile(
+    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
+    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
+    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
+    "xvld           $xr0,             %[src1],              0                      \n\t"
+    "xvldx          $xr1,             %[src1],              %[i_src_stride]        \n\t"
+    "xvldx          $xr2,             %[src1],              %[src_stride2]         \n\t"
+    "xvldx          $xr3,             %[src1],              %[src_stride3]         \n\t"
+    "xvld           $xr4,             %[src2],              0                      \n\t"
+    "xvldx          $xr5,             %[src2],              %[i_src_stride]        \n\t"
+    "xvldx          $xr6,             %[src2],              %[src_stride2]         \n\t"
+    "xvldx          $xr7,             %[src2],              %[src_stride3]         \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
+
+    "xvavgr.bu      $xr0,             $xr0,                 $xr4                   \n\t"
+    "xvavgr.bu      $xr1,             $xr1,                 $xr5                   \n\t"
+    "xvavgr.bu      $xr2,             $xr2,                 $xr6                   \n\t"
+    "xvavgr.bu      $xr3,             $xr3,                 $xr7                   \n\t"
+    "vst            $vr0,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr0,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vst            $vr1,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr1,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vst            $vr2,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr2,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vst            $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr3,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "bge            %[i_height],      %[i_4],          1b                     \n\t"
+    "beqz           %[i_height],      3f                                           \n\t"
+    "2:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
+    "xvld           $xr0,             %[src1],              0                      \n\t"
+    "xvldx          $xr1,             %[src1],              %[i_src_stride]        \n\t"
+    "xvld           $xr2,             %[src2],              0                      \n\t"
+    "xvldx          $xr3,             %[src2],              %[i_src_stride]        \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
+    "xvavgr.bu      $xr0,             $xr0,                 $xr2                   \n\t"
+    "xvavgr.bu      $xr1,             $xr1,                 $xr3                   \n\t"
+    "vst            $vr0,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr0,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vst            $vr1,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr1,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "blt            %[zero],          %[i_height],          2b                     \n\t"
+    "3:                                                                            \n\t"
+    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
+      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+static void (* const pixel_avg_wtab_lasx[6])(uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int ) =
+{
+    NULL,
+    x264_pixel_avg2_w4_lasx,
+    x264_pixel_avg2_w8_lasx,
+    x264_pixel_avg2_w16_lasx,
+    x264_pixel_avg2_w16_lasx,
+    x264_pixel_avg2_w20_lasx,
+};
+
+static uint8_t *get_ref_lasx( uint8_t *p_dst, intptr_t *p_dst_stride,
+                              uint8_t *p_src[4], intptr_t i_src_stride,
+                              int32_t m_vx, int32_t m_vy,
+                              int32_t i_width, int32_t i_height,
+                              const x264_weight_t *pWeight )
+{
+    int32_t i_qpel_idx;
+    int32_t i_offset;
+    uint8_t *p_src1;
+
+    i_qpel_idx = ( ( m_vy & 3 ) << 2 ) + ( m_vx & 3 );
+    i_offset = ( m_vy >> 2 ) * i_src_stride + ( m_vx >> 2 );
+    p_src1 = p_src[x264_hpel_ref0[i_qpel_idx]] + i_offset +
+           ( 3 == ( m_vy & 3 ) ) * i_src_stride;
+
+    if( i_qpel_idx & 5 )
+    {
+        uint8_t *p_src2 = p_src[x264_hpel_ref1[i_qpel_idx]] +
+                          i_offset + ( 3 == ( m_vx & 3 ) );
+        pixel_avg_wtab_lasx[i_width >> 2](
+                p_dst, *p_dst_stride, p_src1, i_src_stride,
+                p_src2, i_height );
+
+        if( pWeight->weightfn )
+        {
+            pWeight->weightfn[i_width>>2](p_dst, *p_dst_stride, p_dst, *p_dst_stride, pWeight, i_height);
+        }
+        return p_dst;
+    }
+    else if ( pWeight->weightfn )
+    {
+        pWeight->weightfn[i_width>>2]( p_dst, *p_dst_stride, p_src1, i_src_stride, pWeight, i_height );
+        return p_dst;
+    }
+    else
+    {
+        *p_dst_stride = i_src_stride;
+        return p_src1;
+    }
+}
+
+static void avc_interleaved_chroma_hv_2x2_lasx( uint8_t *p_src,
+                                                int32_t i_src_stride,
+                                                uint8_t *p_dst_u,
+                                                uint8_t *p_dst_v,
+                                                int32_t i_dst_stride,
+                                                uint32_t u_coef_hor0,
+                                                uint32_t u_coef_hor1,
+                                                uint32_t u_coef_ver0,
+                                                uint32_t u_coef_ver1 )
+{
+    __m256i src0, src1, src2, src3, src4;
+    __m256i mask, mask1;
+
+    __m256i coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
+    __m256i coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
+    __m256i coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
+    __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
+    __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
+
+    mask = LASX_LD( &pu_chroma_mask_arr[0] );
+    mask1 = __lasx_xvaddi_bu(mask, 1);
+
+    src0 = LASX_LD( p_src );
+    p_src += i_src_stride;
+
+    LASX_LD_2( p_src, i_src_stride, src1, src2 );
+
+    LASX_SHUF_B_2_128SV( src1, src0, src2, src1, mask1, mask1, src3, src4 );
+    LASX_SHUF_B_2_128SV( src1, src0, src2, src1, mask, mask, src0, src1 );
+
+    src0 = __lasx_xvpermi_q( src0, src3, 0x02 );
+    src1 = __lasx_xvpermi_q( src1, src4, 0x02 );
+    LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+    src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
+    src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
+    src0 = __lasx_xvadd_h( src0, src1 );
+    src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
+    LASX_ST_H_2( src0, 0, 2, p_dst_u, i_dst_stride );
+    LASX_ST_H_2( src0, 8, 10, p_dst_v, i_dst_stride );
+}
+
+static void avc_interleaved_chroma_hv_2x4_lasx( uint8_t *p_src,
+                                                int32_t i_src_stride,
+                                                uint8_t *p_dst_u,
+                                                uint8_t *p_dst_v,
+                                                int32_t i_dst_stride,
+                                                uint32_t u_coef_hor0,
+                                                uint32_t u_coef_hor1,
+                                                uint32_t u_coef_ver0,
+                                                uint32_t u_coef_ver1 )
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m256i mask, mask1;
+
+    __m256i coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
+    __m256i coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
+    __m256i coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
+    __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
+    __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
+
+    mask = LASX_LD( &pu_chroma_mask_arr[0] );
+    mask1 = __lasx_xvaddi_bu(mask, 1);
+
+    src0 = LASX_LD( p_src );
+    p_src += i_src_stride;
+
+    LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
+
+    LASX_SHUF_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
+                         mask1, mask1, mask1, mask1,
+                         src5, src6, src7, src8 );
+    LASX_SHUF_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
+                         mask, mask, mask, mask, src0, src1, src2, src3 );
+
+    src0 = __lasx_xvpermi_q( src0, src2, 0x02 );
+    src1 = __lasx_xvpermi_q( src1, src3, 0x02 );
+    LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+    src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
+    src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
+    src0 = __lasx_xvadd_h( src0, src1 );
+    src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
+    LASX_ST_H_4( src0, 0, 2, 8, 10, p_dst_u, i_dst_stride );
+
+    src0 = __lasx_xvpermi_q( src5, src7, 0x02 );
+    src1 = __lasx_xvpermi_q( src6, src8, 0x02 );
+    LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+    src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
+    src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
+    src0 = __lasx_xvadd_h( src0, src1 );
+    src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
+    LASX_ST_H_4( src0, 0, 2, 8, 10, p_dst_v, i_dst_stride );
+}
+
+static void avc_interleaved_chroma_hv_2w_lasx( uint8_t *p_src,
+                                               int32_t i_src_stride,
+                                               uint8_t *p_dst_u,
+                                               uint8_t *p_dst_v,
+                                               int32_t i_dst_stride,
+                                               uint32_t u_coef_hor0,
+                                               uint32_t u_coef_hor1,
+                                               uint32_t u_coef_ver0,
+                                               uint32_t u_coef_ver1,
+                                               int32_t i_height )
+{
+    if( 2 == i_height )
+    {
+        avc_interleaved_chroma_hv_2x2_lasx( p_src, i_src_stride,
+                                            p_dst_u, p_dst_v, i_dst_stride,
+                                            u_coef_hor0, u_coef_hor1,
+                                            u_coef_ver0, u_coef_ver1 );
+    }
+    else if( 4 == i_height )
+    {
+        avc_interleaved_chroma_hv_2x4_lasx( p_src, i_src_stride,
+                                            p_dst_u, p_dst_v, i_dst_stride,
+                                            u_coef_hor0, u_coef_hor1,
+                                            u_coef_ver0, u_coef_ver1 );
+    }
+}
+
+static void avc_interleaved_chroma_hv_4x2_lasx( uint8_t *p_src,
+                                                int32_t i_src_stride,
+                                                uint8_t *p_dst_u,
+                                                uint8_t *p_dst_v,
+                                                int32_t i_dst_stride,
+                                                uint32_t u_coef_hor0,
+                                                uint32_t u_coef_hor1,
+                                                uint32_t u_coef_ver0,
+                                                uint32_t u_coef_ver1 )
+{
+    __m256i src0, src1, src2, src3, src4;
+    __m256i mask, mask1;
+
+    __m256i coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
+    __m256i coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
+    __m256i coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
+    __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
+    __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
+
+    mask = LASX_LD( &pu_chroma_mask_arr[0] );
+    mask1 = __lasx_xvaddi_bu(mask, 1);
+
+    src0 = LASX_LD( p_src );
+    p_src += i_src_stride;
+
+    LASX_LD_2( p_src, i_src_stride, src1, src2 );
+
+    LASX_SHUF_B_2_128SV( src1, src0, src2, src1, mask1, mask1, src3, src4 );
+    LASX_SHUF_B_2_128SV( src1, src0, src2, src1, mask, mask, src0, src1 );
+
+    src0 = __lasx_xvpermi_q( src0, src3, 0x02 );
+    src1 = __lasx_xvpermi_q( src1, src4, 0x02 );
+    LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+    src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
+    src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
+    src0 = __lasx_xvadd_h( src0, src1 );
+    src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
+    LASX_ST_W_2( src0, 0, 1, p_dst_u, i_dst_stride );
+    LASX_ST_W_2( src0, 4, 5, p_dst_v, i_dst_stride );
+}
+
+static void avc_interleaved_chroma_hv_4x4mul_lasx( uint8_t *p_src,
+                                                   int32_t i_src_stride,
+                                                   uint8_t *p_dst_u,
+                                                   uint8_t *p_dst_v,
+                                                   int32_t i_dst_stride,
+                                                   uint32_t u_coef_hor0,
+                                                   uint32_t u_coef_hor1,
+                                                   uint32_t u_coef_ver0,
+                                                   uint32_t u_coef_ver1,
+                                                   int32_t i_height )
+{
+    uint32_t u_row;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m256i mask, mask1;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_dst_stride_x4 = i_dst_stride << 2;
+    __m256i coeff_hz_vec0, coeff_hz_vec1;
+    __m256i coeff_hz_vec;
+    __m256i coeff_vt_vec0, coeff_vt_vec1;
+
+    coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
+    coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
+    coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
+    coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
+    coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
+
+    mask = LASX_LD( &pu_chroma_mask_arr[0] );
+    mask1 = __lasx_xvaddi_bu(mask, 1);
+
+    src0 = LASX_LD( p_src );
+    p_src += i_src_stride;
+
+    for( u_row = ( i_height >> 2 ); u_row--; )
+    {
+        LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
+        p_src += i_src_stride_x4;
+
+        LASX_SHUF_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
+                             mask1, mask1, mask1, mask1,
+                             src5, src6, src7, src8 );
+        LASX_SHUF_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
+                             mask, mask, mask, mask, src0, src1, src2, src3 );
+
+        src0 = __lasx_xvpermi_q( src0, src2, 0x02 );
+        src1 = __lasx_xvpermi_q( src1, src3, 0x02 );
+        LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+        src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
+        src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
+        src0 = __lasx_xvadd_h( src0, src1 );
+        src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
+        LASX_ST_W_4( src0, 0, 1, 4, 5, p_dst_u, i_dst_stride );
+        p_dst_u += i_dst_stride_x4;
+
+        src0 = __lasx_xvpermi_q( src5, src7, 0x02 );
+        src1 = __lasx_xvpermi_q( src6, src8, 0x02 );
+        LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+        src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
+        src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
+        src0 = __lasx_xvadd_h( src0, src1 );
+        src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
+        LASX_ST_W_4( src0, 0, 1, 4, 5, p_dst_v, i_dst_stride );
+        p_dst_v += i_dst_stride_x4;
+
+        src0 = src4;
+    }
+}
+
+static void avc_interleaved_chroma_hv_4w_lasx( uint8_t *p_src,
+                                               int32_t i_src_stride,
+                                               uint8_t *p_dst_u,
+                                               uint8_t *p_dst_v,
+                                               int32_t i_dst_stride,
+                                               uint32_t u_coef_hor0,
+                                               uint32_t u_coef_hor1,
+                                               uint32_t u_coef_ver0,
+                                               uint32_t u_coef_ver1,
+                                               int32_t i_height )
+{
+    if( 2 == i_height )
+    {
+        avc_interleaved_chroma_hv_4x2_lasx( p_src, i_src_stride,
+                                            p_dst_u, p_dst_v, i_dst_stride,
+                                            u_coef_hor0, u_coef_hor1,
+                                            u_coef_ver0, u_coef_ver1 );
+    }
+    else
+    {
+        avc_interleaved_chroma_hv_4x4mul_lasx( p_src, i_src_stride,
+                                               p_dst_u, p_dst_v, i_dst_stride,
+                                               u_coef_hor0, u_coef_hor1,
+                                               u_coef_ver0, u_coef_ver1,
+                                               i_height );
+    }
+}
+
+static void avc_interleaved_chroma_hv_8w_lasx( uint8_t *p_src,
+                                               int32_t i_src_stride,
+                                               uint8_t *p_dst_u,
+                                               uint8_t *p_dst_v,
+                                               int32_t i_dst_stride,
+                                               uint32_t u_coef_hor0,
+                                               uint32_t u_coef_hor1,
+                                               uint32_t u_coef_ver0,
+                                               uint32_t u_coef_ver1,
+                                               int32_t i_height )
+{
+    uint32_t u_row;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m256i mask, mask1;
+    __m256i coeff_hz_vec0, coeff_hz_vec1;
+    __m256i coeff_hz_vec;
+    __m256i coeff_vt_vec0, coeff_vt_vec1;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    __m256i head_u, head_v;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_dst_stride_x4 = i_dst_stride << 2;
+
+    coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
+    coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
+    coeff_hz_vec = __lasx_xvilvl_b( coeff_hz_vec0, coeff_hz_vec1 );
+    coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
+    coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
+
+    mask = LASX_LD( &pu_chroma_mask_arr1[0] );
+    mask1 = __lasx_xvaddi_bu(mask, 1);
+
+    src0 = LASX_LD( p_src );
+    p_src += i_src_stride;
+    tmp0 = __lasx_xvpermi_q( src0, src0, 0x11 );
+    LASX_SHUF_B_2_128SV( tmp0, src0, tmp0, src0, mask, mask1, head_u, head_v );
+    LASX_DP2_H_BU_2( head_u, coeff_hz_vec, head_v,
+                     coeff_hz_vec, head_u, head_v );
+
+    for( u_row = ( i_height >> 2 ); u_row--; )
+    {
+        LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
+        p_src += i_src_stride_x4;
+        src5 = __lasx_xvpermi_q( src1, src2, 0x02 );
+        src6 = __lasx_xvpermi_q( src1, src2, 0x13 );
+        src7 = __lasx_xvpermi_q( src3, src4, 0x02 );
+        src8 = __lasx_xvpermi_q( src3, src4, 0x13 );
+
+        LASX_SHUF_B_2_128SV( src6, src5, src8, src7, mask, mask, tmp0, tmp1 );
+        LASX_DP2_H_BU_2( tmp0, coeff_hz_vec, tmp1, coeff_hz_vec, tmp0, tmp1);
+        tmp2 = __lasx_xvpermi_q( head_u, tmp0, 0x02 );
+        tmp3 = __lasx_xvpermi_q( tmp0, tmp1, 0x03 );
+        head_u = __lasx_xvpermi_q( tmp1, tmp1, 0x11 );
+
+        tmp0 = __lasx_xvmul_h( tmp0, coeff_vt_vec0 );
+        tmp1 = __lasx_xvmul_h( tmp1, coeff_vt_vec0 );
+        tmp2 = __lasx_xvmul_h( tmp2, coeff_vt_vec1 );
+        tmp3 = __lasx_xvmul_h( tmp3, coeff_vt_vec1 );
+        tmp0 = __lasx_xvadd_h( tmp0, tmp2 );
+        tmp1 = __lasx_xvadd_h( tmp1, tmp3 );
+
+        tmp0 = __lasx_xvssrlrni_bu_h(tmp1, tmp0, 6);
+        LASX_ST_D_4( tmp0, 0, 2, 1, 3, p_dst_u, i_dst_stride );
+        p_dst_u += i_dst_stride_x4;
+
+        LASX_SHUF_B_2_128SV( src6, src5, src8, src7, mask1, mask1, tmp0, tmp1 );
+        LASX_DP2_H_BU_2( tmp0, coeff_hz_vec, tmp1, coeff_hz_vec, tmp0, tmp1);
+        tmp2 = __lasx_xvpermi_q( head_v, tmp0, 0x02 );
+        tmp3 = __lasx_xvpermi_q( tmp0, tmp1, 0x03 );
+        head_v = __lasx_xvpermi_q( tmp1, tmp1, 0x11 );
+
+        tmp0 = __lasx_xvmul_h( tmp0, coeff_vt_vec0 );
+        tmp1 = __lasx_xvmul_h( tmp1, coeff_vt_vec0 );
+        tmp2 = __lasx_xvmul_h( tmp2, coeff_vt_vec1 );
+        tmp3 = __lasx_xvmul_h( tmp3, coeff_vt_vec1 );
+        tmp0 = __lasx_xvadd_h( tmp0, tmp2 );
+        tmp1 = __lasx_xvadd_h( tmp1, tmp3 );
+
+        tmp0 = __lasx_xvssrlrni_bu_h(tmp1, tmp0, 6);
+        LASX_ST_D_4( tmp0, 0, 2, 1, 3, p_dst_v, i_dst_stride );
+        p_dst_v += i_dst_stride_x4;
+    }
+}
+
+static void mc_chroma_lasx( uint8_t *p_dst_u, uint8_t *p_dst_v,
+                            intptr_t i_dst_stride,
+                            uint8_t *p_src, intptr_t i_src_stride,
+                            int32_t m_vx, int32_t m_vy,
+                            int32_t i_width, int32_t i_height )
+{
+    int32_t i_d8x = m_vx & 0x07;
+    int32_t i_d8y = m_vy & 0x07;
+    int32_t i_coeff_horiz1 = ( 8 - i_d8x );
+    int32_t i_coeff_vert1 = ( 8 - i_d8y );
+    int32_t i_coeff_horiz0 = i_d8x;
+    int32_t i_coeff_vert0 = i_d8y;
+
+    p_src += ( m_vy >> 3 ) * i_src_stride + ( m_vx >> 3 ) * 2;
+
+    if( 4 == i_width )
+    {
+        avc_interleaved_chroma_hv_4w_lasx( p_src, i_src_stride,
+                                           p_dst_u, p_dst_v, i_dst_stride,
+                                           i_coeff_horiz0, i_coeff_horiz1,
+                                           i_coeff_vert0, i_coeff_vert1,
+                                           i_height );
+    }
+    else if( 8 == i_width )
+    {
+        avc_interleaved_chroma_hv_8w_lasx( p_src, i_src_stride,
+                                           p_dst_u, p_dst_v, i_dst_stride,
+                                           i_coeff_horiz0, i_coeff_horiz1,
+                                           i_coeff_vert0, i_coeff_vert1,
+                                           i_height );
+    }
+    else if( 2 == i_width )
+    {
+        avc_interleaved_chroma_hv_2w_lasx( p_src, i_src_stride,
+                                           p_dst_u, p_dst_v, i_dst_stride,
+                                           i_coeff_horiz0, i_coeff_horiz1,
+                                           i_coeff_vert0, i_coeff_vert1,
+                                           i_height );
+    }
+}
+
+static void copy_width4_lasx( uint8_t *p_src, int32_t i_src_stride,
+                              uint8_t *p_dst, int32_t i_dst_stride,
+                              int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = ( i_height >> 1 ); i_cnt--;  )
+    {
+        src0 = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+        src1 = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+
+        LASX_ST_W( src0, 0, p_dst );
+        p_dst += i_dst_stride;
+        LASX_ST_W( src1, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+}
+
+static void copy_width8_lasx( uint8_t *p_src, int32_t i_src_stride,
+                              uint8_t *p_dst, int32_t i_dst_stride,
+                              int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1, src2, src3;
+
+#define COPY_W8_H4                                  \
+    src0 = __lasx_xvldrepl_d( p_src, 0 );           \
+    p_src += i_src_stride;                          \
+    src1 = __lasx_xvldrepl_d( p_src, 0 );           \
+    p_src += i_src_stride;                          \
+    src2 = __lasx_xvldrepl_d( p_src, 0 );           \
+    p_src += i_src_stride;                          \
+    src3 = __lasx_xvldrepl_d( p_src, 0 );           \
+    p_src += i_src_stride;                          \
+                                                    \
+    LASX_ST_D( src0, 0, p_dst );                    \
+    p_dst += i_dst_stride;                          \
+    LASX_ST_D( src1, 0, p_dst );                    \
+    p_dst += i_dst_stride;                          \
+    LASX_ST_D( src2, 0, p_dst );                    \
+    p_dst += i_dst_stride;                          \
+    LASX_ST_D( src3, 0, p_dst );                    \
+    p_dst += i_dst_stride;
+
+    if( 0 == i_height % 12 )
+    {
+        for( i_cnt = i_height; 0 < i_cnt; i_cnt -= 12 )
+        {
+            COPY_W8_H4;
+            COPY_W8_H4;
+            COPY_W8_H4;
+        }
+    }
+    else if( 0 == ( i_height & 7 ) )
+    {
+        for( i_cnt = ( i_height >> 3 ); i_cnt--; )
+        {
+            COPY_W8_H4;
+            COPY_W8_H4;
+        }
+    }
+    else if( 0 == ( i_height & 3 ) )
+    {
+        for( i_cnt = ( i_height >> 2 ); i_cnt--; )
+        {
+            COPY_W8_H4;
+        }
+    }
+
+#undef COPY_W8_H4
+
+}
+
+static void copy_width16_lasx( uint8_t *p_src, int32_t i_src_stride,
+                               uint8_t *p_dst, int32_t i_dst_stride,
+                               int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1, src2, src3;
+
+#define COPY_W16_H4                                 \
+    src0 = LASX_LD(p_src);                          \
+    p_src += i_src_stride;                          \
+    src1 = LASX_LD(p_src);                          \
+    p_src += i_src_stride;                          \
+    src2 = LASX_LD(p_src);                          \
+    p_src += i_src_stride;                          \
+    src3 = LASX_LD(p_src);                          \
+    p_src += i_src_stride;                          \
+                                                    \
+    LASX_ST_D( src0, 0, p_dst );                    \
+    LASX_ST_D( src0, 1, ( p_dst + 8 ) );            \
+    p_dst += i_dst_stride;                          \
+    LASX_ST_D( src1, 0, p_dst );                    \
+    LASX_ST_D( src1, 1, ( p_dst + 8 ) );            \
+    p_dst += i_dst_stride;                          \
+    LASX_ST_D( src2, 0, p_dst );                    \
+    LASX_ST_D( src2, 1, ( p_dst + 8 ) );            \
+    p_dst += i_dst_stride;                          \
+    LASX_ST_D( src3, 0, p_dst );                    \
+    LASX_ST_D( src3, 1, ( p_dst + 8 ) );            \
+    p_dst += i_dst_stride;
+
+    if( 0 == i_height % 12 )
+    {
+        for( i_cnt = i_height; 0 < i_cnt; i_cnt -= 12 )
+        {
+            COPY_W16_H4;
+            COPY_W16_H4;
+            COPY_W16_H4;
+        }
+    }
+    else if( 0 == ( i_height & 7 ) )
+    {
+        for( i_cnt = ( i_height >> 3 ); i_cnt--; )
+        {
+            COPY_W16_H4;
+            COPY_W16_H4;
+        }
+    }
+    else if( 0 == ( i_height & 3 ) )
+    {
+        for( i_cnt = ( i_height >> 2 ); i_cnt--; )
+        {
+            COPY_W16_H4;
+        }
+    }
+
+#undef COPY_W16_H4
+
+}
+
+static void mc_copy_w16_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                              uint8_t *p_src, intptr_t i_src_stride,
+                              int32_t i_height )
+{
+    copy_width16_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
+}
+
+static void mc_copy_w8_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                             uint8_t *p_src, intptr_t i_src_stride,
+                             int32_t i_height )
+{
+    copy_width8_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
+}
+
+static void mc_copy_w4_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                             uint8_t *p_src, intptr_t i_src_stride,
+                             int32_t i_height )
+{
+    copy_width4_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
+}
+
+static void mc_luma_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                          uint8_t *p_src[4], intptr_t i_src_stride,
+                          int32_t m_vx, int32_t m_vy,
+                          int32_t i_width, int32_t i_height,
+                          const x264_weight_t *pWeight )
+{
+    int32_t  i_qpel_idx;
+    int32_t  i_offset;
+    uint8_t  *p_src1;
+
+    i_qpel_idx = ( ( m_vy & 3 ) << 2 ) + ( m_vx & 3 );
+    i_offset = ( m_vy >> 2 ) * i_src_stride + ( m_vx >> 2 );
+    p_src1 = p_src[x264_hpel_ref0[i_qpel_idx]] + i_offset +
+             ( 3 == ( m_vy & 3 ) ) * i_src_stride;
+
+    if( i_qpel_idx & 5 )
+    {
+        uint8_t *p_src2 = p_src[x264_hpel_ref1[i_qpel_idx]] +
+                          i_offset + ( 3 == ( m_vx & 3 ) );
+
+        pixel_avg_wtab_lasx[i_width >> 2](
+                p_dst, i_dst_stride, p_src1, i_src_stride,
+                p_src2, i_height );
+
+        if( pWeight->weightfn )
+        {
+            pWeight->weightfn[i_width>>2]( p_dst, i_dst_stride, p_dst, i_dst_stride, pWeight, i_height );
+        }
+    }
+    else if( pWeight->weightfn )
+    {
+        pWeight->weightfn[i_width>>2]( p_dst, i_dst_stride, p_src1, i_src_stride, pWeight, i_height );
+    }
+    else
+    {
+        if( 16 == i_width )
+        {
+            copy_width16_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
+                               i_height );
+        }
+        else if( 8 == i_width )
+        {
+            copy_width8_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
+                              i_height );
+        }
+        else if( 4 == i_width )
+        {
+            copy_width4_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
+                              i_height );
+        }
+    }
+}
+
+static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                  uint8_t *p_dst, int32_t i_dst_stride,
+                                  int32_t i_height )
+{
+    uint32_t u_loop_cnt, u_h4w;
+    const int16_t i_filt_const0 = 0xfb01;
+    const int16_t i_filt_const1 = 0x1414;
+    const int16_t i_filt_const2 = 0x1fb;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m256i src10_h, src32_h, src54_h, src76_h;
+    __m256i src21_h, src43_h, src65_h, src87_h;
+    __m256i src10_l, src32_l, src54_l, src76_l;
+    __m256i src21_l, src43_l, src65_l, src87_l;
+    __m256i out10_h, out32_h, out10_l, out32_l;
+    __m256i res10_h, res32_h, res10_l, res32_l;
+    __m256i tmp10_h, tmp32_h, tmp10_l, tmp32_l;
+    __m256i filt0, filt1, filt2;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+
+    u_h4w = i_height % 4;
+    filt0 = __lasx_xvreplgr2vr_h( i_filt_const0 );
+    filt1 = __lasx_xvreplgr2vr_h( i_filt_const1 );
+    filt2 = __lasx_xvreplgr2vr_h( i_filt_const2 );
+
+    src0 = LASX_LD( p_src );
+    p_src += i_src_stride;
+    LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
+    p_src += i_src_stride_x4;
+
+    LASX_XORI_B_128( src0 );
+    LASX_XORI_B_4_128( src1, src2, src3, src4 );
+
+    LASX_ILVLH_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
+                          src10_h, src10_l, src21_h, src21_l,
+                          src32_h, src32_l, src43_h, src43_l );
+    res10_h = __lasx_xvpermi_q( src21_h, src10_h, 0x20 );
+    res32_h = __lasx_xvpermi_q( src43_h, src32_h, 0x20 );
+    res10_l = __lasx_xvpermi_q( src21_l, src10_l, 0x20 );
+    res32_l = __lasx_xvpermi_q( src43_l, src32_l, 0x20 );
+
+    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
+    {
+        LASX_LD_4( p_src, i_src_stride, src5, src6, src7, src8 );
+        p_src += i_src_stride_x4;
+
+        LASX_XORI_B_4_128( src5, src6, src7, src8 );
+        LASX_ILVLH_B_4_128SV( src5, src4, src6, src5, src7, src6, src8, src7,
+                              src54_h, src54_l, src65_h, src65_l,
+                              src76_h, src76_l, src87_h, src87_l );
+        tmp10_h = __lasx_xvpermi_q( src65_h, src54_h, 0x20 );
+        tmp32_h = __lasx_xvpermi_q( src87_h, src76_h, 0x20 );
+        tmp10_l = __lasx_xvpermi_q( src65_l, src54_l, 0x20 );
+        tmp32_l = __lasx_xvpermi_q( src87_l, src76_l, 0x20 );
+
+        out10_h = LASX_DPADD_SH_3( res10_h, res32_h, tmp10_h,
+                                   filt0, filt1, filt2 );
+        out32_h = LASX_DPADD_SH_3( res32_h, tmp10_h, tmp32_h,
+                                   filt0, filt1, filt2 );
+        out10_l = LASX_DPADD_SH_3( res10_l, res32_l, tmp10_l,
+                                   filt0, filt1, filt2 );
+        out32_l = LASX_DPADD_SH_3( res32_l, tmp10_l, tmp32_l,
+                                   filt0, filt1, filt2 );
+
+        out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
+        out32_l = __lasx_xvssrarni_b_h(out32_h, out32_l, 5);
+        LASX_XORI_B_2_128( out10_l, out32_l );
+
+        LASX_ST_Q_2(out10_l, 0, 1, p_dst, i_dst_stride );
+        p_dst += i_dst_stride_x2;
+        LASX_ST_Q_2(out32_l, 0, 1, p_dst, i_dst_stride );
+        p_dst += i_dst_stride_x2;
+
+        res10_h = tmp10_h;
+        res32_h = tmp32_h;
+        res10_l = tmp10_l;
+        res32_l = tmp32_l;
+        src4 = src8;
+    }
+
+    for( u_loop_cnt = u_h4w; u_loop_cnt--; )
+    {
+        src5 = LASX_LD( p_src );
+        p_src += i_src_stride;
+        LASX_XORI_B_128( src5 );
+        LASX_ILVLH_B_128SV( src5, src4, src54_h, src54_l );
+        out10_h = LASX_DPADD_SH_3( src10_h, src32_h, src54_h,
+                                   filt0, filt1, filt2 );
+        out10_l = LASX_DPADD_SH_3( src10_l, src32_l, src54_l,
+                                  filt0, filt1, filt2 );
+        out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
+        LASX_XORI_B_128( out10_l );
+        LASX_ST_Q( out10_l, 0, p_dst );
+        p_dst += i_dst_stride;
+
+        src10_h = src21_h;
+        src32_h = src43_h;
+        src10_l = src21_l;
+        src32_l = src43_l;
+
+        src4 = src5;
+    }
+}
+
+#define LASX_HORZ_FILTER_SH( in, mask0, mask1, mask2 )         \
+( {                                                            \
+    __m256i out0_m;                                            \
+    __m256i tmp0_m, tmp1_m;                                    \
+                                                               \
+    tmp0_m = __lasx_xvshuf_b(in, in, mask0);                   \
+    out0_m = __lasx_xvhaddw_h_b( tmp0_m, tmp0_m );             \
+                                                               \
+    tmp0_m = __lasx_xvshuf_b(in, in, mask1);                   \
+    LASX_DP2ADD_H_B( out0_m, minus5b, tmp0_m, out0_m );        \
+                                                               \
+    tmp1_m = __lasx_xvshuf_b(in, in, mask2);                   \
+    LASX_DP2ADD_H_B( out0_m, plus20b, tmp1_m, out0_m );        \
+                                                               \
+    out0_m;                                                    \
+} )
+
+#define LASX_CALC_DPADD_H_6PIX_2COEFF_SH( in0, in1, in2, in3, in4, in5 )   \
+( {                                                                        \
+    __m256i tmp0_m, tmp1_m;                                                \
+    __m256i out0_m, out1_m, out2_m, out3_m;                                \
+                                                                           \
+    LASX_ILVLH_H_128SV( in5, in0, tmp0_m, tmp1_m );                        \
+                                                                           \
+    tmp0_m = __lasx_xvhaddw_w_h( tmp0_m, tmp0_m );                         \
+    tmp1_m = __lasx_xvhaddw_w_h( tmp1_m, tmp1_m );                         \
+                                                                           \
+    LASX_ILVLH_H_128SV( in1, in4, out0_m, out1_m );                        \
+    LASX_DP2ADD_W_H_2( tmp0_m, out0_m, minus5h, tmp1_m,                    \
+                       out1_m, minus5h, tmp0_m, tmp1_m );                  \
+    LASX_ILVLH_H_128SV( in2, in3, out2_m, out3_m );                        \
+    LASX_DP2ADD_W_H_2( tmp0_m, out2_m, plus20h, tmp1_m,                    \
+                       out3_m, plus20h, tmp0_m, tmp1_m );                  \
+                                                                           \
+    out0_m = __lasx_xvssrarni_h_w(tmp0_m, tmp1_m, 10);                     \
+    out0_m = __lasx_xvsat_h(out0_m, 7);                                    \
+                                                                           \
+    out0_m;                                                                \
+} )
+
+static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                   uint8_t *p_dst, int32_t i_dst_stride,
+                                   int32_t i_height )
+{
+    uint32_t u_loop_cnt, u_h4w;
+    int32_t minus = -5, plus = 20;
+    __m256i src0, src1, src2, src3, src4;
+    __m256i src5, src6, src7, src8;
+    __m256i mask0, mask1, mask2;
+    __m256i dst0, dst1, dst2, dst3;
+    __m256i out0, out1;
+    __m256i minus5b, plus20b, minus5h, plus20h;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+
+    minus5b = __lasx_xvreplgr2vr_b( minus );
+    plus20b = __lasx_xvreplgr2vr_b( plus );
+    minus5h = __lasx_xvreplgr2vr_h( minus );
+    plus20h = __lasx_xvreplgr2vr_h( plus );
+
+    u_h4w = i_height & 3;
+    mask0 = LASX_LD( &pu_luma_mask_arr[0] );
+    mask1 = LASX_LD( &pu_luma_mask_arr[32] );
+    mask2 = LASX_LD( &pu_luma_mask_arr[64] );
+
+    src0 = LASX_LD( p_src );
+    p_src += i_src_stride;
+    LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvpermi_d( src0, 0x94);
+    src1 = __lasx_xvpermi_d( src1, 0x94);
+    src2 = __lasx_xvpermi_d( src2, 0x94);
+    src3 = __lasx_xvpermi_d( src3, 0x94);
+    src4 = __lasx_xvpermi_d( src4, 0x94);
+
+    LASX_XORI_B_128( src0 );
+    LASX_XORI_B_4_128( src1, src2, src3, src4 );
+
+    src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
+    src1 = LASX_HORZ_FILTER_SH( src1, mask0, mask1, mask2 );
+    src2 = LASX_HORZ_FILTER_SH( src2, mask0, mask1, mask2 );
+    src3 = LASX_HORZ_FILTER_SH( src3, mask0, mask1, mask2 );
+    src4 = LASX_HORZ_FILTER_SH( src4, mask0, mask1, mask2 );
+
+    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
+    {
+        LASX_LD_4( p_src, i_src_stride, src5, src6, src7, src8 );
+        p_src += i_src_stride_x4;
+        src5 = __lasx_xvpermi_d( src5, 0x94);
+        src6 = __lasx_xvpermi_d( src6, 0x94);
+        src7 = __lasx_xvpermi_d( src7, 0x94);
+        src8 = __lasx_xvpermi_d( src8, 0x94);
+
+        LASX_XORI_B_4_128( src5, src6, src7, src8 );
+
+        src5 = LASX_HORZ_FILTER_SH( src5, mask0, mask1, mask2 );
+        src6 = LASX_HORZ_FILTER_SH( src6, mask0, mask1, mask2 );
+        src7 = LASX_HORZ_FILTER_SH( src7, mask0, mask1, mask2 );
+        src8 = LASX_HORZ_FILTER_SH( src8, mask0, mask1, mask2 );
+        dst0 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src0, src1, src2,
+                                                 src3, src4, src5 );
+        dst1 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src1, src2, src3,
+                                                 src4, src5, src6 );
+        dst2 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src2, src3, src4,
+                                                 src5, src6, src7 );
+        dst3 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src3, src4, src5,
+                                                 src6, src7, src8 );
+        out0 = __lasx_xvpickev_b( dst1, dst0 );
+        out1 = __lasx_xvpickev_b( dst3, dst2 );
+        LASX_XORI_B_2_128( out0, out1 );
+        LASX_ST_D_2( out0, 0, 2, p_dst, 8);
+        p_dst += i_dst_stride;
+        LASX_ST_D_2( out0, 1, 3, p_dst, 8);
+        p_dst += i_dst_stride;
+        LASX_ST_D_2( out1, 0, 2, p_dst, 8);
+        p_dst += i_dst_stride;
+        LASX_ST_D_2( out1, 1, 3, p_dst, 8);
+        p_dst += i_dst_stride;
+
+        src3 = src7;
+        src1 = src5;
+        src5 = src4;
+        src4 = src8;
+        src2 = src6;
+        src0 = src5;
+    }
+
+    for( u_loop_cnt = u_h4w; u_loop_cnt--; )
+    {
+        src5 = LASX_LD( p_src );
+        p_src += i_src_stride;
+        src5 = __lasx_xvpermi_d( src5, 0x94);
+        LASX_XORI_B_128( src5 );
+
+        src5 = LASX_HORZ_FILTER_SH( src5, mask0, mask1, mask2 );
+        dst0 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src0, src1,
+                                                 src2, src3,
+                                                 src4, src5 );
+
+        out0 = __lasx_xvpickev_b( dst0, dst0 );
+        LASX_XORI_B_128( out0 );
+        LASX_ST_D_2( out0, 0, 2, p_dst, 8);
+        p_dst += i_dst_stride;
+
+        src0 = src1;
+        src1 = src2;
+        src2 = src3;
+        src3 = src4;
+        src4 = src5;
+    }
+}
+
+static void avc_luma_hz_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                  uint8_t *p_dst, int32_t i_dst_stride,
+                                  int32_t i_height )
+{
+    uint32_t u_loop_cnt, u_h4w;
+    int32_t minus = -5, plus = 20;
+    __m256i src0, src1, src2, src3;
+    __m256i mask0, mask1, mask2;
+    __m256i minus5b, plus20b;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+
+    minus5b = __lasx_xvreplgr2vr_b( minus );
+    plus20b = __lasx_xvreplgr2vr_b( plus );
+
+    u_h4w = i_height & 3;
+    mask0 = LASX_LD( &pu_luma_mask_arr[0] );
+    mask1 = LASX_LD( &pu_luma_mask_arr[32] );
+    mask2 = LASX_LD( &pu_luma_mask_arr[64] );
+
+    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
+    {
+        LASX_LD_4( p_src, i_src_stride, src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvpermi_d( src0, 0x94);
+        src1 = __lasx_xvpermi_d( src1, 0x94);
+        src2 = __lasx_xvpermi_d( src2, 0x94);
+        src3 = __lasx_xvpermi_d( src3, 0x94);
+
+        LASX_XORI_B_4_128( src0, src1, src2, src3 );
+
+        src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
+        src1 = LASX_HORZ_FILTER_SH( src1, mask0, mask1, mask2 );
+        src2 = LASX_HORZ_FILTER_SH( src2, mask0, mask1, mask2 );
+        src3 = LASX_HORZ_FILTER_SH( src3, mask0, mask1, mask2 );
+
+        src0 = __lasx_xvssrarni_b_h(src1, src0, 5);
+        src1 = __lasx_xvssrarni_b_h(src3, src2, 5);
+        LASX_XORI_B_2_128(src0, src1);
+        LASX_ST_D_2( src0, 0, 2, p_dst, 8);
+        p_dst += i_dst_stride;
+        LASX_ST_D_2( src0, 1, 3, p_dst, 8);
+        p_dst += i_dst_stride;
+        LASX_ST_D_2( src1, 0, 2, p_dst, 8);
+        p_dst += i_dst_stride;
+        LASX_ST_D_2( src1, 1, 3, p_dst, 8);
+        p_dst += i_dst_stride;
+    }
+
+    for( u_loop_cnt = u_h4w; u_loop_cnt--; )
+    {
+        src0 = LASX_LD( p_src );
+        p_src += i_src_stride;
+        src0 = __lasx_xvpermi_d( src0, 0x94);
+
+        LASX_XORI_B_128( src0 );
+        src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
+        src0 = __lasx_xvssrarni_b_h(src0, src0, 5);
+        LASX_XORI_B_128( src0 );
+        LASX_ST_D_2( src0, 0, 2, p_dst, 8);
+        p_dst += i_dst_stride;
+    }
+}
+
+static void hpel_filter_lasx( uint8_t *p_dsth, uint8_t *p_dst_v,
+                              uint8_t *p_dstc, uint8_t *p_src,
+                              intptr_t i_stride, int32_t i_width,
+                              int32_t i_height, int16_t *p_buf )
+{
+    for( int32_t i = 0; i < ( i_width >> 4 ); i++ )
+    {
+        avc_luma_vt_16w_lasx( p_src - 2 - ( 2 * i_stride ), i_stride,
+                              p_dst_v - 2, i_stride, i_height );
+        avc_luma_mid_16w_lasx( p_src - 2 - ( 2 * i_stride ) , i_stride,
+                               p_dstc, i_stride, i_height );
+        avc_luma_hz_16w_lasx( p_src - 2, i_stride, p_dsth, i_stride, i_height );
+
+        p_src += 16;
+        p_dst_v += 16;
+        p_dsth += 16;
+        p_dstc += 16;
+    }
+}
+
+static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
+                                              int32_t i_src_stride,
+                                              uint8_t *p_dst0,
+                                              uint8_t *p_dst1,
+                                              uint8_t *p_dst2,
+                                              uint8_t *p_dst3,
+                                              int32_t i_dst_stride,
+                                              int32_t i_width,
+                                              int32_t i_height )
+{
+    int32_t i_loop_width, i_loop_height, i_w16_mul;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m256i sld1_vec0, sld1_vec1, sld1_vec2, sld1_vec3, sld1_vec4, sld1_vec5;
+    __m256i pckev_vec0, pckev_vec1, pckev_vec2;
+    __m256i pckod_vec0, pckod_vec1, pckod_vec2;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    __m256i mask;
+
+    mask = LASX_LD( &pu_core_mask_arr[0] );
+
+    i_w16_mul = i_width - i_width % 16;
+    for( i_loop_height = i_height; i_loop_height--; )
+    {
+        src0  = LASX_LD( p_src );
+        LASX_LD_2( p_src + i_src_stride, i_src_stride, src1, src2 );
+        p_src += 16;
+        for( i_loop_width = 0; i_loop_width < ( i_w16_mul >> 4 ); i_loop_width++ )
+        {
+            src3  = LASX_LD( p_src );
+            LASX_LD_2( p_src + i_src_stride, i_src_stride, src4, src5 );
+            src6 = __lasx_xvpermi_q( src3, src3, 0x11 );
+            src7 = __lasx_xvpermi_q( src4, src4, 0x11 );
+            src8 = __lasx_xvpermi_q( src5, src5, 0x11 );
+            p_src += 32;
+
+            pckev_vec0 = __lasx_xvpickev_b( src3, src0 );
+            pckod_vec0 = __lasx_xvpickod_b( src3, src0 );
+            pckev_vec1 = __lasx_xvpickev_b( src4, src1 );
+            pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
+            pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
+            pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
+            LASX_AVER_BU_4( pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                            pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
+                            tmp0, tmp1, tmp2, tmp3 );
+            LASX_AVER_BU_2( tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            LASX_ST_Q( tmp0, 0, p_dst0 );
+            LASX_ST_Q( tmp1, 0, p_dst2 );
+
+            LASX_SHUF_B_2_128SV( src3, src0, src4, src1,
+                                 mask, mask, sld1_vec0, sld1_vec1 );
+            LASX_SHUF_B_2_128SV( src5, src2, src6, src3,
+                                 mask, mask, sld1_vec2, sld1_vec3 );
+            LASX_SHUF_B_2_128SV( src7, src4, src8, src5,
+                                 mask, mask, sld1_vec4, sld1_vec5 );
+            pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
+            pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
+            pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
+            LASX_AVER_BU_4( pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                            pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
+                            tmp0, tmp1, tmp2, tmp3 );
+            LASX_AVER_BU_2( tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            LASX_ST_Q( tmp0, 0, p_dst1 );
+            LASX_ST_Q( tmp1, 0, p_dst3 );
+
+            src0 = src6;
+            src1 = src7;
+            src2 = src8;
+            p_dst0 += 16;
+            p_dst1 += 16;
+            p_dst2 += 16;
+            p_dst3 += 16;
+        }
+
+        for( i_loop_width = i_w16_mul; i_loop_width < i_width;
+             i_loop_width += 8 )
+        {
+            src3  = LASX_LD( p_src );
+            LASX_LD_2( p_src + i_src_stride, i_src_stride, src4, src5 );
+            p_src += 16;
+
+            pckev_vec0 = __lasx_xvpickev_b( src3, src0 );
+            pckod_vec0 = __lasx_xvpickod_b( src3, src0 );
+            pckev_vec1 = __lasx_xvpickev_b( src4, src1 );
+            pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
+            pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
+            pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
+            LASX_AVER_BU_4( pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                            pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
+                            tmp0, tmp1, tmp2, tmp3 );
+            LASX_AVER_BU_2( tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            LASX_ST_D( tmp0, 0, p_dst0 );
+            LASX_ST_D( tmp1, 0, p_dst2 );
+
+            LASX_SHUF_B_2_128SV( src3, src0, src4, src1,
+                                 mask, mask, sld1_vec0, sld1_vec1 );
+            LASX_SHUF_B_2_128SV( src5, src2, src3, src3,
+                                 mask, mask, sld1_vec2, sld1_vec3 );
+            LASX_SHUF_B_2_128SV( src4, src4, src5, src5,
+                                 mask, mask, sld1_vec4, sld1_vec5 );
+            pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
+            pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
+            pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
+            LASX_AVER_BU_4( pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                            pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
+                            tmp0, tmp1, tmp2, tmp3 );
+            LASX_AVER_BU_2( tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            LASX_ST_D( tmp0, 0, p_dst1 );
+            LASX_ST_D( tmp1, 0, p_dst3 );
+            p_dst0 += 8;
+            p_dst1 += 8;
+            p_dst2 += 8;
+            p_dst3 += 8;
+        }
+
+        p_src += ( ( i_src_stride << 1 ) - ( ( i_width << 1 ) + 16 ) );
+        p_dst0 += ( i_dst_stride - i_width );
+        p_dst1 += ( i_dst_stride - i_width );
+        p_dst2 += ( i_dst_stride - i_width );
+        p_dst3 += ( i_dst_stride - i_width );
+    }
+}
+
+static void frame_init_lowres_core_lasx( uint8_t *p_src, uint8_t *p_dst0,
+                                         uint8_t *p_dst1, uint8_t *p_dst2,
+                                         uint8_t *p_dst3, intptr_t i_src_stride,
+                                         intptr_t i_dst_stride, int32_t i_width,
+                                         int32_t i_height )
+{
+    core_frame_init_lowres_core_lasx( p_src, i_src_stride, p_dst0,
+                                      p_dst1, p_dst2, p_dst3,
+                                      i_dst_stride, i_width, i_height );
+}
+static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
+                                               int32_t i_src_stride,
+                                               uint8_t *p_dst0,
+                                               int32_t dst0_stride,
+                                               uint8_t *p_dst1,
+                                               int32_t dst1_stride,
+                                               int32_t i_width,
+                                               int32_t i_height )
+{
+    int32_t i_loop_width, i_loop_height;
+    int32_t i_w_mul4, i_w_mul16, i_w_mul32, i_h4w;
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m256i vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3;
+    __m256i vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7;
+    __m256i vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3;
+    __m256i vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7;
+    uint8_t *p_dst, *p_dstA, *p_dstB, *p_srcA;
+    int32_t i_dst0_stride_x2 = dst0_stride << 1;
+    int32_t i_dst1_stride_x2 = dst1_stride << 1;
+
+    i_w_mul32 = i_width - ( i_width & 31 );
+    i_w_mul16 = i_width - ( i_width & 15 );
+    i_w_mul4 = i_width - ( i_width & 3 );
+    i_h4w = i_height - ( i_height & 7 );
+
+    for( i_loop_height = ( i_h4w >> 3 ); i_loop_height--; )
+    {
+        for( i_loop_width = ( i_w_mul32 >> 5 ); i_loop_width--; )
+        {
+            LASX_LD_8( p_src, i_src_stride,
+                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            p_src += 32;
+            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            LASX_LD_8( p_src, i_src_stride,
+                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            p_src += 32;
+            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
+            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
+
+            in0 = __lasx_xvpermi_q( vec_pckev0, vec_pckev4, 0x02 );
+            in1 = __lasx_xvpermi_q( vec_pckev0, vec_pckev4, 0x13 );
+            in2 = __lasx_xvpermi_q( vec_pckev1, vec_pckev5, 0x02 );
+            in3 = __lasx_xvpermi_q( vec_pckev1, vec_pckev5, 0x13 );
+            in4 = __lasx_xvpermi_q( vec_pckev2, vec_pckev6, 0x02 );
+            in5 = __lasx_xvpermi_q( vec_pckev2, vec_pckev6, 0x13 );
+            in6 = __lasx_xvpermi_q( vec_pckev3, vec_pckev7, 0x02 );
+            in7 = __lasx_xvpermi_q( vec_pckev3, vec_pckev7, 0x13 );
+
+            LASX_ILVL_D_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                 vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            LASX_ILVH_D_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                 vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
+
+            LASX_ST_8( vec_pckev0, vec_pckev4, vec_pckev1, vec_pckev5,
+                       vec_pckev2, vec_pckev6, vec_pckev3, vec_pckev7,
+                       p_dst0, dst0_stride );
+
+            in0 = __lasx_xvpermi_q( vec_pckod0, vec_pckod4, 0x02 );
+            in1 = __lasx_xvpermi_q( vec_pckod0, vec_pckod4, 0x13 );
+            in2 = __lasx_xvpermi_q( vec_pckod1, vec_pckod5, 0x02 );
+            in3 = __lasx_xvpermi_q( vec_pckod1, vec_pckod5, 0x13 );
+            in4 = __lasx_xvpermi_q( vec_pckod2, vec_pckod6, 0x02 );
+            in5 = __lasx_xvpermi_q( vec_pckod2, vec_pckod6, 0x13 );
+            in6 = __lasx_xvpermi_q( vec_pckod3, vec_pckod7, 0x02 );
+            in7 = __lasx_xvpermi_q( vec_pckod3, vec_pckod7, 0x13 );
+
+            LASX_ILVL_D_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                 vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+            LASX_ILVH_D_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                 vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
+
+            LASX_ST_8( vec_pckod0, vec_pckod4, vec_pckod1, vec_pckod5,
+                       vec_pckod2, vec_pckod6, vec_pckod3, vec_pckod7,
+                       p_dst1, dst1_stride );
+
+            p_dst0 += 32;
+            p_dst1 += 32;
+        }
+
+        for( i_loop_width = ( ( i_width & 31 ) >> 4 ); i_loop_width--; )
+        {
+            LASX_LD_8( p_src, i_src_stride,
+                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            p_src += 32;
+            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            LASX_ST_D_2( vec_pckev0, 0, 2, p_dst0, 8 );
+            p_dst = p_dst0 + dst0_stride;
+            LASX_ST_D_2( vec_pckev0, 1, 3, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckev1, 0, 2, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckev1, 1, 3, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckev2, 0, 2, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckev2, 1, 3, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckev3, 0, 2, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckev3, 1, 3, p_dst, 8 );
+
+            LASX_ST_D_2( vec_pckod0, 0, 2, p_dst1, 8 );
+            p_dst = p_dst1 + dst0_stride;
+            LASX_ST_D_2( vec_pckod0, 1, 3, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckod1, 0, 2, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckod1, 1, 3, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckod2, 0, 2, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckod2, 1, 3, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckod3, 0, 2, p_dst, 8 );
+            p_dst = p_dst + dst0_stride;
+            LASX_ST_D_2( vec_pckod3, 1, 3, p_dst, 8 );
+
+            p_dst0 += 16;
+            p_dst1 += 16;
+        }
+
+        for( i_loop_width = ( ( i_width & 15 ) >> 3 ); i_loop_width--; )
+        {
+            LASX_LD_8( p_src, i_src_stride,
+                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            p_src += 16;
+            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            LASX_ST_D_2( vec_pckev0, 0, 1, p_dst0, dst0_stride );
+            p_dst = p_dst0 + i_dst0_stride_x2;
+            LASX_ST_D_2( vec_pckev1, 0, 1, p_dst, dst0_stride );
+            p_dst = p_dst + i_dst0_stride_x2;
+            LASX_ST_D_2( vec_pckev2, 0, 1, p_dst, dst0_stride );
+            p_dst = p_dst + i_dst0_stride_x2;
+            LASX_ST_D_2( vec_pckev3, 0, 1, p_dst, dst0_stride );
+
+            LASX_ST_D_2( vec_pckod0, 0, 1, p_dst1, dst0_stride );
+            p_dst = p_dst1 + i_dst1_stride_x2;
+            LASX_ST_D_2( vec_pckod1, 0, 1, p_dst, dst0_stride );
+            p_dst = p_dst + i_dst1_stride_x2;
+            LASX_ST_D_2( vec_pckod2, 0, 1, p_dst, dst0_stride );
+            p_dst = p_dst + i_dst1_stride_x2;
+            LASX_ST_D_2( vec_pckod3, 0, 1, p_dst, dst0_stride );
+
+            p_dst0 += 8;
+            p_dst1 += 8;
+        }
+
+
+        for( i_loop_width = ( ( i_width & 7 ) >> 2 ); i_loop_width--; )
+        {
+            LASX_LD_8( p_src, i_src_stride,
+                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            p_src += 8;
+            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
+                                  vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            LASX_ST_W_2( vec_pckev0, 0, 2, p_dst0, dst0_stride );
+            p_dst = p_dst0 + i_dst0_stride_x2;
+            LASX_ST_W_2( vec_pckev1, 0, 2, p_dst, dst0_stride );
+            p_dst = p_dst + i_dst0_stride_x2;
+            LASX_ST_W_2( vec_pckev2, 0, 2, p_dst, dst0_stride );
+            p_dst = p_dst + i_dst0_stride_x2;
+            LASX_ST_W_2( vec_pckev3, 0, 2, p_dst, dst0_stride );
+
+            LASX_ST_W_2( vec_pckod0, 0, 2, p_dst1, dst0_stride );
+            p_dst = p_dst1 + i_dst1_stride_x2;
+            LASX_ST_W_2( vec_pckod1, 0, 2, p_dst, dst0_stride );
+            p_dst = p_dst + i_dst1_stride_x2;
+            LASX_ST_W_2( vec_pckod2, 0, 2, p_dst, dst0_stride );
+            p_dst = p_dst + i_dst1_stride_x2;
+            LASX_ST_W_2( vec_pckod3, 0, 2, p_dst, dst0_stride );
+
+            p_dst0 += 4;
+            p_dst1 += 4;
+        }
+
+        for( i_loop_width = i_w_mul4; i_loop_width < i_width; i_loop_width++ )
+        {
+            p_dst0[0] = p_src[0];
+            p_dst1[0] = p_src[1];
+
+            p_dstA = p_dst0 + dst0_stride;
+            p_dstB = p_dst1 + dst1_stride;
+            p_srcA = p_src + i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dst0 += 1;
+            p_dst1 += 1;
+            p_src += 2;
+        }
+
+        p_src += ( ( i_src_stride << 3 ) - ( i_width << 1 ) );
+        p_dst0 += ( ( dst0_stride << 3 ) - i_width );
+        p_dst1 += ( ( dst1_stride << 3) - i_width );
+    }
+
+    for( i_loop_height = i_h4w; i_loop_height < i_height; i_loop_height++ )
+    {
+        for( i_loop_width = ( i_w_mul16 >> 4 ); i_loop_width--; )
+        {
+            in0 = LASX_LD( p_src );
+            p_src += 32;
+            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
+            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
+            LASX_ST_D_2( vec_pckev0, 0, 2, p_dst0, 8 );
+            LASX_ST_D_2( vec_pckod0, 0, 2, p_dst1, 8 );
+            p_dst0 += 16;
+            p_dst1 += 16;
+        }
+
+        for( i_loop_width = ( ( i_width & 15 ) >> 3 ); i_loop_width--; )
+        {
+            in0 = LASX_LD( p_src );
+            p_src += 16;
+            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
+            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
+            LASX_ST_D( vec_pckev0, 0, p_dst0 );
+            LASX_ST_D( vec_pckod0, 0, p_dst1 );
+            p_dst0 += 8;
+            p_dst1 += 8;
+        }
+
+        for( i_loop_width = ( ( i_width & 7 ) >> 2 ); i_loop_width--; )
+        {
+            in0 = LASX_LD( p_src );
+            p_src += 8;
+            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
+            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
+            LASX_ST_W( vec_pckev0, 0, p_dst0 );
+            LASX_ST_W( vec_pckod0, 0, p_dst1 );
+            p_dst0 += 4;
+            p_dst1 += 4;
+        }
+
+        for( i_loop_width = i_w_mul4; i_loop_width < i_width; i_loop_width++ )
+        {
+            p_dst0[0] = p_src[0];
+            p_dst1[0] = p_src[1];
+            p_dst0 += 1;
+            p_dst1 += 1;
+            p_src += 2;
+        }
+
+        p_src += ( ( i_src_stride ) - ( i_width << 1 ) );
+        p_dst0 += ( ( dst0_stride ) - i_width );
+        p_dst1 += ( ( dst1_stride ) - i_width );
+    }
+}
+
+static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
+                                             int32_t i_src0_stride,
+                                             uint8_t *p_src1,
+                                             int32_t i_src1_stride,
+                                             uint8_t *p_dst,
+                                             int32_t i_dst_stride,
+                                             int32_t i_width, int32_t i_height )
+{
+    int32_t i_loop_width, i_loop_height, i_w_mul8, i_h4w;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3;
+    __m256i vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3;
+    uint8_t *p_dst_t, *p_srcA, *p_srcB;
+
+    i_w_mul8 = i_width - ( i_width & 7 );
+    i_h4w = i_height - ( i_height & 3 );
+
+    for( i_loop_height = ( i_h4w >> 2 ); i_loop_height--; )
+    {
+        for( i_loop_width = ( i_width >> 5 ); i_loop_width--; )
+        {
+            LASX_LD_4( p_src0, i_src0_stride, src0, src1, src2, src3 );
+            LASX_LD_4( p_src1, i_src1_stride, src4, src5, src6, src7 );
+            LASX_ILVL_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
+                                 vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+            LASX_ILVH_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
+                                 vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
+
+            src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
+            src1 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x02 );
+            src2 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x02 );
+            src3 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x02 );
+
+            src4 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x13 );
+            src5 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x13 );
+            src6 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x13 );
+            src7 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x13 );
+
+            LASX_ST_4( src0, src1, src2, src3, p_dst, i_dst_stride );
+            LASX_ST_4( src4, src5, src6, src7, ( p_dst + 32 ), i_dst_stride );
+
+            p_src0 += 32;
+            p_src1 += 32;
+            p_dst += 64;
+        }
+
+        for( i_loop_width = ( ( i_width & 31 ) >> 4 ); i_loop_width--; )
+        {
+            LASX_LD_4( p_src0, i_src0_stride, src0, src1, src2, src3 );
+            LASX_LD_4( p_src1, i_src1_stride, src4, src5, src6, src7 );
+            LASX_ILVL_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
+                                 vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+            LASX_ILVH_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
+                                 vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
+
+            vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
+            vec_ilv_l1 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x02 );
+            vec_ilv_l2 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x02 );
+            vec_ilv_l3 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x02 );
+
+            LASX_ST_4( vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3,
+                       p_dst, i_dst_stride );
+
+            p_src0 += 16;
+            p_src1 += 16;
+            p_dst += 32;
+        }
+
+        for( i_loop_width = ( i_width & 15 ) >> 3; i_loop_width--; )
+        {
+            LASX_LD_4( p_src0, i_src0_stride, src0, src1, src2, src3 );
+            LASX_LD_4( p_src1, i_src1_stride, src4, src5, src6, src7 );
+            LASX_ILVL_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
+                                 vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+
+            LASX_ST_Q( vec_ilv_l0, 0, p_dst );
+            p_dst_t = p_dst + i_dst_stride;
+            LASX_ST_Q( vec_ilv_l1, 0, p_dst_t );
+            p_dst_t = p_dst_t + i_dst_stride;
+            LASX_ST_Q( vec_ilv_l2, 0, p_dst_t );
+            p_dst_t = p_dst_t + i_dst_stride;
+            LASX_ST_Q( vec_ilv_l3, 0, p_dst_t );
+
+            p_src0 += 8;
+            p_src1 += 8;
+            p_dst += 16;
+        }
+
+        for( i_loop_width = i_w_mul8; i_loop_width < i_width; i_loop_width++ )
+        {
+            p_dst[0] = p_src0[0];
+            p_dst[1] = p_src1[0];
+
+            p_dst_t = p_dst + i_dst_stride;
+            p_srcA = p_src0 + i_src0_stride;
+            p_srcB = p_src1 + i_src1_stride;
+            p_dst_t[0] = p_srcA[0];
+            p_dst_t[1] = p_srcB[0];
+
+            p_dst_t += i_dst_stride;
+            p_srcA += i_src0_stride;
+            p_srcB += i_src1_stride;
+            p_dst_t[0] = p_srcA[0];
+            p_dst_t[1] = p_srcB[0];
+
+            p_dst_t += i_dst_stride;
+            p_srcA += i_src0_stride;
+            p_srcB += i_src1_stride;
+            p_dst_t[0] = p_srcA[0];
+            p_dst_t[1] = p_srcB[0];
+
+            p_src0 += 1;
+            p_src1 += 1;
+            p_dst += 2;
+        }
+
+        p_src0 += ( ( i_src0_stride << 2 ) - i_width );
+        p_src1 += ( ( i_src1_stride << 2 ) - i_width );
+        p_dst += ( ( i_dst_stride << 2 ) - ( i_width << 1 ) );
+    }
+
+    for( i_loop_height = i_h4w; i_loop_height < i_height; i_loop_height++ )
+    {
+        for( i_loop_width = ( i_width >> 5 ); i_loop_width--; )
+        {
+            src0 = LASX_LD( p_src0 );
+            src4 = LASX_LD( p_src1 );
+            LASX_ILVLH_B_128SV( src4, src0, vec_ilv_h0, vec_ilv_l0 );
+
+            src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
+            src1 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x13 );
+            LASX_ST_2( src0, src1, p_dst, 32 );
+
+            p_src0 += 32;
+            p_src1 += 32;
+            p_dst += 64;
+        }
+
+        for( i_loop_width = ( ( i_width &  31 )  >> 4 ); i_loop_width--; )
+        {
+            src0 = LASX_LD( p_src0 );
+            src4 = LASX_LD( p_src1 );
+            LASX_ILVLH_B_128SV( src4, src0, vec_ilv_h0, vec_ilv_l0 );
+
+            vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
+            LASX_ST( vec_ilv_l0, p_dst );
+
+            p_src0 += 16;
+            p_src1 += 16;
+            p_dst += 32;
+        }
+
+        for( i_loop_width = ( i_width & 15 ) >> 3; i_loop_width--; )
+        {
+            src0 = LASX_LD( p_src0 );
+            src4 = LASX_LD( p_src1 );
+            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
+            LASX_ST_Q( vec_ilv_l0, 0, p_dst );
+
+            p_src0 += 8;
+            p_src1 += 8;
+            p_dst += 16;
+        }
+
+        for( i_loop_width = i_w_mul8; i_loop_width < i_width; i_loop_width++ )
+        {
+            p_dst[0] = p_src0[0];
+            p_dst[1] = p_src1[0];
+            p_src0 += 1;
+            p_src1 += 1;
+            p_dst += 2;
+        }
+
+        p_src0 += ( i_src0_stride - i_width );
+        p_src1 += ( i_src1_stride - i_width );
+        p_dst += ( i_dst_stride - ( i_width << 1 ) );
+    }
+}
+
+static void core_store_interleave_chroma_lasx( uint8_t *p_src0,
+                                               int32_t i_src0_stride,
+                                               uint8_t *p_src1,
+                                               int32_t i_src1_stride,
+                                               uint8_t *p_dst,
+                                               int32_t i_dst_stride,
+                                               int32_t i_height )
+{
+    int32_t i_loop_height, i_h4w;
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    int32_t i_src0_stride_x4 = i_src0_stride << 2;
+    int32_t i_src1_stride_x4 = i_src1_stride << 2;
+
+    i_h4w = i_height & 3;
+    for( i_loop_height = ( i_height >> 2 ); i_loop_height--; )
+    {
+        LASX_LD_4( p_src0, i_src0_stride, in0, in1, in2, in3 );
+        p_src0 += i_src0_stride_x4;
+        LASX_LD_4( p_src1, i_src1_stride, in4, in5, in6, in7 );
+        p_src1 += i_src1_stride_x4;
+        LASX_ILVL_B_4_128SV( in4, in0, in5, in1, in6, in2, in7, in3,
+                             tmp0, tmp1, tmp2, tmp3 );
+
+        LASX_ST_Q( tmp0, 0, p_dst );
+        p_dst += i_dst_stride;
+        LASX_ST_Q( tmp1, 0, p_dst );
+        p_dst += i_dst_stride;
+        LASX_ST_Q( tmp2, 0, p_dst );
+        p_dst += i_dst_stride;
+        LASX_ST_Q( tmp3, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+
+    for( i_loop_height = i_h4w; i_loop_height--; )
+    {
+        in0 = LASX_LD( p_src0 );
+        p_src0 += i_src0_stride;
+        in1 = LASX_LD( p_src1 );
+        p_src1 += i_src1_stride;
+
+        tmp0 = __lasx_xvilvl_b( in1, in0 );
+
+        LASX_ST_Q( tmp0, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+}
+
+static void plane_copy_deinterleave_lasx( uint8_t *p_dst0,
+                                          intptr_t i_dst_stride0,
+                                          uint8_t *p_dst1,
+                                          intptr_t i_dst_stride1,
+                                          uint8_t *p_src, intptr_t i_src_stride,
+                                          int32_t i_width, int32_t i_height )
+{
+    core_plane_copy_deinterleave_lasx( p_src, i_src_stride,
+                                       p_dst0, i_dst_stride0,
+                                       p_dst1, i_dst_stride1,
+                                       i_width, i_height );
+}
+
+static void load_deinterleave_chroma_fenc_lasx( uint8_t *p_dst, uint8_t *p_src,
+                                                intptr_t i_src_stride,
+                                                int32_t i_height )
+{
+    core_plane_copy_deinterleave_lasx( p_src, i_src_stride, p_dst, FENC_STRIDE,
+                                       ( p_dst + ( FENC_STRIDE / 2 ) ),
+                                       FENC_STRIDE, 8, i_height );
+}
+
+static void plane_copy_interleave_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                                       uint8_t *p_src0, intptr_t i_src_stride0,
+                                       uint8_t *p_src1, intptr_t i_src_stride1,
+                                       int32_t i_width, int32_t i_height )
+{
+    core_plane_copy_interleave_lasx( p_src0, i_src_stride0,
+                                     p_src1, i_src_stride1,
+                                     p_dst, i_dst_stride,
+                                     i_width, i_height );
+}
+
+static void load_deinterleave_chroma_fdec_lasx( uint8_t *p_dst, uint8_t *p_src,
+                                                intptr_t i_src_stride,
+                                                int32_t i_height )
+{
+    core_plane_copy_deinterleave_lasx( p_src, i_src_stride, p_dst, FDEC_STRIDE,
+                                       ( p_dst + ( FDEC_STRIDE / 2 ) ),
+                                       FDEC_STRIDE, 8, i_height );
+}
+
+static void store_interleave_chroma_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                                          uint8_t *p_src0, uint8_t *p_src1,
+                                          int32_t i_height )
+{
+    core_store_interleave_chroma_lasx( p_src0, FDEC_STRIDE, p_src1, FDEC_STRIDE,
+                                       p_dst, i_dst_stride, i_height );
+}
+
+static void memzero_aligned_lasx( void *p_dst, size_t n )
+{
+    uint32_t i_tot32 = n >> 5;
+    uint32_t i_remain = n - ( i_tot32 << 5 );
+    int8_t i_cnt;
+    __m256i zero = __lasx_xvldi( 0 );
+
+    for ( i_cnt = i_tot32; i_cnt--; )
+    {
+        LASX_ST( zero, p_dst );
+        p_dst += 32;
+    }
+
+    if( i_remain )
+    {
+        memset( p_dst, 0, i_remain );
+    }
+}
+
+static void prefetch_ref_lasx( uint8_t *pix, intptr_t stride, int32_t parity )
+{
+    int32_t tmp = 0;
+    uint8_t *pix_tmp = pix, *pix_tmp2 = pix;
+
+    __asm__ volatile(
+    "addi.d    %[parity],    %[parity],      -1                   \n\t"
+    "addi.d    %[pix],       %[pix],         64                   \n\t"
+    "and       %[parity],    %[parity],      %[stride]            \n\t"
+    "slli.d    %[tmp],       %[parity],      3                    \n\t"
+    "add.d     %[pix_tmp],   %[pix],         %[tmp]               \n\t"
+    "slli.d    %[tmp],       %[stride],      1                    \n\t"
+    "add.d     %[parity],    %[stride],      %[tmp]               \n\t"
+    "preld     0,            %[pix_tmp],     0                    \n\t"
+    "add.d     %[pix_tmp2],  %[pix_tmp],     %[stride]            \n\t"
+    "preld     0,            %[pix_tmp2],    0                    \n\t"
+    "add.d     %[pix_tmp2],  %[pix_tmp2],    %[stride]            \n\t"
+    "preld     0,            %[pix_tmp2],    0                    \n\t"
+    "add.d     %[pix_tmp],   %[pix_tmp],     %[parity]            \n\t"
+    "preld     0,            %[pix_tmp],     0                    \n\t"
+    "add.d     %[pix],       %[pix_tmp2],    %[tmp]               \n\t"
+    "preld     0,            %[pix],         0                    \n\t"
+    "add.d     %[pix_tmp],   %[pix],         %[stride]            \n\t"
+    "preld     0,            %[pix_tmp],     0                    \n\t"
+    "add.d     %[pix_tmp],   %[pix_tmp],     %[stride]            \n\t"
+    "preld     0,            %[pix_tmp],     0                    \n\t"
+    "add.d     %[pix],       %[pix],         %[parity]            \n\t"
+    "preld     0,            %[pix],         0                    \n\t"
+     : [tmp]"+&r"(tmp), [pix_tmp]"+&r"(pix_tmp),
+       [pix_tmp2]"+&r"(pix_tmp2), [pix]"+&r"(pix),
+       [parity]"+&r"(parity)
+     : [stride]"r"(stride)
+     :
+    );
+}
+
+static void prefetch_fenc_422_lasx( uint8_t *pix_y, intptr_t stride_y,
+                                    uint8_t *pix_uv, intptr_t stride_uv,
+                                    int32_t mb_x )
+{
+    int64_t num1 = 0;
+    int64_t num2 = 0;
+    uint8_t *y_tmp = pix_y, *uv_tmp = pix_uv;
+
+    __asm__ volatile(
+    "andi      %[num1],      %[mb_x],         3                  \n\t"
+    "mul.d     %[num1],      %[num1],         %[stride_y]        \n\t"
+    "andi      %[mb_x],      %[mb_x],         6                  \n\t"
+    "mul.d     %[num2],      %[mb_x],         %[stride_uv]       \n\t"
+    "addi.d    %[pix_y],     %[pix_y],        64                 \n\t"
+    "addi.d    %[pix_uv],    %[pix_uv],       64                 \n\t"
+    "slli.d    %[num1],      %[num1],         2                  \n\t"
+    "add.d     %[pix_y],     %[pix_y],        %[num1]            \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "add.d     %[y_tmp],     %[pix_y],        %[stride_y]        \n\t"
+    "preld     0,            %[y_tmp],        0                  \n\t"
+    "add.d     %[pix_y],     %[y_tmp],        %[stride_y]        \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "slli.d    %[num2],      %[num2],         2                  \n\t"
+    "add.d     %[pix_y],     %[pix_y],        %[stride_y]        \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "add.d     %[pix_uv],    %[pix_uv],       %[num2]            \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+    "add.d     %[uv_tmp],    %[pix_uv],       %[stride_uv]       \n\t"
+    "preld     0,            %[uv_tmp],       0                  \n\t"
+    "add.d     %[pix_uv],    %[uv_tmp],       %[stride_uv]       \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+    "add.d     %[pix_uv],    %[pix_uv],       %[stride_uv]       \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+     : [y_tmp]"+&r"(y_tmp),
+       [uv_tmp]"+&r"(uv_tmp),
+       [num2]"+&r"(num2),
+       [num1]"+&r"(num1),
+       [mb_x]"+&r"(mb_x),
+       [pix_y]"+&r"(pix_y),
+       [pix_uv]"+&r"(pix_uv)
+     : [stride_y]"r"(stride_y), [stride_uv]"r"(stride_uv)
+     :
+    );
+}
+
+static void prefetch_fenc_420_lasx( uint8_t *pix_y, intptr_t stride_y,
+                                    uint8_t *pix_uv, intptr_t stride_uv,
+                                    int32_t mb_x )
+{
+    int64_t num1 = 0;
+    int64_t num2 = 0;
+    uint8_t *y_tmp = pix_y;
+
+    __asm__ volatile(
+    "andi      %[num1],      %[mb_x],         3                  \n\t"
+    "mul.d     %[num1],      %[num1],         %[stride_y]        \n\t"
+    "andi      %[mb_x],      %[mb_x],         6                  \n\t"
+    "mul.d     %[num2],      %[mb_x],         %[stride_uv]       \n\t"
+    "addi.d    %[pix_y],     %[pix_y],        64                 \n\t"
+    "addi.d    %[pix_uv],    %[pix_uv],       64                 \n\t"
+    "slli.d    %[num1],      %[num1],         2                  \n\t"
+    "add.d     %[pix_y],     %[pix_y],        %[num1]            \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "add.d     %[y_tmp],     %[pix_y],        %[stride_y]        \n\t"
+    "preld     0,            %[y_tmp],        0                  \n\t"
+    "add.d     %[pix_y],     %[y_tmp],        %[stride_y]        \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "slli.d    %[num2],      %[num2],         2                  \n\t"
+    "add.d     %[pix_y],     %[pix_y],        %[stride_y]        \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "add.d     %[pix_uv],    %[pix_uv],       %[num2]            \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+    "add.d     %[pix_uv],    %[pix_uv],       %[stride_uv]       \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+     : [y_tmp]"+&r"(y_tmp),
+       [num2]"+&r"(num2),
+       [num1]"+&r"(num1),
+       [mb_x]"+&r"(mb_x),
+       [pix_y]"+&r"(pix_y),
+       [pix_uv]"+&r"(pix_uv)
+     : [stride_y]"r"(stride_y), [stride_uv]"r"(stride_uv)
+     :
+    );
+}
+
+#endif // !HIGH_BIT_DEPTH
+
+void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
+{
+#if !HIGH_BIT_DEPTH
+    if( cpu & X264_CPU_LASX )
+    {
+        pf->mc_luma = mc_luma_lasx;
+        pf->mc_chroma = mc_chroma_lasx;
+        pf->get_ref = get_ref_lasx;
+
+        pf->avg[PIXEL_16x16]= pixel_avg_16x16_lasx;
+        pf->avg[PIXEL_16x8] = pixel_avg_16x8_lasx;
+        pf->avg[PIXEL_8x16] = pixel_avg_8x16_lasx;
+        pf->avg[PIXEL_8x8] = pixel_avg_8x8_lasx;
+        pf->avg[PIXEL_8x4] = pixel_avg_8x4_lasx;
+        pf->avg[PIXEL_4x16] = pixel_avg_4x16_lasx;
+        pf->avg[PIXEL_4x8] = pixel_avg_4x8_lasx;
+        pf->avg[PIXEL_4x4] = pixel_avg_4x4_lasx;
+        pf->avg[PIXEL_4x2] = pixel_avg_4x2_lasx;
+
+        pf->weight = mc_weight_wtab_lasx;
+        pf->offsetadd = mc_weight_wtab_lasx;
+        pf->offsetsub = mc_weight_wtab_lasx;
+        pf->weight_cache = weight_cache_lasx;
+
+        pf->copy_16x16_unaligned = mc_copy_w16_lasx;
+        pf->copy[PIXEL_16x16] = mc_copy_w16_lasx;
+        pf->copy[PIXEL_8x8] = mc_copy_w8_lasx;
+        pf->copy[PIXEL_4x4] = mc_copy_w4_lasx;
+
+        pf->store_interleave_chroma = store_interleave_chroma_lasx;
+        pf->load_deinterleave_chroma_fenc = load_deinterleave_chroma_fenc_lasx;
+        pf->load_deinterleave_chroma_fdec = load_deinterleave_chroma_fdec_lasx;
+
+        pf->plane_copy_interleave = plane_copy_interleave_lasx;
+        pf->plane_copy_deinterleave = plane_copy_deinterleave_lasx;
+        pf->plane_copy_deinterleave_yuyv = plane_copy_deinterleave_lasx;
+
+        pf->hpel_filter = hpel_filter_lasx;
+        pf->memcpy_aligned = x264_memcpy_aligned_lasx;
+        pf->memzero_aligned = memzero_aligned_lasx;
+        pf->frame_init_lowres_core = frame_init_lowres_core_lasx;
+
+        pf->prefetch_fenc_420 = prefetch_fenc_420_lasx;
+        pf->prefetch_fenc_422 = prefetch_fenc_422_lasx;
+        pf->prefetch_ref  = prefetch_ref_lasx;
+    }
+#endif // !HIGH_BIT_DEPTH
+}
diff --git a/common/loongarch/mc.h b/common/loongarch/mc.h
new file mode 100644
index 00000000..ce1b9a78
--- /dev/null
+++ b/common/loongarch/mc.h
@@ -0,0 +1,33 @@
+/*****************************************************************************
+ * mc.h: loongarch motion compensation
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_MC_H
+#define X264_LOONGARCH_MC_H
+
+#define x264_mc_init_loongarch x264_template(mc_init_loongarch)
+void x264_mc_init_loongarch( int cpu, x264_mc_functions_t *pf );
+
+#endif
diff --git a/common/loongarch/pixel-c.c b/common/loongarch/pixel-c.c
new file mode 100644
index 00000000..b09f59b1
--- /dev/null
+++ b/common/loongarch/pixel-c.c
@@ -0,0 +1,2608 @@
+/*****************************************************************************
+ * pixel-c.c: loongarch pixel metrics
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "generic_macros_lasx.h"
+#include "pixel.h"
+#include "predict.h"
+
+#if !HIGH_BIT_DEPTH
+
+#define LASX_LOAD_4(p_src, _stride, _stride2, _stride3, _src0, _src1, _src2, _src3)      \
+{                                                                                        \
+    _src0 = __lasx_xvld(p_src, 0);                                                       \
+    _src1 = __lasx_xvldx(p_src, _stride);                                                \
+    _src2 = __lasx_xvldx(p_src, _stride2);                                               \
+    _src3 = __lasx_xvldx(p_src, _stride3);                                               \
+}
+
+static inline int32_t pixel_satd_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                              uint8_t *p_ref, int32_t i_ref_stride,
+                                              uint8_t i_height )
+{
+    int32_t cnt;
+    uint32_t u_sum, sum1, sum2;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff0, diff1, diff2, diff3;
+    __m256i tmp0, tmp1;
+    __m256i sum = __lasx_xvldi(0);
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    for( cnt = i_height >> 3; cnt--; )
+    {
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     diff0, diff1, diff2, diff3 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvilvl_w(src1, src0);
+        src1 = __lasx_xvilvl_w(src3, src2);
+        src2 = __lasx_xvilvl_w(diff1, diff0);
+        src3 = __lasx_xvilvl_w(diff3, diff2);
+        src0 = __lasx_xvpermi_q(src0, src2, 0x02);
+        src1 = __lasx_xvpermi_q(src1, src3, 0x02);
+        src0 = __lasx_xvilvl_d(src1, src0);
+
+
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref0, ref1, ref2, ref3 );
+        p_ref += i_ref_stride_x4;
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     diff0, diff1, diff2, diff3 );
+        p_ref += i_ref_stride_x4;
+        ref0 = __lasx_xvilvl_w(ref1, ref0);
+        ref1 = __lasx_xvilvl_w(ref3, ref2);
+        ref2 = __lasx_xvilvl_w(diff1, diff0);
+        ref3 = __lasx_xvilvl_w(diff3, diff2);
+        ref0 = __lasx_xvpermi_q(ref0, ref2, 0x02);
+        ref1 = __lasx_xvpermi_q(ref1, ref3, 0x02);
+        ref0 = __lasx_xvilvl_d(ref1, ref0);
+
+        diff0 = __lasx_xvsubwev_h_bu(src0, ref0);
+        diff1 = __lasx_xvsubwod_h_bu(src0, ref0);
+
+        tmp0 = __lasx_xvadd_h(diff0, diff1);
+        tmp1 = __lasx_xvsub_h(diff0, diff1);
+
+        diff0 = __lasx_xvpackev_h(tmp1, tmp0);
+        diff1 = __lasx_xvpackod_h(tmp1, tmp0);
+
+        tmp0 = __lasx_xvadd_h(diff0, diff1);
+        tmp1 = __lasx_xvsub_h(diff0, diff1);
+
+        diff0 = __lasx_xvpackev_w(tmp1, tmp0);
+        diff1 = __lasx_xvpackod_w(tmp1, tmp0);
+
+        tmp0 = __lasx_xvadd_h(diff0, diff1);
+        tmp1 = __lasx_xvsub_h(diff0, diff1);
+
+        diff0 = __lasx_xvpackev_d(tmp1, tmp0);
+        diff1 = __lasx_xvpackod_d(tmp1, tmp0);
+
+        tmp0 = __lasx_xvadd_h(diff0, diff1);
+        tmp1 = __lasx_xvsub_h(diff0, diff1);
+
+        diff0 = __lasx_xvpackev_d(tmp1, tmp0);
+        diff1 = __lasx_xvpackod_d(tmp1, tmp0);
+
+        diff0 = __lasx_xvadda_h(diff0, diff1);
+        sum = __lasx_xvadd_h(sum, diff0);
+    }
+    sum = __lasx_xvhaddw_wu_hu( sum, sum );
+    sum = __lasx_xvhaddw_du_wu( sum, sum );
+    sum = __lasx_xvhaddw_qu_du( sum, sum );
+    sum1 = __lasx_xvpickve2gr_wu(sum, 0);
+    sum2 = __lasx_xvpickve2gr_wu(sum, 4);
+    u_sum = sum1 + sum2;
+
+    return ( u_sum >> 1 );
+}
+
+int32_t x264_pixel_satd_4x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    uint32_t sum;
+    intptr_t i_stride_2 = i_stride << 1;
+    intptr_t i_stride2_2 = i_stride2 << 1;
+    intptr_t i_stride_3 = i_stride_2 + i_stride;
+    intptr_t i_stride2_3 = i_stride2_2 + i_stride2;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff0, diff1, tmp0, tmp1;
+
+    src0 = __lasx_xvld(p_pix1, 0);
+    src1 = __lasx_xvldx(p_pix1, i_stride);
+    src2 = __lasx_xvldx(p_pix1, i_stride_2);
+    src3 = __lasx_xvldx(p_pix1, i_stride_3);
+    ref0 = __lasx_xvld(p_pix2, 0);
+    ref1 = __lasx_xvldx(p_pix2, i_stride2);
+    ref2 = __lasx_xvldx(p_pix2, i_stride2_2);
+    ref3 = __lasx_xvldx(p_pix2, i_stride2_3);
+
+    src0 = __lasx_xvilvl_w(src1, src0);
+    src1 = __lasx_xvilvl_w(src3, src2);
+    ref0 = __lasx_xvilvl_w(ref1, ref0);
+    ref1 = __lasx_xvilvl_w(ref3, ref2);
+    src0 = __lasx_xvilvl_d(src1, src0);
+    ref0 = __lasx_xvilvl_d(ref1, ref0);
+
+    diff0 = __lasx_xvsubwev_h_bu(src0, ref0);
+    diff1 = __lasx_xvsubwod_h_bu(src0, ref0);
+    tmp0  = __lasx_xvadd_h(diff0, diff1);
+    tmp1  = __lasx_xvsub_h(diff0, diff1);
+    diff0 = __lasx_xvpackev_h(tmp1, tmp0);
+    diff1 = __lasx_xvpackod_h(tmp1, tmp0);
+    tmp0  = __lasx_xvadd_h(diff0, diff1);
+    tmp1  = __lasx_xvsub_h(diff0, diff1);
+    diff0 = __lasx_xvpackev_w(tmp1, tmp0);
+    diff1 = __lasx_xvpackod_w(tmp1, tmp0);
+    tmp0  = __lasx_xvadd_h(diff0, diff1);
+    tmp1  = __lasx_xvsub_h(diff0, diff1);
+    diff0 = __lasx_xvpackev_d(tmp1, tmp0);
+    diff1 = __lasx_xvpackod_d(tmp1, tmp0);
+    tmp0  = __lasx_xvadd_h(diff0, diff1);
+    tmp1  = __lasx_xvsub_h(diff0, diff1);
+    diff0 = __lasx_xvpackev_d(tmp1, tmp0);
+    diff1 = __lasx_xvpackod_d(tmp1, tmp0);
+    diff0 = __lasx_xvadda_h(diff0, diff1);
+    diff0 = __lasx_xvhaddw_wu_hu( diff0, diff0 );
+    diff0 = __lasx_xvhaddw_du_wu( diff0, diff0 );
+    diff0 = __lasx_xvhaddw_qu_du( diff0, diff0 );
+    sum   = __lasx_xvpickve2gr_wu(diff0, 0);
+    return ( sum >> 1 );
+}
+
+static inline int32_t pixel_satd_8width_lasx( uint8_t *p_pix1, int32_t i_stride,
+                                              uint8_t *p_pix2, int32_t i_stride2,
+                                              uint8_t i_height )
+{
+    int32_t sum, i_8 = 8;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+
+     __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "xvldi          $xr16,            0                                            \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -8                     \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vilvl.d        $vr0,             $vr1,                 $vr0                   \n\t"
+    "vilvl.d        $vr1,             $vr3,                 $vr2                   \n\t"
+    "vilvl.d        $vr2,             $vr5,                 $vr4                   \n\t"
+    "vilvl.d        $vr3,             $vr7,                 $vr6                   \n\t"
+    "xvpermi.q      $xr0,             $xr2,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr3,                 2                      \n\t"
+    "vilvl.d        $vr2,             $vr9,                 $vr8                   \n\t"
+    "vilvl.d        $vr3,             $vr11,                $vr10                  \n\t"
+    "vilvl.d        $vr4,             $vr13,                $vr12                  \n\t"
+    "vilvl.d        $vr5,             $vr15,                $vr14                  \n\t"
+    "xvpermi.q      $xr2,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr5,                 2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr2                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr2                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr3                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvilvl.h       $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr7,             $xr3,                 $xr2                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr0,                 $xr2                   \n\t"
+    "xvadd.h        $xr5,             $xr1,                 $xr3                   \n\t"
+    "xvsub.h        $xr6,             $xr0,                 $xr2                   \n\t"
+    "xvsub.h        $xr7,             $xr1,                 $xr3                   \n\t"
+    "xvadda.h       $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvadda.h       $xr1,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr16,                $xr0                   \n\t"
+    "bge            %[i_height],      %[i_8],               1b                     \n\t"
+    "2:                                                                            \n\t"
+    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2),
+      [i_height]"+&r"(i_height)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2), [i_8]"r"(i_8)
+    : "memory"
+    );
+
+    return ( sum >> 1 );
+}
+
+static inline int32_t pixel_satd_16width_lasx( uint8_t *p_pix1,
+                                               int32_t i_stride,
+                                               uint8_t *p_pix2,
+                                               int32_t i_stride2,
+                                               uint8_t i_height )
+{
+    int32_t sum, i_8 = 8;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+
+    __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "xvldi          $xr16,            0                                            \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -8                     \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
+    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
+    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
+    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
+    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
+    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
+    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
+    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
+    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
+    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
+    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
+
+    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
+    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
+    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
+    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+
+    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
+    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
+    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
+    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
+
+
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
+
+    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
+    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
+    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
+
+    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
+    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
+
+    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr16,                $xr8                   \n\t"
+    "xvadd.h        $xr16,            $xr16,                $xr0                   \n\t"
+    "bge            %[i_height],      %[i_8],               1b                     \n\t"
+    "2:                                                                            \n\t"
+    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2),
+      [i_height]"+&r"(i_height)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2), [i_8]"r"(i_8)
+    : "memory"
+    );
+
+     return ( sum >> 1 );
+}
+
+
+int32_t x264_pixel_satd_4x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    return pixel_satd_4width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 8 );
+}
+
+int32_t x264_pixel_satd_4x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    return pixel_satd_4width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
+}
+
+int32_t x264_pixel_satd_8x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    uint32_t u_sum = 0, sum1, sum2;
+    intptr_t i_stride_2 = i_stride << 1;
+    intptr_t i_stride2_2 = i_stride2 << 1;
+    intptr_t i_stride_3 = i_stride_2 + i_stride;
+    intptr_t i_stride2_3 = i_stride2_2 + i_stride2;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i dif0, dif1;
+    __m256i tmp0, tmp1;
+
+    LASX_LOAD_4(p_pix1, i_stride, i_stride_2, i_stride_3, src0, src1, src2, src3);
+    LASX_LOAD_4(p_pix2, i_stride2, i_stride2_2, i_stride2_3, ref0, ref1, ref2, ref3);
+
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src1 = __lasx_xvilvl_d(src3, src2);
+    ref0 = __lasx_xvilvl_d(ref1, ref0);
+    ref1 = __lasx_xvilvl_d(ref3, ref2);
+    src0 = __lasx_xvpermi_q(src0, src1, 2);
+    ref0 = __lasx_xvpermi_q(ref0, ref1, 2);
+    dif0 = __lasx_xvsubwev_h_bu(src0, ref0);
+    dif1 = __lasx_xvsubwod_h_bu(src0, ref0);
+    tmp0 = __lasx_xvadd_h(dif0, dif1);
+    tmp1 = __lasx_xvsub_h(dif0, dif1);
+    dif0 = __lasx_xvpackev_h(tmp1, tmp0);
+    dif1 = __lasx_xvpackod_h(tmp1, tmp0);
+    tmp0 = __lasx_xvadd_h(dif0, dif1);
+    tmp1 = __lasx_xvsub_h(dif0, dif1);
+    dif0 = __lasx_xvpackev_d(tmp1, tmp0);
+    dif1 = __lasx_xvpackod_d(tmp1, tmp0);
+    tmp0 = __lasx_xvadd_h(dif0, dif1);
+    tmp1 = __lasx_xvsub_h(dif0, dif1);
+    dif0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);
+    dif1 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);
+    tmp0 = __lasx_xvadd_h(dif0, dif1);
+    tmp1 = __lasx_xvsub_h(dif0, dif1);
+    dif0 = __lasx_xvpackev_d(tmp1, tmp0);
+    dif1 = __lasx_xvpackod_d(tmp1, tmp0);
+    dif0 = __lasx_xvadda_h(dif0, dif1);
+    dif0 = __lasx_xvhaddw_wu_hu(dif0, dif0);
+    dif0 = __lasx_xvhaddw_du_wu(dif0, dif0);
+    dif0 = __lasx_xvhaddw_qu_du(dif0, dif0);
+    sum1 = __lasx_xvpickve2gr_wu(dif0, 0);
+    sum2 = __lasx_xvpickve2gr_wu(dif0, 4);
+    u_sum = sum1 + sum2;
+
+    return ( u_sum >> 1 );
+}
+
+int32_t x264_pixel_satd_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    uint32_t sum;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+    uint8_t *pix1, *pix2;
+
+    __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "add.d          %[pix1],          %[p_pix1],            %[stride_4]            \n\t"
+    "add.d          %[pix2],          %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "vld            $vr4,             %[pix1],              0                      \n\t"
+    "vldx           $vr5,             %[pix1],              %[i_stride]            \n\t"
+    "vldx           $vr6,             %[pix1],              %[stride_2]            \n\t"
+    "vldx           $vr7,             %[pix1],              %[stride_3]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "vld            $vr12,            %[pix2],              0                      \n\t"
+    "vldx           $vr13,            %[pix2],              %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[pix2],              %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[pix2],              %[stride2_3]           \n\t"
+    "vilvl.d        $vr0,             $vr1,                 $vr0                   \n\t"
+    "vilvl.d        $vr1,             $vr3,                 $vr2                   \n\t"
+    "vilvl.d        $vr2,             $vr5,                 $vr4                   \n\t"
+    "vilvl.d        $vr3,             $vr7,                 $vr6                   \n\t"
+    "xvpermi.q      $xr0,             $xr2,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr3,                 2                      \n\t"
+    "vilvl.d        $vr2,             $vr9,                 $vr8                   \n\t"
+    "vilvl.d        $vr3,             $vr11,                $vr10                  \n\t"
+    "vilvl.d        $vr4,             $vr13,                $vr12                  \n\t"
+    "vilvl.d        $vr5,             $vr15,                $vr14                  \n\t"
+    "xvpermi.q      $xr2,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr5,                 2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr2                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr2                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr3                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvilvl.h       $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr7,             $xr3,                 $xr2                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr0,                 $xr2                   \n\t"
+    "xvadd.h        $xr5,             $xr1,                 $xr3                   \n\t"
+    "xvsub.h        $xr6,             $xr0,                 $xr2                   \n\t"
+    "xvsub.h        $xr7,             $xr1,                 $xr3                   \n\t"
+    "xvadda.h       $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvadda.h       $xr1,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvhaddw.wu.hu  $xr0,             $xr0,                 $xr0                   \n\t"
+    "xvhaddw.du.wu  $xr0,             $xr0,                 $xr0                   \n\t"
+    "xvhaddw.qu.du  $xr0,             $xr0,                 $xr0                   \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr0,                 0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr0,                 4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [pix1]"=&r"(pix1), [pix2]"=&r"(pix2), [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2), [p_pix1]"r"(p_pix1), [p_pix2]"r"(p_pix2)
+    : "memory"
+    );
+
+    return ( sum >> 1 );
+}
+
+int32_t x264_pixel_satd_8x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    return pixel_satd_8width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
+}
+
+int32_t x264_pixel_satd_16x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    return pixel_satd_16width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 8 );
+}
+
+int32_t x264_pixel_satd_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                    uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    return pixel_satd_16width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
+}
+
+static inline void sad_16width_x4d_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                         uint8_t *p_aref[], int32_t i_ref_stride,
+                                         int32_t i_height, uint32_t *pu_sad_array )
+{
+    int32_t i_ht_cnt;
+    uint8_t *p_ref0, *p_ref1, *p_ref2, *p_ref3;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad0 = __lasx_xvldi( 0 );
+    __m256i sad1 = __lasx_xvldi( 0 );
+    __m256i sad2 = __lasx_xvldi( 0 );
+    __m256i sad3 = __lasx_xvldi( 0 );
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    p_ref0 = p_aref[0];
+    p_ref1 = p_aref[1];
+    p_ref2 = p_aref[2];
+    p_ref3 = p_aref[3];
+
+#define LOAD_REF_DATA_16W( p_ref, sad)                                        \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref0, ref1, ref2, ref3 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref4, ref5, ref6, ref7 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );                              \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src2, ref2 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src3, ref3 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src4, src5, src6, src7 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+        src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+        src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+        LOAD_REF_DATA_16W( p_ref0, sad0 );
+        LOAD_REF_DATA_16W( p_ref1, sad1 );
+        LOAD_REF_DATA_16W( p_ref2, sad2 );
+        LOAD_REF_DATA_16W( p_ref3, sad3 );
+   }
+
+#undef LOAD_REF_DATA_16W
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+    ST_REF_DATA(sad0);
+    pu_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    pu_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    pu_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    ST_REF_DATA(sad3);
+    pu_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+
+#undef ST_REF_DATA
+
+}
+
+static inline void sad_8width_x4d_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                        uint8_t *p_aref[], int32_t i_ref_stride,
+                                        int32_t i_height, uint32_t *pu_sad_array )
+{
+    int32_t i_ht_cnt;
+    uint8_t *p_ref0, *p_ref1, *p_ref2, *p_ref3;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad0 = __lasx_xvldi( 0 );
+    __m256i sad1 = __lasx_xvldi( 0 );
+    __m256i sad2 = __lasx_xvldi( 0 );
+    __m256i sad3 = __lasx_xvldi( 0 );
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    p_ref0 = p_aref[0];
+    p_ref1 = p_aref[1];
+    p_ref2 = p_aref[2];
+    p_ref3 = p_aref[3];
+
+#define LOAD_REF_DATA_8W( p_ref, sad)                                             \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,           \
+                 ref0, ref1, ref2, ref3 );                                        \
+    p_ref += i_ref_stride_x4;                                                     \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,           \
+                 ref4, ref5, ref6, ref7 );                                        \
+    p_ref += i_ref_stride_x4;                                                     \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                         \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                         \
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );                                         \
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );                                         \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                                  \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                                  \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                        \
+    sad += __lasx_xvhaddw_hu_bu( diff, diff );                                    \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                        \
+    sad += __lasx_xvhaddw_hu_bu( diff, diff );
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src4, src5, src6, src7 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvilvl_d( src1, src0 );
+        src1 = __lasx_xvilvl_d( src3, src2 );
+        src2 = __lasx_xvilvl_d( src5, src4 );
+        src3 = __lasx_xvilvl_d( src7, src6 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+        LOAD_REF_DATA_8W( p_ref0, sad0 );
+        LOAD_REF_DATA_8W( p_ref1, sad1 );
+        LOAD_REF_DATA_8W( p_ref2, sad2 );
+        LOAD_REF_DATA_8W( p_ref3, sad3 );
+    }
+
+#undef LOAD_REF_DATA_8W
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+    ST_REF_DATA(sad0);
+    pu_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    pu_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    pu_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    ST_REF_DATA(sad3);
+    pu_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+
+#undef ST_REF_DATA
+
+}
+
+void x264_pixel_sad_x4_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                   uint8_t *p_ref1, uint8_t *p_ref2,
+                                   uint8_t *p_ref3, intptr_t i_ref_stride,
+                                   int32_t p_sad_array[4] )
+{
+    uint8_t *p_aref[4] = { p_ref0, p_ref1, p_ref2, p_ref3 };
+
+    sad_16width_x4d_lasx( p_src, FENC_STRIDE, p_aref, i_ref_stride, 16,
+                          ( uint32_t * ) p_sad_array );
+}
+
+void x264_pixel_sad_x4_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] )
+{
+    uint8_t *p_aref[4] = { p_ref0, p_ref1, p_ref2, p_ref3 };
+
+    sad_16width_x4d_lasx( p_src, FENC_STRIDE, p_aref, i_ref_stride, 8,
+                          ( uint32_t * ) p_sad_array );
+}
+
+void x264_pixel_sad_x4_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] )
+{
+    uint8_t *p_aref[4] = { p_ref0, p_ref1, p_ref2, p_ref3 };
+
+    sad_8width_x4d_lasx( p_src, FENC_STRIDE, p_aref, i_ref_stride, 16,
+                         ( uint32_t * ) p_sad_array );
+}
+
+void x264_pixel_sad_x4_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] )
+{
+    uint8_t *p_aref[4] = { p_ref0, p_ref1, p_ref2, p_ref3 };
+
+    sad_8width_x4d_lasx( p_src, FENC_STRIDE, p_aref, i_ref_stride, 8,
+                         ( uint32_t * ) p_sad_array );
+}
+
+void x264_pixel_sad_x4_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff;
+    __m256i sad0, sad1, sad2, sad3;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+#define LOAD_REF_DATA_8W_4H( p_ref, sad)                                \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3, \
+                 ref0, ref1, ref2, ref3 );                              \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                               \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                               \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                        \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                              \
+    sad = __lasx_xvhaddw_hu_bu( diff, diff );                           \
+    sad = __lasx_xvhaddw_wu_hu( sad, sad );                             \
+    sad = __lasx_xvhaddw_du_wu( sad, sad );                             \
+    sad = __lasx_xvhaddw_qu_du( sad, sad );                             \
+
+    LOAD_REF_DATA_8W_4H( p_ref0, sad0 );
+    LOAD_REF_DATA_8W_4H( p_ref1, sad1 );
+    LOAD_REF_DATA_8W_4H( p_ref2, sad2 );
+    LOAD_REF_DATA_8W_4H( p_ref3, sad3 );
+
+#undef LOAD_REF_DATA_8W_4H
+
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+}
+
+void x264_pixel_sad_x4_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] )
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad0, sad1, sad2, sad3;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    intptr_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    intptr_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    src0 = __lasx_xvld( p_src, 0);
+    src1 = __lasx_xvld( p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld( p_src, 0);
+    src5 = __lasx_xvld( p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx( p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx( p_src, i_src_stride_x3);
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src2 = __lasx_xvilvl_w( src5, src4 );
+    src3 = __lasx_xvilvl_w( src7, src6 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+#define LOAD_REF_DATA_4W_8H( p_ref, sad) \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref0, ref1, ref2, ref3 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref4, ref5, ref6, ref7 );                                    \
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                     \
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                     \
+    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                     \
+    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                     \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                     \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                     \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
+    sad = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
+    sad = __lasx_xvhaddw_wu_hu( sad, sad );                                   \
+    sad = __lasx_xvhaddw_du_wu( sad, sad );                                   \
+    sad = __lasx_xvhaddw_qu_du( sad, sad );                                   \
+
+    LOAD_REF_DATA_4W_8H( p_ref0, sad0 );
+    LOAD_REF_DATA_4W_8H( p_ref1, sad1 );
+    LOAD_REF_DATA_4W_8H( p_ref2, sad2 );
+    LOAD_REF_DATA_4W_8H( p_ref3, sad3 );
+
+#undef LOAD_REF_DATA_4W_8H
+
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+}
+
+void x264_pixel_sad_x4_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src0 = __lasx_xvpermi_q(src0, src0, 0x00);
+
+#define LOAD_REF_DATA_4W_4H( p0, p1 )                                    \
+    LASX_LOAD_4( p0, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,     \
+                 ref0, ref1, ref2, ref3 );                               \
+    LASX_LOAD_4( p1, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,     \
+                 ref4, ref5, ref6, ref7 );                               \
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                \
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                \
+    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                \
+    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                \
+    ref0 = __lasx_xvpermi_q(ref0, ref1, 0x02);                           \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                               \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                           \
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );                           \
+    diff = __lasx_xvhaddw_du_wu( diff, diff );                           \
+    diff = __lasx_xvhaddw_qu_du( diff, diff );                           \
+
+    LOAD_REF_DATA_4W_4H( p_ref0, p_ref1 );
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(diff, 0);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(diff, 4);
+    LOAD_REF_DATA_4W_4H( p_ref2, p_ref3 );
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(diff, 0);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(diff, 4);
+
+#undef LOAD_REF_DATA_4W_4H
+
+}
+
+static inline void sad_16width_x3d_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                         uint8_t *p_ref0, uint8_t *p_ref1,
+                                         uint8_t *p_ref2, int32_t i_ref_stride,
+                                         int32_t i_height, uint32_t *pu_sad_array )
+{
+    int32_t i_ht_cnt;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad0 = __lasx_xvldi(0);
+    __m256i sad1 = __lasx_xvldi(0);
+    __m256i sad2 = __lasx_xvldi(0);
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+#define LOAD_REF_DATA_16W( p_ref, sad)                                        \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref0, ref1, ref2, ref3 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref4, ref5, ref6, ref7 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );                              \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src2, ref2 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src3, ref3 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src4, src5, src6, src7 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+        src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+        src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+        LOAD_REF_DATA_16W( p_ref0, sad0 );
+        LOAD_REF_DATA_16W( p_ref1, sad1 );
+        LOAD_REF_DATA_16W( p_ref2, sad2 );
+   }
+
+#undef LOAD_REF_DATA_16W
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+    ST_REF_DATA(sad0);
+    pu_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    pu_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    pu_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+
+#undef ST_REF_DATA
+
+}
+
+static inline void sad_8width_x3d_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                        uint8_t *p_ref0, uint8_t *p_ref1,
+                                        uint8_t *p_ref2, int32_t i_ref_stride,
+                                        int32_t i_height, uint32_t *pu_sad_array )
+{
+    int32_t i_ht_cnt;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad0 = __lasx_xvldi( 0 );
+    __m256i sad1 = __lasx_xvldi( 0 );
+    __m256i sad2 = __lasx_xvldi( 0 );
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+#define LOAD_REF_DATA_8W( p_ref, sad) \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
+                 ref0, ref1, ref2, ref3 );                                     \
+    p_ref += i_ref_stride_x4;                                                  \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
+                 ref4, ref5, ref6, ref7 );                                     \
+    p_ref += i_ref_stride_x4;                                                  \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                      \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );                                      \
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );                                      \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                               \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                               \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                     \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
+    sad  = __lasx_xvadd_h(sad, diff);                                          \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                     \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
+    sad  = __lasx_xvadd_h(sad, diff);                                          \
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src4, src5, src6, src7 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvilvl_d( src1, src0 );
+        src1 = __lasx_xvilvl_d( src3, src2 );
+        src2 = __lasx_xvilvl_d( src5, src4 );
+        src3 = __lasx_xvilvl_d( src7, src6 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+        LOAD_REF_DATA_8W( p_ref0, sad0 );
+        LOAD_REF_DATA_8W( p_ref1, sad1 );
+        LOAD_REF_DATA_8W( p_ref2, sad2 );
+    }
+
+#undef LOAD_REF_DATA_8W
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+    ST_REF_DATA(sad0);
+    pu_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    pu_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    pu_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+
+#undef ST_REF_DATA
+
+}
+
+void x264_pixel_sad_x3_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                   uint8_t *p_ref1, uint8_t *p_ref2,
+                                   intptr_t i_ref_stride,
+                                   int32_t p_sad_array[3] )
+{
+    sad_16width_x3d_lasx( p_src, FENC_STRIDE, p_ref0, p_ref1, p_ref2,
+                          i_ref_stride, 16, ( uint32_t * ) p_sad_array );
+}
+
+void x264_pixel_sad_x3_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] )
+{
+    sad_16width_x3d_lasx( p_src, FENC_STRIDE, p_ref0, p_ref1, p_ref2,
+                          i_ref_stride, 8, ( uint32_t * ) p_sad_array );
+}
+
+void x264_pixel_sad_x3_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] )
+{
+    sad_8width_x3d_lasx( p_src, FENC_STRIDE, p_ref0, p_ref1, p_ref2,
+                         i_ref_stride, 16, ( uint32_t * ) p_sad_array );
+}
+
+void x264_pixel_sad_x3_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] )
+{
+    sad_8width_x3d_lasx( p_src, FENC_STRIDE, p_ref0, p_ref1, p_ref2,
+                         i_ref_stride, 8, ( uint32_t * ) p_sad_array );
+}
+
+void x264_pixel_sad_x3_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff;
+    __m256i sad0, sad1, sad2;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
+    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+#define LOAD_REF_DATA_8W_4H( p_ref, sad)                                \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3, \
+                 ref0, ref1, ref2, ref3 );                              \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                               \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                               \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                        \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                              \
+    sad = __lasx_xvhaddw_hu_bu( diff, diff );
+
+    LOAD_REF_DATA_8W_4H( p_ref0, sad0 );
+    LOAD_REF_DATA_8W_4H( p_ref1, sad1 );
+    LOAD_REF_DATA_8W_4H( p_ref2, sad2 );
+
+#undef LOAD_REF_DATA_8W_4H
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+
+#undef ST_REF_DATA
+
+}
+
+void x264_pixel_sad_x3_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] )
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad0 = __lasx_xvldi( 0 );
+    __m256i sad1 = __lasx_xvldi( 0 );
+    __m256i sad2 = __lasx_xvldi( 0 );
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
+    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    intptr_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    intptr_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld( p_src, 0 );
+    src5 = __lasx_xvld( p_src, FENC_STRIDE );
+    src6 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src7 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src2 = __lasx_xvilvl_w( src5, src4 );
+    src3 = __lasx_xvilvl_w( src7, src6 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+#define LOAD_REF_DATA_4W_8H( p_ref, sad)                                       \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
+                 ref0, ref1, ref2, ref3 );                                     \
+    p_ref += i_ref_stride_x4;                                                  \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
+                 ref4, ref5, ref6, ref7 );                                     \
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                      \
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                      \
+    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                      \
+    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                      \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                      \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                               \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                     \
+    sad = __lasx_xvhaddw_hu_bu( diff, diff );
+
+    LOAD_REF_DATA_4W_8H( p_ref0, sad0 );
+    LOAD_REF_DATA_4W_8H( p_ref1, sad1 );
+    LOAD_REF_DATA_4W_8H( p_ref2, sad2 );
+
+#undef LOAD_REF_DATA_4W_8H
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+
+#undef ST_REF_DATA
+
+}
+
+void x264_pixel_sad_x3_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
+    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src0 = __lasx_xvpermi_q(src0, src0, 0x00);
+
+    LASX_LOAD_4( p_ref0, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    LASX_LOAD_4( p_ref1, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref2 = __lasx_xvilvl_w( ref5, ref4 );
+    ref3 = __lasx_xvilvl_w( ref7, ref6 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref0 = __lasx_xvpermi_q(ref0, ref1, 0x02);
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );
+    diff = __lasx_xvhaddw_du_wu( diff, diff );
+    diff = __lasx_xvhaddw_qu_du( diff, diff );
+
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(diff, 0);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(diff, 4);
+    LASX_LOAD_4( p_ref2, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );
+    diff = __lasx_xvhaddw_du_wu( diff, diff );
+    diff = __lasx_xvhaddw_qu_du( diff, diff );
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(diff, 0);
+
+}
+
+static inline uint32_t sad_16width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                         uint8_t *p_ref, int32_t i_ref_stride,
+                                         int32_t i_height )
+{
+    int32_t i_ht_cnt;
+    uint32_t result;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad = __lasx_xvldi( 0 );
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src4, src5, src6, src7 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+        src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+        src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref0, ref1, ref2, ref3 );
+        p_ref += i_ref_stride_x4;
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref4, ref5, ref6, ref7 );
+        p_ref += i_ref_stride_x4;
+        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+        ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+        ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
+        ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
+        diff = __lasx_xvabsd_bu( src0, ref0 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h(sad, diff);
+        diff = __lasx_xvabsd_bu( src1, ref1 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h(sad, diff);
+        diff = __lasx_xvabsd_bu( src2, ref2 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h(sad, diff);
+        diff = __lasx_xvabsd_bu( src3, ref3 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h(sad, diff);
+   }
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+
+    return ( result );
+}
+
+static inline uint32_t sad_8width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                        uint8_t *p_ref, int32_t i_ref_stride,
+                                        int32_t i_height )
+{
+    int32_t i_ht_cnt;
+    uint32_t result;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad = __lasx_xvldi( 0 );
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src4, src5, src6, src7 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvilvl_d( src1, src0 );
+        src1 = __lasx_xvilvl_d( src3, src2 );
+        src2 = __lasx_xvilvl_d( src5, src4 );
+        src3 = __lasx_xvilvl_d( src7, src6 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref0, ref1, ref2, ref3 );
+        p_ref += i_ref_stride_x4;
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref4, ref5, ref6, ref7 );
+        p_ref += i_ref_stride_x4;
+        ref0 = __lasx_xvilvl_d( ref1, ref0 );
+        ref1 = __lasx_xvilvl_d( ref3, ref2 );
+        ref2 = __lasx_xvilvl_d( ref5, ref4 );
+        ref3 = __lasx_xvilvl_d( ref7, ref6 );
+        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+        ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+        diff = __lasx_xvabsd_bu( src0, ref0 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h(sad, diff);
+        diff = __lasx_xvabsd_bu( src1, ref1 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h(sad, diff);
+    }
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+
+    return ( result );
+}
+
+static inline uint32_t sad_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                        uint8_t *p_ref, int32_t i_ref_stride,
+                                        int32_t i_height )
+{
+    int32_t i_ht_cnt;
+    uint32_t result;
+    uint8_t * p_src2;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad = __lasx_xvldi( 0 );
+    int32_t i_src_stride_x2 = FENC_STRIDE << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+    int32_t i_src_stride_x8 = i_src_stride << 3;
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src, 0 );
+        src1 = __lasx_xvld( p_src, FENC_STRIDE );
+        src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+        src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+        p_src2 = p_src + i_src_stride_x4;
+        src4 = __lasx_xvld( p_src2, 0 );
+        src5 = __lasx_xvld( p_src2, FENC_STRIDE );
+        src6 = __lasx_xvldx( p_src2, i_src_stride_x2 );
+        src7 = __lasx_xvldx( p_src2, i_src_stride_x3 );
+        p_src += i_src_stride_x8;
+        src0 = __lasx_xvilvl_w( src1, src0 );
+        src1 = __lasx_xvilvl_w( src3, src2 );
+        src2 = __lasx_xvilvl_w( src5, src4 );
+        src3 = __lasx_xvilvl_w( src7, src6 );
+        src0 = __lasx_xvilvl_d( src1, src0 );
+        src1 = __lasx_xvilvl_d( src3, src2 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref0, ref1, ref2, ref3 );
+        p_ref += i_ref_stride_x4;
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref4, ref5, ref6, ref7 );
+        p_ref += i_ref_stride_x4;
+        ref0 = __lasx_xvilvl_w( ref1, ref0 );
+        ref1 = __lasx_xvilvl_w( ref3, ref2 );
+        ref2 = __lasx_xvilvl_w( ref5, ref4 );
+        ref3 = __lasx_xvilvl_w( ref7, ref6 );
+        ref0 = __lasx_xvilvl_d( ref1, ref0 );
+        ref1 = __lasx_xvilvl_d( ref3, ref2 );
+        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+        diff = __lasx_xvabsd_bu( src0, ref0 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h( sad, diff );
+    }
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+
+    return ( result );
+}
+
+int32_t x264_pixel_sad_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                   uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sad_16width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+}
+
+int32_t x264_pixel_sad_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sad_16width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+}
+
+int32_t x264_pixel_sad_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sad_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+}
+
+int32_t x264_pixel_sad_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sad_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+}
+
+int32_t x264_pixel_sad_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff;
+    int32_t result;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+    ref0 = __lasx_xvld( p_ref, 0 );
+    ref1 = __lasx_xvldx( p_ref, i_ref_stride );
+    ref2 = __lasx_xvldx( p_ref, i_ref_stride_x2 );
+    ref3 = __lasx_xvldx( p_ref, i_ref_stride_x3 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );
+    diff = __lasx_xvhaddw_du_wu( diff, diff );
+    diff = __lasx_xvhaddw_qu_du( diff, diff );
+    result = __lasx_xvpickve2gr_wu(diff, 0) + __lasx_xvpickve2gr_wu(diff, 4);
+    return ( result );
+}
+
+int32_t x264_pixel_sad_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sad_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+}
+
+int32_t x264_pixel_sad_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sad_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+}
+
+int32_t x264_pixel_sad_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff;
+    int32_t result;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+
+    src0 = __lasx_xvld( p_src, 0);
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+
+    ref0 = __lasx_xvld( p_ref, 0 );
+    ref1 = __lasx_xvldx( p_ref, i_ref_stride );
+    ref2 = __lasx_xvldx( p_ref, i_ref_stride_x2 );
+    ref3 = __lasx_xvldx( p_ref, i_ref_stride_x3 );
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );
+    diff = __lasx_xvhaddw_du_wu( diff, diff );
+    diff = __lasx_xvhaddw_qu_du( diff, diff );
+    result = __lasx_xvpickve2gr_w(diff, 0);
+
+    return ( result );
+}
+
+static inline uint64_t pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix,
+                                                   int32_t i_stride )
+{
+    uint32_t u_sum4 = 0, u_sum8 = 0, u_dc;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
+    __m256i sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7;
+    __m256i temp0, temp1, temp2, temp3;
+    v16i16  dc;
+    v4u64 out;
+
+    LASX_LD_8( p_pix, i_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7 );
+    LASX_ILVL_B_4_128SV( zero, src0, zero, src1, zero, src2,
+                         zero, src3, diff0, diff1, diff2, diff3 );
+    LASX_ILVL_B_4_128SV( zero, src4, zero, src5, zero, src6,
+                         zero, src7, diff4, diff5, diff6, diff7 );
+    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7,
+                               diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7,
+                               diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+
+    dc = (v16i16 )diff0;
+    u_dc = ( uint16_t ) ( dc[0] + dc[4] );
+    dc = (v16i16 )diff4;
+    u_dc += ( uint16_t ) ( dc[0] + dc[4] );
+
+    sub0 = __lasx_xvadda_h( diff0, diff1 );
+    sub2 = __lasx_xvadda_h( diff2, diff3 );
+    sub4 = __lasx_xvadda_h( diff4, diff5 );
+    sub6 = __lasx_xvadda_h( diff6, diff7 );
+
+    sub0 = __lasx_xvadd_h( sub0, sub2);
+    sub0 = __lasx_xvadd_h( sub0, sub4);
+    sub0 = __lasx_xvadd_h( sub0, sub6);
+    sub0 = __lasx_xvhaddw_wu_hu( sub0, sub0 );
+    out = ( v4u64 ) __lasx_xvhaddw_du_wu( sub0, sub0 );
+    u_sum4 = out[0] + out[1];
+
+    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7,
+                               sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7 );
+
+    LASX_ILVL_D_2_128SV( sub2, sub0, sub6, sub4, diff0, diff1 );
+    LASX_ILVL_D_2_128SV( sub3, sub1, sub7, sub5, diff4, diff6 );
+
+    diff2 = __lasx_xvilvh_d( sub2, sub0 );
+    diff3 = __lasx_xvilvh_d( sub6, sub4 );
+    diff5 = __lasx_xvilvh_d( sub3, sub1 );
+    diff7 = __lasx_xvilvh_d( sub7, sub5 );
+
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+
+    sub0 = __lasx_xvadda_h( diff0, diff1 );
+    sub2 = __lasx_xvadda_h( diff2, diff3 );
+    sub4 = __lasx_xvadda_h( diff4, diff5 );
+    sub6 = __lasx_xvadda_h( diff6, diff7 );
+
+    sub0 = __lasx_xvadd_h( sub0, sub2);
+    sub0 = __lasx_xvadd_h( sub0, sub4);
+    sub0 = __lasx_xvadd_h( sub0, sub6);
+    sub0 = __lasx_xvhaddw_wu_hu( sub0, sub0 );
+    out = ( v4u64 ) __lasx_xvhaddw_du_wu( sub0, sub0 );
+    u_sum8 = out[0] + out[1];
+
+    u_sum4 = u_sum4 - u_dc;
+    u_sum8 = u_sum8 - u_dc;
+
+    return ( ( uint64_t ) u_sum8 << 32 ) + u_sum4;
+}
+
+static inline uint64_t pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix,
+                                                    int32_t i_stride )
+{
+    uint32_t u_sum4 = 0, u_sum8 = 0, u_dc;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
+    __m256i sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7;
+    __m256i temp0, temp1, temp2, temp3;
+    v16i16  dc;
+
+    LASX_LD_8( p_pix, i_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7 );
+    src0 = __lasx_xvpermi_d(src0, 0x50);
+    src1 = __lasx_xvpermi_d(src1, 0x50);
+    src2 = __lasx_xvpermi_d(src2, 0x50);
+    src3 = __lasx_xvpermi_d(src3, 0x50);
+    src4 = __lasx_xvpermi_d(src4, 0x50);
+    src5 = __lasx_xvpermi_d(src5, 0x50);
+    src6 = __lasx_xvpermi_d(src6, 0x50);
+    src7 = __lasx_xvpermi_d(src7, 0x50);
+
+    LASX_ILVL_B_4_128SV( zero, src0, zero, src1, zero, src2,
+                         zero, src3, diff0, diff1, diff2, diff3 );
+    LASX_ILVL_B_4_128SV( zero, src4, zero, src5, zero, src6,
+                         zero, src7, diff4, diff5, diff6, diff7 );
+    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7,
+                               diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7,
+                               diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+
+    dc = (v16i16 )diff0;
+    u_dc = ( uint16_t ) ( dc[0] + dc[4] + dc[8] + dc[12] );
+    dc = (v16i16 )diff4;
+    u_dc += ( uint16_t ) ( dc[0] + dc[4] + dc[8] + dc[12] );
+
+    sub0 = __lasx_xvadda_h( diff0, diff1 );
+    sub2 = __lasx_xvadda_h( diff2, diff3 );
+    sub4 = __lasx_xvadda_h( diff4, diff5 );
+    sub6 = __lasx_xvadda_h( diff6, diff7 );
+
+    sub0 = __lasx_xvadd_h( sub0, sub2);
+    sub0 = __lasx_xvadd_h( sub0, sub4);
+    sub0 = __lasx_xvadd_h( sub0, sub6);
+    u_sum4 = LASX_HADD_UH_U32( sub0 );
+
+    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7,
+                               sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7 );
+
+    LASX_ILVL_D_2_128SV( sub2, sub0, sub6, sub4, diff0, diff1 );
+    LASX_ILVL_D_2_128SV( sub3, sub1, sub7, sub5, diff4, diff6 );
+
+    diff2 = __lasx_xvilvh_d( sub2, sub0 );
+    diff3 = __lasx_xvilvh_d( sub6, sub4 );
+    diff5 = __lasx_xvilvh_d( sub3, sub1 );
+    diff7 = __lasx_xvilvh_d( sub7, sub5 );
+
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+
+    sub0 = __lasx_xvadda_h( diff0, diff1 );
+    sub2 = __lasx_xvadda_h( diff2, diff3 );
+    sub4 = __lasx_xvadda_h( diff4, diff5 );
+    sub6 = __lasx_xvadda_h( diff6, diff7 );
+
+    sub0 = __lasx_xvadd_h( sub0, sub2);
+    sub0 = __lasx_xvadd_h( sub0, sub4);
+    sub0 = __lasx_xvadd_h( sub0, sub6);
+    u_sum8 = LASX_HADD_UH_U32( sub0 );
+
+    u_sum4 = u_sum4 - u_dc;
+    u_sum8 = u_sum8 - u_dc;
+
+    return ( ( uint64_t ) u_sum8 << 32 ) + u_sum4;
+}
+
+#define LASX_CALC_MSE_B( src, ref, var )                                   \
+{                                                                          \
+    __m256i src_l0_m, src_l1_m;                                            \
+    __m256i res_l0_m, res_l1_m;                                            \
+                                                                           \
+    LASX_ILVLH_B_128SV( src, ref, src_l1_m, src_l0_m );                    \
+    LASX_HSUB_UB_2( src_l0_m, src_l1_m, res_l0_m, res_l1_m );              \
+    LASX_DP2ADD_W_H_2( var, res_l0_m, res_l0_m,                            \
+                       var, res_l1_m, res_l1_m, var, var );                \
+}
+
+static uint32_t sse_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                 uint8_t *p_ref, int32_t i_ref_stride,
+                                 int32_t i_height )
+{
+    int32_t i_ht_cnt;
+    uint32_t u_sse;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i var = __lasx_xvldi( 0 );
+    v4i64  out;
+
+    for( i_ht_cnt = ( i_height >> 2 ); i_ht_cnt--; )
+    {
+        src0 = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+        src1 = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+        src2 = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+        src3 = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        src0 = __lasx_xvpackev_d( src1, src0 );
+
+        ref0 = __lasx_xvldrepl_w( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref1 = __lasx_xvldrepl_w( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref2 = __lasx_xvldrepl_w( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref3 = __lasx_xvldrepl_w( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref0 = __lasx_xvpackev_w( ref1, ref0 );
+        ref1 = __lasx_xvpackev_w( ref3, ref2 );
+        ref0 = __lasx_xvpackev_d( ref1, ref0 );
+
+        LASX_CALC_MSE_B( src0, ref0, var );
+    }
+
+    out = __lasx_xvhaddw_d_w( var, var );
+    u_sse = out[0] + out[1];
+
+    return u_sse;
+}
+
+static uint32_t sse_8width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                 uint8_t *p_ref, int32_t i_ref_stride,
+                                 int32_t i_height )
+{
+    int32_t i_ht_cnt;
+    uint32_t u_sse;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i var = __lasx_xvldi( 0 );
+
+    for( i_ht_cnt = ( i_height >> 2 ); i_ht_cnt--; )
+    {
+        src0 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src1 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src2 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src3 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+
+        ref0 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref1 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref2 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref3 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+
+        LASX_PCKEV_D_4_128SV( src1, src0, src3, src2, ref1, ref0, ref3, ref2,
+                              src0, src1, ref0, ref1 );
+        src0 = __lasx_xvpermi_q( src1, src0, 0x20 );
+        ref0 = __lasx_xvpermi_q( ref1, ref0, 0x20 );
+        LASX_CALC_MSE_B( src0, ref0, var );
+    }
+
+    u_sse = LASX_HADD_SW_S32( var );
+
+    return u_sse;
+}
+
+static uint32_t sse_16width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                  uint8_t *p_ref, int32_t i_ref_stride,
+                                  int32_t i_height )
+{
+    int32_t i_ht_cnt;
+    uint32_t u_sse;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i var = __lasx_xvldi( 0 );
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        LASX_LD_2( p_src, i_src_stride, src0, src1 );
+        p_src += ( i_src_stride << 1 );
+        LASX_LD_2( p_src, i_src_stride, src2, src3 );
+        p_src += ( i_src_stride << 1 );
+        LASX_LD_2( p_src, i_src_stride, src4, src5 );
+        p_src += ( i_src_stride << 1 );
+        LASX_LD_2( p_src, i_src_stride, src6, src7 );
+        p_src += ( i_src_stride << 1 );
+
+        LASX_LD_2( p_ref, i_ref_stride, ref0, ref1 );
+        p_ref += ( i_ref_stride << 1 );
+        LASX_LD_2( p_ref, i_ref_stride, ref2, ref3 );
+        p_ref += ( i_ref_stride << 1 );
+        LASX_LD_2( p_ref, i_ref_stride, ref4, ref5 );
+        p_ref += ( i_ref_stride << 1 );
+        LASX_LD_2( p_ref, i_ref_stride, ref6, ref7 );
+        p_ref += ( i_ref_stride << 1 );
+
+        src0 = __lasx_xvpermi_q( src1, src0, 0x20 );
+        ref0 = __lasx_xvpermi_q( ref1, ref0, 0x20 );
+        LASX_CALC_MSE_B( src0, ref0, var );
+        src0 = __lasx_xvpermi_q( src3, src2, 0x20 );
+        ref0 = __lasx_xvpermi_q( ref3, ref2, 0x20 );
+        LASX_CALC_MSE_B( src0, ref0, var );
+        src0 = __lasx_xvpermi_q( src5, src4, 0x20 );
+        ref0 = __lasx_xvpermi_q( ref5, ref4, 0x20 );
+        LASX_CALC_MSE_B( src0, ref0, var );
+        src0 = __lasx_xvpermi_q( src7, src6, 0x20 );
+        ref0 = __lasx_xvpermi_q( ref7, ref6, 0x20 );
+        LASX_CALC_MSE_B( src0, ref0, var );
+    }
+
+    u_sse = LASX_HADD_SW_S32( var );
+
+    return u_sse;
+}
+
+uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = pixel_hadamard_ac_8x8_lasx( p_pix, i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_8x16_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = pixel_hadamard_ac_8x8_lasx( p_pix, i_stride );
+    u_sum += pixel_hadamard_ac_8x8_lasx( p_pix + ( i_stride << 3 ), i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = pixel_hadamard_ac_16x8_lasx( p_pix, i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_16x16_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = pixel_hadamard_ac_16x8_lasx( p_pix, i_stride );
+    u_sum += pixel_hadamard_ac_16x8_lasx( p_pix + ( i_stride << 3 ), i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+static int32_t sa8d_8x8_lasx( uint8_t *p_src, int32_t i_src_stride,
+                              uint8_t *p_ref, int32_t i_ref_stride )
+{
+    uint32_t u_sum = 0;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
+    __m256i temp0, temp1, temp2, temp3;
+    v4u64 out;
+
+    LASX_LD_8( p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7 );
+    LASX_LD_8( p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7 );
+
+    LASX_ILVL_B_4_128SV( src0, ref0, src1, ref1,
+                         src2, ref2, src3, ref3,
+                         src0, src1, src2, src3 );
+    LASX_ILVL_B_4_128SV( src4, ref4, src5, ref5,
+                         src6, ref6, src7, ref7,
+                         src4, src5, src6, src7 );
+    LASX_HSUB_UB_4( src0, src1, src2, src3,
+                    src0, src1, src2, src3 );
+    LASX_HSUB_UB_4( src4, src5, src6, src7,
+                    src4, src5, src6, src7 );
+    LASX_TRANSPOSE8x8_H_128SV( src0, src1, src2, src3,
+                               src4, src5, src6, src7,
+                               src0, src1, src2, src3,
+                               src4, src5, src6, src7 );
+    LASX_BUTTERFLY_4( v16i16, src0, src2, src3, src1,
+                      diff0, diff1, diff4, diff5 );
+    LASX_BUTTERFLY_4( v16i16, src4, src6, src7, src5,
+                      diff2, diff3, diff7, diff6 );
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7,
+                               diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+
+    temp0 = __lasx_xvadd_h( diff0, diff4 );
+    temp1 = __lasx_xvadd_h( diff1, diff5 );
+    temp2 = __lasx_xvadd_h( diff2, diff6 );
+    temp3 = __lasx_xvadd_h( diff3, diff7 );
+
+    diff0 = __lasx_xvabsd_h( diff0, diff4 );
+    diff1 = __lasx_xvabsd_h( diff1, diff5 );
+    diff2 = __lasx_xvabsd_h( diff2, diff6 );
+    diff3 = __lasx_xvabsd_h( diff3, diff7 );
+    diff0 = __lasx_xvadda_h( diff0, temp0 );
+    diff1 = __lasx_xvadda_h( diff1, temp1 );
+    diff2 = __lasx_xvadda_h( diff2, temp2 );
+    diff3 = __lasx_xvadda_h( diff3, temp3 );
+
+    diff0 = __lasx_xvadd_h( diff0, diff1 );
+    diff0 = __lasx_xvadd_h( diff0, diff2 );
+    diff0 = __lasx_xvadd_h( diff0, diff3 );
+
+    diff0 = __lasx_xvhaddw_wu_hu( diff0, diff0 );
+    out = ( v4u64 ) __lasx_xvhaddw_du_wu( diff0, diff0 );
+    u_sum = out[0] + out[1];
+
+    return u_sum;
+}
+
+static int32_t sa8d_8x16_lasx( uint8_t *p_src, int32_t i_src_stride,
+                               uint8_t *p_ref, int32_t i_ref_stride )
+{
+    uint32_t u_sum = 0;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
+    __m256i temp0, temp1, temp2, temp3;
+
+    LASX_LD_8( p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7 );
+    src0 = __lasx_xvpermi_d(src0, 0x50);
+    src1 = __lasx_xvpermi_d(src1, 0x50);
+    src2 = __lasx_xvpermi_d(src2, 0x50);
+    src3 = __lasx_xvpermi_d(src3, 0x50);
+    src4 = __lasx_xvpermi_d(src4, 0x50);
+    src5 = __lasx_xvpermi_d(src5, 0x50);
+    src6 = __lasx_xvpermi_d(src6, 0x50);
+    src7 = __lasx_xvpermi_d(src7, 0x50);
+
+    LASX_LD_8( p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvpermi_d(ref0, 0x50);
+    ref1 = __lasx_xvpermi_d(ref1, 0x50);
+    ref2 = __lasx_xvpermi_d(ref2, 0x50);
+    ref3 = __lasx_xvpermi_d(ref3, 0x50);
+    ref4 = __lasx_xvpermi_d(ref4, 0x50);
+    ref5 = __lasx_xvpermi_d(ref5, 0x50);
+    ref6 = __lasx_xvpermi_d(ref6, 0x50);
+    ref7 = __lasx_xvpermi_d(ref7, 0x50);
+
+    LASX_ILVL_B_4_128SV( src0, ref0, src1, ref1,
+                         src2, ref2, src3, ref3,
+                         src0, src1, src2, src3 );
+    LASX_ILVL_B_4_128SV( src4, ref4, src5, ref5,
+                         src6, ref6, src7, ref7,
+                         src4, src5, src6, src7 );
+    LASX_HSUB_UB_4( src0, src1, src2, src3,
+                    src0, src1, src2, src3 );
+    LASX_HSUB_UB_4( src4, src5, src6, src7,
+                    src4, src5, src6, src7 );
+    LASX_TRANSPOSE8x8_H_128SV( src0, src1, src2, src3,
+                               src4, src5, src6, src7,
+                               src0, src1, src2, src3,
+                               src4, src5, src6, src7 );
+    LASX_BUTTERFLY_4( v16i16, src0, src2, src3, src1,
+                      diff0, diff1, diff4, diff5 );
+    LASX_BUTTERFLY_4( v16i16, src4, src6, src7, src5,
+                      diff2, diff3, diff7, diff6 );
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7,
+                               diff0, diff1, diff2, diff3,
+                               diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
+                      temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
+                      diff4, diff5, diff7, diff6 );
+
+    temp0 = __lasx_xvadd_h( diff0, diff4 );
+    temp1 = __lasx_xvadd_h( diff1, diff5 );
+    temp2 = __lasx_xvadd_h( diff2, diff6 );
+    temp3 = __lasx_xvadd_h( diff3, diff7 );
+
+    diff0 = __lasx_xvabsd_h( diff0, diff4 );
+    diff1 = __lasx_xvabsd_h( diff1, diff5 );
+    diff2 = __lasx_xvabsd_h( diff2, diff6 );
+    diff3 = __lasx_xvabsd_h( diff3, diff7 );
+    diff0 = __lasx_xvadda_h( diff0, temp0 );
+    diff1 = __lasx_xvadda_h( diff1, temp1 );
+    diff2 = __lasx_xvadda_h( diff2, temp2 );
+    diff3 = __lasx_xvadda_h( diff3, temp3 );
+
+    diff0 = __lasx_xvadd_h( diff0, diff1 );
+    diff0 = __lasx_xvadd_h( diff0, diff2 );
+    diff0 = __lasx_xvadd_h( diff0, diff3 );
+
+    u_sum = LASX_HADD_UH_U32( diff0 );
+
+    return u_sum;
+}
+
+int32_t x264_pixel_sa8d_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    int32_t i32Sum = sa8d_8x8_lasx( p_pix1, i_stride, p_pix2, i_stride2 );
+
+    return ( i32Sum + 2 ) >> 2;
+}
+
+int32_t x264_pixel_sa8d_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                    uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    int32_t i32Sum = sa8d_8x16_lasx( p_pix1, i_stride, p_pix2, i_stride2 ) +
+                     sa8d_8x16_lasx( p_pix1 + 8 * i_stride, i_stride,
+                                     p_pix2 + 8 * i_stride2, i_stride2 );
+
+    return ( i32Sum + 2 ) >> 2;
+}
+
+void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
+                                  int32_t p_sad_array[3] )
+{
+    ALIGNED_ARRAY_16( uint8_t, pix, [8 * FDEC_STRIDE] );
+
+    x264_intra_predict_v_8x8_lasx( pix, p_edge );
+    p_sad_array[0] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_h_8x8_lasx( pix, p_edge );
+    p_sad_array[1] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_8x8_lasx( pix, p_edge );
+    p_sad_array[2] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+}
+
+void x264_intra_satd_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] )
+{
+    x264_intra_predict_vert_4x4_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_4x4_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_4x4_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+}
+
+void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                    int32_t p_sad_array[3] )
+{
+    x264_intra_predict_vert_16x16_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                 p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_16x16_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                 p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_16x16_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                 p_enc, FENC_STRIDE );
+}
+
+void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] )
+{
+    x264_intra_predict_dc_4blk_8x8_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_8x8_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_vert_8x8_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                 int32_t p_sad_array[3] )
+{
+    x264_intra_predict_vert_4x4_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_4x4_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_4x4_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] )
+{
+    x264_intra_predict_vert_16x16_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_16x16_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_16x16_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
+                                 int32_t p_sad_array[3] )
+{
+    ALIGNED_ARRAY_16( uint8_t, pix, [8 * FDEC_STRIDE] );
+
+    x264_intra_predict_v_8x8_lasx( pix, p_edge );
+    p_sad_array[0] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_h_8x8_lasx( pix, p_edge );
+    p_sad_array[1] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_8x8_lasx( pix, p_edge );
+    p_sad_array[2] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] )
+{
+    x264_intra_predict_dc_4blk_8x8_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_8x8_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_vert_8x8_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
+int32_t x264_pixel_ssd_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                   uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sse_16width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+}
+
+int32_t x264_pixel_ssd_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sse_16width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+}
+
+int32_t x264_pixel_ssd_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sse_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+}
+
+int32_t x264_pixel_ssd_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sse_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+}
+
+int32_t x264_pixel_ssd_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sse_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 4 );
+}
+
+int32_t x264_pixel_ssd_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sse_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+}
+
+int32_t x264_pixel_ssd_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sse_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+}
+
+int32_t x264_pixel_ssd_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sse_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 4 );
+}
+
+#define LASX_CALC_MSE_AVG_B( src, ref, var, sub )                          \
+{                                                                          \
+    __m256i src_l0_m, src_l1_m;                                            \
+    __m256i res_l0_m, res_l1_m;                                            \
+                                                                           \
+    LASX_ILVLH_B_128SV( src, ref, src_l1_m, src_l0_m );                    \
+    LASX_HSUB_UB_2( src_l0_m, src_l1_m, res_l0_m, res_l1_m );              \
+    LASX_DP2ADD_W_H_2( var, res_l0_m, res_l0_m,                            \
+                       var, res_l1_m, res_l1_m, var, var );                \
+                                                                           \
+    res_l0_m = __lasx_xvadd_h( res_l0_m, res_l1_m );                       \
+    sub = __lasx_xvadd_h( sub, res_l0_m );                                 \
+}
+
+#define VARIANCE_WxH( sse, diff, shift )                                \
+    ( ( sse ) - ( ( ( uint32_t )( diff ) * ( diff ) ) >> ( shift ) ) )
+
+static inline uint32_t sse_diff_8width_lasx( uint8_t *p_src,
+                                             int32_t i_src_stride,
+                                             uint8_t *p_ref,
+                                             int32_t i_ref_stride,
+                                             int32_t i_height,
+                                             int32_t *p_diff )
+{
+    int32_t i_ht_cnt;
+    uint32_t u_sse;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i avg = __lasx_xvldi( 0 );
+    __m256i var = __lasx_xvldi( 0 );
+
+    for( i_ht_cnt = ( i_height >> 2 ); i_ht_cnt--; )
+    {
+        src0 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src1 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src2 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src3 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+
+        ref0 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref1 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref2 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref3 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+
+        LASX_PCKEV_D_4_128SV( src1, src0, src3, src2, ref1, ref0, ref3, ref2,
+                              src0, src1, ref0, ref1 );
+        src0 = __lasx_xvpermi_q( src1, src0, 0x20 );
+        ref0 = __lasx_xvpermi_q( ref1, ref0, 0x20 );
+        LASX_CALC_MSE_AVG_B( src0, ref0, var, avg );
+    }
+
+    avg = __lasx_xvhaddw_w_h( avg, avg );
+    *p_diff = LASX_HADD_SW_S32( avg );
+    u_sse = LASX_HADD_SW_S32( var );
+
+    return u_sse;
+}
+
+static uint64_t avc_pixel_var16width_lasx( uint8_t *p_pix, int32_t i_stride,
+                                           uint8_t i_height )
+{
+    uint32_t u_sum = 0, u_sqr_out = 0, u_cnt;
+    __m256i pix0, pix1, pix2, pix3, pix4, pix5, pix6, pix7;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i add, pix_h, pix_l;
+    __m256i sqr = __lasx_xvldi( 0 );
+
+#define LASX_PIXEL_VAR_16W( src0, src1 )                       \
+    src0 = __lasx_xvpermi_q( src1, src0, 0x20 );               \
+    add = __lasx_xvhaddw_hu_bu( src0, src0 );                  \
+    u_sum += LASX_HADD_UH_U32( add );                          \
+    LASX_ILVLH_B_128SV( zero, src0, pix_h, pix_l );            \
+    LASX_DP2ADD_W_H_2( sqr, pix_h, pix_h,                      \
+                       sqr, pix_l, pix_l, sqr, sqr );
+
+    for( u_cnt = ( i_height >> 3 ); u_cnt--; )
+    {
+        LASX_LD_8( p_pix, i_stride, pix0, pix1, pix2, pix3,
+                   pix4, pix5, pix6, pix7 );
+        p_pix += ( i_stride << 3 );
+
+        LASX_PIXEL_VAR_16W( pix0, pix1 );
+        LASX_PIXEL_VAR_16W( pix2, pix3 );
+        LASX_PIXEL_VAR_16W( pix4, pix5 );
+        LASX_PIXEL_VAR_16W( pix6, pix7 );
+    }
+
+    u_sqr_out = LASX_HADD_SW_S32( sqr );
+
+#undef LASX_PIXEL_VAR_16W
+
+    return ( u_sum + ( ( uint64_t ) u_sqr_out << 32 ) );
+}
+
+static uint64_t avc_pixel_var8width_lasx( uint8_t *p_pix, int32_t i_stride,
+                                          uint8_t i_height )
+{
+    uint32_t u_sum = 0, u_sqr_out = 0, u_cnt;
+    __m256i pix0, pix1, pix2, pix3, pix4, pix5, pix6, pix7;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i add, pix_h, pix_l;
+    __m256i sqr = __lasx_xvldi( 0 );
+
+#define LASX_PIXEL_VAR_8W( src0, src1, src2, src3 )            \
+    LASX_PCKEV_D_128SV( src1, src0, src0 );                    \
+    LASX_PCKEV_D_128SV( src3, src2, src1 );                    \
+    src0 = __lasx_xvpermi_q( src1, src0, 0x20 );               \
+    add = __lasx_xvhaddw_hu_bu( src0, src0 );                  \
+    u_sum += LASX_HADD_UH_U32( add );                          \
+    LASX_ILVLH_B_128SV( zero, src0, pix_h, pix_l );            \
+    LASX_DP2ADD_W_H_2( sqr, pix_h, pix_h,                      \
+                       sqr, pix_l, pix_l, sqr, sqr );
+
+    for( u_cnt = ( i_height >> 3 ); u_cnt--; )
+    {
+        pix0 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix1 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix2 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix3 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix4 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix5 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix6 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix7 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+
+        LASX_PIXEL_VAR_8W( pix0, pix1, pix2, pix3 );
+        LASX_PIXEL_VAR_8W( pix4, pix5, pix6, pix7 );
+    }
+
+    u_sqr_out = LASX_HADD_SW_S32( sqr );
+
+#undef LASX_PIXEL_VAR_8W
+
+    return ( u_sum + ( ( uint64_t ) u_sqr_out << 32 ) );
+}
+
+uint64_t x264_pixel_var_16x16_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    return avc_pixel_var16width_lasx( p_pix, i_stride, 16 );
+}
+
+uint64_t x264_pixel_var_8x16_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    return avc_pixel_var8width_lasx( p_pix, i_stride, 16 );
+}
+
+uint64_t x264_pixel_var_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    return avc_pixel_var8width_lasx( p_pix, i_stride, 8 );
+}
+
+int32_t x264_pixel_var2_8x16_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                   int32_t ssd[2] )
+{
+    int32_t i_var = 0, i_diff_u = 0, i_sqr_u = 0;
+    int32_t i_diff_v = 0, i_sqr_v = 0;
+
+    i_sqr_u = sse_diff_8width_lasx( p_pix1, FENC_STRIDE,
+                                    p_pix2, FDEC_STRIDE, 16, &i_diff_u );
+    i_sqr_v = sse_diff_8width_lasx( p_pix1 + (FENC_STRIDE >> 1),
+                                    FENC_STRIDE,
+                                    p_pix2 + (FDEC_STRIDE >> 1),
+                                    FDEC_STRIDE, 16, &i_diff_v );
+    i_var = VARIANCE_WxH( i_sqr_u, i_diff_u, 7 ) +
+            VARIANCE_WxH( i_sqr_v, i_diff_v, 7 );
+    ssd[0] = i_sqr_u;
+    ssd[1] = i_sqr_v;
+
+    return i_var;
+}
+
+int32_t x264_pixel_var2_8x8_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                  int32_t ssd[2] )
+{
+    int32_t i_var = 0, i_diff_u = 0, i_sqr_u = 0;
+    int32_t i_diff_v = 0, i_sqr_v = 0;
+
+    i_sqr_u = sse_diff_8width_lasx( p_pix1, FENC_STRIDE,
+                                    p_pix2, FDEC_STRIDE, 8, &i_diff_u );
+    i_sqr_v = sse_diff_8width_lasx( p_pix1 + (FENC_STRIDE >> 1),
+                                    FENC_STRIDE,
+                                    p_pix2 + (FDEC_STRIDE >> 1),
+                                    FDEC_STRIDE, 8, &i_diff_v );
+    i_var = VARIANCE_WxH( i_sqr_u, i_diff_u, 6 ) +
+            VARIANCE_WxH( i_sqr_v, i_diff_v, 6 );
+    ssd[0] = i_sqr_u;
+    ssd[1] = i_sqr_v;
+
+    return i_var;
+}
+
+#endif
diff --git a/common/loongarch/pixel.h b/common/loongarch/pixel.h
new file mode 100644
index 00000000..ca56f2a6
--- /dev/null
+++ b/common/loongarch/pixel.h
@@ -0,0 +1,232 @@
+/*****************************************************************************
+ * pixel.h: loongarch pixel metrics
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_PIXEL_H
+#define X264_LOONGARCH_PIXEL_H
+
+#define x264_pixel_satd_4x4_lasx x264_template(pixel_satd_4x4_lasx)
+int32_t x264_pixel_satd_4x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_4x8_lasx x264_template(pixel_satd_4x8_lasx)
+int32_t x264_pixel_satd_4x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_4x16_lasx x264_template(pixel_satd_4x16_lasx)
+int32_t x264_pixel_satd_4x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x4_lasx x264_template(pixel_satd_8x4_lasx)
+int32_t x264_pixel_satd_8x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x8_lasx x264_template(pixel_satd_8x8_lasx)
+int32_t x264_pixel_satd_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x16_lasx x264_template(pixel_satd_8x16_lasx)
+int32_t x264_pixel_satd_8x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_16x8_lasx x264_template(pixel_satd_16x8_lasx)
+int32_t x264_pixel_satd_16x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_16x16_lasx x264_template(pixel_satd_16x16_lasx)
+int32_t x264_pixel_satd_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                    uint8_t *p_pix2, intptr_t i_stride2 );
+
+#define x264_pixel_sad_x4_16x16_lasx x264_template(pixel_sad_x4_16x16_lasx)
+void x264_pixel_sad_x4_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                   uint8_t *p_ref1, uint8_t *p_ref2,
+                                   uint8_t *p_ref3, intptr_t i_ref_stride,
+                                   int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_16x8_lasx x264_template(pixel_sad_x4_16x8_lasx)
+void x264_pixel_sad_x4_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x16_lasx x264_template(pixel_sad_x4_8x16_lasx)
+void x264_pixel_sad_x4_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x8_lasx x264_template(pixel_sad_x4_8x8_lasx)
+void x264_pixel_sad_x4_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x4_lasx x264_template(pixel_sad_x4_8x4_lasx)
+void x264_pixel_sad_x4_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_4x8_lasx x264_template(pixel_sad_x4_4x8_lasx)
+void x264_pixel_sad_x4_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_4x4_lasx x264_template(pixel_sad_x4_4x4_lasx)
+void x264_pixel_sad_x4_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+
+#define x264_pixel_sad_x3_16x16_lasx x264_template(pixel_sad_x3_16x16_lasx)
+void x264_pixel_sad_x3_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_16x8_lasx x264_template(pixel_sad_x3_16x8_lasx)
+void x264_pixel_sad_x3_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x16_lasx x264_template(pixel_sad_x3_8x16_lasx)
+void x264_pixel_sad_x3_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x8_lasx x264_template(pixel_sad_x3_8x8_lasx)
+void x264_pixel_sad_x3_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x4_lasx x264_template(pixel_sad_x3_8x4_lasx)
+void x264_pixel_sad_x3_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_4x8_lasx x264_template(pixel_sad_x3_4x8_lasx)
+void x264_pixel_sad_x3_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_4x4_lasx x264_template(pixel_sad_x3_4x4_lasx)
+void x264_pixel_sad_x3_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+
+#define x264_pixel_sad_16x16_lasx x264_template(pixel_sad_16x16_lasx)
+int32_t x264_pixel_sad_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                   uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_16x8_lasx x264_template(pixel_sad_16x8_lasx)
+int32_t x264_pixel_sad_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x16_lasx x264_template(pixel_sad_8x16_lasx)
+int32_t x264_pixel_sad_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x8_lasx x264_template(pixel_sad_8x8_lasx)
+int32_t x264_pixel_sad_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x4_lasx x264_template(pixel_sad_8x4_lasx)
+int32_t x264_pixel_sad_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x16_lasx x264_template(pixel_sad_4x16_lasx)
+int32_t x264_pixel_sad_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x8_lasx x264_template(pixel_sad_4x8_lasx)
+int32_t x264_pixel_sad_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x4_lasx x264_template(pixel_sad_4x4_lasx)
+int32_t x264_pixel_sad_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+
+#define x264_pixel_hadamard_ac_8x8_lasx x264_template(pixel_hadamard_ac_8x8_lasx)
+uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_8x16_lasx x264_template(pixel_hadamard_ac_8x16_lasx)
+uint64_t x264_pixel_hadamard_ac_8x16_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_16x8_lasx x264_template(pixel_hadamard_ac_16x8_lasx)
+uint64_t x264_pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_16x16_lasx x264_template(pixel_hadamard_ac_16x16_lasx)
+uint64_t x264_pixel_hadamard_ac_16x16_lasx( uint8_t *p_pix, intptr_t i_stride );
+
+#define x264_intra_satd_x3_4x4_lasx x264_template(intra_satd_x3_4x4_lasx)
+void x264_intra_satd_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] );
+#define x264_intra_satd_x3_16x16_lasx x264_template(intra_satd_x3_16x16_lasx)
+void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                    int32_t p_sad_array[3] );
+#define x264_intra_satd_x3_8x8c_lasx x264_template(intra_satd_x3_8x8c_lasx)
+void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] );
+
+#define x264_pixel_ssd_16x16_lasx x264_template(pixel_ssd_16x16_lasx)
+int32_t x264_pixel_ssd_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                   uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_16x8_lasx x264_template(pixel_ssd_16x8_lasx)
+int32_t x264_pixel_ssd_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x16_lasx x264_template(pixel_ssd_8x16_lasx)
+int32_t x264_pixel_ssd_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x8_lasx x264_template(pixel_ssd_8x8_lasx)
+int32_t x264_pixel_ssd_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x4_lasx x264_template(pixel_ssd_8x4_lasx)
+int32_t x264_pixel_ssd_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x16_lasx x264_template(pixel_ssd_4x16_lasx)
+int32_t x264_pixel_ssd_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x8_lasx x264_template(pixel_ssd_4x8_lasx)
+int32_t x264_pixel_ssd_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x4_lasx x264_template(pixel_ssd_4x4_lasx)
+int32_t x264_pixel_ssd_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+
+#define x264_pixel_var_16x16_lasx x264_template(pixel_var_16x16_lasx)
+uint64_t x264_pixel_var_16x16_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_var_8x16_lasx x264_template(pixel_var_8x16_lasx)
+uint64_t x264_pixel_var_8x16_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_var_8x8_lasx x264_template(pixel_var_8x8_lasx)
+uint64_t x264_pixel_var_8x8_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_var2_8x16_lasx x264_template(pixel_var2_8x16_lasx)
+int32_t x264_pixel_var2_8x16_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                   int32_t ssd[2] );
+#define x264_pixel_var2_8x8_lasx x264_template(pixel_var2_8x8_lasx)
+int32_t x264_pixel_var2_8x8_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                  int32_t ssd[2] );
+
+#define x264_intra_sa8d_x3_8x8_lasx x264_template(intra_sa8d_x3_8x8_lasx)
+void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sa8d_8x8_lasx x264_template(pixel_sa8d_8x8_lasx)
+int32_t x264_pixel_sa8d_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_sa8d_16x16_lasx x264_template(pixel_sa8d_16x16_lasx)
+int32_t x264_pixel_sa8d_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                    uint8_t *p_pix2, intptr_t i_stride2 );
+
+#define x264_intra_sad_x3_4x4_lasx x264_template(intra_sad_x3_4x4_lasx)
+void x264_intra_sad_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                 int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_16x16_lasx x264_template(intra_sad_x3_16x16_lasx)
+void x264_intra_sad_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_8x8_lasx x264_template(intra_sad_x3_8x8_lasx)
+void x264_intra_sad_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
+                                 int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_8x8c_lasx x264_template(intra_sad_x3_8x8c_lasx)
+void x264_intra_sad_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] );
+
+#endif
diff --git a/common/loongarch/predict-c.c b/common/loongarch/predict-c.c
new file mode 100644
index 00000000..d372235d
--- /dev/null
+++ b/common/loongarch/predict-c.c
@@ -0,0 +1,484 @@
+/*****************************************************************************
+ * predict-c.c: loongarch intra prediction
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "generic_macros_lasx.h"
+#include "predict.h"
+
+#if !HIGH_BIT_DEPTH
+
+static inline void intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src,
+                                                   int32_t i_stride )
+{
+    uint32_t u_mask = 0x01010101;
+    uint8_t *p_src1, *p_src2;
+    __m256i sum, mask;
+    v8u32 out;
+
+    sum = LASX_LD( p_src - i_stride );
+    sum = __lasx_xvhaddw_hu_bu( sum, sum );
+    out = ( v8u32 ) __lasx_xvhaddw_wu_hu( sum, sum );
+    mask = __lasx_xvreplgr2vr_w( u_mask );
+
+    p_src1 = p_src - 1;
+    p_src2 = p_src1 + ( i_stride << 2 );
+    out[0] += p_src1[0];
+    out[2] = p_src2[0];
+
+    p_src1 += i_stride;
+    p_src2 += i_stride;
+    out[0] += p_src1[0];
+    out[2] += p_src2[0];
+
+    p_src1 += i_stride;
+    p_src2 += i_stride;
+    out[0] += p_src1[0];
+    out[2] += p_src2[0];
+
+    p_src1 += i_stride;
+    p_src2 += i_stride;
+    out[0] += p_src1[0];
+    out[2] += p_src2[0];
+
+    out[0] = ( out[0] + 4 ) >> 3;
+    out[3] = ( out[1] + out[2] + 4 ) >> 3;
+    out[1] = ( out[1] + 2 ) >> 2;
+    out[2] = ( out[2] + 2 ) >> 2;
+
+    out = ( v8u32 ) __lasx_xvmul_w( ( __m256i ) out, mask );
+
+    LASX_ST_D( out, 0, p_src );
+    LASX_ST_D( out, 1, ( p_src + ( i_stride << 2 ) ) );
+    p_src += i_stride;
+
+    LASX_ST_D( out, 0, p_src );
+    LASX_ST_D( out, 1, ( p_src + ( i_stride << 2 ) ) );
+    p_src += i_stride;
+
+    LASX_ST_D( out, 0, p_src );
+    LASX_ST_D( out, 1, ( p_src + ( i_stride << 2 ) ) );
+    p_src += i_stride;
+
+    LASX_ST_D( out, 0, p_src );
+    LASX_ST_D( out, 1, ( p_src + ( i_stride << 2 ) ) );
+    p_src += i_stride;
+}
+
+static inline void intra_predict_dc_4x4_lasx( uint8_t *p_src_top,
+                                              uint8_t *p_src_left,
+                                              int32_t i_src_stride_left,
+                                              uint8_t *p_dst,
+                                              int32_t i_dst_stride,
+                                              uint8_t is_above,
+                                              uint8_t is_left )
+{
+    uint32_t u_row;
+    uint32_t u_addition = 0;
+    __m256i src, store;
+    v8u32 sum;
+
+    if( is_left && is_above )
+    {
+        src  = LASX_LD( p_src_top );
+        src = __lasx_xvhaddw_hu_bu( src, src );
+        sum = ( v8u32 ) __lasx_xvhaddw_wu_hu( src, src );
+        u_addition = sum[0];
+
+        for( u_row = 0; u_row < 4; u_row++ )
+        {
+            u_addition += p_src_left[u_row * i_src_stride_left];
+        }
+
+        u_addition = ( u_addition + 4 ) >> 3;
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+    else if( is_left )
+    {
+        for( u_row = 0; u_row < 4; u_row++ )
+        {
+            u_addition += p_src_left[u_row * i_src_stride_left];
+        }
+
+        u_addition = ( u_addition + 2 ) >> 2;
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+    else if( is_above )
+    {
+        src  = LASX_LD( p_src_top );
+        src = __lasx_xvhaddw_hu_bu( src, src );
+        src = __lasx_xvhaddw_wu_hu( src, src );
+        src = __lasx_xvsrari_w( src, 2 );
+
+        store = __lasx_xvrepl128vei_b( src, 0 );
+    }
+    else
+    {
+        u_addition = 128;
+
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+
+    LASX_ST_W_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+}
+
+static inline void intra_predict_dc_8x8_lasx( uint8_t *p_src_top,
+                                              uint8_t *p_src_left,
+                                              uint8_t *p_dst,
+                                              int32_t i_dst_stride )
+{
+    __m256i src0, src1, store;
+
+    src0 = __lasx_xvldrepl_d( p_src_top, 0 );
+    src1 = __lasx_xvldrepl_d( p_src_left, 0 );
+    LASX_PCKEV_D_128SV( src1, src0, src0 );
+
+    src0 = __lasx_xvhaddw_hu_bu( src0, src0 );
+    src0 = __lasx_xvhaddw_wu_hu( src0, src0 );
+    src0 = __lasx_xvhaddw_du_wu( src0, src0 );
+    src0 = __lasx_xvpickev_w( src0, src0 );
+    src0 = __lasx_xvhaddw_du_wu( src0, src0 );
+    src0 = __lasx_xvsrari_w( src0, 4 );
+    store = __lasx_xvrepl128vei_b( src0, 0 );
+
+    LASX_ST_D_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    p_dst += ( i_dst_stride  << 2);
+    LASX_ST_D_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+}
+
+static inline void intra_predict_dc_16x16_lasx( uint8_t *p_src_top,
+                                                uint8_t *p_src_left,
+                                                int32_t i_src_stride_left,
+                                                uint8_t *p_dst,
+                                                int32_t i_dst_stride,
+                                                uint8_t is_above,
+                                                uint8_t is_left )
+{
+    uint32_t u_row;
+    int32_t i_index = 0;
+    uint32_t u_addition = 0;
+    __m256i src, store;
+    v4u64 sum;
+
+    if( is_left && is_above )
+    {
+        src  = LASX_LD( p_src_top );
+        src = __lasx_xvhaddw_hu_bu( src, src );
+        src = __lasx_xvhaddw_wu_hu( src, src );
+        src = __lasx_xvhaddw_du_wu( src, src );
+        src = __lasx_xvpickev_w( src, src );
+        sum = ( v4u64 ) __lasx_xvhaddw_du_wu( src, src );
+        u_addition = sum[0];
+
+        for( u_row = 0; u_row < 4; u_row++ )
+        {
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+        }
+
+        u_addition = ( u_addition + 16 ) >> 5;
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+    else if( is_left )
+    {
+        for( u_row = 0; u_row < 4; u_row++ )
+        {
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+        }
+
+        u_addition = ( u_addition + 8 ) >> 4;
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+    else if( is_above )
+    {
+        src  = LASX_LD( p_src_top );
+        src = __lasx_xvhaddw_hu_bu( src, src );
+        src = __lasx_xvhaddw_wu_hu( src, src );
+        src = __lasx_xvhaddw_du_wu( src, src );
+        src = __lasx_xvpickev_w( src, src );
+        src = __lasx_xvhaddw_du_wu( src, src );
+        src = __lasx_xvsrari_d( src, 4 );
+
+        store = __lasx_xvrepl128vei_b( src, 0 );
+    }
+    else
+    {
+        u_addition = 128;
+
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+
+    LASX_ST_Q_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    p_dst += ( i_dst_stride  << 2);
+    LASX_ST_Q_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    p_dst += ( i_dst_stride  << 2);
+    LASX_ST_Q_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    p_dst += ( i_dst_stride  << 2);
+    LASX_ST_Q_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+}
+
+static inline void intra_predict_horiz_16x16_lasx( uint8_t *p_src,
+                                                   int32_t i_src_stride,
+                                                   uint8_t *p_dst,
+                                                   int32_t i_dst_stride )
+{
+    uint32_t u_row;
+    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
+    __m256i src0, src1, src2, src3;
+
+    for( u_row = 4; u_row--; )
+    {
+        u_inp0 = p_src[0];
+        p_src += i_src_stride;
+        u_inp1 = p_src[0];
+        p_src += i_src_stride;
+        u_inp2 = p_src[0];
+        p_src += i_src_stride;
+        u_inp3 = p_src[0];
+        p_src += i_src_stride;
+
+        src0 = __lasx_xvreplgr2vr_b( u_inp0 );
+        src1 = __lasx_xvreplgr2vr_b( u_inp1 );
+        src2 = __lasx_xvreplgr2vr_b( u_inp2 );
+        src3 = __lasx_xvreplgr2vr_b( u_inp3 );
+
+        LASX_ST_Q( src0, 0, p_dst );
+        p_dst += i_dst_stride;
+        LASX_ST_Q( src1, 0, p_dst );
+        p_dst += i_dst_stride;
+        LASX_ST_Q( src2, 0, p_dst );
+        p_dst += i_dst_stride;
+        LASX_ST_Q( src3, 0, p_dst );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void intra_predict_horiz_8x8_lasx( uint8_t *p_src,
+                                                 int32_t i_src_stride,
+                                                 uint8_t *p_dst,
+                                                 int32_t i_dst_stride )
+{
+    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
+    __m256i src0, src1, src2, src3;
+
+    u_inp0 = p_src[0];
+    p_src += i_src_stride;
+    u_inp1 = p_src[0];
+    p_src += i_src_stride;
+    u_inp2 = p_src[0];
+    p_src += i_src_stride;
+    u_inp3 = p_src[0];
+    p_src += i_src_stride;
+
+    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
+    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
+    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
+    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
+
+    LASX_ST_D( src0, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_D( src1, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_D( src2, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_D( src3, 0, p_dst );
+    p_dst += i_dst_stride;
+
+    u_inp0 = p_src[0];
+    p_src += i_src_stride;
+    u_inp1 = p_src[0];
+    p_src += i_src_stride;
+    u_inp2 = p_src[0];
+    p_src += i_src_stride;
+    u_inp3 = p_src[0];
+    p_src += i_src_stride;
+
+    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
+    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
+    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
+    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
+
+    LASX_ST_D( src0, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_D( src1, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_D( src2, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_D( src3, 0, p_dst );
+    p_dst += i_dst_stride;
+}
+
+static inline void intra_predict_horiz_4x4_lasx( uint8_t *p_src,
+                                                 int32_t i_src_stride,
+                                                 uint8_t *p_dst,
+                                                 int32_t i_dst_stride )
+{
+    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
+    __m256i src0, src1, src2, src3;
+
+    u_inp0 = p_src[0];
+    p_src += i_src_stride;
+    u_inp1 = p_src[0];
+    p_src += i_src_stride;
+    u_inp2 = p_src[0];
+    p_src += i_src_stride;
+    u_inp3 = p_src[0];
+    p_src += i_src_stride;
+
+    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
+    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
+    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
+    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
+
+    LASX_ST_W( src0, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_W( src1, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_W( src2, 0, p_dst );
+    p_dst += i_dst_stride;
+    LASX_ST_W( src3, 0, p_dst );
+    p_dst += i_dst_stride;
+}
+
+static inline void intra_predict_vert_16x16_lasx( uint8_t *p_src,
+                                                  uint8_t *p_dst,
+                                                  int32_t i_dst_stride )
+{
+    __m256i src;
+    src  = LASX_LD( p_src );
+
+    LASX_ST_Q_4( src, 0, 0, 0, 0, p_dst, i_dst_stride );
+    p_dst += ( i_dst_stride  << 2);
+    LASX_ST_Q_4( src, 0, 0, 0, 0, p_dst, i_dst_stride );
+    p_dst += ( i_dst_stride  << 2);
+    LASX_ST_Q_4( src, 0, 0, 0, 0, p_dst, i_dst_stride );
+    p_dst += ( i_dst_stride  << 2);
+    LASX_ST_Q_4( src, 0, 0, 0, 0, p_dst, i_dst_stride );
+}
+
+static inline void intra_predict_vert_8x8_lasx( uint8_t *p_src,
+                                                uint8_t *p_dst,
+                                                int32_t i_dst_stride )
+{
+    __m256i out;
+
+    out = __lasx_xvldrepl_d( p_src, 0 );
+
+    LASX_ST_D_4( out, 0, 0, 0, 0, p_dst, i_dst_stride );
+    p_dst += ( i_dst_stride << 2 );
+    LASX_ST_D_4( out, 0, 0, 0, 0, p_dst, i_dst_stride );
+}
+
+static inline void intra_predict_vert_4x4_lasx( uint8_t *p_src,
+                                                uint8_t *p_dst,
+                                                int32_t i_dst_stride )
+{
+    __m256i out;
+
+    out = __lasx_xvldrepl_w( p_src, 0 );
+
+    LASX_ST_W_4( out, 0, 0, 0, 0, p_dst, i_dst_stride );
+}
+
+void x264_intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src )
+{
+    intra_predict_dc_4blk_8x8_lasx( p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_hor_8x8_lasx( uint8_t *p_src )
+{
+    intra_predict_horiz_8x8_lasx( ( p_src - 1 ), FDEC_STRIDE,
+                                  p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_vert_8x8_lasx( uint8_t *p_src )
+{
+    intra_predict_vert_8x8_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_dc_4x4_lasx( uint8_t *p_src )
+{
+    intra_predict_dc_4x4_lasx( ( p_src - FDEC_STRIDE ), ( p_src - 1 ),
+                               FDEC_STRIDE, p_src, FDEC_STRIDE, 1, 1 );
+}
+
+void x264_intra_predict_hor_4x4_lasx( uint8_t *p_src )
+{
+    intra_predict_horiz_4x4_lasx( ( p_src - 1 ), FDEC_STRIDE,
+                                  p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_vert_4x4_lasx( uint8_t *p_src )
+{
+    intra_predict_vert_4x4_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_hor_16x16_lasx( uint8_t *p_src )
+{
+    intra_predict_horiz_16x16_lasx( ( p_src - 1 ), FDEC_STRIDE,
+                                    p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_vert_16x16_lasx( uint8_t *p_src )
+{
+    intra_predict_vert_16x16_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_dc_16x16_lasx( uint8_t *p_src )
+{
+    intra_predict_dc_16x16_lasx( ( p_src - FDEC_STRIDE ), ( p_src - 1 ),
+                                 FDEC_STRIDE, p_src, FDEC_STRIDE, 1, 1 );
+}
+
+void x264_intra_predict_dc_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
+{
+    intra_predict_dc_8x8_lasx( ( pu_xyz + 16 ), ( pu_xyz + 7 ),
+                               p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_h_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
+{
+    intra_predict_horiz_8x8_lasx( ( pu_xyz + 14 ), -1, p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_v_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
+{
+    intra_predict_vert_8x8_lasx( ( pu_xyz + 16 ), p_src, FDEC_STRIDE );
+}
+
+#endif
diff --git a/common/loongarch/predict.h b/common/loongarch/predict.h
new file mode 100644
index 00000000..8db72ad5
--- /dev/null
+++ b/common/loongarch/predict.h
@@ -0,0 +1,58 @@
+/*****************************************************************************
+ * predict.h: loongarch intra prediction
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_PREDICT_H
+#define X264_LOONGARCH_PREDICT_H
+
+#define x264_intra_predict_dc_16x16_lasx x264_template(intra_predict_dc_16x16_lasx)
+void x264_intra_predict_dc_16x16_lasx( uint8_t *p_src );
+#define x264_intra_predict_hor_16x16_lasx x264_template(intra_predict_hor_16x16_lasx)
+void x264_intra_predict_hor_16x16_lasx( uint8_t *p_src );
+#define x264_intra_predict_vert_16x16_lasx x264_template(intra_predict_vert_16x16_lasx)
+void x264_intra_predict_vert_16x16_lasx( uint8_t *p_src );
+
+#define x264_intra_predict_dc_4blk_8x8_lasx x264_template(intra_predict_dc_4blk_8x8_lasx)
+void x264_intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src );
+#define x264_intra_predict_hor_8x8_lasx x264_template(intra_predict_hor_8x8_lasx)
+void x264_intra_predict_hor_8x8_lasx( uint8_t *p_src );
+#define x264_intra_predict_vert_8x8_lasx x264_template(intra_predict_vert_8x8_lasx)
+void x264_intra_predict_vert_8x8_lasx( uint8_t *p_src );
+
+#define x264_intra_predict_dc_4x4_lasx x264_template(intra_predict_dc_4x4_lasx)
+void x264_intra_predict_dc_4x4_lasx( uint8_t *p_src );
+#define x264_intra_predict_hor_4x4_lasx x264_template(intra_predict_hor_4x4_lasx)
+void x264_intra_predict_hor_4x4_lasx( uint8_t *p_src );
+#define x264_intra_predict_vert_4x4_lasx x264_template(intra_predict_vert_4x4_lasx)
+void x264_intra_predict_vert_4x4_lasx( uint8_t *p_src );
+
+#define x264_intra_predict_dc_8x8_lasx x264_template(intra_predict_dc_8x8_lasx)
+void x264_intra_predict_dc_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
+#define x264_intra_predict_h_8x8_lasx x264_template(intra_predict_h_8x8_lasx)
+void x264_intra_predict_h_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
+#define x264_intra_predict_v_8x8_lasx x264_template(intra_predict_v_8x8_lasx)
+void x264_intra_predict_v_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
+
+#endif
diff --git a/common/loongarch/quant-c.c b/common/loongarch/quant-c.c
new file mode 100644
index 00000000..6167672a
--- /dev/null
+++ b/common/loongarch/quant-c.c
@@ -0,0 +1,273 @@
+/*****************************************************************************
+ * quant-c.c: loongarch quantization and level-run
+ *****************************************************************************
+ * Copyright (C) 2020 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "generic_macros_lasx.h"
+#include "quant.h"
+
+#if !HIGH_BIT_DEPTH
+
+static inline int32_t avc_coeff_last64_lasx( int16_t *p_src )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    v16i16 result;
+    v32u8 mask = { 1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128,
+                   1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128 };
+
+    LASX_LD_4( p_src, 16, src0, src1, src2, src3 );
+
+    tmp0 = __lasx_xvseqi_h( src0, 0 );
+    tmp1 = __lasx_xvseqi_h( src1, 0 );
+    tmp2 = __lasx_xvseqi_h( src2, 0 );
+    tmp3 = __lasx_xvseqi_h( src3, 0 );
+
+    tmp0 = __lasx_xvpickev_b( tmp1, tmp0 );
+    tmp1 = __lasx_xvpickev_b( tmp3, tmp2 );
+
+    tmp0 = tmp0 & (__m256i)mask;
+    tmp1 = tmp1 & (__m256i)mask;
+
+    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
+
+    tmp1 = __lasx_xvhaddw_h_b( tmp1, tmp1 );
+    tmp1 = __lasx_xvpickev_b( tmp1, tmp1 );
+    tmp1 = __lasx_xvhaddw_h_b( tmp1, tmp1 );
+    tmp1 = __lasx_xvpickev_b( tmp1, tmp1 );
+    tmp1 = __lasx_xvhaddw_h_b( tmp1, tmp1 );
+    tmp1 = __lasx_xvpickev_b( tmp1, tmp1 );
+
+    tmp2 = __lasx_xvpermi_q( tmp0, tmp0, 0x11 );
+    tmp0 = __lasx_xvilvl_b( tmp2, tmp0 );
+    tmp3 = __lasx_xvpermi_q( tmp1, tmp1, 0x11 );
+    tmp1 = __lasx_xvilvl_b( tmp3, tmp1 );
+    tmp0 = __lasx_xvpackev_w( tmp1, tmp0 );
+    result = ( v16i16 ) __lasx_xvclo_d( tmp0 );
+
+    return ( 63 - result[0] );
+}
+
+static inline int32_t avc_coeff_last16_lasx( int16_t *p_src )
+{
+    __m256i src0, tmp0, out0, out1;
+    v16i16 result;
+    v32u8 mask = { 1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128,
+                   1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128 };
+
+    src0 = LASX_LD( p_src );
+    out0 = __lasx_xvseqi_h( src0, 0 );
+    out1 = __lasx_xvpermi_q(out0, out0, 0x11);
+    tmp0 = __lasx_xvpickev_b( out1, out0 );
+    tmp0 = tmp0 & (__m256i)mask;
+
+    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
+    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
+    result = ( v16i16 ) __lasx_xvclo_h( tmp0 );
+
+    return ( 15 - result[0] );
+}
+
+static inline int32_t avc_quant_4x4_lasx( int16_t *p_dct,
+                                          uint16_t *p_mf,
+                                          uint16_t *p_bias )
+{
+    int32_t non_zero = 0;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i dct_mask;
+    __m256i dct, dct0, dct1;
+    __m256i mf, mf0, mf1;
+    __m256i bias, bias0, bias1;
+    __m256i tmp;
+
+    dct = LASX_LD( p_dct );
+    bias = LASX_LD( p_bias );
+    mf = LASX_LD( p_mf );
+
+    dct_mask = __lasx_xvslei_h( dct, 0 );
+
+    LASX_UNPCK_SH_128SV( dct, dct0, dct1 );
+    LASX_ILVLH_H_128SV( zero, bias, bias1, bias0 );
+    LASX_ILVLH_H_128SV( zero, mf, mf1, mf0 );
+
+    dct0 = __lasx_xvadda_w( dct0, bias0 );
+    dct1 = __lasx_xvadda_w( dct1, bias1 );
+    dct0 = __lasx_xvmul_w(dct0, mf0);
+    dct1 = __lasx_xvmul_w(dct1, mf1);
+
+    LASX_SRAI_W_2( dct0, dct1, dct0, dct1, 16);
+    dct = __lasx_xvpickev_h( dct1, dct0 );
+    tmp = __lasx_xvhaddw_w_h( dct, dct );
+    non_zero = LASX_HADD_SW_S32( tmp );
+
+    dct0 = __lasx_xvsub_h( zero, dct );
+    LASX_BMNZ( dct, dct0, dct_mask, dct );
+
+    LASX_ST( dct, p_dct );
+
+    return !!non_zero;
+}
+
+static inline int32_t avc_quant_8x8_lasx( int16_t *p_dct,
+                                          uint16_t *p_mf,
+                                          uint16_t *p_bias )
+{
+    int32_t non_zero = 0;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i dct_mask0, dct_mask1;
+    __m256i dct0, dct1, dct0_0, dct0_1, dct1_0, dct1_1;
+    __m256i mf0, mf1, mf0_0, mf0_1, mf1_0, mf1_1;
+    __m256i bias0, bias1, bias0_0, bias0_1, bias1_0, bias1_1;
+    __m256i tmp;
+
+    LASX_LD_2( p_dct, 16, dct0, dct1 );
+    LASX_LD_2( p_bias, 16, bias0, bias1 );
+    LASX_LD_2( p_mf, 16, mf0, mf1 );
+
+    dct_mask0 = __lasx_xvslei_h( dct0, 0 );
+    dct_mask1 = __lasx_xvslei_h( dct1, 0 );
+
+    LASX_UNPCK_SH_128SV( dct0, dct0_0, dct0_1 );
+    LASX_UNPCK_SH_128SV( dct1, dct1_0, dct1_1 );
+    LASX_ILVLH_H_2_128SV( zero, bias0, zero, bias1,
+                          bias0_1, bias0_0, bias1_1, bias1_0 );
+    LASX_ILVLH_H_2_128SV( zero, mf0, zero, mf1,
+                          mf0_1, mf0_0, mf1_1, mf1_0 );
+
+    dct0_0 = __lasx_xvadda_w( dct0_0, bias0_0 );
+    dct0_1 = __lasx_xvadda_w( dct0_1, bias0_1 );
+    dct1_0 = __lasx_xvadda_w( dct1_0, bias1_0 );
+    dct1_1 = __lasx_xvadda_w( dct1_1, bias1_1 );
+
+    dct0_0 = __lasx_xvmul_w( dct0_0, mf0_0 );
+    dct0_1 = __lasx_xvmul_w( dct0_1, mf0_1 );
+    dct1_0 = __lasx_xvmul_w( dct1_0, mf1_0 );
+    dct1_1 = __lasx_xvmul_w( dct1_1, mf1_1 );
+
+    LASX_SRAI_W_4( dct0_0, dct0_1, dct1_0, dct1_1,
+                   dct0_0, dct0_1, dct1_0, dct1_1, 16);
+    dct0 = __lasx_xvpickev_h( dct0_1, dct0_0 );
+    dct1 = __lasx_xvpickev_h( dct1_1, dct1_0 );
+
+    tmp = __lasx_xvadd_h( dct0, dct1 );
+    tmp = __lasx_xvhaddw_w_h( tmp, tmp );
+    non_zero = LASX_HADD_SW_S32( tmp );
+
+    dct0_0 = __lasx_xvsub_h( zero, dct0 );
+    dct1_0 = __lasx_xvsub_h( zero, dct1 );
+    LASX_BMNZ( dct0, dct0_0, dct_mask0, dct0 );
+    LASX_BMNZ( dct1, dct1_0, dct_mask1, dct1 );
+
+    LASX_ST_2( dct0, dct1, p_dct, 16 );
+
+    /* next part */
+    LASX_LD_2( p_dct + 32, 16, dct0, dct1 );
+    LASX_LD_2( p_bias + 32, 16, bias0, bias1 );
+    LASX_LD_2( p_mf + 32, 16, mf0, mf1 );
+
+    dct_mask0 = __lasx_xvslei_h( dct0, 0 );
+    dct_mask1 = __lasx_xvslei_h( dct1, 0 );
+
+    LASX_UNPCK_SH_128SV( dct0, dct0_0, dct0_1 );
+    LASX_UNPCK_SH_128SV( dct1, dct1_0, dct1_1 );
+    LASX_ILVLH_H_2_128SV( zero, bias0, zero, bias1,
+                          bias0_1, bias0_0, bias1_1, bias1_0 );
+    LASX_ILVLH_H_2_128SV( zero, mf0, zero, mf1,
+                          mf0_1, mf0_0, mf1_1, mf1_0 );
+
+    dct0_0 = __lasx_xvadda_w( dct0_0, bias0_0 );
+    dct0_1 = __lasx_xvadda_w( dct0_1, bias0_1 );
+    dct1_0 = __lasx_xvadda_w( dct1_0, bias1_0 );
+    dct1_1 = __lasx_xvadda_w( dct1_1, bias1_1 );
+
+    dct0_0 = __lasx_xvmul_w( dct0_0, mf0_0 );
+    dct0_1 = __lasx_xvmul_w( dct0_1, mf0_1 );
+    dct1_0 = __lasx_xvmul_w( dct1_0, mf1_0 );
+    dct1_1 = __lasx_xvmul_w( dct1_1, mf1_1 );
+
+    LASX_SRAI_W_4( dct0_0, dct0_1, dct1_0, dct1_1,
+                   dct0_0, dct0_1, dct1_0, dct1_1, 16);
+    dct0 = __lasx_xvpickev_h( dct0_1, dct0_0 );
+    dct1 = __lasx_xvpickev_h( dct1_1, dct1_0 );
+
+    tmp = __lasx_xvadd_h( dct0, dct1 );
+    tmp = __lasx_xvhaddw_w_h( tmp, tmp );
+    non_zero += LASX_HADD_SW_S32( tmp );
+
+    dct0_0 = __lasx_xvsub_h( zero, dct0 );
+    dct1_0 = __lasx_xvsub_h( zero, dct1 );
+    LASX_BMNZ( dct0, dct0_0, dct_mask0, dct0 );
+    LASX_BMNZ( dct1, dct1_0, dct_mask1, dct1 );
+
+    LASX_ST_2( dct0, dct1, p_dct + 32, 16 );
+
+    return !!non_zero;
+}
+
+int32_t x264_coeff_last16_lasx( int16_t *p_src )
+{
+    return avc_coeff_last16_lasx( p_src );
+}
+
+int32_t x264_coeff_last64_lasx( int16_t *p_src )
+{
+    return avc_coeff_last64_lasx( p_src );
+}
+
+int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
+{
+    return avc_quant_4x4_lasx( p_dct, p_mf, p_bias );
+}
+
+int32_t x264_quant_4x4x4_lasx( int16_t p_dct[4][16],
+                               uint16_t pu_mf[16], uint16_t pu_bias[16] )
+{
+    int32_t i_non_zero, i_non_zero_acc = 0;
+
+    for( int32_t j = 0; j < 4; j++  )
+    {
+        i_non_zero = x264_quant_4x4_lasx( p_dct[j], pu_mf, pu_bias );
+
+        i_non_zero_acc |= ( !!i_non_zero ) << j;
+    }
+
+    return i_non_zero_acc;
+}
+
+int32_t x264_quant_8x8_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
+{
+    return avc_quant_8x8_lasx( p_dct, p_mf, p_bias );
+}
+
+#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/quant.h b/common/loongarch/quant.h
new file mode 100644
index 00000000..da11ed5e
--- /dev/null
+++ b/common/loongarch/quant.h
@@ -0,0 +1,42 @@
+/*****************************************************************************
+ * quant.h: loongarch quantization and level-run
+ *****************************************************************************
+ * Copyright (C) 2020 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_QUANT_H
+#define X264_LOONGARCH_QUANT_H
+
+#define x264_coeff_last64_lasx x264_template(coeff_last64_lasx)
+int32_t x264_coeff_last64_lasx( int16_t *p_src );
+#define x264_coeff_last16_lasx x264_template(coeff_last16_lasx)
+int32_t x264_coeff_last16_lasx( int16_t *p_src );
+#define x264_quant_4x4_lasx x264_template(quant_4x4_lasx)
+int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
+#define x264_quant_4x4x4_lasx x264_template(quant_4x4x4_lasx)
+int32_t x264_quant_4x4x4_lasx( int16_t p_dct[4][16],
+                               uint16_t pu_mf[16], uint16_t pu_bias[16] );
+#define x264_quant_8x8_lasx x264_template(quant_8x8_lasx)
+int32_t x264_quant_8x8_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
+
+#endif/* X264_LOONGARCH_QUANT_H */
diff --git a/common/mc.c b/common/mc.c
index 4ca69f1a..e81334a4 100644
--- a/common/mc.c
+++ b/common/mc.c
@@ -41,6 +41,9 @@
 #if ARCH_MIPS
 #include "mips/mc.h"
 #endif
+#if ARCH_LOONGARCH
+#   include "loongarch/mc.h"
+#endif
 
 
 static inline void pixel_avg( pixel *dst,  intptr_t i_dst_stride,
@@ -686,6 +689,9 @@ void x264_mc_init( int cpu, x264_mc_functions_t *pf, int cpu_independent )
     if( cpu&X264_CPU_MSA )
         x264_mc_init_mips( cpu, pf );
 #endif
+#if HAVE_LASX
+    x264_mc_init_loongarch( cpu, pf );
+#endif
 
     if( cpu_independent )
     {
diff --git a/common/pixel.c b/common/pixel.c
index 9c3a54ee..0d84d2c7 100644
--- a/common/pixel.c
+++ b/common/pixel.c
@@ -45,7 +45,9 @@
 #if ARCH_MIPS
 #   include "mips/pixel.h"
 #endif
-
+#if ARCH_LOONGARCH
+#   include "loongarch/pixel.h"
+#endif
 
 /****************************************************************************
  * pixel_sad_WxH
@@ -1509,6 +1511,36 @@ void x264_pixel_init( int cpu, x264_pixel_function_t *pixf )
     }
 #endif // HAVE_MSA
 
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        INIT8( sad, _lasx );
+        INIT8_NAME( sad_aligned, sad, _lasx );
+        INIT8( ssd, _lasx );
+        INIT7( sad_x3, _lasx );
+        INIT7( sad_x4, _lasx );
+        INIT8( satd, _lasx );
+        INIT4( hadamard_ac, _lasx );
+
+        pixf->intra_sad_x3_4x4   = x264_intra_sad_x3_4x4_lasx;
+        pixf->intra_sad_x3_8x8   = x264_intra_sad_x3_8x8_lasx;
+        pixf->intra_sad_x3_8x8c  = x264_intra_sad_x3_8x8c_lasx;
+        pixf->intra_sad_x3_16x16 = x264_intra_sad_x3_16x16_lasx;
+        pixf->intra_satd_x3_4x4   = x264_intra_satd_x3_4x4_lasx;
+        pixf->intra_satd_x3_16x16 = x264_intra_satd_x3_16x16_lasx;
+        pixf->intra_satd_x3_8x8c  = x264_intra_satd_x3_8x8c_lasx;
+        pixf->intra_sa8d_x3_8x8   = x264_intra_sa8d_x3_8x8_lasx;
+
+        pixf->var[PIXEL_16x16] = x264_pixel_var_16x16_lasx;
+        pixf->var[PIXEL_8x16]  = x264_pixel_var_8x16_lasx;
+        pixf->var[PIXEL_8x8]   = x264_pixel_var_8x8_lasx;
+        pixf->var2[PIXEL_8x16]  = x264_pixel_var2_8x16_lasx;
+        pixf->var2[PIXEL_8x8]   = x264_pixel_var2_8x8_lasx;
+        pixf->sa8d[PIXEL_16x16] = x264_pixel_sa8d_16x16_lasx;
+        pixf->sa8d[PIXEL_8x8]   = x264_pixel_sa8d_8x8_lasx;
+    }
+#endif // HAVE_LASX
+
 #endif // HIGH_BIT_DEPTH
 #if HAVE_ALTIVEC
     if( cpu&X264_CPU_ALTIVEC )
diff --git a/common/quant.c b/common/quant.c
index 78366673..324f654e 100644
--- a/common/quant.c
+++ b/common/quant.c
@@ -43,6 +43,9 @@
 #if ARCH_MIPS
 #   include "mips/quant.h"
 #endif
+#if ARCH_LOONGARCH
+#   include "loongarch/quant.h"
+#endif
 
 #define QUANT_ONE( coef, mf, f ) \
 { \
@@ -804,6 +807,17 @@ void x264_quant_init( x264_t *h, int cpu, x264_quant_function_t *pf )
         pf->coeff_last[DCT_LUMA_8x8] = x264_coeff_last64_msa;
     }
 #endif
+
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf->quant_4x4      = x264_quant_4x4_lasx;
+        pf->quant_4x4x4    = x264_quant_4x4x4_lasx;
+        pf->quant_8x8      = x264_quant_8x8_lasx;
+        pf->coeff_last[DCT_LUMA_4x4] = x264_coeff_last16_lasx;
+        pf->coeff_last[DCT_LUMA_8x8] = x264_coeff_last64_lasx;
+    }
+#endif
 #endif // HIGH_BIT_DEPTH
     pf->coeff_last[DCT_LUMA_DC]     = pf->coeff_last[DCT_CHROMAU_DC]  = pf->coeff_last[DCT_CHROMAV_DC] =
     pf->coeff_last[DCT_CHROMAU_4x4] = pf->coeff_last[DCT_CHROMAV_4x4] = pf->coeff_last[DCT_LUMA_4x4];
diff --git a/config.guess b/config.guess
index 375fa2a8..015efd1a 100755
--- a/config.guess
+++ b/config.guess
@@ -1,14 +1,12 @@
 #! /bin/sh
 # Attempt to guess a canonical system name.
-#   Copyright (C) 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
-#   2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,
-#   2011, 2012 Free Software Foundation, Inc.
+#   Copyright 1992-2018 Free Software Foundation, Inc.
 
-timestamp='2012-09-25'
+timestamp='2018-02-24'
 
 # This file is free software; you can redistribute it and/or modify it
 # under the terms of the GNU General Public License as published by
-# the Free Software Foundation; either version 2 of the License, or
+# the Free Software Foundation; either version 3 of the License, or
 # (at your option) any later version.
 #
 # This program is distributed in the hope that it will be useful, but
@@ -17,24 +15,22 @@ timestamp='2012-09-25'
 # General Public License for more details.
 #
 # You should have received a copy of the GNU General Public License
-# along with this program; if not, see <http://www.gnu.org/licenses/>.
+# along with this program; if not, see <https://www.gnu.org/licenses/>.
 #
 # As a special exception to the GNU General Public License, if you
 # distribute this file as part of a program that contains a
 # configuration script generated by Autoconf, you may include it under
-# the same distribution terms that you use for the rest of that program.
-
-
-# Originally written by Per Bothner.  Please send patches (context
-# diff format) to <config-patches@gnu.org> and include a ChangeLog
-# entry.
+# the same distribution terms that you use for the rest of that
+# program.  This Exception is an additional permission under section 7
+# of the GNU General Public License, version 3 ("GPLv3").
 #
-# This script attempts to guess a canonical system name similar to
-# config.sub.  If it succeeds, it prints the system name on stdout, and
-# exits with 0.  Otherwise, it exits with 1.
+# Originally written by Per Bothner; maintained since 2000 by Ben Elliston.
 #
 # You can get the latest version of this script from:
-# http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess;hb=HEAD
+# https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess
+#
+# Please send patches to <config-patches@gnu.org>.
+
 
 me=`echo "$0" | sed -e 's,.*/,,'`
 
@@ -43,7 +39,7 @@ Usage: $0 [OPTION]
 
 Output the configuration name of the system \`$me' is run on.
 
-Operation modes:
+Options:
   -h, --help         print this help, then exit
   -t, --time-stamp   print date of last modification, then exit
   -v, --version      print version number, then exit
@@ -54,9 +50,7 @@ version="\
 GNU config.guess ($timestamp)
 
 Originally written by Per Bothner.
-Copyright (C) 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000,
-2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012
-Free Software Foundation, Inc.
+Copyright 1992-2018 Free Software Foundation, Inc.
 
 This is free software; see the source for copying conditions.  There is NO
 warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE."
@@ -113,9 +107,9 @@ trap "rm -f \$tmpfiles 2>/dev/null; rmdir \$tmp 2>/dev/null; exit 1" 1 2 13 15 ;
 dummy=$tmp/dummy ;
 tmpfiles="$dummy.c $dummy.o $dummy.rel $dummy" ;
 case $CC_FOR_BUILD,$HOST_CC,$CC in
- ,,)    echo "int x;" > $dummy.c ;
+ ,,)    echo "int x;" > "$dummy.c" ;
 	for c in cc gcc c89 c99 ; do
-	  if ($c -c -o $dummy.o $dummy.c) >/dev/null 2>&1 ; then
+	  if ($c -c -o "$dummy.o" "$dummy.c") >/dev/null 2>&1 ; then
 	     CC_FOR_BUILD="$c"; break ;
 	  fi ;
 	done ;
@@ -138,9 +132,37 @@ UNAME_RELEASE=`(uname -r) 2>/dev/null` || UNAME_RELEASE=unknown
 UNAME_SYSTEM=`(uname -s) 2>/dev/null`  || UNAME_SYSTEM=unknown
 UNAME_VERSION=`(uname -v) 2>/dev/null` || UNAME_VERSION=unknown
 
+case "$UNAME_SYSTEM" in
+Linux|GNU|GNU/*)
+	# If the system lacks a compiler, then just pick glibc.
+	# We could probably try harder.
+	LIBC=gnu
+
+	eval "$set_cc_for_build"
+	cat <<-EOF > "$dummy.c"
+	#include <features.h>
+	#if defined(__UCLIBC__)
+	LIBC=uclibc
+	#elif defined(__dietlibc__)
+	LIBC=dietlibc
+	#else
+	LIBC=gnu
+	#endif
+	EOF
+	eval "`$CC_FOR_BUILD -E "$dummy.c" 2>/dev/null | grep '^LIBC' | sed 's, ,,g'`"
+
+	# If ldd exists, use it to detect musl libc.
+	if command -v ldd >/dev/null && \
+		ldd --version 2>&1 | grep -q ^musl
+	then
+	    LIBC=musl
+	fi
+	;;
+esac
+
 # Note: order is significant - the case branches are not exclusive.
 
-case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
+case "$UNAME_MACHINE:$UNAME_SYSTEM:$UNAME_RELEASE:$UNAME_VERSION" in
     *:NetBSD:*:*)
 	# NetBSD (nbsd) targets should (where applicable) match one or
 	# more of the tuples: *-*-netbsdelf*, *-*-netbsdaout*,
@@ -153,21 +175,31 @@ case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
 	# Note: NetBSD doesn't particularly care about the vendor
 	# portion of the name.  We always set it to "unknown".
 	sysctl="sysctl -n hw.machine_arch"
-	UNAME_MACHINE_ARCH=`(/sbin/$sysctl 2>/dev/null || \
-	    /usr/sbin/$sysctl 2>/dev/null || echo unknown)`
-	case "${UNAME_MACHINE_ARCH}" in
+	UNAME_MACHINE_ARCH=`(uname -p 2>/dev/null || \
+	    "/sbin/$sysctl" 2>/dev/null || \
+	    "/usr/sbin/$sysctl" 2>/dev/null || \
+	    echo unknown)`
+	case "$UNAME_MACHINE_ARCH" in
 	    armeb) machine=armeb-unknown ;;
 	    arm*) machine=arm-unknown ;;
 	    sh3el) machine=shl-unknown ;;
 	    sh3eb) machine=sh-unknown ;;
 	    sh5el) machine=sh5le-unknown ;;
-	    *) machine=${UNAME_MACHINE_ARCH}-unknown ;;
+	    earmv*)
+		arch=`echo "$UNAME_MACHINE_ARCH" | sed -e 's,^e\(armv[0-9]\).*$,\1,'`
+		endian=`echo "$UNAME_MACHINE_ARCH" | sed -ne 's,^.*\(eb\)$,\1,p'`
+		machine="${arch}${endian}"-unknown
+		;;
+	    *) machine="$UNAME_MACHINE_ARCH"-unknown ;;
 	esac
 	# The Operating System including object format, if it has switched
-	# to ELF recently, or will in the future.
-	case "${UNAME_MACHINE_ARCH}" in
+	# to ELF recently (or will in the future) and ABI.
+	case "$UNAME_MACHINE_ARCH" in
+	    earm*)
+		os=netbsdelf
+		;;
 	    arm*|i386|m68k|ns32k|sh3*|sparc|vax)
-		eval $set_cc_for_build
+		eval "$set_cc_for_build"
 		if echo __ELF__ | $CC_FOR_BUILD -E - 2>/dev/null \
 			| grep -q __ELF__
 		then
@@ -182,44 +214,67 @@ case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
 		os=netbsd
 		;;
 	esac
+	# Determine ABI tags.
+	case "$UNAME_MACHINE_ARCH" in
+	    earm*)
+		expr='s/^earmv[0-9]/-eabi/;s/eb$//'
+		abi=`echo "$UNAME_MACHINE_ARCH" | sed -e "$expr"`
+		;;
+	esac
 	# The OS release
 	# Debian GNU/NetBSD machines have a different userland, and
 	# thus, need a distinct triplet. However, they do not need
 	# kernel version information, so it can be replaced with a
 	# suitable tag, in the style of linux-gnu.
-	case "${UNAME_VERSION}" in
+	case "$UNAME_VERSION" in
 	    Debian*)
 		release='-gnu'
 		;;
 	    *)
-		release=`echo ${UNAME_RELEASE}|sed -e 's/[-_].*/\./'`
+		release=`echo "$UNAME_RELEASE" | sed -e 's/[-_].*//' | cut -d. -f1,2`
 		;;
 	esac
 	# Since CPU_TYPE-MANUFACTURER-KERNEL-OPERATING_SYSTEM:
 	# contains redundant information, the shorter form:
 	# CPU_TYPE-MANUFACTURER-OPERATING_SYSTEM is used.
-	echo "${machine}-${os}${release}"
+	echo "$machine-${os}${release}${abi}"
 	exit ;;
     *:Bitrig:*:*)
 	UNAME_MACHINE_ARCH=`arch | sed 's/Bitrig.//'`
-	echo ${UNAME_MACHINE_ARCH}-unknown-bitrig${UNAME_RELEASE}
+	echo "$UNAME_MACHINE_ARCH"-unknown-bitrig"$UNAME_RELEASE"
 	exit ;;
     *:OpenBSD:*:*)
 	UNAME_MACHINE_ARCH=`arch | sed 's/OpenBSD.//'`
-	echo ${UNAME_MACHINE_ARCH}-unknown-openbsd${UNAME_RELEASE}
+	echo "$UNAME_MACHINE_ARCH"-unknown-openbsd"$UNAME_RELEASE"
+	exit ;;
+    *:LibertyBSD:*:*)
+	UNAME_MACHINE_ARCH=`arch | sed 's/^.*BSD\.//'`
+	echo "$UNAME_MACHINE_ARCH"-unknown-libertybsd"$UNAME_RELEASE"
+	exit ;;
+    *:MidnightBSD:*:*)
+	echo "$UNAME_MACHINE"-unknown-midnightbsd"$UNAME_RELEASE"
 	exit ;;
     *:ekkoBSD:*:*)
-	echo ${UNAME_MACHINE}-unknown-ekkobsd${UNAME_RELEASE}
+	echo "$UNAME_MACHINE"-unknown-ekkobsd"$UNAME_RELEASE"
 	exit ;;
     *:SolidBSD:*:*)
-	echo ${UNAME_MACHINE}-unknown-solidbsd${UNAME_RELEASE}
+	echo "$UNAME_MACHINE"-unknown-solidbsd"$UNAME_RELEASE"
 	exit ;;
     macppc:MirBSD:*:*)
-	echo powerpc-unknown-mirbsd${UNAME_RELEASE}
+	echo powerpc-unknown-mirbsd"$UNAME_RELEASE"
 	exit ;;
     *:MirBSD:*:*)
-	echo ${UNAME_MACHINE}-unknown-mirbsd${UNAME_RELEASE}
+	echo "$UNAME_MACHINE"-unknown-mirbsd"$UNAME_RELEASE"
+	exit ;;
+    *:Sortix:*:*)
+	echo "$UNAME_MACHINE"-unknown-sortix
 	exit ;;
+    *:Redox:*:*)
+	echo "$UNAME_MACHINE"-unknown-redox
+	exit ;;
+    mips:OSF1:*.*)
+        echo mips-dec-osf1
+        exit ;;
     alpha:OSF1:*:*)
 	case $UNAME_RELEASE in
 	*4.0)
@@ -236,63 +291,54 @@ case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
 	ALPHA_CPU_TYPE=`/usr/sbin/psrinfo -v | sed -n -e 's/^  The alpha \(.*\) processor.*$/\1/p' | head -n 1`
 	case "$ALPHA_CPU_TYPE" in
 	    "EV4 (21064)")
-		UNAME_MACHINE="alpha" ;;
+		UNAME_MACHINE=alpha ;;
 	    "EV4.5 (21064)")
-		UNAME_MACHINE="alpha" ;;
+		UNAME_MACHINE=alpha ;;
 	    "LCA4 (21066/21068)")
-		UNAME_MACHINE="alpha" ;;
+		UNAME_MACHINE=alpha ;;
 	    "EV5 (21164)")
-		UNAME_MACHINE="alphaev5" ;;
+		UNAME_MACHINE=alphaev5 ;;
 	    "EV5.6 (21164A)")
-		UNAME_MACHINE="alphaev56" ;;
+		UNAME_MACHINE=alphaev56 ;;
 	    "EV5.6 (21164PC)")
-		UNAME_MACHINE="alphapca56" ;;
+		UNAME_MACHINE=alphapca56 ;;
 	    "EV5.7 (21164PC)")
-		UNAME_MACHINE="alphapca57" ;;
+		UNAME_MACHINE=alphapca57 ;;
 	    "EV6 (21264)")
-		UNAME_MACHINE="alphaev6" ;;
+		UNAME_MACHINE=alphaev6 ;;
 	    "EV6.7 (21264A)")
-		UNAME_MACHINE="alphaev67" ;;
+		UNAME_MACHINE=alphaev67 ;;
 	    "EV6.8CB (21264C)")
-		UNAME_MACHINE="alphaev68" ;;
+		UNAME_MACHINE=alphaev68 ;;
 	    "EV6.8AL (21264B)")
-		UNAME_MACHINE="alphaev68" ;;
+		UNAME_MACHINE=alphaev68 ;;
 	    "EV6.8CX (21264D)")
-		UNAME_MACHINE="alphaev68" ;;
+		UNAME_MACHINE=alphaev68 ;;
 	    "EV6.9A (21264/EV69A)")
-		UNAME_MACHINE="alphaev69" ;;
+		UNAME_MACHINE=alphaev69 ;;
 	    "EV7 (21364)")
-		UNAME_MACHINE="alphaev7" ;;
+		UNAME_MACHINE=alphaev7 ;;
 	    "EV7.9 (21364A)")
-		UNAME_MACHINE="alphaev79" ;;
+		UNAME_MACHINE=alphaev79 ;;
 	esac
 	# A Pn.n version is a patched version.
 	# A Vn.n version is a released version.
 	# A Tn.n version is a released field test version.
 	# A Xn.n version is an unreleased experimental baselevel.
 	# 1.2 uses "1.2" for uname -r.
-	echo ${UNAME_MACHINE}-dec-osf`echo ${UNAME_RELEASE} | sed -e 's/^[PVTX]//' | tr 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' 'abcdefghijklmnopqrstuvwxyz'`
+	echo "$UNAME_MACHINE"-dec-osf"`echo "$UNAME_RELEASE" | sed -e 's/^[PVTX]//' | tr ABCDEFGHIJKLMNOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz`"
 	# Reset EXIT trap before exiting to avoid spurious non-zero exit code.
 	exitcode=$?
 	trap '' 0
 	exit $exitcode ;;
-    Alpha\ *:Windows_NT*:*)
-	# How do we know it's Interix rather than the generic POSIX subsystem?
-	# Should we change UNAME_MACHINE based on the output of uname instead
-	# of the specific Alpha model?
-	echo alpha-pc-interix
-	exit ;;
-    21064:Windows_NT:50:3)
-	echo alpha-dec-winnt3.5
-	exit ;;
     Amiga*:UNIX_System_V:4.0:*)
 	echo m68k-unknown-sysv4
 	exit ;;
     *:[Aa]miga[Oo][Ss]:*:*)
-	echo ${UNAME_MACHINE}-unknown-amigaos
+	echo "$UNAME_MACHINE"-unknown-amigaos
 	exit ;;
     *:[Mm]orph[Oo][Ss]:*:*)
-	echo ${UNAME_MACHINE}-unknown-morphos
+	echo "$UNAME_MACHINE"-unknown-morphos
 	exit ;;
     *:OS/390:*:*)
 	echo i370-ibm-openedition
@@ -304,7 +350,7 @@ case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
 	echo powerpc-ibm-os400
 	exit ;;
     arm:RISC*:1.[012]*:*|arm:riscix:1.[012]*:*)
-	echo arm-acorn-riscix${UNAME_RELEASE}
+	echo arm-acorn-riscix"$UNAME_RELEASE"
 	exit ;;
     arm*:riscos:*:*|arm*:RISCOS:*:*)
 	echo arm-unknown-riscos
@@ -331,38 +377,38 @@ case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
 	    sparc) echo sparc-icl-nx7; exit ;;
 	esac ;;
     s390x:SunOS:*:*)
-	echo ${UNAME_MACHINE}-ibm-solaris2`echo ${UNAME_RELEASE}|sed -e 's/[^.]*//'`
+	echo "$UNAME_MACHINE"-ibm-solaris2"`echo "$UNAME_RELEASE" | sed -e 's/[^.]*//'`"
 	exit ;;
     sun4H:SunOS:5.*:*)
-	echo sparc-hal-solaris2`echo ${UNAME_RELEASE}|sed -e 's/[^.]*//'`
+	echo sparc-hal-solaris2"`echo "$UNAME_RELEASE"|sed -e 's/[^.]*//'`"
 	exit ;;
     sun4*:SunOS:5.*:* | tadpole*:SunOS:5.*:*)
-	echo sparc-sun-solaris2`echo ${UNAME_RELEASE}|sed -e 's/[^.]*//'`
+	echo sparc-sun-solaris2"`echo "$UNAME_RELEASE" | sed -e 's/[^.]*//'`"
 	exit ;;
     i86pc:AuroraUX:5.*:* | i86xen:AuroraUX:5.*:*)
-	echo i386-pc-auroraux${UNAME_RELEASE}
+	echo i386-pc-auroraux"$UNAME_RELEASE"
 	exit ;;
     i86pc:SunOS:5.*:* | i86xen:SunOS:5.*:*)
-	eval $set_cc_for_build
-	SUN_ARCH="i386"
+	eval "$set_cc_for_build"
+	SUN_ARCH=i386
 	# If there is a compiler, see if it is configured for 64-bit objects.
 	# Note that the Sun cc does not turn __LP64__ into 1 like gcc does.
 	# This test works for both compilers.
-	if [ "$CC_FOR_BUILD" != 'no_compiler_found' ]; then
+	if [ "$CC_FOR_BUILD" != no_compiler_found ]; then
 	    if (echo '#ifdef __amd64'; echo IS_64BIT_ARCH; echo '#endif') | \
-		(CCOPTS= $CC_FOR_BUILD -E - 2>/dev/null) | \
+		(CCOPTS="" $CC_FOR_BUILD -E - 2>/dev/null) | \
 		grep IS_64BIT_ARCH >/dev/null
 	    then
-		SUN_ARCH="x86_64"
+		SUN_ARCH=x86_64
 	    fi
 	fi
-	echo ${SUN_ARCH}-pc-solaris2`echo ${UNAME_RELEASE}|sed -e 's/[^.]*//'`
+	echo "$SUN_ARCH"-pc-solaris2"`echo "$UNAME_RELEASE"|sed -e 's/[^.]*//'`"
 	exit ;;
     sun4*:SunOS:6*:*)
 	# According to config.sub, this is the proper way to canonicalize
 	# SunOS6.  Hard to guess exactly what SunOS6 will be like, but
 	# it's likely to be more like Solaris than SunOS4.
-	echo sparc-sun-solaris3`echo ${UNAME_RELEASE}|sed -e 's/[^.]*//'`
+	echo sparc-sun-solaris3"`echo "$UNAME_RELEASE"|sed -e 's/[^.]*//'`"
 	exit ;;
     sun4*:SunOS:*:*)
 	case "`/usr/bin/arch -k`" in
@@ -371,25 +417,25 @@ case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
 		;;
 	esac
 	# Japanese Language versions have a version number like `4.1.3-JL'.
-	echo sparc-sun-sunos`echo ${UNAME_RELEASE}|sed -e 's/-/_/'`
+	echo sparc-sun-sunos"`echo "$UNAME_RELEASE"|sed -e 's/-/_/'`"
 	exit ;;
     sun3*:SunOS:*:*)
-	echo m68k-sun-sunos${UNAME_RELEASE}
+	echo m68k-sun-sunos"$UNAME_RELEASE"
 	exit ;;
     sun*:*:4.2BSD:*)
 	UNAME_RELEASE=`(sed 1q /etc/motd | awk '{print substr($5,1,3)}') 2>/dev/null`
-	test "x${UNAME_RELEASE}" = "x" && UNAME_RELEASE=3
+	test "x$UNAME_RELEASE" = x && UNAME_RELEASE=3
 	case "`/bin/arch`" in
 	    sun3)
-		echo m68k-sun-sunos${UNAME_RELEASE}
+		echo m68k-sun-sunos"$UNAME_RELEASE"
 		;;
 	    sun4)
-		echo sparc-sun-sunos${UNAME_RELEASE}
+		echo sparc-sun-sunos"$UNAME_RELEASE"
 		;;
 	esac
 	exit ;;
     aushp:SunOS:*:*)
-	echo sparc-auspex-sunos${UNAME_RELEASE}
+	echo sparc-auspex-sunos"$UNAME_RELEASE"
 	exit ;;
     # The situation for MiNT is a little confusing.  The machine name
     # can be virtually everything (everything which is not
@@ -400,44 +446,44 @@ case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
     # MiNT.  But MiNT is downward compatible to TOS, so this should
     # be no problem.
     atarist[e]:*MiNT:*:* | atarist[e]:*mint:*:* | atarist[e]:*TOS:*:*)
-	echo m68k-atari-mint${UNAME_RELEASE}
+	echo m68k-atari-mint"$UNAME_RELEASE"
 	exit ;;
     atari*:*MiNT:*:* | atari*:*mint:*:* | atarist[e]:*TOS:*:*)
-	echo m68k-atari-mint${UNAME_RELEASE}
+	echo m68k-atari-mint"$UNAME_RELEASE"
 	exit ;;
     *falcon*:*MiNT:*:* | *falcon*:*mint:*:* | *falcon*:*TOS:*:*)
-	echo m68k-atari-mint${UNAME_RELEASE}
+	echo m68k-atari-mint"$UNAME_RELEASE"
 	exit ;;
     milan*:*MiNT:*:* | milan*:*mint:*:* | *milan*:*TOS:*:*)
-	echo m68k-milan-mint${UNAME_RELEASE}
+	echo m68k-milan-mint"$UNAME_RELEASE"
 	exit ;;
     hades*:*MiNT:*:* | hades*:*mint:*:* | *hades*:*TOS:*:*)
-	echo m68k-hades-mint${UNAME_RELEASE}
+	echo m68k-hades-mint"$UNAME_RELEASE"
 	exit ;;
     *:*MiNT:*:* | *:*mint:*:* | *:*TOS:*:*)
-	echo m68k-unknown-mint${UNAME_RELEASE}
+	echo m68k-unknown-mint"$UNAME_RELEASE"
 	exit ;;
     m68k:machten:*:*)
-	echo m68k-apple-machten${UNAME_RELEASE}
+	echo m68k-apple-machten"$UNAME_RELEASE"
 	exit ;;
     powerpc:machten:*:*)
-	echo powerpc-apple-machten${UNAME_RELEASE}
+	echo powerpc-apple-machten"$UNAME_RELEASE"
 	exit ;;
     RISC*:Mach:*:*)
 	echo mips-dec-mach_bsd4.3
 	exit ;;
     RISC*:ULTRIX:*:*)
-	echo mips-dec-ultrix${UNAME_RELEASE}
+	echo mips-dec-ultrix"$UNAME_RELEASE"
 	exit ;;
     VAX*:ULTRIX*:*:*)
-	echo vax-dec-ultrix${UNAME_RELEASE}
+	echo vax-dec-ultrix"$UNAME_RELEASE"
 	exit ;;
     2020:CLIX:*:* | 2430:CLIX:*:*)
-	echo clipper-intergraph-clix${UNAME_RELEASE}
+	echo clipper-intergraph-clix"$UNAME_RELEASE"
 	exit ;;
     mips:*:*:UMIPS | mips:*:*:RISCos)
-	eval $set_cc_for_build
-	sed 's/^	//' << EOF >$dummy.c
+	eval "$set_cc_for_build"
+	sed 's/^	//' << EOF > "$dummy.c"
 #ifdef __cplusplus
 #include <stdio.h>  /* for printf() prototype */
 	int main (int argc, char *argv[]) {
@@ -446,23 +492,23 @@ case "${UNAME_MACHINE}:${UNAME_SYSTEM}:${UNAME_RELEASE}:${UNAME_VERSION}" in
 #endif
 	#if defined (host_mips) && defined (MIPSEB)
 	#if defined (SYSTYPE_SYSV)
-	  printf ("mips-mips-riscos%ssysv\n", argv[1]); exit (0);
+	  printf ("mips-mips-riscos%ssysv\\n", argv[1]); exit (0);
 	#endif
 	#if defined (SYSTYPE_SVR4)
-	  printf ("mips-mips-riscos%ssvr4\n", argv[1]); exit (0);
+	  printf ("mips-mips-riscos%ssvr4\\n", argv[1]); exit (0);
 	#endif
 	#if defined (SYSTYPE_BSD43) || defined(SYSTYPE_BSD)
-	  printf ("mips-mips-riscos%sbsd\n", argv[1]); exit (0);
+	  printf ("mips-mips-riscos%sbsd\\n", argv[1]); exit (0);
 	#endif
 	#endif
 	  exit (-1);
 	}
 EOF
-	$CC_FOR_BUILD -o $dummy $dummy.c &&
-	  dummyarg=`echo "${UNAME_RELEASE}" | sed -n 's/\([0-9]*\).*/\1/p'` &&
-	  SYSTEM_NAME=`$dummy $dummyarg` &&
+	$CC_FOR_BUILD -o "$dummy" "$dummy.c" &&
+	  dummyarg=`echo "$UNAME_RELEASE" | sed -n 's/\([0-9]*\).*/\1/p'` &&
+	  SYSTEM_NAME=`"$dummy" "$dummyarg"` &&
 	    { echo "$SYSTEM_NAME"; exit; }
-	echo mips-mips-riscos${UNAME_RELEASE}
+	echo mips-mips-riscos"$UNAME_RELEASE"
 	exit ;;
     Motorola:PowerMAX_OS:*:*)
 	echo powerpc-motorola-powermax
@@ -488,17 +534,17 @@ EOF
     AViiON:dgux:*:*)
 	# DG/UX returns AViiON for all architectures
 	UNAME_PROCESSOR=`/usr/bin/uname -p`
-	if [ $UNAME_PROCESSOR = mc88100 ] || [ $UNAME_PROCESSOR = mc88110 ]
+	if [ "$UNAME_PROCESSOR" = mc88100 ] || [ "$UNAME_PROCESSOR" = mc88110 ]
 	then
-	    if [ ${TARGET_BINARY_INTERFACE}x = m88kdguxelfx ] || \
-	       [ ${TARGET_BINARY_INTERFACE}x = x ]
+	    if [ "$TARGET_BINARY_INTERFACE"x = m88kdguxelfx ] || \
+	       [ "$TARGET_BINARY_INTERFACE"x = x ]
 	    then
-		echo m88k-dg-dgux${UNAME_RELEASE}
+		echo m88k-dg-dgux"$UNAME_RELEASE"
 	    else
-		echo m88k-dg-dguxbcs${UNAME_RELEASE}
+		echo m88k-dg-dguxbcs"$UNAME_RELEASE"
 	    fi
 	else
-	    echo i586-dg-dgux${UNAME_RELEASE}
+	    echo i586-dg-dgux"$UNAME_RELEASE"
 	fi
 	exit ;;
     M88*:DolphinOS:*:*)	# DolphinOS (SVR3)
@@ -515,7 +561,7 @@ EOF
 	echo m68k-tektronix-bsd
 	exit ;;
     *:IRIX*:*:*)
-	echo mips-sgi-irix`echo ${UNAME_RELEASE}|sed -e 's/-/_/g'`
+	echo mips-sgi-irix"`echo "$UNAME_RELEASE"|sed -e 's/-/_/g'`"
 	exit ;;
     ????????:AIX?:[12].1:2)   # AIX 2.2.1 or AIX 2.1.1 is RT/PC AIX.
 	echo romp-ibm-aix     # uname -m gives an 8 hex-code CPU id
@@ -527,14 +573,14 @@ EOF
 	if [ -x /usr/bin/oslevel ] ; then
 		IBM_REV=`/usr/bin/oslevel`
 	else
-		IBM_REV=${UNAME_VERSION}.${UNAME_RELEASE}
+		IBM_REV="$UNAME_VERSION.$UNAME_RELEASE"
 	fi
-	echo ${UNAME_MACHINE}-ibm-aix${IBM_REV}
+	echo "$UNAME_MACHINE"-ibm-aix"$IBM_REV"
 	exit ;;
     *:AIX:2:3)
 	if grep bos325 /usr/include/stdio.h >/dev/null 2>&1; then
-		eval $set_cc_for_build
-		sed 's/^		//' << EOF >$dummy.c
+		eval "$set_cc_for_build"
+		sed 's/^		//' << EOF > "$dummy.c"
 		#include <sys/systemcfg.h>
 
 		main()
@@ -545,7 +591,7 @@ EOF
 			exit(0);
 			}
 EOF
-		if $CC_FOR_BUILD -o $dummy $dummy.c && SYSTEM_NAME=`$dummy`
+		if $CC_FOR_BUILD -o "$dummy" "$dummy.c" && SYSTEM_NAME=`"$dummy"`
 		then
 			echo "$SYSTEM_NAME"
 		else
@@ -559,26 +605,27 @@ EOF
 	exit ;;
     *:AIX:*:[4567])
 	IBM_CPU_ID=`/usr/sbin/lsdev -C -c processor -S available | sed 1q | awk '{ print $1 }'`
-	if /usr/sbin/lsattr -El ${IBM_CPU_ID} | grep ' POWER' >/dev/null 2>&1; then
+	if /usr/sbin/lsattr -El "$IBM_CPU_ID" | grep ' POWER' >/dev/null 2>&1; then
 		IBM_ARCH=rs6000
 	else
 		IBM_ARCH=powerpc
 	fi
-	if [ -x /usr/bin/oslevel ] ; then
-		IBM_REV=`/usr/bin/oslevel`
+	if [ -x /usr/bin/lslpp ] ; then
+		IBM_REV=`/usr/bin/lslpp -Lqc bos.rte.libc |
+			   awk -F: '{ print $3 }' | sed s/[0-9]*$/0/`
 	else
-		IBM_REV=${UNAME_VERSION}.${UNAME_RELEASE}
+		IBM_REV="$UNAME_VERSION.$UNAME_RELEASE"
 	fi
-	echo ${IBM_ARCH}-ibm-aix${IBM_REV}
+	echo "$IBM_ARCH"-ibm-aix"$IBM_REV"
 	exit ;;
     *:AIX:*:*)
 	echo rs6000-ibm-aix
 	exit ;;
-    ibmrt:4.4BSD:*|romp-ibm:BSD:*)
+    ibmrt:4.4BSD:*|romp-ibm:4.4BSD:*)
 	echo romp-ibm-bsd4.4
 	exit ;;
     ibmrt:*BSD:*|romp-ibm:BSD:*)            # covers RT/PC BSD and
-	echo romp-ibm-bsd${UNAME_RELEASE}   # 4.3 with uname added to
+	echo romp-ibm-bsd"$UNAME_RELEASE"   # 4.3 with uname added to
 	exit ;;                             # report: romp-ibm BSD 4.3
     *:BOSX:*:*)
 	echo rs6000-bull-bosx
@@ -593,28 +640,28 @@ EOF
 	echo m68k-hp-bsd4.4
 	exit ;;
     9000/[34678]??:HP-UX:*:*)
-	HPUX_REV=`echo ${UNAME_RELEASE}|sed -e 's/[^.]*.[0B]*//'`
-	case "${UNAME_MACHINE}" in
-	    9000/31? )            HP_ARCH=m68000 ;;
-	    9000/[34]?? )         HP_ARCH=m68k ;;
+	HPUX_REV=`echo "$UNAME_RELEASE"|sed -e 's/[^.]*.[0B]*//'`
+	case "$UNAME_MACHINE" in
+	    9000/31?)            HP_ARCH=m68000 ;;
+	    9000/[34]??)         HP_ARCH=m68k ;;
 	    9000/[678][0-9][0-9])
 		if [ -x /usr/bin/getconf ]; then
 		    sc_cpu_version=`/usr/bin/getconf SC_CPU_VERSION 2>/dev/null`
 		    sc_kernel_bits=`/usr/bin/getconf SC_KERNEL_BITS 2>/dev/null`
-		    case "${sc_cpu_version}" in
-		      523) HP_ARCH="hppa1.0" ;; # CPU_PA_RISC1_0
-		      528) HP_ARCH="hppa1.1" ;; # CPU_PA_RISC1_1
+		    case "$sc_cpu_version" in
+		      523) HP_ARCH=hppa1.0 ;; # CPU_PA_RISC1_0
+		      528) HP_ARCH=hppa1.1 ;; # CPU_PA_RISC1_1
 		      532)                      # CPU_PA_RISC2_0
-			case "${sc_kernel_bits}" in
-			  32) HP_ARCH="hppa2.0n" ;;
-			  64) HP_ARCH="hppa2.0w" ;;
-			  '') HP_ARCH="hppa2.0" ;;   # HP-UX 10.20
+			case "$sc_kernel_bits" in
+			  32) HP_ARCH=hppa2.0n ;;
+			  64) HP_ARCH=hppa2.0w ;;
+			  '') HP_ARCH=hppa2.0 ;;   # HP-UX 10.20
 			esac ;;
 		    esac
 		fi
-		if [ "${HP_ARCH}" = "" ]; then
-		    eval $set_cc_for_build
-		    sed 's/^		//' << EOF >$dummy.c
+		if [ "$HP_ARCH" = "" ]; then
+		    eval "$set_cc_for_build"
+		    sed 's/^		//' << EOF > "$dummy.c"
 
 		#define _HPUX_SOURCE
 		#include <stdlib.h>
@@ -647,13 +694,13 @@ EOF
 		    exit (0);
 		}
 EOF
-		    (CCOPTS= $CC_FOR_BUILD -o $dummy $dummy.c 2>/dev/null) && HP_ARCH=`$dummy`
+		    (CCOPTS="" $CC_FOR_BUILD -o "$dummy" "$dummy.c" 2>/dev/null) && HP_ARCH=`"$dummy"`
 		    test -z "$HP_ARCH" && HP_ARCH=hppa
 		fi ;;
 	esac
-	if [ ${HP_ARCH} = "hppa2.0w" ]
+	if [ "$HP_ARCH" = hppa2.0w ]
 	then
-	    eval $set_cc_for_build
+	    eval "$set_cc_for_build"
 
 	    # hppa2.0w-hp-hpux* has a 64-bit kernel and a compiler generating
 	    # 32-bit code.  hppa64-hp-hpux* has the same kernel and a compiler
@@ -664,23 +711,23 @@ EOF
 	    # $ CC_FOR_BUILD="cc +DA2.0w" ./config.guess
 	    # => hppa64-hp-hpux11.23
 
-	    if echo __LP64__ | (CCOPTS= $CC_FOR_BUILD -E - 2>/dev/null) |
+	    if echo __LP64__ | (CCOPTS="" $CC_FOR_BUILD -E - 2>/dev/null) |
 		grep -q __LP64__
 	    then
-		HP_ARCH="hppa2.0w"
+		HP_ARCH=hppa2.0w
 	    else
-		HP_ARCH="hppa64"
+		HP_ARCH=hppa64
 	    fi
 	fi
-	echo ${HP_ARCH}-hp-hpux${HPUX_REV}
+	echo "$HP_ARCH"-hp-hpux"$HPUX_REV"
 	exit ;;
     ia64:HP-UX:*:*)
-	HPUX_REV=`echo ${UNAME_RELEASE}|sed -e 's/[^.]*.[0B]*//'`
-	echo ia64-hp-hpux${HPUX_REV}
+	HPUX_REV=`echo "$UNAME_RELEASE"|sed -e 's/[^.]*.[0B]*//'`
+	echo ia64-hp-hpux"$HPUX_REV"
 	exit ;;
     3050*:HI-UX:*:*)
-	eval $set_cc_for_build
-	sed 's/^	//' << EOF >$dummy.c
+	eval "$set_cc_for_build"
+	sed 's/^	//' << EOF > "$dummy.c"
 	#include <unistd.h>
 	int
 	main ()
@@ -705,11 +752,11 @@ EOF
 	  exit (0);
 	}
 EOF
-	$CC_FOR_BUILD -o $dummy $dummy.c && SYSTEM_NAME=`$dummy` &&
+	$CC_FOR_BUILD -o "$dummy" "$dummy.c" && SYSTEM_NAME=`"$dummy"` &&
 		{ echo "$SYSTEM_NAME"; exit; }
 	echo unknown-hitachi-hiuxwe2
 	exit ;;
-    9000/7??:4.3bsd:*:* | 9000/8?[79]:4.3bsd:*:* )
+    9000/7??:4.3bsd:*:* | 9000/8?[79]:4.3bsd:*:*)
 	echo hppa1.1-hp-bsd
 	exit ;;
     9000/8??:4.3bsd:*:*)
@@ -718,7 +765,7 @@ EOF
     *9??*:MPE/iX:*:* | *3000*:MPE/iX:*:*)
 	echo hppa1.0-hp-mpeix
 	exit ;;
-    hp7??:OSF1:*:* | hp8?[79]:OSF1:*:* )
+    hp7??:OSF1:*:* | hp8?[79]:OSF1:*:*)
 	echo hppa1.1-hp-osf
 	exit ;;
     hp8??:OSF1:*:*)
@@ -726,9 +773,9 @@ EOF
 	exit ;;
     i*86:OSF1:*:*)
 	if [ -x /usr/sbin/sysversion ] ; then
-	    echo ${UNAME_MACHINE}-unknown-osf1mk
+	    echo "$UNAME_MACHINE"-unknown-osf1mk
 	else
-	    echo ${UNAME_MACHINE}-unknown-osf1
+	    echo "$UNAME_MACHINE"-unknown-osf1
 	fi
 	exit ;;
     parisc*:Lites*:*:*)
@@ -753,127 +800,109 @@ EOF
 	echo c4-convex-bsd
 	exit ;;
     CRAY*Y-MP:*:*:*)
-	echo ymp-cray-unicos${UNAME_RELEASE} | sed -e 's/\.[^.]*$/.X/'
+	echo ymp-cray-unicos"$UNAME_RELEASE" | sed -e 's/\.[^.]*$/.X/'
 	exit ;;
     CRAY*[A-Z]90:*:*:*)
-	echo ${UNAME_MACHINE}-cray-unicos${UNAME_RELEASE} \
+	echo "$UNAME_MACHINE"-cray-unicos"$UNAME_RELEASE" \
 	| sed -e 's/CRAY.*\([A-Z]90\)/\1/' \
 	      -e y/ABCDEFGHIJKLMNOPQRSTUVWXYZ/abcdefghijklmnopqrstuvwxyz/ \
 	      -e 's/\.[^.]*$/.X/'
 	exit ;;
     CRAY*TS:*:*:*)
-	echo t90-cray-unicos${UNAME_RELEASE} | sed -e 's/\.[^.]*$/.X/'
+	echo t90-cray-unicos"$UNAME_RELEASE" | sed -e 's/\.[^.]*$/.X/'
 	exit ;;
     CRAY*T3E:*:*:*)
-	echo alphaev5-cray-unicosmk${UNAME_RELEASE} | sed -e 's/\.[^.]*$/.X/'
+	echo alphaev5-cray-unicosmk"$UNAME_RELEASE" | sed -e 's/\.[^.]*$/.X/'
 	exit ;;
     CRAY*SV1:*:*:*)
-	echo sv1-cray-unicos${UNAME_RELEASE} | sed -e 's/\.[^.]*$/.X/'
+	echo sv1-cray-unicos"$UNAME_RELEASE" | sed -e 's/\.[^.]*$/.X/'
 	exit ;;
     *:UNICOS/mp:*:*)
-	echo craynv-cray-unicosmp${UNAME_RELEASE} | sed -e 's/\.[^.]*$/.X/'
+	echo craynv-cray-unicosmp"$UNAME_RELEASE" | sed -e 's/\.[^.]*$/.X/'
 	exit ;;
     F30[01]:UNIX_System_V:*:* | F700:UNIX_System_V:*:*)
-	FUJITSU_PROC=`uname -m | tr 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' 'abcdefghijklmnopqrstuvwxyz'`
-	FUJITSU_SYS=`uname -p | tr 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' 'abcdefghijklmnopqrstuvwxyz' | sed -e 's/\///'`
-	FUJITSU_REL=`echo ${UNAME_RELEASE} | sed -e 's/ /_/'`
+	FUJITSU_PROC=`uname -m | tr ABCDEFGHIJKLMNOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz`
+	FUJITSU_SYS=`uname -p | tr ABCDEFGHIJKLMNOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz | sed -e 's/\///'`
+	FUJITSU_REL=`echo "$UNAME_RELEASE" | sed -e 's/ /_/'`
 	echo "${FUJITSU_PROC}-fujitsu-${FUJITSU_SYS}${FUJITSU_REL}"
 	exit ;;
     5000:UNIX_System_V:4.*:*)
-	FUJITSU_SYS=`uname -p | tr 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' 'abcdefghijklmnopqrstuvwxyz' | sed -e 's/\///'`
-	FUJITSU_REL=`echo ${UNAME_RELEASE} | tr 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' 'abcdefghijklmnopqrstuvwxyz' | sed -e 's/ /_/'`
+	FUJITSU_SYS=`uname -p | tr ABCDEFGHIJKLMNOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz | sed -e 's/\///'`
+	FUJITSU_REL=`echo "$UNAME_RELEASE" | tr ABCDEFGHIJKLMNOPQRSTUVWXYZ abcdefghijklmnopqrstuvwxyz | sed -e 's/ /_/'`
 	echo "sparc-fujitsu-${FUJITSU_SYS}${FUJITSU_REL}"
 	exit ;;
     i*86:BSD/386:*:* | i*86:BSD/OS:*:* | *:Ascend\ Embedded/OS:*:*)
-	echo ${UNAME_MACHINE}-pc-bsdi${UNAME_RELEASE}
+	echo "$UNAME_MACHINE"-pc-bsdi"$UNAME_RELEASE"
 	exit ;;
     sparc*:BSD/OS:*:*)
-	echo sparc-unknown-bsdi${UNAME_RELEASE}
+	echo sparc-unknown-bsdi"$UNAME_RELEASE"
 	exit ;;
     *:BSD/OS:*:*)
-	echo ${UNAME_MACHINE}-unknown-bsdi${UNAME_RELEASE}
+	echo "$UNAME_MACHINE"-unknown-bsdi"$UNAME_RELEASE"
 	exit ;;
     *:FreeBSD:*:*)
 	UNAME_PROCESSOR=`/usr/bin/uname -p`
-	case ${UNAME_PROCESSOR} in
+	case "$UNAME_PROCESSOR" in
 	    amd64)
-		echo x86_64-unknown-freebsd`echo ${UNAME_RELEASE}|sed -e 's/[-(].*//'` ;;
-	    *)
-		echo ${UNAME_PROCESSOR}-unknown-freebsd`echo ${UNAME_RELEASE}|sed -e 's/[-(].*//'` ;;
+		UNAME_PROCESSOR=x86_64 ;;
+	    i386)
+		UNAME_PROCESSOR=i586 ;;
 	esac
+	echo "$UNAME_PROCESSOR"-unknown-freebsd"`echo "$UNAME_RELEASE"|sed -e 's/[-(].*//'`"
 	exit ;;
     i*:CYGWIN*:*)
-	echo ${UNAME_MACHINE}-pc-cygwin
+	echo "$UNAME_MACHINE"-pc-cygwin
 	exit ;;
     *:MINGW64*:*)
-	echo ${UNAME_MACHINE}-pc-mingw64
+	echo "$UNAME_MACHINE"-pc-mingw64
 	exit ;;
     *:MINGW*:*)
-	echo ${UNAME_MACHINE}-pc-mingw32
+	echo "$UNAME_MACHINE"-pc-mingw32
 	exit ;;
-    i*:MSYS*:*)
-	echo ${UNAME_MACHINE}-pc-msys
-	exit ;;
-    i*:windows32*:*)
-	# uname -m includes "-pc" on this system.
-	echo ${UNAME_MACHINE}-mingw32
+    *:MSYS*:*)
+	echo "$UNAME_MACHINE"-pc-msys
 	exit ;;
     i*:PW*:*)
-	echo ${UNAME_MACHINE}-pc-pw32
+	echo "$UNAME_MACHINE"-pc-pw32
 	exit ;;
     *:Interix*:*)
-	case ${UNAME_MACHINE} in
+	case "$UNAME_MACHINE" in
 	    x86)
-		echo i586-pc-interix${UNAME_RELEASE}
+		echo i586-pc-interix"$UNAME_RELEASE"
 		exit ;;
 	    authenticamd | genuineintel | EM64T)
-		echo x86_64-unknown-interix${UNAME_RELEASE}
+		echo x86_64-unknown-interix"$UNAME_RELEASE"
 		exit ;;
 	    IA64)
-		echo ia64-unknown-interix${UNAME_RELEASE}
+		echo ia64-unknown-interix"$UNAME_RELEASE"
 		exit ;;
 	esac ;;
-    [345]86:Windows_95:* | [345]86:Windows_98:* | [345]86:Windows_NT:*)
-	echo i${UNAME_MACHINE}-pc-mks
-	exit ;;
-    8664:Windows_NT:*)
-	echo x86_64-pc-mks
-	exit ;;
-    i*:Windows_NT*:* | Pentium*:Windows_NT*:*)
-	# How do we know it's Interix rather than the generic POSIX subsystem?
-	# It also conflicts with pre-2.0 versions of AT&T UWIN. Should we
-	# UNAME_MACHINE based on the output of uname instead of i386?
-	echo i586-pc-interix
-	exit ;;
     i*:UWIN*:*)
-	echo ${UNAME_MACHINE}-pc-uwin
+	echo "$UNAME_MACHINE"-pc-uwin
 	exit ;;
     amd64:CYGWIN*:*:* | x86_64:CYGWIN*:*:*)
 	echo x86_64-unknown-cygwin
 	exit ;;
-    p*:CYGWIN*:*)
-	echo powerpcle-unknown-cygwin
-	exit ;;
     prep*:SunOS:5.*:*)
-	echo powerpcle-unknown-solaris2`echo ${UNAME_RELEASE}|sed -e 's/[^.]*//'`
+	echo powerpcle-unknown-solaris2"`echo "$UNAME_RELEASE"|sed -e 's/[^.]*//'`"
 	exit ;;
     *:GNU:*:*)
 	# the GNU system
-	echo `echo ${UNAME_MACHINE}|sed -e 's,[-/].*$,,'`-unknown-gnu`echo ${UNAME_RELEASE}|sed -e 's,/.*$,,'`
+	echo "`echo "$UNAME_MACHINE"|sed -e 's,[-/].*$,,'`-unknown-$LIBC`echo "$UNAME_RELEASE"|sed -e 's,/.*$,,'`"
 	exit ;;
     *:GNU/*:*:*)
 	# other systems with GNU libc and userland
-	echo ${UNAME_MACHINE}-unknown-`echo ${UNAME_SYSTEM} | sed 's,^[^/]*/,,' | tr '[A-Z]' '[a-z]'``echo ${UNAME_RELEASE}|sed -e 's/[-(].*//'`-gnu
+	echo "$UNAME_MACHINE-unknown-`echo "$UNAME_SYSTEM" | sed 's,^[^/]*/,,' | tr "[:upper:]" "[:lower:]"``echo "$UNAME_RELEASE"|sed -e 's/[-(].*//'`-$LIBC"
 	exit ;;
     i*86:Minix:*:*)
-	echo ${UNAME_MACHINE}-pc-minix
+	echo "$UNAME_MACHINE"-pc-minix
 	exit ;;
     aarch64:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     aarch64_be:Linux:*:*)
 	UNAME_MACHINE=aarch64_be
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     alpha:Linux:*:*)
 	case `sed -n '/^cpu model/s/^.*: \(.*\)/\1/p' < /proc/cpuinfo` in
@@ -886,63 +915,67 @@ EOF
 	  EV68*) UNAME_MACHINE=alphaev68 ;;
 	esac
 	objdump --private-headers /bin/sh | grep -q ld.so.1
-	if test "$?" = 0 ; then LIBC="libc1" ; else LIBC="" ; fi
-	echo ${UNAME_MACHINE}-unknown-linux-gnu${LIBC}
+	if test "$?" = 0 ; then LIBC=gnulibc1 ; fi
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
+	exit ;;
+    arc:Linux:*:* | arceb:Linux:*:*)
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     arm*:Linux:*:*)
-	eval $set_cc_for_build
+	eval "$set_cc_for_build"
 	if echo __ARM_EABI__ | $CC_FOR_BUILD -E - 2>/dev/null \
 	    | grep -q __ARM_EABI__
 	then
-	    echo ${UNAME_MACHINE}-unknown-linux-gnu
+	    echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	else
 	    if echo __ARM_PCS_VFP | $CC_FOR_BUILD -E - 2>/dev/null \
 		| grep -q __ARM_PCS_VFP
 	    then
-		echo ${UNAME_MACHINE}-unknown-linux-gnueabi
+		echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"eabi
 	    else
-		echo ${UNAME_MACHINE}-unknown-linux-gnueabihf
+		echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"eabihf
 	    fi
 	fi
 	exit ;;
     avr32*:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     cris:Linux:*:*)
-	echo ${UNAME_MACHINE}-axis-linux-gnu
+	echo "$UNAME_MACHINE"-axis-linux-"$LIBC"
 	exit ;;
     crisv32:Linux:*:*)
-	echo ${UNAME_MACHINE}-axis-linux-gnu
+	echo "$UNAME_MACHINE"-axis-linux-"$LIBC"
+	exit ;;
+    e2k:Linux:*:*)
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     frv:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     hexagon:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     i*86:Linux:*:*)
-	LIBC=gnu
-	eval $set_cc_for_build
-	sed 's/^	//' << EOF >$dummy.c
-	#ifdef __dietlibc__
-	LIBC=dietlibc
-	#endif
-EOF
-	eval `$CC_FOR_BUILD -E $dummy.c 2>/dev/null | grep '^LIBC'`
-	echo "${UNAME_MACHINE}-pc-linux-${LIBC}"
+	echo "$UNAME_MACHINE"-pc-linux-"$LIBC"
 	exit ;;
     ia64:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
+	exit ;;
+    k1om:Linux:*:*)
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
+	exit ;;
+    loongarch32:Linux:*:* | loongarch64:Linux:*:*)
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     m32r*:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     m68*:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     mips:Linux:*:* | mips64:Linux:*:*)
-	eval $set_cc_for_build
-	sed 's/^	//' << EOF >$dummy.c
+	eval "$set_cc_for_build"
+	sed 's/^	//' << EOF > "$dummy.c"
 	#undef CPU
 	#undef ${UNAME_MACHINE}
 	#undef ${UNAME_MACHINE}el
@@ -956,58 +989,74 @@ EOF
 	#endif
 	#endif
 EOF
-	eval `$CC_FOR_BUILD -E $dummy.c 2>/dev/null | grep '^CPU'`
-	test x"${CPU}" != x && { echo "${CPU}-unknown-linux-gnu"; exit; }
+	eval "`$CC_FOR_BUILD -E "$dummy.c" 2>/dev/null | grep '^CPU'`"
+	test "x$CPU" != x && { echo "$CPU-unknown-linux-$LIBC"; exit; }
 	;;
-    or32:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+    mips64el:Linux:*:*)
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
+	exit ;;
+    openrisc*:Linux:*:*)
+	echo or1k-unknown-linux-"$LIBC"
+	exit ;;
+    or32:Linux:*:* | or1k*:Linux:*:*)
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     padre:Linux:*:*)
-	echo sparc-unknown-linux-gnu
+	echo sparc-unknown-linux-"$LIBC"
 	exit ;;
     parisc64:Linux:*:* | hppa64:Linux:*:*)
-	echo hppa64-unknown-linux-gnu
+	echo hppa64-unknown-linux-"$LIBC"
 	exit ;;
     parisc:Linux:*:* | hppa:Linux:*:*)
 	# Look for CPU level
 	case `grep '^cpu[^a-z]*:' /proc/cpuinfo 2>/dev/null | cut -d' ' -f2` in
-	  PA7*) echo hppa1.1-unknown-linux-gnu ;;
-	  PA8*) echo hppa2.0-unknown-linux-gnu ;;
-	  *)    echo hppa-unknown-linux-gnu ;;
+	  PA7*) echo hppa1.1-unknown-linux-"$LIBC" ;;
+	  PA8*) echo hppa2.0-unknown-linux-"$LIBC" ;;
+	  *)    echo hppa-unknown-linux-"$LIBC" ;;
 	esac
 	exit ;;
     ppc64:Linux:*:*)
-	echo powerpc64-unknown-linux-gnu
+	echo powerpc64-unknown-linux-"$LIBC"
+	exit ;;
+    ppc:Linux:*:*)
+	echo powerpc-unknown-linux-"$LIBC"
 	exit ;;
     ppc64le:Linux:*:*)
-	echo powerpc64le-unknown-linux-gnu
+	echo powerpc64le-unknown-linux-"$LIBC"
 	exit ;;
-    ppc:Linux:*:*)
-	echo powerpc-unknown-linux-gnu
+    ppcle:Linux:*:*)
+	echo powerpcle-unknown-linux-"$LIBC"
+	exit ;;
+    riscv32:Linux:*:* | riscv64:Linux:*:*)
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     s390:Linux:*:* | s390x:Linux:*:*)
-	echo ${UNAME_MACHINE}-ibm-linux
+	echo "$UNAME_MACHINE"-ibm-linux-"$LIBC"
 	exit ;;
     sh64*:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     sh*:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     sparc:Linux:*:* | sparc64:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     tile*:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     vax:Linux:*:*)
-	echo ${UNAME_MACHINE}-dec-linux-gnu
+	echo "$UNAME_MACHINE"-dec-linux-"$LIBC"
 	exit ;;
     x86_64:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	if objdump -f /bin/sh | grep -q elf32-x86-64; then
+	    echo "$UNAME_MACHINE"-pc-linux-"$LIBC"x32
+	else
+	    echo "$UNAME_MACHINE"-pc-linux-"$LIBC"
+	fi
 	exit ;;
     xtensa*:Linux:*:*)
-	echo ${UNAME_MACHINE}-unknown-linux-gnu
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
     i*86:DYNIX/ptx:4*:*)
 	# ptx 4.0 does uname -s correctly, with DYNIX/ptx in there.
@@ -1021,34 +1070,34 @@ EOF
 	# I am not positive that other SVR4 systems won't match this,
 	# I just have to hope.  -- rms.
 	# Use sysv4.2uw... so that sysv4* matches it.
-	echo ${UNAME_MACHINE}-pc-sysv4.2uw${UNAME_VERSION}
+	echo "$UNAME_MACHINE"-pc-sysv4.2uw"$UNAME_VERSION"
 	exit ;;
     i*86:OS/2:*:*)
 	# If we were able to find `uname', then EMX Unix compatibility
 	# is probably installed.
-	echo ${UNAME_MACHINE}-pc-os2-emx
+	echo "$UNAME_MACHINE"-pc-os2-emx
 	exit ;;
     i*86:XTS-300:*:STOP)
-	echo ${UNAME_MACHINE}-unknown-stop
+	echo "$UNAME_MACHINE"-unknown-stop
 	exit ;;
     i*86:atheos:*:*)
-	echo ${UNAME_MACHINE}-unknown-atheos
+	echo "$UNAME_MACHINE"-unknown-atheos
 	exit ;;
     i*86:syllable:*:*)
-	echo ${UNAME_MACHINE}-pc-syllable
+	echo "$UNAME_MACHINE"-pc-syllable
 	exit ;;
     i*86:LynxOS:2.*:* | i*86:LynxOS:3.[01]*:* | i*86:LynxOS:4.[02]*:*)
-	echo i386-unknown-lynxos${UNAME_RELEASE}
+	echo i386-unknown-lynxos"$UNAME_RELEASE"
 	exit ;;
     i*86:*DOS:*:*)
-	echo ${UNAME_MACHINE}-pc-msdosdjgpp
+	echo "$UNAME_MACHINE"-pc-msdosdjgpp
 	exit ;;
-    i*86:*:4.*:* | i*86:SYSTEM_V:4.*:*)
-	UNAME_REL=`echo ${UNAME_RELEASE} | sed 's/\/MP$//'`
+    i*86:*:4.*:*)
+	UNAME_REL=`echo "$UNAME_RELEASE" | sed 's/\/MP$//'`
 	if grep Novell /usr/include/link.h >/dev/null 2>/dev/null; then
-		echo ${UNAME_MACHINE}-univel-sysv${UNAME_REL}
+		echo "$UNAME_MACHINE"-univel-sysv"$UNAME_REL"
 	else
-		echo ${UNAME_MACHINE}-pc-sysv${UNAME_REL}
+		echo "$UNAME_MACHINE"-pc-sysv"$UNAME_REL"
 	fi
 	exit ;;
     i*86:*:5:[678]*)
@@ -1058,12 +1107,12 @@ EOF
 	    *Pentium)	     UNAME_MACHINE=i586 ;;
 	    *Pent*|*Celeron) UNAME_MACHINE=i686 ;;
 	esac
-	echo ${UNAME_MACHINE}-unknown-sysv${UNAME_RELEASE}${UNAME_SYSTEM}${UNAME_VERSION}
+	echo "$UNAME_MACHINE-unknown-sysv${UNAME_RELEASE}${UNAME_SYSTEM}{$UNAME_VERSION}"
 	exit ;;
     i*86:*:3.2:*)
 	if test -f /usr/options/cb.name; then
 		UNAME_REL=`sed -n 's/.*Version //p' </usr/options/cb.name`
-		echo ${UNAME_MACHINE}-pc-isc$UNAME_REL
+		echo "$UNAME_MACHINE"-pc-isc"$UNAME_REL"
 	elif /bin/uname -X 2>/dev/null >/dev/null ; then
 		UNAME_REL=`(/bin/uname -X|grep Release|sed -e 's/.*= //')`
 		(/bin/uname -X|grep i80486 >/dev/null) && UNAME_MACHINE=i486
@@ -1073,9 +1122,9 @@ EOF
 			&& UNAME_MACHINE=i686
 		(/bin/uname -X|grep '^Machine.*Pentium Pro' >/dev/null) \
 			&& UNAME_MACHINE=i686
-		echo ${UNAME_MACHINE}-pc-sco$UNAME_REL
+		echo "$UNAME_MACHINE"-pc-sco"$UNAME_REL"
 	else
-		echo ${UNAME_MACHINE}-pc-sysv32
+		echo "$UNAME_MACHINE"-pc-sysv32
 	fi
 	exit ;;
     pc:*:*:*)
@@ -1083,7 +1132,7 @@ EOF
 	# uname -m prints for DJGPP always 'pc', but it prints nothing about
 	# the processor, so we play safe by assuming i586.
 	# Note: whatever this is, it MUST be the same as what config.sub
-	# prints for the "djgpp" host, or else GDB configury will decide that
+	# prints for the "djgpp" host, or else GDB configure will decide that
 	# this is a cross-build.
 	echo i586-pc-msdosdjgpp
 	exit ;;
@@ -1095,9 +1144,9 @@ EOF
 	exit ;;
     i860:*:4.*:*) # i860-SVR4
 	if grep Stardent /usr/include/sys/uadmin.h >/dev/null 2>&1 ; then
-	  echo i860-stardent-sysv${UNAME_RELEASE} # Stardent Vistra i860-SVR4
+	  echo i860-stardent-sysv"$UNAME_RELEASE" # Stardent Vistra i860-SVR4
 	else # Add other i860-SVR4 vendors below as they are discovered.
-	  echo i860-unknown-sysv${UNAME_RELEASE}  # Unknown i860-SVR4
+	  echo i860-unknown-sysv"$UNAME_RELEASE"  # Unknown i860-SVR4
 	fi
 	exit ;;
     mini*:CTIX:SYS*5:*)
@@ -1117,9 +1166,9 @@ EOF
 	test -r /etc/.relid \
 	&& OS_REL=.`sed -n 's/[^ ]* [^ ]* \([0-9][0-9]\).*/\1/p' < /etc/.relid`
 	/bin/uname -p 2>/dev/null | grep 86 >/dev/null \
-	  && { echo i486-ncr-sysv4.3${OS_REL}; exit; }
+	  && { echo i486-ncr-sysv4.3"$OS_REL"; exit; }
 	/bin/uname -p 2>/dev/null | /bin/grep entium >/dev/null \
-	  && { echo i586-ncr-sysv4.3${OS_REL}; exit; } ;;
+	  && { echo i586-ncr-sysv4.3"$OS_REL"; exit; } ;;
     3[34]??:*:4.0:* | 3[34]??,*:*:4.0:*)
 	/bin/uname -p 2>/dev/null | grep 86 >/dev/null \
 	  && { echo i486-ncr-sysv4; exit; } ;;
@@ -1128,28 +1177,28 @@ EOF
 	test -r /etc/.relid \
 	    && OS_REL=.`sed -n 's/[^ ]* [^ ]* \([0-9][0-9]\).*/\1/p' < /etc/.relid`
 	/bin/uname -p 2>/dev/null | grep 86 >/dev/null \
-	    && { echo i486-ncr-sysv4.3${OS_REL}; exit; }
+	    && { echo i486-ncr-sysv4.3"$OS_REL"; exit; }
 	/bin/uname -p 2>/dev/null | /bin/grep entium >/dev/null \
-	    && { echo i586-ncr-sysv4.3${OS_REL}; exit; }
+	    && { echo i586-ncr-sysv4.3"$OS_REL"; exit; }
 	/bin/uname -p 2>/dev/null | /bin/grep pteron >/dev/null \
-	    && { echo i586-ncr-sysv4.3${OS_REL}; exit; } ;;
+	    && { echo i586-ncr-sysv4.3"$OS_REL"; exit; } ;;
     m68*:LynxOS:2.*:* | m68*:LynxOS:3.0*:*)
-	echo m68k-unknown-lynxos${UNAME_RELEASE}
+	echo m68k-unknown-lynxos"$UNAME_RELEASE"
 	exit ;;
     mc68030:UNIX_System_V:4.*:*)
 	echo m68k-atari-sysv4
 	exit ;;
     TSUNAMI:LynxOS:2.*:*)
-	echo sparc-unknown-lynxos${UNAME_RELEASE}
+	echo sparc-unknown-lynxos"$UNAME_RELEASE"
 	exit ;;
     rs6000:LynxOS:2.*:*)
-	echo rs6000-unknown-lynxos${UNAME_RELEASE}
+	echo rs6000-unknown-lynxos"$UNAME_RELEASE"
 	exit ;;
     PowerPC:LynxOS:2.*:* | PowerPC:LynxOS:3.[01]*:* | PowerPC:LynxOS:4.[02]*:*)
-	echo powerpc-unknown-lynxos${UNAME_RELEASE}
+	echo powerpc-unknown-lynxos"$UNAME_RELEASE"
 	exit ;;
     SM[BE]S:UNIX_SV:*:*)
-	echo mips-dde-sysv${UNAME_RELEASE}
+	echo mips-dde-sysv"$UNAME_RELEASE"
 	exit ;;
     RM*:ReliantUNIX-*:*:*)
 	echo mips-sni-sysv4
@@ -1160,7 +1209,7 @@ EOF
     *:SINIX-*:*:*)
 	if uname -p 2>/dev/null >/dev/null ; then
 		UNAME_MACHINE=`(uname -p) 2>/dev/null`
-		echo ${UNAME_MACHINE}-sni-sysv4
+		echo "$UNAME_MACHINE"-sni-sysv4
 	else
 		echo ns32k-sni-sysv
 	fi
@@ -1180,23 +1229,23 @@ EOF
 	exit ;;
     i*86:VOS:*:*)
 	# From Paul.Green@stratus.com.
-	echo ${UNAME_MACHINE}-stratus-vos
+	echo "$UNAME_MACHINE"-stratus-vos
 	exit ;;
     *:VOS:*:*)
 	# From Paul.Green@stratus.com.
 	echo hppa1.1-stratus-vos
 	exit ;;
     mc68*:A/UX:*:*)
-	echo m68k-apple-aux${UNAME_RELEASE}
+	echo m68k-apple-aux"$UNAME_RELEASE"
 	exit ;;
     news*:NEWS-OS:6*:*)
 	echo mips-sony-newsos6
 	exit ;;
     R[34]000:*System_V*:*:* | R4000:UNIX_SYSV:*:* | R*000:UNIX_SV:*:*)
 	if [ -d /usr/nec ]; then
-		echo mips-nec-sysv${UNAME_RELEASE}
+		echo mips-nec-sysv"$UNAME_RELEASE"
 	else
-		echo mips-unknown-sysv${UNAME_RELEASE}
+		echo mips-unknown-sysv"$UNAME_RELEASE"
 	fi
 	exit ;;
     BeBox:BeOS:*:*)	# BeOS running on hardware made by Be, PPC only.
@@ -1215,65 +1264,93 @@ EOF
 	echo x86_64-unknown-haiku
 	exit ;;
     SX-4:SUPER-UX:*:*)
-	echo sx4-nec-superux${UNAME_RELEASE}
+	echo sx4-nec-superux"$UNAME_RELEASE"
 	exit ;;
     SX-5:SUPER-UX:*:*)
-	echo sx5-nec-superux${UNAME_RELEASE}
+	echo sx5-nec-superux"$UNAME_RELEASE"
 	exit ;;
     SX-6:SUPER-UX:*:*)
-	echo sx6-nec-superux${UNAME_RELEASE}
+	echo sx6-nec-superux"$UNAME_RELEASE"
 	exit ;;
     SX-7:SUPER-UX:*:*)
-	echo sx7-nec-superux${UNAME_RELEASE}
+	echo sx7-nec-superux"$UNAME_RELEASE"
 	exit ;;
     SX-8:SUPER-UX:*:*)
-	echo sx8-nec-superux${UNAME_RELEASE}
+	echo sx8-nec-superux"$UNAME_RELEASE"
 	exit ;;
     SX-8R:SUPER-UX:*:*)
-	echo sx8r-nec-superux${UNAME_RELEASE}
+	echo sx8r-nec-superux"$UNAME_RELEASE"
+	exit ;;
+    SX-ACE:SUPER-UX:*:*)
+	echo sxace-nec-superux"$UNAME_RELEASE"
 	exit ;;
     Power*:Rhapsody:*:*)
-	echo powerpc-apple-rhapsody${UNAME_RELEASE}
+	echo powerpc-apple-rhapsody"$UNAME_RELEASE"
 	exit ;;
     *:Rhapsody:*:*)
-	echo ${UNAME_MACHINE}-apple-rhapsody${UNAME_RELEASE}
+	echo "$UNAME_MACHINE"-apple-rhapsody"$UNAME_RELEASE"
 	exit ;;
     *:Darwin:*:*)
 	UNAME_PROCESSOR=`uname -p` || UNAME_PROCESSOR=unknown
-	case $UNAME_PROCESSOR in
-	    i386)
-		eval $set_cc_for_build
-		if [ "$CC_FOR_BUILD" != 'no_compiler_found' ]; then
-		  if (echo '#ifdef __LP64__'; echo IS_64BIT_ARCH; echo '#endif') | \
-		      (CCOPTS= $CC_FOR_BUILD -E - 2>/dev/null) | \
-		      grep IS_64BIT_ARCH >/dev/null
-		  then
-		      UNAME_PROCESSOR="x86_64"
-		  fi
-		fi ;;
-	    unknown) UNAME_PROCESSOR=powerpc ;;
-	esac
-	echo ${UNAME_PROCESSOR}-apple-darwin${UNAME_RELEASE}
+	eval "$set_cc_for_build"
+	if test "$UNAME_PROCESSOR" = unknown ; then
+	    UNAME_PROCESSOR=powerpc
+	fi
+	if test "`echo "$UNAME_RELEASE" | sed -e 's/\..*//'`" -le 10 ; then
+	    if [ "$CC_FOR_BUILD" != no_compiler_found ]; then
+		if (echo '#ifdef __LP64__'; echo IS_64BIT_ARCH; echo '#endif') | \
+		       (CCOPTS="" $CC_FOR_BUILD -E - 2>/dev/null) | \
+		       grep IS_64BIT_ARCH >/dev/null
+		then
+		    case $UNAME_PROCESSOR in
+			i386) UNAME_PROCESSOR=x86_64 ;;
+			powerpc) UNAME_PROCESSOR=powerpc64 ;;
+		    esac
+		fi
+		# On 10.4-10.6 one might compile for PowerPC via gcc -arch ppc
+		if (echo '#ifdef __POWERPC__'; echo IS_PPC; echo '#endif') | \
+		       (CCOPTS="" $CC_FOR_BUILD -E - 2>/dev/null) | \
+		       grep IS_PPC >/dev/null
+		then
+		    UNAME_PROCESSOR=powerpc
+		fi
+	    fi
+	elif test "$UNAME_PROCESSOR" = i386 ; then
+	    # Avoid executing cc on OS X 10.9, as it ships with a stub
+	    # that puts up a graphical alert prompting to install
+	    # developer tools.  Any system running Mac OS X 10.7 or
+	    # later (Darwin 11 and later) is required to have a 64-bit
+	    # processor. This is not true of the ARM version of Darwin
+	    # that Apple uses in portable devices.
+	    UNAME_PROCESSOR=x86_64
+	fi
+	echo "$UNAME_PROCESSOR"-apple-darwin"$UNAME_RELEASE"
 	exit ;;
     *:procnto*:*:* | *:QNX:[0123456789]*:*)
 	UNAME_PROCESSOR=`uname -p`
-	if test "$UNAME_PROCESSOR" = "x86"; then
+	if test "$UNAME_PROCESSOR" = x86; then
 		UNAME_PROCESSOR=i386
 		UNAME_MACHINE=pc
 	fi
-	echo ${UNAME_PROCESSOR}-${UNAME_MACHINE}-nto-qnx${UNAME_RELEASE}
+	echo "$UNAME_PROCESSOR"-"$UNAME_MACHINE"-nto-qnx"$UNAME_RELEASE"
 	exit ;;
     *:QNX:*:4*)
 	echo i386-pc-qnx
 	exit ;;
-    NEO-?:NONSTOP_KERNEL:*:*)
-	echo neo-tandem-nsk${UNAME_RELEASE}
+    NEO-*:NONSTOP_KERNEL:*:*)
+	echo neo-tandem-nsk"$UNAME_RELEASE"
 	exit ;;
     NSE-*:NONSTOP_KERNEL:*:*)
-	echo nse-tandem-nsk${UNAME_RELEASE}
+	echo nse-tandem-nsk"$UNAME_RELEASE"
+	exit ;;
+    NSR-*:NONSTOP_KERNEL:*:*)
+	echo nsr-tandem-nsk"$UNAME_RELEASE"
 	exit ;;
-    NSR-?:NONSTOP_KERNEL:*:*)
-	echo nsr-tandem-nsk${UNAME_RELEASE}
+    NSV-*:NONSTOP_KERNEL:*:*)
+	echo nsv-tandem-nsk"$UNAME_RELEASE"
+	exit ;;
+    NSX-*:NONSTOP_KERNEL:*:*)
+	echo nsx-tandem-nsk"$UNAME_RELEASE"
 	exit ;;
     *:NonStop-UX:*:*)
 	echo mips-compaq-nonstopux
@@ -1282,18 +1359,18 @@ EOF
 	echo bs2000-siemens-sysv
 	exit ;;
     DS/*:UNIX_System_V:*:*)
-	echo ${UNAME_MACHINE}-${UNAME_SYSTEM}-${UNAME_RELEASE}
+	echo "$UNAME_MACHINE"-"$UNAME_SYSTEM"-"$UNAME_RELEASE"
 	exit ;;
     *:Plan9:*:*)
 	# "uname -m" is not consistent, so use $cputype instead. 386
 	# is converted to i386 for consistency with other x86
 	# operating systems.
-	if test "$cputype" = "386"; then
+	if test "$cputype" = 386; then
 	    UNAME_MACHINE=i386
 	else
 	    UNAME_MACHINE="$cputype"
 	fi
-	echo ${UNAME_MACHINE}-unknown-plan9
+	echo "$UNAME_MACHINE"-unknown-plan9
 	exit ;;
     *:TOPS-10:*:*)
 	echo pdp10-unknown-tops10
@@ -1314,14 +1391,14 @@ EOF
 	echo pdp10-unknown-its
 	exit ;;
     SEI:*:*:SEIUX)
-	echo mips-sei-seiux${UNAME_RELEASE}
+	echo mips-sei-seiux"$UNAME_RELEASE"
 	exit ;;
     *:DragonFly:*:*)
-	echo ${UNAME_MACHINE}-unknown-dragonfly`echo ${UNAME_RELEASE}|sed -e 's/[-(].*//'`
+	echo "$UNAME_MACHINE"-unknown-dragonfly"`echo "$UNAME_RELEASE"|sed -e 's/[-(].*//'`"
 	exit ;;
     *:*VMS:*:*)
 	UNAME_MACHINE=`(uname -p) 2>/dev/null`
-	case "${UNAME_MACHINE}" in
+	case "$UNAME_MACHINE" in
 	    A*) echo alpha-dec-vms ; exit ;;
 	    I*) echo ia64-dec-vms ; exit ;;
 	    V*) echo vax-dec-vms ; exit ;;
@@ -1330,182 +1407,48 @@ EOF
 	echo i386-pc-xenix
 	exit ;;
     i*86:skyos:*:*)
-	echo ${UNAME_MACHINE}-pc-skyos`echo ${UNAME_RELEASE}` | sed -e 's/ .*$//'
+	echo "$UNAME_MACHINE"-pc-skyos"`echo "$UNAME_RELEASE" | sed -e 's/ .*$//'`"
 	exit ;;
     i*86:rdos:*:*)
-	echo ${UNAME_MACHINE}-pc-rdos
+	echo "$UNAME_MACHINE"-pc-rdos
 	exit ;;
     i*86:AROS:*:*)
-	echo ${UNAME_MACHINE}-pc-aros
+	echo "$UNAME_MACHINE"-pc-aros
 	exit ;;
     x86_64:VMkernel:*:*)
-	echo ${UNAME_MACHINE}-unknown-esx
+	echo "$UNAME_MACHINE"-unknown-esx
+	exit ;;
+    amd64:Isilon\ OneFS:*:*)
+	echo x86_64-unknown-onefs
 	exit ;;
 esac
 
-eval $set_cc_for_build
-cat >$dummy.c <<EOF
-#ifdef _SEQUENT_
-# include <sys/types.h>
-# include <sys/utsname.h>
-#endif
-main ()
-{
-#if defined (sony)
-#if defined (MIPSEB)
-  /* BFD wants "bsd" instead of "newsos".  Perhaps BFD should be changed,
-     I don't know....  */
-  printf ("mips-sony-bsd\n"); exit (0);
-#else
-#include <sys/param.h>
-  printf ("m68k-sony-newsos%s\n",
-#ifdef NEWSOS4
-	"4"
-#else
-	""
-#endif
-	); exit (0);
-#endif
-#endif
-
-#if defined (__arm) && defined (__acorn) && defined (__unix)
-  printf ("arm-acorn-riscix\n"); exit (0);
-#endif
-
-#if defined (hp300) && !defined (hpux)
-  printf ("m68k-hp-bsd\n"); exit (0);
-#endif
-
-#if defined (NeXT)
-#if !defined (__ARCHITECTURE__)
-#define __ARCHITECTURE__ "m68k"
-#endif
-  int version;
-  version=`(hostinfo | sed -n 's/.*NeXT Mach \([0-9]*\).*/\1/p') 2>/dev/null`;
-  if (version < 4)
-    printf ("%s-next-nextstep%d\n", __ARCHITECTURE__, version);
-  else
-    printf ("%s-next-openstep%d\n", __ARCHITECTURE__, version);
-  exit (0);
-#endif
-
-#if defined (MULTIMAX) || defined (n16)
-#if defined (UMAXV)
-  printf ("ns32k-encore-sysv\n"); exit (0);
-#else
-#if defined (CMU)
-  printf ("ns32k-encore-mach\n"); exit (0);
-#else
-  printf ("ns32k-encore-bsd\n"); exit (0);
-#endif
-#endif
-#endif
-
-#if defined (__386BSD__)
-  printf ("i386-pc-bsd\n"); exit (0);
-#endif
-
-#if defined (sequent)
-#if defined (i386)
-  printf ("i386-sequent-dynix\n"); exit (0);
-#endif
-#if defined (ns32000)
-  printf ("ns32k-sequent-dynix\n"); exit (0);
-#endif
-#endif
-
-#if defined (_SEQUENT_)
-    struct utsname un;
-
-    uname(&un);
-
-    if (strncmp(un.version, "V2", 2) == 0) {
-	printf ("i386-sequent-ptx2\n"); exit (0);
-    }
-    if (strncmp(un.version, "V1", 2) == 0) { /* XXX is V1 correct? */
-	printf ("i386-sequent-ptx1\n"); exit (0);
-    }
-    printf ("i386-sequent-ptx\n"); exit (0);
+echo "$0: unable to guess system type" >&2
 
-#endif
-
-#if defined (vax)
-# if !defined (ultrix)
-#  include <sys/param.h>
-#  if defined (BSD)
-#   if BSD == 43
-      printf ("vax-dec-bsd4.3\n"); exit (0);
-#   else
-#    if BSD == 199006
-      printf ("vax-dec-bsd4.3reno\n"); exit (0);
-#    else
-      printf ("vax-dec-bsd\n"); exit (0);
-#    endif
-#   endif
-#  else
-    printf ("vax-dec-bsd\n"); exit (0);
-#  endif
-# else
-    printf ("vax-dec-ultrix\n"); exit (0);
-# endif
-#endif
+case "$UNAME_MACHINE:$UNAME_SYSTEM" in
+    mips:Linux | mips64:Linux)
+	# If we got here on MIPS GNU/Linux, output extra information.
+	cat >&2 <<EOF
 
-#if defined (alliant) && defined (i860)
-  printf ("i860-alliant-bsd\n"); exit (0);
-#endif
-
-  exit (1);
-}
+NOTE: MIPS GNU/Linux systems require a C compiler to fully recognize
+the system type. Please install a C compiler and try again.
 EOF
-
-$CC_FOR_BUILD -o $dummy $dummy.c 2>/dev/null && SYSTEM_NAME=`$dummy` &&
-	{ echo "$SYSTEM_NAME"; exit; }
-
-# Apollos put the system type in the environment.
-
-test -d /usr/apollo && { echo ${ISP}-apollo-${SYSTYPE}; exit; }
-
-# Convex versions that predate uname can use getsysinfo(1)
-
-if [ -x /usr/convex/getsysinfo ]
-then
-    case `getsysinfo -f cpu_type` in
-    c1*)
-	echo c1-convex-bsd
-	exit ;;
-    c2*)
-	if getsysinfo -f scalar_acc
-	then echo c32-convex-bsd
-	else echo c2-convex-bsd
-	fi
-	exit ;;
-    c34*)
-	echo c34-convex-bsd
-	exit ;;
-    c38*)
-	echo c38-convex-bsd
-	exit ;;
-    c4*)
-	echo c4-convex-bsd
-	exit ;;
-    esac
-fi
+	;;
+esac
 
 cat >&2 <<EOF
-$0: unable to guess system type
 
-This script, last modified $timestamp, has failed to recognize
-the operating system you are using. It is advised that you
-download the most up to date version of the config scripts from
+This script (version $timestamp), has failed to recognize the
+operating system you are using. If your script is old, overwrite *all*
+copies of config.guess and config.sub with the latest versions from:
 
-  http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess;hb=HEAD
+  https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess
 and
-  http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub;hb=HEAD
+  https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub
 
-If the version you run ($0) is already up to date, please
-send the following data and any information you think might be
-pertinent to <config-patches@gnu.org> in order to provide the needed
-information to handle your system.
+If $0 has already been updated, send the following data and any
+information you think might be pertinent to config-patches@gnu.org to
+provide the necessary information to handle your system.
 
 config.guess timestamp = $timestamp
 
@@ -1524,16 +1467,16 @@ hostinfo               = `(hostinfo) 2>/dev/null`
 /usr/bin/oslevel       = `(/usr/bin/oslevel) 2>/dev/null`
 /usr/convex/getsysinfo = `(/usr/convex/getsysinfo) 2>/dev/null`
 
-UNAME_MACHINE = ${UNAME_MACHINE}
-UNAME_RELEASE = ${UNAME_RELEASE}
-UNAME_SYSTEM  = ${UNAME_SYSTEM}
-UNAME_VERSION = ${UNAME_VERSION}
+UNAME_MACHINE = "$UNAME_MACHINE"
+UNAME_RELEASE = "$UNAME_RELEASE"
+UNAME_SYSTEM  = "$UNAME_SYSTEM"
+UNAME_VERSION = "$UNAME_VERSION"
 EOF
 
 exit 1
 
 # Local variables:
-# eval: (add-hook 'write-file-hooks 'time-stamp)
+# eval: (add-hook 'write-file-functions 'time-stamp)
 # time-stamp-start: "timestamp='"
 # time-stamp-format: "%:y-%02m-%02d"
 # time-stamp-end: "'"
diff --git a/config.sub b/config.sub
index 8df55110..682f5976 100755
--- a/config.sub
+++ b/config.sub
@@ -1,36 +1,31 @@
 #! /bin/sh
 # Configuration validation subroutine script.
-#   Copyright (C) 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
-#   2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,
-#   2011, 2012 Free Software Foundation, Inc.
+#   Copyright 1992-2018 Free Software Foundation, Inc.
 
-timestamp='2012-12-06'
+timestamp='2018-02-22'
 
-# This file is (in principle) common to ALL GNU software.
-# The presence of a machine in this file suggests that SOME GNU software
-# can handle that machine.  It does not imply ALL GNU software can.
-#
-# This file is free software; you can redistribute it and/or modify
-# it under the terms of the GNU General Public License as published by
-# the Free Software Foundation; either version 2 of the License, or
+# This file is free software; you can redistribute it and/or modify it
+# under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3 of the License, or
 # (at your option) any later version.
 #
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-# GNU General Public License for more details.
+# This program is distributed in the hope that it will be useful, but
+# WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+# General Public License for more details.
 #
 # You should have received a copy of the GNU General Public License
-# along with this program; if not, see <http://www.gnu.org/licenses/>.
+# along with this program; if not, see <https://www.gnu.org/licenses/>.
 #
 # As a special exception to the GNU General Public License, if you
 # distribute this file as part of a program that contains a
 # configuration script generated by Autoconf, you may include it under
-# the same distribution terms that you use for the rest of that program.
+# the same distribution terms that you use for the rest of that
+# program.  This Exception is an additional permission under section 7
+# of the GNU General Public License, version 3 ("GPLv3").
 
 
-# Please send patches to <config-patches@gnu.org>.  Submit a context
-# diff and a properly formatted GNU ChangeLog entry.
+# Please send patches to <config-patches@gnu.org>.
 #
 # Configuration subroutine to validate and canonicalize a configuration type.
 # Supply the specified configuration type as an argument.
@@ -38,7 +33,7 @@ timestamp='2012-12-06'
 # Otherwise, we print the canonical config type on stdout and succeed.
 
 # You can get the latest version of this script from:
-# http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub;hb=HEAD
+# https://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub
 
 # This file is supposed to be the same for all GNU packages
 # and recognize all the CPU types, system types and aliases
@@ -58,12 +53,11 @@ timestamp='2012-12-06'
 me=`echo "$0" | sed -e 's,.*/,,'`
 
 usage="\
-Usage: $0 [OPTION] CPU-MFR-OPSYS
-       $0 [OPTION] ALIAS
+Usage: $0 [OPTION] CPU-MFR-OPSYS or ALIAS
 
 Canonicalize a configuration name.
 
-Operation modes:
+Options:
   -h, --help         print this help, then exit
   -t, --time-stamp   print date of last modification, then exit
   -v, --version      print version number, then exit
@@ -73,9 +67,7 @@ Report bugs and patches to <config-patches@gnu.org>."
 version="\
 GNU config.sub ($timestamp)
 
-Copyright (C) 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000,
-2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012
-Free Software Foundation, Inc.
+Copyright 1992-2018 Free Software Foundation, Inc.
 
 This is free software; see the source for copying conditions.  There is NO
 warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE."
@@ -102,7 +94,7 @@ while test $# -gt 0 ; do
 
     *local*)
        # First pass through any local machine types.
-       echo $1
+       echo "$1"
        exit ;;
 
     * )
@@ -120,24 +112,24 @@ esac
 
 # Separate what the user gave into CPU-COMPANY and OS or KERNEL-OS (if any).
 # Here we must recognize all the valid KERNEL-OS combinations.
-maybe_os=`echo $1 | sed 's/^\(.*\)-\([^-]*-[^-]*\)$/\2/'`
+maybe_os=`echo "$1" | sed 's/^\(.*\)-\([^-]*-[^-]*\)$/\2/'`
 case $maybe_os in
   nto-qnx* | linux-gnu* | linux-android* | linux-dietlibc | linux-newlib* | \
   linux-musl* | linux-uclibc* | uclinux-uclibc* | uclinux-gnu* | kfreebsd*-gnu* | \
-  knetbsd*-gnu* | netbsd*-gnu* | \
-  kopensolaris*-gnu* | \
+  knetbsd*-gnu* | netbsd*-gnu* | netbsd*-eabi* | \
+  kopensolaris*-gnu* | cloudabi*-eabi* | \
   storm-chaos* | os2-emx* | rtmk-nova*)
     os=-$maybe_os
-    basic_machine=`echo $1 | sed 's/^\(.*\)-\([^-]*-[^-]*\)$/\1/'`
+    basic_machine=`echo "$1" | sed 's/^\(.*\)-\([^-]*-[^-]*\)$/\1/'`
     ;;
   android-linux)
     os=-linux-android
-    basic_machine=`echo $1 | sed 's/^\(.*\)-\([^-]*-[^-]*\)$/\1/'`-unknown
+    basic_machine=`echo "$1" | sed 's/^\(.*\)-\([^-]*-[^-]*\)$/\1/'`-unknown
     ;;
   *)
-    basic_machine=`echo $1 | sed 's/-[^-]*$//'`
-    if [ $basic_machine != $1 ]
-    then os=`echo $1 | sed 's/.*-/-/'`
+    basic_machine=`echo "$1" | sed 's/-[^-]*$//'`
+    if [ "$basic_machine" != "$1" ]
+    then os=`echo "$1" | sed 's/.*-/-/'`
     else os=; fi
     ;;
 esac
@@ -156,7 +148,7 @@ case $os in
 	-convergent* | -ncr* | -news | -32* | -3600* | -3100* | -hitachi* |\
 	-c[123]* | -convex* | -sun | -crds | -omron* | -dg | -ultra | -tti* | \
 	-harris | -dolphin | -highlevel | -gould | -cbm | -ns | -masscomp | \
-	-apple | -axis | -knuth | -cray | -microblaze*)
+	-apple | -axis | -knuth | -cray | -microblaze* | loongson)
 		os=
 		basic_machine=$1
 		;;
@@ -186,44 +178,44 @@ case $os in
 		;;
 	-sco6)
 		os=-sco5v6
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-sco5)
 		os=-sco3.2v5
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-sco4)
 		os=-sco3.2v4
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-sco3.2.[4-9]*)
 		os=`echo $os | sed -e 's/sco3.2./sco3.2v/'`
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-sco3.2v[4-9]*)
 		# Don't forget version if it is 3.2v4 or newer.
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-sco5v6*)
 		# Don't forget version if it is 3.2v4 or newer.
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-sco*)
 		os=-sco3.2v2
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-udk*)
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-isc)
 		os=-isc2.2
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-clix*)
 		basic_machine=clipper-intergraph
 		;;
 	-isc*)
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-pc/'`
 		;;
 	-lynx*178)
 		os=-lynxos178
@@ -235,10 +227,7 @@ case $os in
 		os=-lynxos
 		;;
 	-ptx*)
-		basic_machine=`echo $1 | sed -e 's/86-.*/86-sequent/'`
-		;;
-	-windowsnt*)
-		os=`echo $os | sed -e 's/windowsnt/winnt/'`
+		basic_machine=`echo "$1" | sed -e 's/86-.*/86-sequent/'`
 		;;
 	-psos*)
 		os=-psos
@@ -259,19 +248,21 @@ case $basic_machine in
 	| alpha | alphaev[4-8] | alphaev56 | alphaev6[78] | alphapca5[67] \
 	| alpha64 | alpha64ev[4-8] | alpha64ev56 | alpha64ev6[78] | alpha64pca5[67] \
 	| am33_2.0 \
-	| arc \
+	| arc | arceb \
 	| arm | arm[bl]e | arme[lb] | armv[2-8] | armv[3-8][lb] | armv7[arm] \
 	| avr | avr32 \
+	| ba \
 	| be32 | be64 \
 	| bfin \
-	| c4x | clipper \
+	| c4x | c8051 | clipper \
 	| d10v | d30v | dlx | dsp16xx \
-	| epiphany \
-	| fido | fr30 | frv \
+	| e2k | epiphany \
+	| fido | fr30 | frv | ft32 \
 	| h8300 | h8500 | hppa | hppa1.[01] | hppa2.0 | hppa2.0[nw] | hppa64 \
 	| hexagon \
-	| i370 | i860 | i960 | ia64 \
+	| i370 | i860 | i960 | ia16 | ia64 \
 	| ip2k | iq2000 \
+	| k1om \
 	| le32 | le64 \
 	| lm32 \
 	| m32c | m32r | m32rle | m68000 | m68k | m88k \
@@ -289,26 +280,31 @@ case $basic_machine in
 	| mips64vr5900 | mips64vr5900el \
 	| mipsisa32 | mipsisa32el \
 	| mipsisa32r2 | mipsisa32r2el \
+	| mipsisa32r6 | mipsisa32r6el \
 	| mipsisa64 | mipsisa64el \
 	| mipsisa64r2 | mipsisa64r2el \
+	| mipsisa64r6 | mipsisa64r6el \
 	| mipsisa64sb1 | mipsisa64sb1el \
 	| mipsisa64sr71k | mipsisa64sr71kel \
+	| mipsr5900 | mipsr5900el \
 	| mipstx39 | mipstx39el \
 	| mn10200 | mn10300 \
 	| moxie \
 	| mt \
 	| msp430 \
+	| loongarch32 | loongarch64 \
 	| nds32 | nds32le | nds32be \
-	| nios | nios2 \
+	| nios | nios2 | nios2eb | nios2el \
 	| ns16k | ns32k \
-	| open8 \
-	| or32 \
-	| pdp10 | pdp11 | pj | pjl \
+	| open8 | or1k | or1knd | or32 \
+	| pdp10 | pj | pjl \
 	| powerpc | powerpc64 | powerpc64le | powerpcle \
+	| pru \
 	| pyramid \
+	| riscv32 | riscv64 \
 	| rl78 | rx \
 	| score \
-	| sh | sh[1234] | sh[24]a | sh[24]aeb | sh[23]e | sh[34]eb | sheb | shbe | shle | sh[1234]le | sh3ele \
+	| sh | sh[1234] | sh[24]a | sh[24]aeb | sh[23]e | sh[234]eb | sheb | shbe | shle | sh[1234]le | sh3ele \
 	| sh64 | sh64le \
 	| sparc | sparc64 | sparc64b | sparc64v | sparc86x | sparclet | sparclite \
 	| sparcv8 | sparcv9 | sparcv9b | sparcv9v \
@@ -316,7 +312,8 @@ case $basic_machine in
 	| tahoe | tic4x | tic54x | tic55x | tic6x | tic80 | tron \
 	| ubicom32 \
 	| v850 | v850e | v850e1 | v850e2 | v850es | v850e2v3 \
-	| we32k \
+	| visium \
+	| wasm32 \
 	| x86 | xc16x | xstormy16 | xtensa \
 	| z8k | z80)
 		basic_machine=$basic_machine-unknown
@@ -330,11 +327,14 @@ case $basic_machine in
 	c6x)
 		basic_machine=tic6x-unknown
 		;;
-	m6811 | m68hc11 | m6812 | m68hc12 | m68hcs12x | picochip)
+	leon|leon[3-9])
+		basic_machine=sparc-$basic_machine
+		;;
+	m6811 | m68hc11 | m6812 | m68hc12 | m68hcs12x | nvptx | picochip)
 		basic_machine=$basic_machine-unknown
 		os=-none
 		;;
-	m88110 | m680[12346]0 | m683?2 | m68360 | m5200 | v70 | w65 | z8k)
+	m88110 | m680[12346]0 | m683?2 | m68360 | m5200 | v70 | w65)
 		;;
 	ms1)
 		basic_machine=mt-unknown
@@ -363,7 +363,7 @@ case $basic_machine in
 	  ;;
 	# Object if more than one company name word.
 	*-*-*)
-		echo Invalid configuration \`$1\': machine \`$basic_machine\' not recognized 1>&2
+		echo Invalid configuration \`"$1"\': machine \`"$basic_machine"\' not recognized 1>&2
 		exit 1
 		;;
 	# Recognize the basic CPU types with company name.
@@ -372,22 +372,25 @@ case $basic_machine in
 	| aarch64-* | aarch64_be-* \
 	| alpha-* | alphaev[4-8]-* | alphaev56-* | alphaev6[78]-* \
 	| alpha64-* | alpha64ev[4-8]-* | alpha64ev56-* | alpha64ev6[78]-* \
-	| alphapca5[67]-* | alpha64pca5[67]-* | arc-* \
+	| alphapca5[67]-* | alpha64pca5[67]-* | arc-* | arceb-* \
 	| arm-*  | armbe-* | armle-* | armeb-* | armv*-* \
 	| avr-* | avr32-* \
+	| ba-* \
 	| be32-* | be64-* \
 	| bfin-* | bs2000-* \
 	| c[123]* | c30-* | [cjt]90-* | c4x-* \
-	| clipper-* | craynv-* | cydra-* \
+	| c8051-* | clipper-* | craynv-* | cydra-* \
 	| d10v-* | d30v-* | dlx-* \
-	| elxsi-* \
+	| e2k-* | elxsi-* \
 	| f30[01]-* | f700-* | fido-* | fr30-* | frv-* | fx80-* \
 	| h8300-* | h8500-* \
 	| hppa-* | hppa1.[01]-* | hppa2.0-* | hppa2.0[nw]-* | hppa64-* \
 	| hexagon-* \
-	| i*86-* | i860-* | i960-* | ia64-* \
+	| i*86-* | i860-* | i960-* | ia16-* | ia64-* \
 	| ip2k-* | iq2000-* \
+	| k1om-* \
 	| le32-* | le64-* \
+	| loongarch32-* | loongarch64-* \
 	| lm32-* \
 	| m32c-* | m32r-* | m32rle-* \
 	| m68000-* | m680[012346]0-* | m68360-* | m683?2-* | m68k-* \
@@ -406,28 +409,34 @@ case $basic_machine in
 	| mips64vr5900-* | mips64vr5900el-* \
 	| mipsisa32-* | mipsisa32el-* \
 	| mipsisa32r2-* | mipsisa32r2el-* \
+	| mipsisa32r6-* | mipsisa32r6el-* \
 	| mipsisa64-* | mipsisa64el-* \
 	| mipsisa64r2-* | mipsisa64r2el-* \
+	| mipsisa64r6-* | mipsisa64r6el-* \
 	| mipsisa64sb1-* | mipsisa64sb1el-* \
 	| mipsisa64sr71k-* | mipsisa64sr71kel-* \
+	| mipsr5900-* | mipsr5900el-* \
 	| mipstx39-* | mipstx39el-* \
 	| mmix-* \
 	| mt-* \
 	| msp430-* \
 	| nds32-* | nds32le-* | nds32be-* \
-	| nios-* | nios2-* \
+	| nios-* | nios2-* | nios2eb-* | nios2el-* \
 	| none-* | np1-* | ns16k-* | ns32k-* \
 	| open8-* \
+	| or1k*-* \
 	| orion-* \
 	| pdp10-* | pdp11-* | pj-* | pjl-* | pn-* | power-* \
 	| powerpc-* | powerpc64-* | powerpc64le-* | powerpcle-* \
+	| pru-* \
 	| pyramid-* \
+	| riscv32-* | riscv64-* \
 	| rl78-* | romp-* | rs6000-* | rx-* \
 	| sh-* | sh[1234]-* | sh[24]a-* | sh[24]aeb-* | sh[23]e-* | sh[34]eb-* | sheb-* | shbe-* \
 	| shle-* | sh[1234]le-* | sh3ele-* | sh64-* | sh64le-* \
 	| sparc-* | sparc64-* | sparc64b-* | sparc64v-* | sparc86x-* | sparclet-* \
 	| sparclite-* \
-	| sparcv8-* | sparcv9-* | sparcv9b-* | sparcv9v-* | sv1-* | sx?-* \
+	| sparcv8-* | sparcv9-* | sparcv9b-* | sparcv9v-* | sv1-* | sx*-* \
 	| tahoe-* \
 	| tic30-* | tic4x-* | tic54x-* | tic55x-* | tic6x-* | tic80-* \
 	| tile*-* \
@@ -435,6 +444,8 @@ case $basic_machine in
 	| ubicom32-* \
 	| v850-* | v850e-* | v850e1-* | v850es-* | v850e2-* | v850e2v3-* \
 	| vax-* \
+	| visium-* \
+	| wasm32-* \
 	| we32k-* \
 	| x86-* | x86_64-* | xc16x-* | xps100-* \
 	| xstormy16-* | xtensa*-* \
@@ -448,7 +459,7 @@ case $basic_machine in
 	# Recognize the various machine names and aliases which stand
 	# for a CPU type and a company and sometimes even an OS.
 	386bsd)
-		basic_machine=i386-unknown
+		basic_machine=i386-pc
 		os=-bsd
 		;;
 	3b1 | 7300 | 7300-att | att-7300 | pc7300 | safari | unixpc)
@@ -482,7 +493,7 @@ case $basic_machine in
 		basic_machine=x86_64-pc
 		;;
 	amd64-*)
-		basic_machine=x86_64-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=x86_64-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	amdahl)
 		basic_machine=580-amdahl
@@ -511,6 +522,9 @@ case $basic_machine in
 		basic_machine=i386-pc
 		os=-aros
 		;;
+	asmjs)
+		basic_machine=asmjs-unknown
+		;;
 	aux)
 		basic_machine=m68k-apple
 		os=-aux
@@ -524,7 +538,7 @@ case $basic_machine in
 		os=-linux
 		;;
 	blackfin-*)
-		basic_machine=bfin-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=bfin-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		os=-linux
 		;;
 	bluegene*)
@@ -532,13 +546,13 @@ case $basic_machine in
 		os=-cnk
 		;;
 	c54x-*)
-		basic_machine=tic54x-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=tic54x-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	c55x-*)
-		basic_machine=tic55x-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=tic55x-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	c6x-*)
-		basic_machine=tic6x-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=tic6x-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	c90)
 		basic_machine=c90-cray
@@ -627,10 +641,18 @@ case $basic_machine in
 		basic_machine=rs6000-bull
 		os=-bosx
 		;;
-	dpx2* | dpx2*-bull)
+	dpx2*)
 		basic_machine=m68k-bull
 		os=-sysv3
 		;;
+	e500v[12])
+		basic_machine=powerpc-unknown
+		os=$os"spe"
+		;;
+	e500v[12]-*)
+		basic_machine=powerpc-`echo "$basic_machine" | sed 's/^[^-]*-//'`
+		os=$os"spe"
+		;;
 	ebmon29k)
 		basic_machine=a29k-amd
 		os=-ebmon
@@ -720,9 +742,6 @@ case $basic_machine in
 	hp9k8[0-9][0-9] | hp8[0-9][0-9])
 		basic_machine=hppa1.0-hp
 		;;
-	hppa-next)
-		os=-nextstep3
-		;;
 	hppaosf)
 		basic_machine=hppa1.1-hp
 		os=-osf
@@ -735,26 +754,26 @@ case $basic_machine in
 		basic_machine=i370-ibm
 		;;
 	i*86v32)
-		basic_machine=`echo $1 | sed -e 's/86.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86.*/86-pc/'`
 		os=-sysv32
 		;;
 	i*86v4*)
-		basic_machine=`echo $1 | sed -e 's/86.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86.*/86-pc/'`
 		os=-sysv4
 		;;
 	i*86v)
-		basic_machine=`echo $1 | sed -e 's/86.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86.*/86-pc/'`
 		os=-sysv
 		;;
 	i*86sol2)
-		basic_machine=`echo $1 | sed -e 's/86.*/86-pc/'`
+		basic_machine=`echo "$1" | sed -e 's/86.*/86-pc/'`
 		os=-solaris2
 		;;
 	i386mach)
 		basic_machine=i386-mach
 		os=-mach
 		;;
-	i386-vsta | vsta)
+	vsta)
 		basic_machine=i386-unknown
 		os=-vsta
 		;;
@@ -772,17 +791,17 @@ case $basic_machine in
 		basic_machine=m68k-isi
 		os=-sysv
 		;;
+	leon-*|leon[3-9]-*)
+		basic_machine=sparc-`echo "$basic_machine" | sed 's/-.*//'`
+		;;
 	m68knommu)
 		basic_machine=m68k-unknown
 		os=-linux
 		;;
 	m68knommu-*)
-		basic_machine=m68k-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=m68k-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		os=-linux
 		;;
-	m88k-omron*)
-		basic_machine=m88k-omron
-		;;
 	magnum | m3230)
 		basic_machine=mips-mips
 		os=-sysv
@@ -799,7 +818,7 @@ case $basic_machine in
 		os=-mingw64
 		;;
 	mingw32)
-		basic_machine=i386-pc
+		basic_machine=i686-pc
 		os=-mingw32
 		;;
 	mingw32ce)
@@ -814,10 +833,10 @@ case $basic_machine in
 		os=-mint
 		;;
 	mips3*-*)
-		basic_machine=`echo $basic_machine | sed -e 's/mips3/mips64/'`
+		basic_machine=`echo "$basic_machine" | sed -e 's/mips3/mips64/'`
 		;;
 	mips3*)
-		basic_machine=`echo $basic_machine | sed -e 's/mips3/mips64/'`-unknown
+		basic_machine=`echo "$basic_machine" | sed -e 's/mips3/mips64/'`-unknown
 		;;
 	monitor)
 		basic_machine=m68k-rom68k
@@ -827,15 +846,19 @@ case $basic_machine in
 		basic_machine=powerpc-unknown
 		os=-morphos
 		;;
+	moxiebox)
+		basic_machine=moxie-unknown
+		os=-moxiebox
+		;;
 	msdos)
 		basic_machine=i386-pc
 		os=-msdos
 		;;
 	ms1-*)
-		basic_machine=`echo $basic_machine | sed -e 's/ms1-/mt-/'`
+		basic_machine=`echo "$basic_machine" | sed -e 's/ms1-/mt-/'`
 		;;
 	msys)
-		basic_machine=i386-pc
+		basic_machine=i686-pc
 		os=-msys
 		;;
 	mvs)
@@ -874,7 +897,7 @@ case $basic_machine in
 		basic_machine=v70-nec
 		os=-sysv
 		;;
-	next | m*-next )
+	next | m*-next)
 		basic_machine=m68k-next
 		case $os in
 		    -nextstep* )
@@ -919,6 +942,12 @@ case $basic_machine in
 	nsr-tandem)
 		basic_machine=nsr-tandem
 		;;
+	nsv-tandem)
+		basic_machine=nsv-tandem
+		;;
+	nsx-tandem)
+		basic_machine=nsx-tandem
+		;;
 	op50n-* | op60c-*)
 		basic_machine=hppa1.1-oki
 		os=-proelf
@@ -951,7 +980,7 @@ case $basic_machine in
 		os=-linux
 		;;
 	parisc-*)
-		basic_machine=hppa-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=hppa-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		os=-linux
 		;;
 	pbd)
@@ -967,7 +996,7 @@ case $basic_machine in
 		basic_machine=i386-pc
 		;;
 	pc98-*)
-		basic_machine=i386-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=i386-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	pentium | p5 | k5 | k6 | nexgen | viac3)
 		basic_machine=i586-pc
@@ -982,16 +1011,16 @@ case $basic_machine in
 		basic_machine=i786-pc
 		;;
 	pentium-* | p5-* | k5-* | k6-* | nexgen-* | viac3-*)
-		basic_machine=i586-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=i586-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	pentiumpro-* | p6-* | 6x86-* | athlon-*)
-		basic_machine=i686-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=i686-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	pentiumii-* | pentium2-* | pentiumiii-* | pentium3-*)
-		basic_machine=i686-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=i686-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	pentium4-*)
-		basic_machine=i786-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=i786-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	pn)
 		basic_machine=pn-gould
@@ -1001,23 +1030,23 @@ case $basic_machine in
 	ppc | ppcbe)	basic_machine=powerpc-unknown
 		;;
 	ppc-* | ppcbe-*)
-		basic_machine=powerpc-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=powerpc-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
-	ppcle | powerpclittle | ppc-le | powerpc-little)
+	ppcle | powerpclittle)
 		basic_machine=powerpcle-unknown
 		;;
 	ppcle-* | powerpclittle-*)
-		basic_machine=powerpcle-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=powerpcle-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	ppc64)	basic_machine=powerpc64-unknown
 		;;
-	ppc64-*) basic_machine=powerpc64-`echo $basic_machine | sed 's/^[^-]*-//'`
+	ppc64-*) basic_machine=powerpc64-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
-	ppc64le | powerpc64little | ppc64-le | powerpc64-little)
+	ppc64le | powerpc64little)
 		basic_machine=powerpc64le-unknown
 		;;
 	ppc64le-* | powerpc64little-*)
-		basic_machine=powerpc64le-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=powerpc64le-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	ps2)
 		basic_machine=i386-ibm
@@ -1071,17 +1100,10 @@ case $basic_machine in
 	sequent)
 		basic_machine=i386-sequent
 		;;
-	sh)
-		basic_machine=sh-hitachi
-		os=-hms
-		;;
 	sh5el)
 		basic_machine=sh5le-unknown
 		;;
-	sh64)
-		basic_machine=sh64-unknown
-		;;
-	sparclite-wrs | simso-wrs)
+	simso-wrs)
 		basic_machine=sparclite-wrs
 		os=-vxworks
 		;;
@@ -1100,7 +1122,7 @@ case $basic_machine in
 		os=-sysv4
 		;;
 	strongarm-* | thumb-*)
-		basic_machine=arm-`echo $basic_machine | sed 's/^[^-]*-//'`
+		basic_machine=arm-`echo "$basic_machine" | sed 's/^[^-]*-//'`
 		;;
 	sun2)
 		basic_machine=m68000-sun
@@ -1222,6 +1244,9 @@ case $basic_machine in
 		basic_machine=hppa1.1-winbond
 		os=-proelf
 		;;
+	x64)
+		basic_machine=x86_64-pc
+		;;
 	xbox)
 		basic_machine=i686-pc
 		os=-mingw32
@@ -1230,20 +1255,12 @@ case $basic_machine in
 		basic_machine=xps100-honeywell
 		;;
 	xscale-* | xscalee[bl]-*)
-		basic_machine=`echo $basic_machine | sed 's/^xscale/arm/'`
+		basic_machine=`echo "$basic_machine" | sed 's/^xscale/arm/'`
 		;;
 	ymp)
 		basic_machine=ymp-cray
 		os=-unicos
 		;;
-	z8k-*-coff)
-		basic_machine=z8k-unknown
-		os=-sim
-		;;
-	z80-*-coff)
-		basic_machine=z80-unknown
-		os=-sim
-		;;
 	none)
 		basic_machine=none-none
 		os=-none
@@ -1272,10 +1289,6 @@ case $basic_machine in
 	vax)
 		basic_machine=vax-dec
 		;;
-	pdp10)
-		# there are many clones, so DEC is not a safe bet
-		basic_machine=pdp10-unknown
-		;;
 	pdp11)
 		basic_machine=pdp11-dec
 		;;
@@ -1285,9 +1298,6 @@ case $basic_machine in
 	sh[1234] | sh[24]a | sh[24]aeb | sh[34]eb | sh[1234]le | sh[23]ele)
 		basic_machine=sh-unknown
 		;;
-	sparc | sparcv8 | sparcv9 | sparcv9b | sparcv9v)
-		basic_machine=sparc-sun
-		;;
 	cydra)
 		basic_machine=cydra-cydrome
 		;;
@@ -1307,7 +1317,7 @@ case $basic_machine in
 		# Make sure to match an already-canonicalized machine name.
 		;;
 	*)
-		echo Invalid configuration \`$1\': machine \`$basic_machine\' not recognized 1>&2
+		echo Invalid configuration \`"$1"\': machine \`"$basic_machine"\' not recognized 1>&2
 		exit 1
 		;;
 esac
@@ -1315,10 +1325,10 @@ esac
 # Here we canonicalize certain aliases for manufacturers.
 case $basic_machine in
 	*-digital*)
-		basic_machine=`echo $basic_machine | sed 's/digital.*/dec/'`
+		basic_machine=`echo "$basic_machine" | sed 's/digital.*/dec/'`
 		;;
 	*-commodore*)
-		basic_machine=`echo $basic_machine | sed 's/commodore.*/cbm/'`
+		basic_machine=`echo "$basic_machine" | sed 's/commodore.*/cbm/'`
 		;;
 	*)
 		;;
@@ -1329,8 +1339,8 @@ esac
 if [ x"$os" != x"" ]
 then
 case $os in
-	# First match some system type aliases
-	# that might get confused with valid system types.
+	# First match some system type aliases that might get confused
+	# with valid system types.
 	# -solaris* is a basic system type, with this one exception.
 	-auroraux)
 		os=-auroraux
@@ -1341,45 +1351,48 @@ case $os in
 	-solaris)
 		os=-solaris2
 		;;
-	-svr4*)
-		os=-sysv4
-		;;
 	-unixware*)
 		os=-sysv4.2uw
 		;;
 	-gnu/linux*)
 		os=`echo $os | sed -e 's|gnu/linux|linux-gnu|'`
 		;;
-	# First accept the basic system types.
+	# es1800 is here to avoid being matched by es* (a different OS)
+	-es1800*)
+		os=-ose
+		;;
+	# Now accept the basic system types.
 	# The portable systems comes first.
-	# Each alternative MUST END IN A *, to match a version number.
+	# Each alternative MUST end in a * to match a version number.
 	# -sysv* is not here because it comes later, after sysvr4.
 	-gnu* | -bsd* | -mach* | -minix* | -genix* | -ultrix* | -irix* \
 	      | -*vms* | -sco* | -esix* | -isc* | -aix* | -cnk* | -sunos | -sunos[34]*\
 	      | -hpux* | -unos* | -osf* | -luna* | -dgux* | -auroraux* | -solaris* \
-	      | -sym* | -kopensolaris* \
+	      | -sym* | -kopensolaris* | -plan9* \
 	      | -amigaos* | -amigados* | -msdos* | -newsos* | -unicos* | -aof* \
-	      | -aos* | -aros* \
+	      | -aos* | -aros* | -cloudabi* | -sortix* \
 	      | -nindy* | -vxsim* | -vxworks* | -ebmon* | -hms* | -mvs* \
 	      | -clix* | -riscos* | -uniplus* | -iris* | -rtu* | -xenix* \
-	      | -hiux* | -386bsd* | -knetbsd* | -mirbsd* | -netbsd* \
-	      | -bitrig* | -openbsd* | -solidbsd* \
+	      | -hiux* | -knetbsd* | -mirbsd* | -netbsd* \
+	      | -bitrig* | -openbsd* | -solidbsd* | -libertybsd* \
 	      | -ekkobsd* | -kfreebsd* | -freebsd* | -riscix* | -lynxos* \
 	      | -bosx* | -nextstep* | -cxux* | -aout* | -elf* | -oabi* \
 	      | -ptx* | -coff* | -ecoff* | -winnt* | -domain* | -vsta* \
 	      | -udi* | -eabi* | -lites* | -ieee* | -go32* | -aux* \
-	      | -chorusos* | -chorusrdb* | -cegcc* \
+	      | -chorusos* | -chorusrdb* | -cegcc* | -glidix* \
 	      | -cygwin* | -msys* | -pe* | -psos* | -moss* | -proelf* | -rtems* \
-	      | -mingw32* | -mingw64* | -linux-gnu* | -linux-android* \
+	      | -midipix* | -mingw32* | -mingw64* | -linux-gnu* | -linux-android* \
 	      | -linux-newlib* | -linux-musl* | -linux-uclibc* \
-	      | -uxpv* | -beos* | -mpeix* | -udk* \
-	      | -interix* | -uwin* | -mks* | -rhapsody* | -darwin* | -opened* \
+	      | -uxpv* | -beos* | -mpeix* | -udk* | -moxiebox* \
+	      | -interix* | -uwin* | -mks* | -rhapsody* | -darwin* \
 	      | -openstep* | -oskit* | -conix* | -pw32* | -nonstopux* \
 	      | -storm-chaos* | -tops10* | -tenex* | -tops20* | -its* \
 	      | -os2* | -vos* | -palmos* | -uclinux* | -nucleus* \
-	      | -morphos* | -superux* | -rtmk* | -rtmk-nova* | -windiss* \
+	      | -morphos* | -superux* | -rtmk* | -windiss* \
 	      | -powermax* | -dnix* | -nx6 | -nx7 | -sei* | -dragonfly* \
-	      | -skyos* | -haiku* | -rdos* | -toppers* | -drops* | -es*)
+	      | -skyos* | -haiku* | -rdos* | -toppers* | -drops* | -es* \
+	      | -onefs* | -tirtos* | -phoenix* | -fuchsia* | -redox* | -bme* \
+	      | -midnightbsd*)
 	# Remember, each alternative MUST END IN *, to match a version number.
 		;;
 	-qnx*)
@@ -1396,12 +1409,12 @@ case $os in
 	-nto*)
 		os=`echo $os | sed -e 's|nto|nto-qnx|'`
 		;;
-	-sim | -es1800* | -hms* | -xray | -os68k* | -none* | -v88r* \
-	      | -windows* | -osx | -abug | -netware* | -os9* | -beos* | -haiku* \
+	-sim | -xray | -os68k* | -v88r* \
+	      | -windows* | -osx | -abug | -netware* | -os9* \
 	      | -macos* | -mpw* | -magic* | -mmixware* | -mon960* | -lnews*)
 		;;
 	-mac*)
-		os=`echo $os | sed -e 's|mac|macos|'`
+		os=`echo "$os" | sed -e 's|mac|macos|'`
 		;;
 	-linux-dietlibc)
 		os=-linux-dietlibc
@@ -1410,10 +1423,10 @@ case $os in
 		os=`echo $os | sed -e 's|linux|linux-gnu|'`
 		;;
 	-sunos5*)
-		os=`echo $os | sed -e 's|sunos5|solaris2|'`
+		os=`echo "$os" | sed -e 's|sunos5|solaris2|'`
 		;;
 	-sunos6*)
-		os=`echo $os | sed -e 's|sunos6|solaris3|'`
+		os=`echo "$os" | sed -e 's|sunos6|solaris3|'`
 		;;
 	-opened*)
 		os=-openedition
@@ -1424,12 +1437,6 @@ case $os in
 	-wince*)
 		os=-wince
 		;;
-	-osfrose*)
-		os=-osfrose
-		;;
-	-osf*)
-		os=-osf
-		;;
 	-utek*)
 		os=-bsd
 		;;
@@ -1454,7 +1461,7 @@ case $os in
 	-nova*)
 		os=-rtmk-nova
 		;;
-	-ns2 )
+	-ns2)
 		os=-nextstep2
 		;;
 	-nsk*)
@@ -1476,7 +1483,7 @@ case $os in
 	-oss*)
 		os=-sysv3
 		;;
-	-svr4)
+	-svr4*)
 		os=-sysv4
 		;;
 	-svr3)
@@ -1491,35 +1498,38 @@ case $os in
 	-ose*)
 		os=-ose
 		;;
-	-es1800*)
-		os=-ose
-		;;
-	-xenix)
-		os=-xenix
-		;;
 	-*mint | -mint[0-9]* | -*MiNT | -MiNT[0-9]*)
 		os=-mint
 		;;
-	-aros*)
-		os=-aros
-		;;
-	-kaos*)
-		os=-kaos
-		;;
 	-zvmoe)
 		os=-zvmoe
 		;;
 	-dicos*)
 		os=-dicos
 		;;
+	-pikeos*)
+		# Until real need of OS specific support for
+		# particular features comes up, bare metal
+		# configurations are quite functional.
+		case $basic_machine in
+		    arm*)
+			os=-eabi
+			;;
+		    *)
+			os=-elf
+			;;
+		esac
+		;;
 	-nacl*)
 		;;
+	-ios)
+		;;
 	-none)
 		;;
 	*)
 		# Get rid of the `-' at the beginning of $os.
 		os=`echo $os | sed 's/[^-]*-//'`
-		echo Invalid configuration \`$1\': system \`$os\' not recognized 1>&2
+		echo Invalid configuration \`"$1"\': system \`"$os"\' not recognized 1>&2
 		exit 1
 		;;
 esac
@@ -1554,6 +1564,9 @@ case $basic_machine in
 	c4x-* | tic4x-*)
 		os=-coff
 		;;
+	c8051-*)
+		os=-elf
+		;;
 	hexagon-*)
 		os=-elf
 		;;
@@ -1606,12 +1619,12 @@ case $basic_machine in
 	sparc-* | *-sun)
 		os=-sunos4.1.1
 		;;
+	pru-*)
+		os=-elf
+		;;
 	*-be)
 		os=-beos
 		;;
-	*-haiku)
-		os=-haiku
-		;;
 	*-ibm)
 		os=-aix
 		;;
@@ -1651,7 +1664,7 @@ case $basic_machine in
 	m88k-omron*)
 		os=-luna
 		;;
-	*-next )
+	*-next)
 		os=-nextstep
 		;;
 	*-sequent)
@@ -1666,9 +1679,6 @@ case $basic_machine in
 	i370-*)
 		os=-mvs
 		;;
-	*-next)
-		os=-nextstep3
-		;;
 	*-gould)
 		os=-sysv
 		;;
@@ -1778,15 +1788,15 @@ case $basic_machine in
 				vendor=stratus
 				;;
 		esac
-		basic_machine=`echo $basic_machine | sed "s/unknown/$vendor/"`
+		basic_machine=`echo "$basic_machine" | sed "s/unknown/$vendor/"`
 		;;
 esac
 
-echo $basic_machine$os
+echo "$basic_machine$os"
 exit
 
 # Local variables:
-# eval: (add-hook 'write-file-hooks 'time-stamp)
+# eval: (add-hook 'write-file-functions 'time-stamp)
 # time-stamp-start: "timestamp='"
 # time-stamp-format: "%:y-%02m-%02d"
 # time-stamp-end: "'"
diff --git a/configure b/configure
index 433dbc88..a5f0ca70 100755
--- a/configure
+++ b/configure
@@ -396,7 +396,7 @@ NL="
 # list of all preprocessor HAVE values we can define
 CONFIG_HAVE="MALLOC_H ALTIVEC ALTIVEC_H MMX ARMV6 ARMV6T2 NEON BEOSTHREAD POSIXTHREAD WIN32THREAD THREAD LOG2F SWSCALE \
              LAVF FFMS GPAC AVS GPL VECTOREXT INTERLACED CPU_COUNT OPENCL THP LSMASH X86_INLINE_ASM AS_FUNC INTEL_DISPATCHER \
-             MSA MMAP WINRT VSX ARM_INLINE_ASM STRTOK_R BITDEPTH8 BITDEPTH10"
+             MSA LSX LASX MMAP WINRT VSX ARM_INLINE_ASM STRTOK_R BITDEPTH8 BITDEPTH10"
 
 # parse options
 
@@ -788,6 +788,11 @@ case $host_cpu in
         AS="${AS-${CC}}"
         AS_EXT=".c"
         ;;
+    loongarch*)
+        ARCH="LOONGARCH"
+        AS="${AS-${CC}}"
+        AS_EXT=".c"
+        ;;
     arm*)
         ARCH="ARM"
         if [ "$SYS" = MACOSX ] ; then
@@ -882,7 +887,7 @@ if [ $compiler_style = GNU ]; then
     fi
 fi
 
-if [ $shared = yes -a \( $ARCH = "X86_64" -o $ARCH = "PPC" -o $ARCH = "ALPHA" -o $ARCH = "ARM" -o $ARCH = "IA64" -o $ARCH = "PARISC" -o $ARCH = "MIPS" -o $ARCH = "AARCH64" \) ] ; then
+if [ $shared = yes -a \( $ARCH = "X86_64" -o $ARCH = "PPC" -o $ARCH = "ALPHA" -o $ARCH = "ARM" -o $ARCH = "IA64" -o $ARCH = "PARISC" -o $ARCH = "MIPS" -o $ARCH = "AARCH64" -o $ARCH = "LOONGARCH" \) ] ; then
     pic="yes"
 fi
 
@@ -967,11 +972,22 @@ if [ $asm = auto -a \( $ARCH = ARM -o $ARCH = AARCH64 \) ] ; then
     as_check ".func test${NL}.endfunc" && define HAVE_AS_FUNC 1
 fi
 
+if [ $asm = auto -a $ARCH = LOONGARCH ] ; then
+    if cc_check '' '-mlsx' '__asm__("vadd.b $vr0, $vr1, $vr2");' ; then
+        LSX_CFLAGS="-mlsx"
+        define HAVE_LSX
+    fi
+    if cc_check '' '-mlasx' '__asm__("xvadd.b $xr0, $xr1, $xr2");' ; then
+        LASX_CFLAGS="-mlasx"
+        define HAVE_LASX
+    fi
+fi
+
 if [ $asm = auto -a $ARCH = MIPS ] ; then
     if cc_check 'msa.h' '-mmsa -mfp64 -mhard-float' '__asm__("addvi.b $w0, $w1, 1");' ; then
-        MSA_CFLAGS="-mmsa -mfp64 -mhard-float"             
+        MSA_CFLAGS="-mmsa -mfp64 -mhard-float"
         define HAVE_MSA
-    fi 
+    fi
 fi
 
 [ $asm = no ] && AS=""
@@ -1435,6 +1451,8 @@ SYS=$SYS
 CC=$CC
 CFLAGS=$CFLAGS
 MSA_CFLAGS=$MSA_CFLAGS
+LSX_CFLAGS=$LSX_CFLAGS
+LASX_CFLAGS=$LASX_CFLAGS
 COMPILER=$compiler
 COMPILER_STYLE=$compiler_style
 DEPMM=$DEPMM
@@ -1563,7 +1581,7 @@ cat conftest.log >> config.log
 cat conftest.log
 
 [ "$SRCPATH" != "." ] && ln -sf ${SRCPATH}/Makefile ./Makefile
-mkdir -p common/{aarch64,arm,mips,ppc,x86} encoder extras filters/video input output tools
+mkdir -p common/{aarch64,arm,mips,ppc,x86,loongarch} encoder extras filters/video input output tools
 
 echo
 echo "You can run 'make' or 'make fprofiled' now."
diff --git a/x264.h b/x264.h
index 77b6b8af..4b0e486f 100644
--- a/x264.h
+++ b/x264.h
@@ -164,6 +164,10 @@ typedef struct x264_nal_t
 /* MIPS */
 #define X264_CPU_MSA             0x0000001  /* MIPS MSA */
 
+/* LOONGARCH */
+#define X264_CPU_LSX             0x0000001  /* LOONGARCH LSX */
+#define X264_CPU_LASX            0x0000002  /* LOONGARCH LASX */
+
 /* Analyse flags */
 #define X264_ANALYSE_I4x4       0x0001  /* Analyse i4x4 */
 #define X264_ANALYSE_I8x8       0x0002  /* Analyse i8x8 (requires 8x8 transform) */
-- 
2.20.1

