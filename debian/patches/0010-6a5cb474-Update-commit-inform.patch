diff --git a/common/dct.c b/common/dct.c
index 14417700..a24a0f21 100644
--- a/common/dct.c
+++ b/common/dct.c
@@ -739,9 +739,11 @@ void x264_dct_init( int cpu, x264_dct_function_t *dctf )
         dctf->sub16x16_dct     = x264_sub16x16_dct_lasx;
         dctf->add4x4_idct      = x264_add4x4_idct_lasx;
         dctf->add8x8_idct      = x264_add8x8_idct_lasx;
+        dctf->add8x8_idct8     = x264_add8x8_idct8_lasx;
         dctf->add16x16_idct    = x264_add16x16_idct_lasx;
         dctf->sub8x8_dct8      = x264_sub8x8_dct8_lasx;
         dctf->sub16x16_dct8    = x264_sub16x16_dct8_lasx;
+        dctf->add8x8_idct_dc   = x264_add8x8_idct_dc_lasx;
     }
 #endif
 
diff --git a/common/loongarch/dct-c.c b/common/loongarch/dct-c.c
index 1a454442..38631969 100644
--- a/common/loongarch/dct-c.c
+++ b/common/loongarch/dct-c.c
@@ -23,16 +23,15 @@
  * This program is also available under a commercial proprietary license.
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
-
 #include "common/common.h"
-#include "generic_macros_lasx.h"
+#include "loongson_intrinsics.h"
 #include "dct.h"
 
 #if !HIGH_BIT_DEPTH
 
 #define LASX_LD4x4( p_src, out0, out1, out2, out3 )     \
 {                                                       \
-    out0 = LASX_LD( p_src );                            \
+    out0 = __lasx_xvld( p_src, 0 );                     \
     out1 = __lasx_xvpermi_d( out0, 0x55 );              \
     out2 = __lasx_xvpermi_d( out0, 0xAA );              \
     out3 = __lasx_xvpermi_d( out0, 0xFF );              \
@@ -49,8 +48,8 @@
     tmp3_m = __lasx_xvsrai_h( in3, 1 );                                     \
     tmp3_m = __lasx_xvadd_h( in1, tmp3_m );                                 \
                                                                             \
-    LASX_BUTTERFLY_4( v16i16, tmp0_m, tmp1_m, tmp2_m, tmp3_m,               \
-                      out0, out1, out2, out3 );                             \
+    LASX_BUTTERFLY_4_H( tmp0_m, tmp1_m, tmp2_m, tmp3_m,                     \
+                        out0, out1, out2, out3 );                           \
 }
 
 #define LASX_ADDBLK_ST4x4_128SV( in0, in1, in2, in3, p_dst, stride )        \
@@ -62,7 +61,7 @@
     __m256i dst1_m = __lasx_xvldi( 0 );                                     \
     __m256i zero_m = __lasx_xvldi( 0 );                                     \
                                                                             \
-    LASX_ILVL_D_2_128SV( in1, in0, in3, in2, inp0_m, inp1_m )               \
+    DUP2_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, inp0_m, inp1_m );       \
     src0_m = *( uint32_t* )( p_dst );                                       \
     p_dst0 = p_dst + stride;                                                \
     src1_m = *( uint32_t* )( p_dst0 );                                      \
@@ -74,19 +73,21 @@
     dst0_m = __lasx_xvinsgr2vr_w( dst0_m, src1_m, 1 );                      \
     dst1_m = __lasx_xvinsgr2vr_w( dst1_m, src2_m, 0 );                      \
     dst1_m = __lasx_xvinsgr2vr_w( dst1_m, src3_m, 1 );                      \
-    LASX_ILVL_B_2_128SV( zero_m, dst0_m, zero_m, dst1_m, res0_m, res1_m );  \
+    DUP2_ARG2( __lasx_xvilvl_b, zero_m, dst0_m, zero_m, dst1_m, res0_m,     \
+               res1_m );                                                    \
     res0_m = __lasx_xvadd_h( res0_m, inp0_m );                              \
     res1_m = __lasx_xvadd_h( res1_m, inp1_m );                              \
-    LASX_CLIP_H_0_255_2( res0_m, res1_m, res0_m, res1_m );                  \
-    LASX_PCKEV_B_2_128SV( res0_m, res0_m, res1_m, res1_m, dst0_m, dst1_m ); \
+    DUP2_ARG1( __lasx_xvclip255_h, res0_m, res1_m, res0_m, res1_m );        \
+    DUP2_ARG2( __lasx_xvpickev_b, res0_m, res0_m, res1_m, res1_m, dst0_m,   \
+               dst1_m );                                                    \
                                                                             \
-    LASX_ST_W( dst0_m, 0, p_dst );                                          \
+    __lasx_xvstelm_w( dst0_m, p_dst, 0, 0 );                                \
     p_dst0 = p_dst + stride;                                                \
-    LASX_ST_W( dst0_m, 1, p_dst0 );                                         \
+    __lasx_xvstelm_w( dst0_m, p_dst0, 0, 1 );                               \
     p_dst0 += stride;                                                       \
-    LASX_ST_W( dst1_m, 0, p_dst0 );                                         \
+    __lasx_xvstelm_w( dst1_m, p_dst0, 0, 0 );                               \
     p_dst0 += stride;                                                       \
-    LASX_ST_W( dst1_m, 1, p_dst0 );                                         \
+    __lasx_xvstelm_w( dst1_m, p_dst0, 0, 1 );                               \
 }
 
 static void avc_sub4x4_dct_lasx( uint8_t *p_src, int32_t i_src_stride,
@@ -121,12 +122,12 @@ static void avc_sub4x4_dct_lasx( uint8_t *p_src, int32_t i_src_stride,
     ref1 = __lasx_xvpackev_w( ref3, ref2 );
     ref0 = __lasx_xvpackev_d( ref1, ref0 );
 
-    LASX_ILVLH_B_128SV( src0, ref0, inp1, inp0 );
-    LASX_HSUB_UB_2( inp0, inp1, diff0, diff2 );
-    LASX_ILVH_D_2_128SV( diff0, diff0, diff2, diff2, diff1, diff3 );
+    inp0 = __lasx_xvilvl_b( src0, ref0 );
+    inp1 = __lasx_xvilvh_b( src0, ref0 );
+    DUP2_ARG2( __lasx_xvhsubw_hu_bu, inp0, inp0, inp1, inp1, diff0, diff2 );
+    DUP2_ARG2( __lasx_xvilvh_d, diff0, diff0, diff2, diff2, diff1, diff3 );
 
-    LASX_BUTTERFLY_4( v16i16, diff0, diff1, diff2, diff3,
-                      temp0, temp1, temp2, temp3 );
+    LASX_BUTTERFLY_4_H( diff0, diff1, diff2, diff3, temp0, temp1, temp2, temp3 );
 
     diff0 = __lasx_xvadd_h( temp0, temp1);
     tmp = __lasx_xvslli_h( temp3, 1);
@@ -135,10 +136,8 @@ static void avc_sub4x4_dct_lasx( uint8_t *p_src, int32_t i_src_stride,
     tmp = __lasx_xvslli_h( temp2, 1);
     diff3 = __lasx_xvsub_h( temp3, tmp );
 
-    LASX_TRANSPOSE4x4_H_128SV( diff0, diff1, diff2, diff3,
-                               temp0, temp1, temp2, temp3 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp2, temp3,
-                      diff0, diff1, diff2, diff3 );
+    LASX_TRANSPOSE4x4_H( diff0, diff1, diff2, diff3, temp0, temp1, temp2, temp3 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp2, temp3, diff0, diff1, diff2, diff3 );
 
     temp0 = __lasx_xvadd_h( diff0, diff1);
     tmp = __lasx_xvslli_h( diff3, 1);
@@ -147,9 +146,9 @@ static void avc_sub4x4_dct_lasx( uint8_t *p_src, int32_t i_src_stride,
     tmp = __lasx_xvslli_h( diff2, 1);
     temp3 = __lasx_xvsub_h( diff3, tmp );
 
-    LASX_ILVL_D_2_128SV( temp1, temp0, temp3, temp2, inp0, inp1 );
+    DUP2_ARG2( __lasx_xvilvl_d, temp1, temp0, temp3, temp2, inp0, inp1 );
     inp0 = __lasx_xvpermi_q(inp1, inp0, 0x20);
-    LASX_ST( inp0, p_dst );
+    __lasx_xvst( inp0, p_dst, 0 );
 }
 
 void x264_sub4x4_dct_lasx( int16_t p_dst[16], uint8_t *p_src,
@@ -194,11 +193,11 @@ static void avc_idct4x4_addblk_lasx( uint8_t *p_dst, int16_t *p_src,
 
     LASX_LD4x4( p_src, src0, src1, src2, src3 );
     LASX_ITRANS_H( src0, src1, src2, src3, hres0, hres1, hres2, hres3 );
-    LASX_TRANSPOSE4x4_H_128SV( hres0, hres1, hres2, hres3,
-                               hres0, hres1, hres2, hres3 );
+    LASX_TRANSPOSE4x4_H( hres0, hres1, hres2, hres3, hres0, hres1, hres2, hres3 );
     LASX_ITRANS_H( hres0, hres1, hres2, hres3, vres0, vres1, vres2, vres3 );
-    LASX_SRARI_H_4( vres0, vres1, vres2, vres3,
-                    vres0, vres1, vres2, vres3, 6 );
+
+    DUP4_ARG2( __lasx_xvsrari_h, vres0, 6, vres1, 6, vres2, 6, vres3, 6,
+               vres0, vres1, vres2, vres3 );
     LASX_ADDBLK_ST4x4_128SV( vres0, vres1, vres2, vres3, p_dst, i_dst_stride );
 }
 
@@ -207,6 +206,199 @@ void x264_add4x4_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16] )
     avc_idct4x4_addblk_lasx( p_dst, pi_dct, FDEC_STRIDE );
 }
 
+void x264_add8x8_idct8_lasx( uint8_t *dst, int16_t dct[64] )
+{
+    int32_t stride2, stride3, stride4;
+    uint8_t* dst_tmp;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m256i reg0, reg1, reg2, reg3, reg4, reg5, reg6, reg7;
+    __m256i shift = {0x0000000400000000, 0x0000000500000001,
+                     0x0000000600000002, 0x0000000700000003};
+
+    dct[0] += 32;
+    stride2 = FDEC_STRIDE << 1;
+    stride3 = FDEC_STRIDE + stride2;
+    stride4 = stride2 << 1;
+    dst_tmp = dst + stride4;
+
+    src0 = __lasx_xvld(dct, 0);
+    src2 = __lasx_xvld(dct, 32);
+    src4 = __lasx_xvld(dct, 64);
+    src6 = __lasx_xvld(dct, 96);
+    src1 = __lasx_xvpermi_d(src0, 0x4E);
+    src3 = __lasx_xvpermi_d(src2, 0x4E);
+    src5 = __lasx_xvpermi_d(src4, 0x4E);
+    src7 = __lasx_xvpermi_d(src6, 0x4E);
+
+    src0 = __lasx_vext2xv_w_h(src0);
+    src1 = __lasx_vext2xv_w_h(src1);
+    src2 = __lasx_vext2xv_w_h(src2);
+    src3 = __lasx_vext2xv_w_h(src3);
+    src4 = __lasx_vext2xv_w_h(src4);
+    src5 = __lasx_vext2xv_w_h(src5);
+    src6 = __lasx_vext2xv_w_h(src6);
+    src7 = __lasx_vext2xv_w_h(src7);
+
+    tmp0 = __lasx_xvadd_w(src0, src4);
+    tmp2 = __lasx_xvsub_w(src0, src4);
+    tmp4 = __lasx_xvsrai_w(src2, 1);
+    tmp4 = __lasx_xvsub_w(tmp4, src6);
+    tmp6 = __lasx_xvsrai_w(src6, 1);
+    tmp6 = __lasx_xvadd_w(tmp6, src2);
+    reg7 = __lasx_xvsrai_w(src7, 1);
+    reg3 = __lasx_xvsrai_w(src3, 1);
+    reg5 = __lasx_xvsrai_w(src5, 1);
+    reg1 = __lasx_xvsrai_w(src1, 1);
+    tmp1 = __lasx_xvsub_w(src5, src3);
+    tmp3 = __lasx_xvadd_w(src1, src7);
+    tmp5 = __lasx_xvsub_w(src7, src1);
+    tmp7 = __lasx_xvadd_w(src3, src5);
+    reg7 = __lasx_xvadd_w(src7, reg7);
+    reg3 = __lasx_xvadd_w(src3, reg3);
+    reg5 = __lasx_xvadd_w(reg5, src5);
+    reg1 = __lasx_xvadd_w(reg1, src1);
+    tmp1 = __lasx_xvsub_w(tmp1, reg7);
+    tmp3 = __lasx_xvsub_w(tmp3, reg3);
+    tmp5 = __lasx_xvadd_w(reg5, tmp5);
+    tmp7 = __lasx_xvadd_w(tmp7, reg1);
+    reg0 = __lasx_xvadd_w(tmp0, tmp6);
+    reg2 = __lasx_xvadd_w(tmp2, tmp4);
+    reg4 = __lasx_xvsub_w(tmp2, tmp4);
+    reg6 = __lasx_xvsub_w(tmp0, tmp6);
+    reg1 = __lasx_xvsrai_w(tmp7, 2);
+    reg3 = __lasx_xvsrai_w(tmp5, 2);
+    reg5 = __lasx_xvsrai_w(tmp3, 2);
+    reg7 = __lasx_xvsrai_w(tmp1, 2);
+    reg1 = __lasx_xvadd_w(tmp1, reg1);
+    reg3 = __lasx_xvadd_w(tmp3, reg3);
+    reg5 = __lasx_xvsub_w(reg5, tmp5);
+    reg7 = __lasx_xvsub_w(tmp7, reg7);
+
+    src0 = __lasx_xvadd_w(reg0, reg7);
+    src1 = __lasx_xvadd_w(reg2, reg5);
+    src2 = __lasx_xvadd_w(reg4, reg3);
+    src3 = __lasx_xvadd_w(reg6, reg1);
+    src4 = __lasx_xvsub_w(reg6, reg1);
+    src5 = __lasx_xvsub_w(reg4, reg3);
+    src6 = __lasx_xvsub_w(reg2, reg5);
+    src7 = __lasx_xvsub_w(reg0, reg7);
+
+    LASX_TRANSPOSE8x8_W(src0, src1, src2, src3, src4, src5, src6, src7,
+                        src0, src1, src2, src3, src4, src5, src6, src7);
+
+    tmp0 = __lasx_xvaddwev_w_h(src0, src4);
+    tmp2 = __lasx_xvsubwev_w_h(src0, src4);
+    tmp4 = __lasx_xvsrai_h(src2, 1);
+    tmp4 = __lasx_xvsubwev_w_h(tmp4, src6);
+    tmp6 = __lasx_xvsrai_h(src6, 1);
+    tmp6 = __lasx_xvaddwev_w_h(tmp6, src2);
+    reg7 = __lasx_xvsrai_h(src7, 1);
+    reg3 = __lasx_xvsrai_h(src3, 1);
+    reg5 = __lasx_xvsrai_h(src5, 1);
+    reg1 = __lasx_xvsrai_h(src1, 1);
+    tmp1 = __lasx_xvsubwev_w_h(src5, src3);
+    tmp3 = __lasx_xvaddwev_w_h(src1, src7);
+    tmp5 = __lasx_xvsubwev_w_h(src7, src1);
+    tmp7 = __lasx_xvaddwev_w_h(src3, src5);
+    reg7 = __lasx_xvaddwev_w_h(src7, reg7);
+    reg3 = __lasx_xvaddwev_w_h(src3, reg3);
+    reg5 = __lasx_xvaddwev_w_h(reg5, src5);
+    reg1 = __lasx_xvaddwev_w_h(reg1, src1);
+
+    tmp1 = __lasx_xvsub_w(tmp1, reg7);
+    tmp3 = __lasx_xvsub_w(tmp3, reg3);
+    tmp5 = __lasx_xvadd_w(reg5, tmp5);
+    tmp7 = __lasx_xvadd_w(tmp7, reg1);
+    reg0 = __lasx_xvadd_w(tmp0, tmp6);
+    reg2 = __lasx_xvadd_w(tmp2, tmp4);
+    reg4 = __lasx_xvsub_w(tmp2, tmp4);
+    reg6 = __lasx_xvsub_w(tmp0, tmp6);
+    reg1 = __lasx_xvsrai_w(tmp7, 2);
+    reg3 = __lasx_xvsrai_w(tmp5, 2);
+    reg5 = __lasx_xvsrai_w(tmp3, 2);
+    reg7 = __lasx_xvsrai_w(tmp1, 2);
+    reg1 = __lasx_xvadd_w(tmp1, reg1);
+    reg3 = __lasx_xvadd_w(tmp3, reg3);
+    reg5 = __lasx_xvsub_w(reg5, tmp5);
+    reg7 = __lasx_xvsub_w(tmp7, reg7);
+    src0 = __lasx_xvadd_w(reg0, reg7);
+    src1 = __lasx_xvadd_w(reg2, reg5);
+    src2 = __lasx_xvadd_w(reg4, reg3);
+    src3 = __lasx_xvadd_w(reg6, reg1);
+    src4 = __lasx_xvsub_w(reg6, reg1);
+    src5 = __lasx_xvsub_w(reg4, reg3);
+    src6 = __lasx_xvsub_w(reg2, reg5);
+    src7 = __lasx_xvsub_w(reg0, reg7);
+
+    src0 = __lasx_xvsrai_w(src0, 6);
+    src1 = __lasx_xvsrai_w(src1, 6);
+    src2 = __lasx_xvsrai_w(src2, 6);
+    src3 = __lasx_xvsrai_w(src3, 6);
+    src4 = __lasx_xvsrai_w(src4, 6);
+    src5 = __lasx_xvsrai_w(src5, 6);
+    src6 = __lasx_xvsrai_w(src6, 6);
+    src7 = __lasx_xvsrai_w(src7, 6);
+
+    reg0 = __lasx_xvld(dst, 0);
+    reg1 = __lasx_xvld(dst, FDEC_STRIDE);
+    reg2 = __lasx_xvldx(dst, stride2);
+    reg3 = __lasx_xvldx(dst, stride3);
+    reg4 = __lasx_xvld(dst_tmp, 0);
+    reg5 = __lasx_xvld(dst_tmp, FDEC_STRIDE);
+    reg6 = __lasx_xvldx(dst_tmp, stride2);
+    reg7 = __lasx_xvldx(dst_tmp, stride3);
+
+    reg0 = __lasx_vext2xv_wu_bu(reg0);
+    reg1 = __lasx_vext2xv_wu_bu(reg1);
+    reg2 = __lasx_vext2xv_wu_bu(reg2);
+    reg3 = __lasx_vext2xv_wu_bu(reg3);
+    reg4 = __lasx_vext2xv_wu_bu(reg4);
+    reg5 = __lasx_vext2xv_wu_bu(reg5);
+    reg6 = __lasx_vext2xv_wu_bu(reg6);
+    reg7 = __lasx_vext2xv_wu_bu(reg7);
+    reg0 = __lasx_xvadd_w(reg0, src0);
+    reg1 = __lasx_xvadd_w(reg1, src1);
+    reg2 = __lasx_xvadd_w(reg2, src2);
+    reg3 = __lasx_xvadd_w(reg3, src3);
+    reg4 = __lasx_xvadd_w(reg4, src4);
+    reg5 = __lasx_xvadd_w(reg5, src5);
+    reg6 = __lasx_xvadd_w(reg6, src6);
+    reg7 = __lasx_xvadd_w(reg7, src7);
+
+    reg0 = __lasx_xvmaxi_w(reg0, 0);
+    reg1 = __lasx_xvmaxi_w(reg1, 0);
+    reg2 = __lasx_xvmaxi_w(reg2, 0);
+    reg3 = __lasx_xvmaxi_w(reg3, 0);
+    reg4 = __lasx_xvmaxi_w(reg4, 0);
+    reg5 = __lasx_xvmaxi_w(reg5, 0);
+    reg6 = __lasx_xvmaxi_w(reg6, 0);
+    reg7 = __lasx_xvmaxi_w(reg7, 0);
+    src0 = __lasx_xvssrlni_hu_w(reg1, reg0, 0);
+    src1 = __lasx_xvssrlni_hu_w(reg3, reg2, 0);
+    src2 = __lasx_xvssrlni_hu_w(reg5, reg4, 0);
+    src3 = __lasx_xvssrlni_hu_w(reg7, reg6, 0);
+    src0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
+    src1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
+    src0 = __lasx_xvperm_w(src0, shift);
+    src1 = __lasx_xvperm_w(src1, shift);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, dst, 0, 1);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, dst, 0, 3);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, dst, 0, 1);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, dst, 0, 2);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, dst, 0, 3);
+}
+
 void x264_add8x8_idct_lasx( uint8_t *p_dst, int16_t pi_dct[4][16] )
 {
     avc_idct4x4_addblk_lasx( &p_dst[0], &pi_dct[0][0], FDEC_STRIDE );
@@ -225,6 +417,69 @@ void x264_add16x16_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16][16] )
     x264_add8x8_idct_lasx( &p_dst[8 * FDEC_STRIDE + 8], &pi_dct[12] );
 }
 
+void x264_add8x8_idct_dc_lasx( uint8_t *pdst, int16_t dct[4] )
+{
+    int32_t stride2 = FDEC_STRIDE << 1;
+    int32_t stride3 = FDEC_STRIDE + stride2;
+    int32_t stride4 = stride2 << 1;
+    uint8_t *pdst_tmp = pdst + stride4;
+    __m256i vec_dct, vec_dct0, vec_dct1;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i const_32 = __lasx_xvldi(0x420);
+
+    vec_dct = __lasx_xvldrepl_d(dct, 0);
+    vec_dct = __lasx_xvadd_h(vec_dct, const_32);
+    vec_dct = __lasx_xvsrai_h(vec_dct, 6);
+    vec_dct = __lasx_xvilvl_h(vec_dct, vec_dct);
+    vec_dct0 = __lasx_xvilvl_w(vec_dct, vec_dct);
+    vec_dct1 = __lasx_xvilvh_w(vec_dct, vec_dct);
+
+    src0 = __lasx_xvld(pdst, 0);
+    src1 = __lasx_xvld(pdst, FDEC_STRIDE);
+    src2 = __lasx_xvldx(pdst, stride2);
+    src3 = __lasx_xvldx(pdst, stride3);
+    src4 = __lasx_xvld(pdst_tmp, 0);
+    src5 = __lasx_xvld(pdst_tmp, FDEC_STRIDE);
+    src6 = __lasx_xvldx(pdst_tmp, stride2);
+    src7 = __lasx_xvldx(pdst_tmp, stride3);
+
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src1 = __lasx_xvilvl_d(src3, src2);
+    src2 = __lasx_xvilvl_d(src5, src4);
+    src3 = __lasx_xvilvl_d(src7, src6);
+
+    src0 = __lasx_vext2xv_hu_bu(src0);
+    src1 = __lasx_vext2xv_hu_bu(src1);
+    src2 = __lasx_vext2xv_hu_bu(src2);
+    src3 = __lasx_vext2xv_hu_bu(src3);
+
+    src0 = __lasx_xvadd_h(src0, vec_dct0);
+    src1 = __lasx_xvadd_h(src1, vec_dct0);
+    src2 = __lasx_xvadd_h(src2, vec_dct1);
+    src3 = __lasx_xvadd_h(src3, vec_dct1);
+
+    src0 = __lasx_xvmaxi_h(src0, 0);
+    src1 = __lasx_xvmaxi_h(src1, 0);
+    src2 = __lasx_xvmaxi_h(src2, 0);
+    src3 = __lasx_xvmaxi_h(src3, 0);
+    src0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
+    src1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
+    __lasx_xvstelm_d(src0, pdst, 0, 0);
+    pdst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, pdst, 0, 2);
+    pdst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, pdst, 0, 1);
+    pdst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, pdst, 0, 3);
+    __lasx_xvstelm_d(src1, pdst_tmp, 0, 0);
+    pdst_tmp += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, pdst_tmp, 0, 2);
+    pdst_tmp += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, pdst_tmp, 0, 1);
+    pdst_tmp += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, pdst_tmp, 0, 3);
+}
+
 /****************************************************************************
  * 8x8 transform:
  ****************************************************************************/
@@ -239,15 +494,14 @@ void x264_sub8x8_dct8_lasx( int16_t pi_dct[64], uint8_t *p_pix1,
     __m256i s07, s16, s25, s34, d07, d16, d25, d34;
     __m256i a0, a1, a2, a3, a4, a5, a6, a7;
 
-#define LOAD_PIX_DATA_2(data1, data2)             \
-    LASX_LD_2( p_pix1, FENC_STRIDE, src0, src1 ); \
-    LASX_LD_2( p_pix2, FDEC_STRIDE, src2, src3 ); \
-    LASX_ILVL_B_4_128SV( zero, src0, zero, src1,  \
-                         zero, src2, zero, src3,  \
-                         src0, src1, src2, src3 );\
-    data1 = __lasx_xvsub_h( src0, src2 );         \
-    data2 = __lasx_xvsub_h( src1, src3 );         \
-    p_pix1 += ( FENC_STRIDE << 1 );               \
+#define LOAD_PIX_DATA_2(data1, data2)                                     \
+    DUP2_ARG2( __lasx_xvld, p_pix1, 0, p_pix1, FENC_STRIDE, src0, src1 ); \
+    DUP2_ARG2( __lasx_xvld, p_pix2, 0, p_pix2, FDEC_STRIDE, src2, src3 ); \
+    DUP4_ARG2( __lasx_xvilvl_b, zero, src0, zero, src1, zero, src2, zero, \
+               src3, src0, src1, src2, src3 );                            \
+    data1 = __lasx_xvsub_h( src0, src2 );                                 \
+    data2 = __lasx_xvsub_h( src1, src3 );                                 \
+    p_pix1 += ( FENC_STRIDE << 1 );                                       \
     p_pix2 += ( FDEC_STRIDE << 1 );
 
     LOAD_PIX_DATA_2(tmp0, tmp1);
@@ -308,20 +562,28 @@ void x264_sub8x8_dct8_lasx( int16_t pi_dct[64], uint8_t *p_pix1,
     tmp7 = __lasx_xvsub_h( temp, a7 );
 
     LASX_DCT8_1D;
-    LASX_TRANSPOSE8x8_H_128SV( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
-                               tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    LASX_TRANSPOSE8x8_H( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                         tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
     LASX_DCT8_1D;
 
 #undef LASX_DCT8_1D
 
-    LASX_ST_D_2( tmp0, 0, 1, &pi_dct[0], 4 );
-    LASX_ST_D_2( tmp1, 0, 1, &pi_dct[8], 4 );
-    LASX_ST_D_2( tmp2, 0, 1, &pi_dct[16], 4 );
-    LASX_ST_D_2( tmp3, 0, 1, &pi_dct[24], 4 );
-    LASX_ST_D_2( tmp4, 0, 1, &pi_dct[32], 4 );
-    LASX_ST_D_2( tmp5, 0, 1, &pi_dct[40], 4 );
-    LASX_ST_D_2( tmp6, 0, 1, &pi_dct[48], 4 );
-    LASX_ST_D_2( tmp7, 0, 1, &pi_dct[56], 4 );
+    __lasx_xvstelm_d( tmp0, &pi_dct[0], 0, 0 );
+    __lasx_xvstelm_d( tmp0, &pi_dct[4], 0, 1 );
+    __lasx_xvstelm_d( tmp1, &pi_dct[8], 0, 0 );
+    __lasx_xvstelm_d( tmp1, &pi_dct[12], 0, 1 );
+    __lasx_xvstelm_d( tmp2, &pi_dct[16], 0, 0 );
+    __lasx_xvstelm_d( tmp2, &pi_dct[20], 0, 1 );
+    __lasx_xvstelm_d( tmp3, &pi_dct[24], 0, 0 );
+    __lasx_xvstelm_d( tmp3, &pi_dct[28], 0, 1 );
+    __lasx_xvstelm_d( tmp4, &pi_dct[32], 0, 0 );
+    __lasx_xvstelm_d( tmp4, &pi_dct[36], 0, 1 );
+    __lasx_xvstelm_d( tmp5, &pi_dct[40], 0, 0 );
+    __lasx_xvstelm_d( tmp5, &pi_dct[44], 0, 1 );
+    __lasx_xvstelm_d( tmp6, &pi_dct[48], 0, 0 );
+    __lasx_xvstelm_d( tmp6, &pi_dct[52], 0, 1 );
+    __lasx_xvstelm_d( tmp7, &pi_dct[56], 0, 0 );
+    __lasx_xvstelm_d( tmp7, &pi_dct[60], 0, 1 );
 }
 
 static void x264_sub8x8_dct8_ext_lasx( int16_t pi_dct1[64],
@@ -335,20 +597,19 @@ static void x264_sub8x8_dct8_ext_lasx( int16_t pi_dct1[64],
     __m256i s07, s16, s25, s34, d07, d16, d25, d34;
     __m256i a0, a1, a2, a3, a4, a5, a6, a7;
 
-#define LOAD_PIX_DATA_2_EXT(data1, data2)         \
-    LASX_LD_2( p_pix1, FENC_STRIDE, src0, src1 ); \
-    LASX_LD_2( p_pix2, FDEC_STRIDE, src2, src3 ); \
-    src0 = __lasx_xvpermi_d( src0, 0x50 );        \
-    src1 = __lasx_xvpermi_d( src1, 0x50 );        \
-    src2 = __lasx_xvpermi_d( src2, 0x50 );        \
-    src3 = __lasx_xvpermi_d( src3, 0x50 );        \
-                                                  \
-    LASX_ILVL_B_4_128SV( zero, src0, zero, src1,  \
-                         zero, src2, zero, src3,  \
-                         src0, src1, src2, src3 );\
-    data1 = __lasx_xvsub_h( src0, src2 );         \
-    data2 = __lasx_xvsub_h( src1, src3 );         \
-    p_pix1 += ( FENC_STRIDE << 1 );               \
+#define LOAD_PIX_DATA_2_EXT(data1, data2)                                 \
+    DUP2_ARG2( __lasx_xvld, p_pix1, 0, p_pix1, FENC_STRIDE, src0, src1 ); \
+    DUP2_ARG2( __lasx_xvld, p_pix2, 0, p_pix2, FDEC_STRIDE, src2, src3 ); \
+    src0 = __lasx_xvpermi_d( src0, 0x50 );                                \
+    src1 = __lasx_xvpermi_d( src1, 0x50 );                                \
+    src2 = __lasx_xvpermi_d( src2, 0x50 );                                \
+    src3 = __lasx_xvpermi_d( src3, 0x50 );                                \
+                                                                          \
+    DUP4_ARG2( __lasx_xvilvl_b, zero, src0, zero, src1, zero, src2, zero, \
+               src3, src0, src1, src2, src3 );                            \
+    data1 = __lasx_xvsub_h( src0, src2 );                                 \
+    data2 = __lasx_xvsub_h( src1, src3 );                                 \
+    p_pix1 += ( FENC_STRIDE << 1 );                                       \
     p_pix2 += ( FDEC_STRIDE << 1 );
 
     LOAD_PIX_DATA_2_EXT(tmp0, tmp1);
@@ -409,29 +670,46 @@ static void x264_sub8x8_dct8_ext_lasx( int16_t pi_dct1[64],
     tmp7 = __lasx_xvsub_h( temp, a7 );
 
     LASX_DCT8_1D_EXT;
-    LASX_TRANSPOSE8x8_H_128SV( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
-                               tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    LASX_TRANSPOSE8x8_H( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                         tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
     LASX_DCT8_1D_EXT;
 
 #undef LASX_DCT8_1D_EXT
 
-    LASX_ST_D_2( tmp0, 0, 1, &pi_dct1[0], 4 );
-    LASX_ST_D_2( tmp1, 0, 1, &pi_dct1[8], 4 );
-    LASX_ST_D_2( tmp2, 0, 1, &pi_dct1[16], 4 );
-    LASX_ST_D_2( tmp3, 0, 1, &pi_dct1[24], 4 );
-    LASX_ST_D_2( tmp4, 0, 1, &pi_dct1[32], 4 );
-    LASX_ST_D_2( tmp5, 0, 1, &pi_dct1[40], 4 );
-    LASX_ST_D_2( tmp6, 0, 1, &pi_dct1[48], 4 );
-    LASX_ST_D_2( tmp7, 0, 1, &pi_dct1[56], 4 );
-
-    LASX_ST_D_2( tmp0, 2, 3, &pi_dct2[0], 4 );
-    LASX_ST_D_2( tmp1, 2, 3, &pi_dct2[8], 4 );
-    LASX_ST_D_2( tmp2, 2, 3, &pi_dct2[16], 4 );
-    LASX_ST_D_2( tmp3, 2, 3, &pi_dct2[24], 4 );
-    LASX_ST_D_2( tmp4, 2, 3, &pi_dct2[32], 4 );
-    LASX_ST_D_2( tmp5, 2, 3, &pi_dct2[40], 4 );
-    LASX_ST_D_2( tmp6, 2, 3, &pi_dct2[48], 4 );
-    LASX_ST_D_2( tmp7, 2, 3, &pi_dct2[56], 4 );
+    __lasx_xvstelm_d( tmp0, &pi_dct1[0], 0, 0 );
+    __lasx_xvstelm_d( tmp0, &pi_dct1[4], 0, 1 );
+    __lasx_xvstelm_d( tmp1, &pi_dct1[8], 0, 0 );
+    __lasx_xvstelm_d( tmp1, &pi_dct1[12], 0, 1 );
+    __lasx_xvstelm_d( tmp2, &pi_dct1[16], 0, 0 );
+    __lasx_xvstelm_d( tmp2, &pi_dct1[20], 0, 1 );
+    __lasx_xvstelm_d( tmp3, &pi_dct1[24], 0, 0 );
+    __lasx_xvstelm_d( tmp3, &pi_dct1[28], 0, 1 );
+    __lasx_xvstelm_d( tmp4, &pi_dct1[32], 0, 0 );
+    __lasx_xvstelm_d( tmp4, &pi_dct1[36], 0, 1 );
+    __lasx_xvstelm_d( tmp5, &pi_dct1[40], 0, 0 );
+    __lasx_xvstelm_d( tmp5, &pi_dct1[44], 0, 1 );
+    __lasx_xvstelm_d( tmp6, &pi_dct1[48], 0, 0 );
+    __lasx_xvstelm_d( tmp6, &pi_dct1[52], 0, 1 );
+    __lasx_xvstelm_d( tmp7, &pi_dct1[56], 0, 0 );
+    __lasx_xvstelm_d( tmp7, &pi_dct1[60], 0, 1 );
+
+    __lasx_xvstelm_d( tmp0, &pi_dct2[0], 0, 2 );
+    __lasx_xvstelm_d( tmp0, &pi_dct2[4], 0, 3 );
+    __lasx_xvstelm_d( tmp1, &pi_dct2[8], 0, 2 );
+    __lasx_xvstelm_d( tmp1, &pi_dct2[12], 0, 3 );
+    __lasx_xvstelm_d( tmp2, &pi_dct2[16], 0, 2 );
+    __lasx_xvstelm_d( tmp2, &pi_dct2[20], 0, 3 );
+    __lasx_xvstelm_d( tmp3, &pi_dct2[24], 0, 2 );
+    __lasx_xvstelm_d( tmp3, &pi_dct2[28], 0, 3 );
+    __lasx_xvstelm_d( tmp4, &pi_dct2[32], 0, 2 );
+    __lasx_xvstelm_d( tmp4, &pi_dct2[36], 0, 3 );
+    __lasx_xvstelm_d( tmp5, &pi_dct2[40], 0, 2 );
+    __lasx_xvstelm_d( tmp5, &pi_dct2[44], 0, 3 );
+    __lasx_xvstelm_d( tmp6, &pi_dct2[48], 0, 2 );
+    __lasx_xvstelm_d( tmp6, &pi_dct2[52], 0, 3 );
+    __lasx_xvstelm_d( tmp7, &pi_dct2[56], 0, 2 );
+    __lasx_xvstelm_d( tmp7, &pi_dct2[60], 0, 3 );
+
 }
 
 void x264_sub16x16_dct8_lasx( int16_t pi_dct[4][64], uint8_t *p_pix1,
diff --git a/common/loongarch/dct.h b/common/loongarch/dct.h
index 9b1d1003..8a0991e4 100644
--- a/common/loongarch/dct.h
+++ b/common/loongarch/dct.h
@@ -49,5 +49,9 @@ void x264_add4x4_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16] );
 void x264_add8x8_idct_lasx( uint8_t *p_dst, int16_t pi_dct[4][16] );
 #define x264_add16x16_idct_lasx x264_template(add16x16_idct_lasx)
 void x264_add16x16_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16][16] );
+#define x264_add8x8_idct8_lasx x264_template(add8x8_idct8_lasx)
+void x264_add8x8_idct8_lasx( uint8_t *p_dst, int16_t pi_dct[64] );
+#define x264_add8x8_idct_dc_lasx x264_template(add8x8_idct_dc_lasx)
+void x264_add8x8_idct_dc_lasx( uint8_t *p_dst, int16_t dct[4] );
 
 #endif
diff --git a/common/loongarch/deblock-c.c b/common/loongarch/deblock-c.c
index 1338cd24..b6912662 100644
--- a/common/loongarch/deblock-c.c
+++ b/common/loongarch/deblock-c.c
@@ -25,7 +25,7 @@
  *****************************************************************************/
 
 #include "common/common.h"
-#include "generic_macros_lasx.h"
+#include "loongson_intrinsics.h"
 #include "deblock.h"
 
 #if !HIGH_BIT_DEPTH
@@ -40,7 +40,7 @@
     temp = __lasx_xvslli_h( p1_or_q1_org_in, 1 );                    \
     clip0 = __lasx_xvsub_h( clip0, temp );                           \
     clip0 = __lasx_xvavg_h( p2_or_q2_org_in, clip0 );                \
-    LASX_CLIP_H( clip0, negate_tc_in, tc_in );                       \
+    clip0 = __lasx_xvclip_h( clip0, negate_tc_in, tc_in );           \
     p1_or_q1_out = __lasx_xvadd_h( p1_or_q1_org_in, clip0 );         \
 }
 
@@ -58,23 +58,27 @@
     delta = __lasx_xvadd_h( q0_sub_p0, p1_sub_q1 );                  \
     delta = __lasx_xvsrai_h( delta, 3 );                             \
                                                                      \
-    LASX_CLIP_H( delta, negate_threshold_in, threshold_in );         \
+    delta = __lasx_xvclip_h(delta, negate_threshold_in,              \
+            threshold_in);                                           \
                                                                      \
     p0_or_q0_out = __lasx_xvadd_h( p0_or_q0_org_in, delta );         \
     q0_or_p0_out = __lasx_xvsub_h( q0_or_p0_org_in, delta );         \
                                                                      \
-    LASX_CLIP_H_0_255_2( p0_or_q0_out, q0_or_p0_out,                 \
-                         p0_or_q0_out, q0_or_p0_out );               \
+    DUP2_ARG1( __lasx_xvclip255_h, p0_or_q0_out, q0_or_p0_out,       \
+               p0_or_q0_out, q0_or_p0_out );                         \
 }
 
 void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
                                int32_t i_alpha, int32_t i_beta, int8_t *p_tc0 )
 {
     uint8_t *p_src;
+    intptr_t i_stride_2x = ( i_stride << 1 );
+    intptr_t i_stride_4x = ( i_stride << 2 );
+    intptr_t i_stride_3x = i_stride_2x + i_stride;
     __m256i beta, bs, tc;
     __m256i zero = __lasx_xvldi( 0 );
 
-    tc = LASX_LD( p_tc0 );
+    tc = __lasx_xvld( p_tc0, 0 );
     tc = __lasx_xvilvl_b( tc, tc );
     tc = __lasx_xvilvl_h( tc, tc );
 
@@ -96,20 +100,25 @@ void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
 
         {
             p_src = p_pix - 4;
-
-            LASX_LD_8( p_src, i_stride,
-                       src0, src1, src2, src3, src4, src5, src6, src7 );
-            p_src += ( i_stride << 3 );
-            LASX_LD_8( p_src, i_stride,
-                       p2_org_l, p1_org_l, p0_org_l, q0_org_l,
-                       q1_org_l, q2_org_l, p2_org_h, p1_org_h );
-
-            LASX_TRANSPOSE16x8_B_128SV( src0, src1, src2, src3,
-                                        src4, src5, src6, src7,
-                                        p2_org_l, p1_org_l, p0_org_l, q0_org_l,
-                                        q1_org_l, q2_org_l, p2_org_h, p1_org_h,
-                                        p3_org, p2_org, p1_org, p0_org,
-                                        q0_org, q1_org, q2_org, q3_org );
+            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
+                       i_stride_3x, src0, src1, src2, src3 );
+            p_src += i_stride_4x;
+            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
+                       i_stride_3x, src4, src5, src6, src7 );
+            p_src += i_stride_4x;
+            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
+                       i_stride_3x, p2_org_l, p1_org_l, p0_org_l, q0_org_l );
+            p_src += i_stride_4x;
+            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
+                       i_stride_3x, q1_org_l, q2_org_l, p2_org_h, p1_org_h );
+            p_src -= i_stride_4x;
+
+            LASX_TRANSPOSE16x8_B( src0, src1, src2, src3,
+                                  src4, src5, src6, src7,
+                                  p2_org_l, p1_org_l, p0_org_l, q0_org_l,
+                                  q1_org_l, q2_org_l, p2_org_h, p1_org_h,
+                                  p3_org, p2_org, p1_org, p0_org,
+                                  q0_org, q1_org, q2_org, q3_org );
         }
         {
             src0 = __lasx_xvabsd_bu( p0_org, q0_org );
@@ -136,13 +145,17 @@ void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
             negate_tc = __lasx_xvsub_b( zero, tc );
             sign_negate_tc = __lasx_xvslti_b( negate_tc, 0 );
 
-            LASX_ILVLH_B_128SV( sign_negate_tc, negate_tc, i16_negatetc_h,
-                                negate_tc_l );
+            negate_tc_l = __lasx_xvilvl_b( sign_negate_tc, negate_tc );
+            i16_negatetc_h = __lasx_xvilvh_b( sign_negate_tc, negate_tc );
 
-            LASX_ILVLH_B_128SV( zero, tc, tc_h, tc_l );
-            LASX_ILVLH_B_128SV( zero, p1_org, p1_org_h, p1_org_l );
-            LASX_ILVLH_B_128SV( zero, p0_org, p0_org_h, p0_org_l );
-            LASX_ILVLH_B_128SV( zero, q0_org, q0_org_h, q0_org_l );
+            tc_l = __lasx_xvilvl_b( zero, tc );
+            tc_h = __lasx_xvilvh_b( zero, tc );
+            p1_org_l = __lasx_xvilvl_b( zero, p1_org );
+            p1_org_h = __lasx_xvilvh_b( zero, p1_org );
+            p0_org_l = __lasx_xvilvl_b( zero, p0_org );
+            p0_org_h = __lasx_xvilvh_b( zero, p0_org );
+            q0_org_l = __lasx_xvilvl_b( zero, q0_org );
+            q0_org_h = __lasx_xvilvh_b( zero, q0_org );
 
             {
                 __m256i p2_asub_p0;
@@ -177,7 +190,7 @@ void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
             if( !__lasx_xbz_v( is_less_than_beta ) )
             {
                 src6 = __lasx_xvpickev_b( src6, src2 );
-                LASX_BMNZ( p1_org, src6, is_less_than_beta, p1_org );
+                p1_org = __lasx_xvbitsel_v( p1_org, src6, is_less_than_beta );
 
                 is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
                 tc = __lasx_xvadd_b( tc, is_less_than_beta );
@@ -218,7 +231,7 @@ void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
             if( !__lasx_xbz_v( is_less_than_beta ) )
             {
                 src7 = __lasx_xvpickev_b( src7, src3 );
-                LASX_BMNZ( q1_org, src7, is_less_than_beta, q1_org );
+                q1_org = __lasx_xvbitsel_v( q1_org, src7, is_less_than_beta );
 
                 is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
                 tc = __lasx_xvadd_b( tc, is_less_than_beta );
@@ -232,8 +245,8 @@ void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
                 negate_thresh = __lasx_xvsub_b( zero, tc );
                 sign_negate_thresh = __lasx_xvslti_b( negate_thresh, 0 );
 
-                LASX_ILVL_B_2_128SV( zero, tc, sign_negate_thresh, negate_thresh,
-                                     threshold_l, negate_thresh_l );
+                DUP2_ARG2( __lasx_xvilvl_b, zero, tc, sign_negate_thresh, negate_thresh,
+                           threshold_l, negate_thresh_l );
 
                 LASX_LPF_P0Q0( q0_org_l, p0_org_l, p1_org_l, q1_org_l,
                                negate_thresh_l, threshold_l, src0, src1 );
@@ -249,73 +262,78 @@ void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
             src4 = __lasx_xvpickev_b( src4, src0 );
             src5 = __lasx_xvpickev_b( src5, src1 );
 
-            LASX_BMNZ( p0_org, src4, is_less_than, p0_org );
-            LASX_BMNZ( q0_org, src5, is_less_than, q0_org );
+            p0_org = __lasx_xvbitsel_v( p0_org, src4, is_less_than );
+            q0_org = __lasx_xvbitsel_v( q0_org, src5, is_less_than );
         }
         {
             p_src = p_pix - 3;
 
-            LASX_ILVLH_B_128SV( p1_org, p2_org, src2, src0 );
-            LASX_ILVLH_B_128SV( q0_org, p0_org, src3, src1 );
-            LASX_ILVLH_B_128SV( q2_org, q1_org, src5, src4 );
+            src0 = __lasx_xvilvl_b( p1_org, p2_org );
+            src2 = __lasx_xvilvh_b( p1_org, p2_org );
+            src1 = __lasx_xvilvl_b( q0_org, p0_org );
+            src3 = __lasx_xvilvh_b( q0_org, p0_org );
+            src4 = __lasx_xvilvl_b( q2_org, q1_org );
+            src5 = __lasx_xvilvh_b( q2_org, q1_org );
 
-            LASX_ILVLH_H_128SV( src1, src0, src7, src6 );
-            LASX_ILVLH_H_128SV( src3, src2, src1, src0 );
+            src6 = __lasx_xvilvl_h( src1, src0 );
+            src7 = __lasx_xvilvh_h( src1, src0 );
+            src0 = __lasx_xvilvl_h( src3, src2 );
+            src1 = __lasx_xvilvh_h( src3, src2 );
 
-            LASX_ST_W( src6, 0, p_src );
-            LASX_ST_H( src4, 0, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src6, p_src, 0, 0 );
+            __lasx_xvstelm_h( src4, p_src, 4, 0 );
             p_src += i_stride;
-            LASX_ST_W( src6, 1, p_src );
-            LASX_ST_H( src4, 1, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src6, p_src, 0, 1 );
+            __lasx_xvstelm_h( src4, p_src, 4, 1 );
 
             p_src += i_stride;
-            LASX_ST_W( src6, 2, p_src );
-            LASX_ST_H( src4, 2, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src6, p_src, 0, 2 );
+            __lasx_xvstelm_h( src4, p_src, 4, 2 );
             p_src += i_stride;
-            LASX_ST_W( src6, 3, p_src );
-            LASX_ST_H( src4, 3, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src6, p_src, 0, 3 );
+            __lasx_xvstelm_h( src4, p_src, 4, 3 );
 
             p_src += i_stride;
-            LASX_ST_W( src7, 0, p_src );
-            LASX_ST_H( src4, 4, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src7, p_src, 0, 0 );
+            __lasx_xvstelm_h( src4, p_src, 4, 4 );
             p_src += i_stride;
-            LASX_ST_W( src7, 1, p_src );
-            LASX_ST_H( src4, 5, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src7, p_src, 0, 1 );
+            __lasx_xvstelm_h( src4, p_src, 4, 5 );
 
             p_src += i_stride;
-            LASX_ST_W( src7, 2, p_src );
-            LASX_ST_H( src4, 6, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src7, p_src, 0, 2 );
+            __lasx_xvstelm_h( src4, p_src, 4, 6 );
             p_src += i_stride;
-            LASX_ST_W( src7, 3, p_src );
-            LASX_ST_H( src4, 7, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src7, p_src, 0, 3 );
+            __lasx_xvstelm_h( src4, p_src, 4, 7 );
 
             p_src += i_stride;
-            LASX_ST_W( src0, 0, p_src );
-            LASX_ST_H( src5, 0, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src0, p_src, 0, 0 );
+            __lasx_xvstelm_h( src5, p_src, 4, 0 );
             p_src += i_stride;
-            LASX_ST_W( src0, 1, p_src );
-            LASX_ST_H( src5, 1, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src0, p_src, 0, 1 );
+            __lasx_xvstelm_h( src5, p_src, 4, 1 );
 
             p_src += i_stride;
-            LASX_ST_W( src0, 2, p_src );
-            LASX_ST_H( src5, 2, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src0, p_src, 0, 2 );
+            __lasx_xvstelm_h( src5, p_src, 4, 2 );
             p_src += i_stride;
-            LASX_ST_W( src0, 3, p_src );
-            LASX_ST_H( src5, 3, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src0, p_src, 0, 3 );
+            __lasx_xvstelm_h( src5, p_src, 4, 3 );
 
             p_src += i_stride;
-            LASX_ST_W( src1, 0, p_src );
-            LASX_ST_H( src5, 4, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src1, p_src, 0, 0 );
+            __lasx_xvstelm_h( src5, p_src, 4, 4 );
             p_src += i_stride;
-            LASX_ST_W( src1, 1, p_src );
-            LASX_ST_H( src5, 5, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src1, p_src, 0, 1 );
+            __lasx_xvstelm_h( src5, p_src, 4, 5 );
 
             p_src += i_stride;
-            LASX_ST_W( src1, 2, p_src );
-            LASX_ST_H( src5, 6, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src1, p_src, 0, 2 );
+            __lasx_xvstelm_h( src5, p_src, 4, 6 );
             p_src += i_stride;
-            LASX_ST_W( src1, 3, p_src );
-            LASX_ST_H( src5, 7, ( p_src + 4 ) );
+            __lasx_xvstelm_w( src1, p_src, 0, 3 );
+            __lasx_xvstelm_h( src5, p_src, 4, 7 );
         }
     }
 }
@@ -325,8 +343,10 @@ void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
 {
     __m256i bs, tc, beta;
     __m256i zero = __lasx_xvldi( 0 );
+    intptr_t i_stride_2x = ( i_stride << 1 );
+    intptr_t i_stride_3x = i_stride_2x + i_stride;
 
-    tc = LASX_LD( p_tc0 );
+    tc = __lasx_xvld( p_tc0, 0 );
     tc = __lasx_xvilvl_b( tc, tc );
     tc = __lasx_xvilvl_h( tc, tc );
 
@@ -347,9 +367,11 @@ void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
         alpha = __lasx_xvreplgr2vr_b( i_alpha );
         beta = __lasx_xvreplgr2vr_b( i_beta );
 
-        p2_org = LASX_LD( p_pix - ( 3 * i_stride ) );
-        LASX_LD_4( p_pix - ( i_stride << 1 ), i_stride,
-                   p1_org, p0_org, q0_org, q1_org );
+        p2_org = __lasx_xvldx( p_pix , -i_stride_3x );
+        p_pix -= i_stride_2x;
+        DUP4_ARG2(__lasx_xvldx, p_pix, 0, p_pix, i_stride, p_pix, i_stride_2x, p_pix,
+                  i_stride_3x, p1_org, p0_org, q0_org, q1_org );
+        p_pix += i_stride_2x;
         {
             src5 = __lasx_xvslt_bu( zero, bs );
             src0 = __lasx_xvabsd_bu( p0_org, q0_org );
@@ -371,17 +393,21 @@ void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
             __m256i sign_negate_tc, negate_tc;
             __m256i negate_tc_l, i16_negatetc_h, tc_h, tc_l;
 
-            q2_org = LASX_LD( p_pix + ( i_stride << 1 ) );
+            q2_org = __lasx_xvldx( p_pix, i_stride_2x );
             negate_tc = __lasx_xvsub_b( zero, tc );
             sign_negate_tc = __lasx_xvslti_b( negate_tc, 0 );
 
-            LASX_ILVLH_B_128SV( sign_negate_tc, negate_tc,
-                                i16_negatetc_h, negate_tc_l );
+            negate_tc_l = __lasx_xvilvl_b( sign_negate_tc, negate_tc );
+            i16_negatetc_h = __lasx_xvilvh_b( sign_negate_tc, negate_tc );
 
-            LASX_ILVLH_B_128SV( zero, tc, tc_h, tc_l );
-            LASX_ILVLH_B_128SV( zero, p1_org, p1_org_h, p1_org_l );
-            LASX_ILVLH_B_128SV( zero, p0_org, p0_org_h, p0_org_l );
-            LASX_ILVLH_B_128SV( zero, q0_org, q0_org_h, q0_org_l );
+            tc_l = __lasx_xvilvl_b( zero, tc );
+            tc_h = __lasx_xvilvh_b( zero, tc );
+            p1_org_l = __lasx_xvilvl_b( zero, p1_org );
+            p1_org_h = __lasx_xvilvh_b( zero, p1_org );
+            p0_org_l = __lasx_xvilvl_b( zero, p0_org );
+            p0_org_h = __lasx_xvilvh_b( zero, p0_org );
+            q0_org_l = __lasx_xvilvl_b( zero, q0_org );
+            q0_org_h = __lasx_xvilvh_b( zero, q0_org );
 
             p2_asub_p0 = __lasx_xvabsd_bu( p2_org, p0_org );
             is_less_than_beta = __lasx_xvslt_bu( p2_asub_p0, beta );
@@ -413,8 +439,9 @@ void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
             if( !__lasx_xbz_v( is_less_than_beta ) )
             {
                 src6 = __lasx_xvpickev_b( src6, src2 );
-                LASX_BMNZ( p1_org, src6, is_less_than_beta, p1_org );
-                LASX_ST_Q( p1_org, 0, p_pix - ( i_stride << 1 ) );
+                p1_org = __lasx_xvbitsel_v( p1_org, src6, is_less_than_beta );
+                __lasx_xvstelm_d( p1_org, p_pix - i_stride_2x, 0, 0 );
+                __lasx_xvstelm_d( p1_org, p_pix - i_stride_2x, 8, 1 );
 
                 is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
                 tc = __lasx_xvadd_b( tc, is_less_than_beta );
@@ -453,8 +480,9 @@ void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
             if( !__lasx_xbz_v( is_less_than_beta ) )
             {
                 src7 = __lasx_xvpickev_b( src7, src3 );
-                LASX_BMNZ( q1_org, src7, is_less_than_beta, q1_org );
-                LASX_ST_Q( q1_org, 0, p_pix + i_stride );
+                q1_org = __lasx_xvbitsel_v( q1_org, src7, is_less_than_beta );
+                __lasx_xvstelm_d( q1_org, p_pix + i_stride, 0, 0 );
+                __lasx_xvstelm_d( q1_org, p_pix + i_stride, 8, 1 );
 
                 is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
                 tc = __lasx_xvadd_b( tc, is_less_than_beta );
@@ -467,8 +495,8 @@ void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
                 negate_thresh = __lasx_xvsub_b( zero, tc );
                 sign_negate_thresh = __lasx_xvslti_b( negate_thresh, 0 );
 
-                LASX_ILVL_B_2_128SV( zero, tc, sign_negate_thresh, negate_thresh,
-                                     threshold_l, negate_thresh_l );
+                DUP2_ARG2( __lasx_xvilvl_b, zero, tc, sign_negate_thresh, negate_thresh,
+                           threshold_l, negate_thresh_l );
                 LASX_LPF_P0Q0( q0_org_l, p0_org_l, p1_org_l, q1_org_l,
                                negate_thresh_l, threshold_l, src0, src1 );
 
@@ -482,11 +510,13 @@ void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
             src4 = __lasx_xvpickev_b( src4, src0 );
             src5 = __lasx_xvpickev_b( src5, src1 );
 
-            LASX_BMNZ( p0_org, src4, is_less_than, p0_org );
-            LASX_BMNZ( q0_org, src5, is_less_than, q0_org );
+            p0_org = __lasx_xvbitsel_v( p0_org, src4, is_less_than );
+            q0_org = __lasx_xvbitsel_v( q0_org, src5, is_less_than );
 
-            LASX_ST_Q( p0_org, 0, ( p_pix - i_stride ) );
-            LASX_ST_Q( q0_org, 0, p_pix );
+            __lasx_xvstelm_d( p0_org, p_pix - i_stride, 0, 0 );
+            __lasx_xvstelm_d( p0_org, p_pix - i_stride, 8, 1 );
+            __lasx_xvstelm_d( q0_org, p_pix, 0, 0 );
+            __lasx_xvstelm_d( q0_org, p_pix, 8, 1 );
         }
     }
 }
@@ -506,19 +536,14 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     int8_t* p_lef = pi_lef[0];
     int16_t* p_mv = pi_mv[0][0];
 
-    nnz0 = LASX_LD( nnz + 4 );
-    nnz2 = LASX_LD( nnz + 20 );
-    nnz4 = LASX_LD( nnz + 36 );
+    DUP2_ARG2(__lasx_xvld, nnz, 4, nnz, 20, nnz0, nnz2 );
+    nnz4 = __lasx_xvld( nnz, 36 );
 
-    ref0 = LASX_LD( p_lef + 4 );
-    ref2 = LASX_LD( p_lef + 20 );
-    ref4 = LASX_LD( p_lef + 36 );
+    DUP2_ARG2(__lasx_xvld, p_lef, 4, p_lef, 20, ref0, ref2 );
+    ref4 = __lasx_xvld( p_lef, 36 );
 
-    mv0 = LASX_LD( p_mv + 8 );
-    mv1 = LASX_LD( p_mv + 24 );
-    mv2 = LASX_LD( p_mv + 40 );
-    mv3 = LASX_LD( p_mv + 56 );
-    mv4 = LASX_LD( p_mv + 72 );
+    DUP4_ARG2(__lasx_xvld, p_mv, 16, p_mv, 48, p_mv, 80, p_mv, 112, mv0, mv1, mv2, mv3 );
+    mv4 = __lasx_xvld( p_mv, 144 );
 
     mvy_himit_vec = __lasx_xvreplgr2vr_h( i_mvy_himit );
     four = __lasx_xvreplgr2vr_h( 4 );
@@ -534,7 +559,7 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref1 = __lasx_xvrepl128vei_w( ref0, 2 );
     nnz_mask = __lasx_xvor_v( nnz0, nnz1 );
     nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    LASX_BMNZ( two, mask, nnz_mask, two );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
 
     ref_mask = __lasx_xvseq_b( ref0, ref1 );
     ref_mask = __lasx_xvxori_b( ref_mask, 255 );
@@ -550,10 +575,10 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref_mask = __lasx_xvor_v( ref_mask, sub0 );
     ref_mask = __lasx_xvor_v( ref_mask, sub1 );
 
-    LASX_BMNZ( dst, one, ref_mask, dst );
-    LASX_BMNZ( two, dst, nnz_mask, dst );
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
 
-    LASX_ST_W( dst, 0, pu_bs[1][0] );
+    __lasx_xvstelm_w( dst, pu_bs[1][0], 0, 0 );
 
     dst = __lasx_xvldi( 0 );
     two = __lasx_xvldi( 2 );
@@ -565,7 +590,7 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
 
     nnz_mask = __lasx_xvor_v( nnz2, nnz1 );
     nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    LASX_BMNZ( two, mask, nnz_mask, two );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
 
     ref_mask = __lasx_xvseq_b( ref1, ref2 );
     ref_mask = __lasx_xvxori_b( ref_mask, 255 );
@@ -580,10 +605,10 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref_mask = __lasx_xvor_v( ref_mask, sub0 );
     ref_mask = __lasx_xvor_v( ref_mask, sub1 );
 
-    LASX_BMNZ( dst, one, ref_mask, dst );
-    LASX_BMNZ( two, dst, nnz_mask, dst );
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
 
-    LASX_ST_W( dst, 0, pu_bs[1][1] );
+    __lasx_xvstelm_w( dst, pu_bs[1][1], 0, 0 );
 
     dst = __lasx_xvldi( 0 );
     two = __lasx_xvldi( 2 );
@@ -598,7 +623,7 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
 
     nnz_mask = __lasx_xvor_v( nnz3, nnz2 );
     nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    LASX_BMNZ( two, mask, nnz_mask, two );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
 
     ref_mask = __lasx_xvseq_b( ref2, ref3 );
     ref_mask = __lasx_xvxori_b( ref_mask, 255 );
@@ -614,10 +639,10 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref_mask = __lasx_xvor_v( ref_mask, sub0 );
     ref_mask = __lasx_xvor_v( ref_mask, sub1 );
 
-    LASX_BMNZ( dst, one, ref_mask, dst );
-    LASX_BMNZ( two, dst, nnz_mask, dst );
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
 
-    LASX_ST_W( dst, 0, pu_bs[1][2] );
+    __lasx_xvstelm_w( dst, pu_bs[1][2], 0, 0 );
 
     dst = __lasx_xvldi( 0 );
     two = __lasx_xvldi( 2 );
@@ -629,7 +654,7 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
 
     nnz_mask = __lasx_xvor_v( nnz4, nnz3 );
     nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    LASX_BMNZ( two, mask, nnz_mask, two );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
 
     ref_mask = __lasx_xvseq_b( ref3, ref4 );
     ref_mask = __lasx_xvxori_b( ref_mask, 255 );
@@ -645,32 +670,24 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref_mask = __lasx_xvor_v( ref_mask, sub0 );
     ref_mask = __lasx_xvor_v( ref_mask, sub1 );
 
-    LASX_BMNZ( dst, one, ref_mask, dst );
-    LASX_BMNZ( two, dst, nnz_mask, dst );
-
-    LASX_ST_W( dst, 0, pu_bs[1][3] );
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
 
-    nnz0 = LASX_LD( nnz + 8 );
-    nnz2 = LASX_LD( nnz + 24 );
+    __lasx_xvstelm_w( dst, pu_bs[1][3], 0, 0 );
 
-    ref0 = LASX_LD( p_lef + 8 );
-    ref2 = LASX_LD( p_lef + 24 );
+    DUP2_ARG2( __lasx_xvld, nnz, 8, nnz, 24, nnz0, nnz2 );
+    DUP2_ARG2( __lasx_xvld, p_lef, 8, p_lef, 24, ref0, ref2);
 
-    mv0 = LASX_LD( p_mv + 16 );
-    mv1 = LASX_LD( p_mv + 24 );
-    mv2 = LASX_LD( p_mv + 32 );
-    mv3 = LASX_LD( p_mv + 40 );
-    mv4 = LASX_LD( p_mv + 48 );
-    mv7 = LASX_LD( p_mv + 56 );
-    mv8 = LASX_LD( p_mv + 64 );
-    mv9 = LASX_LD( p_mv + 72 );
+    DUP4_ARG2(__lasx_xvld, p_mv, 32, p_mv, 48, p_mv, 64, p_mv, 80, mv0, mv1, mv2, mv3 );
+    DUP4_ARG2(__lasx_xvld, p_mv, 96, p_mv, 112, p_mv, 128, p_mv, 144, mv4, mv7, mv8, mv9 );
 
     nnz1 = __lasx_xvrepl128vei_d( nnz0, 1 );
     nnz3 = __lasx_xvrepl128vei_d( nnz2, 1 );
 
-    LASX_ILVL_B_2_128SV( nnz2, nnz0, nnz3, nnz1, temp_vec0, temp_vec1 );
+    DUP2_ARG2( __lasx_xvilvl_b, nnz2, nnz0, nnz3, nnz1, temp_vec0, temp_vec1 );
 
-    LASX_ILVLH_B_128SV( temp_vec1, temp_vec0, nnz1, temp_vec2 );
+    temp_vec2 = __lasx_xvilvl_b( temp_vec1, temp_vec0 );
+    nnz1 = __lasx_xvilvh_b( temp_vec1, temp_vec0 );
 
     nnz0 = __lasx_xvrepl128vei_w( temp_vec2, 3 );
     nnz2 = __lasx_xvrepl128vei_w( nnz1, 1 );
@@ -680,9 +697,10 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref1 = __lasx_xvrepl128vei_d( ref0, 1 );
     ref3 = __lasx_xvrepl128vei_d( ref2, 1 );
 
-    LASX_ILVL_B_2_128SV( ref2, ref0, ref3, ref1, temp_vec0, temp_vec1 );
+    DUP2_ARG2( __lasx_xvilvl_b, ref2, ref0, ref3, ref1, temp_vec0, temp_vec1 );
 
-    LASX_ILVLH_B_128SV( temp_vec1, temp_vec0, ref1, temp_vec2 );
+    temp_vec2 = __lasx_xvilvl_b( temp_vec1, temp_vec0 );
+    ref1 = __lasx_xvilvh_b( temp_vec1, temp_vec0 );
 
     ref0 = __lasx_xvrepl128vei_w( temp_vec2, 3 );
 
@@ -690,8 +708,8 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref3 = __lasx_xvrepl128vei_w( ref1, 2 );
     ref4 = __lasx_xvrepl128vei_w( ref1, 3 );
 
-    LASX_TRANSPOSE8X4_H_128SV( mv0, mv2, mv4, mv8, mv5, mv5, mv5, mv0 );
-    LASX_TRANSPOSE8X4_H_128SV( mv1, mv3, mv7, mv9, mv1, mv2, mv3, mv4 );
+    LASX_TRANSPOSE8X4_H( mv0, mv2, mv4, mv8, mv5, mv5, mv5, mv0 );
+    LASX_TRANSPOSE8X4_H( mv1, mv3, mv7, mv9, mv1, mv2, mv3, mv4 );
 
     mvy_himit_vec = __lasx_xvreplgr2vr_h( i_mvy_himit );
     four = __lasx_xvreplgr2vr_h( 4 );
@@ -705,7 +723,7 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
 
     nnz_mask = __lasx_xvor_v( nnz0, nnz1 );
     nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    LASX_BMNZ( two, mask, nnz_mask, two );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
 
     ref_mask = __lasx_xvseq_b( ref0, ref1 );
     ref_mask = __lasx_xvxori_b( ref_mask, 255 );
@@ -721,10 +739,10 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref_mask = __lasx_xvor_v( ref_mask, sub0 );
     ref_mask = __lasx_xvor_v( ref_mask, sub1 );
 
-    LASX_BMNZ( dst, one, ref_mask, dst );
-    LASX_BMNZ( two, dst, nnz_mask, dst );
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
 
-    LASX_ST_W( dst, 0, pu_bs[0][0] );
+    __lasx_xvstelm_w( dst, pu_bs[0][0], 0, 0 );
 
     two = __lasx_xvldi( 2 );
     dst = __lasx_xvldi( 0 );
@@ -734,7 +752,7 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
 
     nnz_mask = __lasx_xvor_v( nnz1, nnz2 );
     nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    LASX_BMNZ( two, mask, nnz_mask, two );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
 
     ref_mask = __lasx_xvseq_b( ref1, ref2 );
     ref_mask = __lasx_xvxori_b( ref_mask, 255 );
@@ -749,10 +767,10 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref_mask = __lasx_xvor_v( ref_mask, sub0 );
     ref_mask = __lasx_xvor_v( ref_mask, sub1 );
 
-    LASX_BMNZ( dst, one, ref_mask, dst );
-    LASX_BMNZ( two, dst, nnz_mask, dst );
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
 
-    LASX_ST_W( dst, 0, pu_bs[0][1] );
+    __lasx_xvstelm_w( dst, pu_bs[0][1], 0, 0 );
 
     two = __lasx_xvldi( 2 );
     dst = __lasx_xvldi( 0 );
@@ -762,7 +780,7 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
 
     nnz_mask = __lasx_xvor_v( nnz2, nnz3 );
     nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    LASX_BMNZ( two, mask, nnz_mask, two );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
 
     ref_mask = __lasx_xvseq_b( ref2, ref3 );
     ref_mask = __lasx_xvxori_b( ref_mask, 255 );
@@ -777,10 +795,10 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref_mask = __lasx_xvor_v( ref_mask, sub0 );
     ref_mask = __lasx_xvor_v( ref_mask, sub1 );
 
-    LASX_BMNZ( dst, one, ref_mask, dst );
-    LASX_BMNZ( two, dst, nnz_mask, dst );
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
 
-    LASX_ST_W( dst, 0, pu_bs[0][2] );
+    __lasx_xvstelm_w( dst, pu_bs[0][2], 0, 0 );
 
     two = __lasx_xvldi( 2 );
     dst = __lasx_xvldi( 0 );
@@ -790,7 +808,7 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
 
     nnz_mask = __lasx_xvor_v( nnz3, nnz4 );
     nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
-    LASX_BMNZ( two, mask, nnz_mask, two );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
 
     ref_mask = __lasx_xvseq_b( ref3, ref4 );
     ref_mask = __lasx_xvxori_b( ref_mask, 255 );
@@ -805,10 +823,10 @@ static void avc_deblock_strength_lasx( uint8_t *nnz,
     ref_mask = __lasx_xvor_v( ref_mask, sub0 );
     ref_mask = __lasx_xvor_v( ref_mask, sub1 );
 
-    LASX_BMNZ( dst, one, ref_mask, dst );
-    LASX_BMNZ( two, dst, nnz_mask, dst );
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
 
-    LASX_ST_W( dst, 0, pu_bs[0][3] );
+    __lasx_xvstelm_w( dst, pu_bs[0][3], 0, 0 );
 }
 
 void x264_deblock_strength_lasx( uint8_t u_nnz[X264_SCAN8_SIZE],
diff --git a/common/loongarch/generic_macros_lasx.h b/common/loongarch/generic_macros_lasx.h
deleted file mode 100644
index de45bead..00000000
--- a/common/loongarch/generic_macros_lasx.h
+++ /dev/null
@@ -1,3792 +0,0 @@
-/*****************************************************************************
- * generic_macros_lasx.h: loongarch macros
- *****************************************************************************
- * Copyright (C) 2020 x264 project
- * Copyright (C) 2020 Loongson Technology Corporation Limited
- *
- * Authors: Shiyou Yin   <yinshiyou-hf@loongson.cn>
- *          Xiwei Gu     <guxiwei-hf@loongson.cn>
- *          Jin Bo       <jinbo@loongson.cn>
- *          Hao Chen     <chenhao@loongson.cn>
- *          Lu Wang      <wanglu@loongson.cn>
- *          Peng Zhou    <zhoupeng@loongson.cn>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
- *
- * This program is also available under a commercial proprietary license.
- * For more information, contact us at licensing@x264.com.
- *****************************************************************************/
-
-#ifndef GENERIC_MACROS_LASX_H
-#define GENERIC_MACROS_LASX_H
-
-#include <stdint.h>
-#include <lasxintrin.h>
-
-/**
- * MAJOR version: Macro usage changes.
- * MINOR version: Add new macros, or bug fix.
- * MICRO version: Comment changes or implementation changes。
- */
-#define LSOM_LASX_VERSION_MAJOR 3
-#define LSOM_LASX_VERSION_MINOR 1
-#define LSOM_LASX_VERSION_MICRO 0
-
-/* Description : Load 256-bit vector data with stride
- * Arguments   : Inputs  - psrc    (source pointer to load from)
- *                       - stride
- *               Outputs - out0, out1, ~
- * Details     : Load 256-bit data in 'out0' from (psrc)
- *               Load 256-bit data in 'out1' from (psrc + stride)
- */
-#define LASX_LD(psrc) *((__m256i *)(psrc))
-
-#define LASX_LD_2(psrc, stride, out0, out1)                                 \
-{                                                                           \
-    out0 = LASX_LD(psrc);                                                   \
-    out1 = LASX_LD((psrc) + stride);                                        \
-}
-
-#define LASX_LD_4(psrc, stride, out0, out1, out2, out3)                     \
-{                                                                           \
-    LASX_LD_2((psrc), stride, out0, out1);                                  \
-    LASX_LD_2((psrc) + 2 * stride , stride, out2, out3);                    \
-}
-
-#define LASX_LD_8(psrc, stride, out0, out1, out2, out3, out4, out5,         \
-                  out6, out7)                                               \
-{                                                                           \
-    LASX_LD_4((psrc), stride, out0, out1, out2, out3);                      \
-    LASX_LD_4((psrc) + 4 * stride, stride, out4, out5, out6, out7);         \
-}
-
-/* Description : Store 256-bit vector data with stride
- * Arguments   : Inputs  - in0, in1, ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store 256-bit data from 'in0' to (pdst)
- *               Store 256-bit data from 'in1' to (pdst + stride)
- */
-#define LASX_ST(in, pdst) *((__m256i *)(pdst)) = (in)
-
-#define LASX_ST_2(in0, in1, pdst, stride)                                   \
-{                                                                           \
-    LASX_ST(in0, (pdst));                                                   \
-    LASX_ST(in1, (pdst) + stride);                                          \
-}
-
-#define LASX_ST_4(in0, in1, in2, in3, pdst, stride)                         \
-{                                                                           \
-    LASX_ST_2(in0, in1, (pdst), stride);                                    \
-    LASX_ST_2(in2, in3, (pdst) + 2 * stride, stride);                       \
-}
-
-#define LASX_ST_8(in0, in1, in2, in3, in4, in5, in6, in7, pdst, stride)     \
-{                                                                           \
-    LASX_ST_4(in0, in1, in2, in3, (pdst), stride);                          \
-    LASX_ST_4(in4, in5, in6, in7, (pdst) + 4 * stride, stride);             \
-}
-
-/* Description : Store half word elements of vector with stride
- * Arguments   : Inputs  - in   source vector
- *                       - idx, idx0, idx1,  ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store half word 'idx0' from 'in' to (pdst)
- *               Store half word 'idx1' from 'in' to (pdst + stride)
- *               Similar for other elements
- * Example     : LASX_ST_H(in, idx, pdst)
- *          in : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
- *        idx0 : 0x01
- *        out0 : 2
- */
-#define LASX_ST_H(in, idx, pdst)                                          \
-{                                                                         \
-    __lasx_xvstelm_h(in, pdst, 0, idx);                                   \
-}
-
-#define LASX_ST_H_2(in, idx0, idx1, pdst, stride)                         \
-{                                                                         \
-    LASX_ST_H(in, idx0, (pdst));                                          \
-    LASX_ST_H(in, idx1, (pdst) + stride);                                 \
-}
-
-#define LASX_ST_H_4(in, idx0, idx1, idx2, idx3, pdst, stride)             \
-{                                                                         \
-    LASX_ST_H_2(in, idx0, idx1, (pdst), stride);                          \
-    LASX_ST_H_2(in, idx2, idx3, (pdst) + 2 * stride, stride);             \
-}
-
-
-/* Description : Store word elements of vector with stride
- * Arguments   : Inputs  - in   source vector
- *                       - idx, idx0, idx1,  ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store word 'idx0' from 'in' to (pdst)
- *               Store word 'idx1' from 'in' to (pdst + stride)
- *               Similar for other elements
- * Example     : LASX_ST_W(in, idx, pdst)
- *          in : 1, 2, 3, 4, 5, 6, 7, 8
- *        idx0 : 0x01
- *        out0 : 2
- */
-#define LASX_ST_W(in, idx, pdst)                                          \
-{                                                                         \
-    __lasx_xvstelm_w(in, pdst, 0, idx);                                   \
-}
-
-#define LASX_ST_W_2(in, idx0, idx1, pdst, stride)                         \
-{                                                                         \
-    LASX_ST_W(in, idx0, (pdst));                                          \
-    LASX_ST_W(in, idx1, (pdst) + stride);                                 \
-}
-
-#define LASX_ST_W_4(in, idx0, idx1, idx2, idx3, pdst, stride)             \
-{                                                                         \
-    LASX_ST_W_2(in, idx0, idx1, (pdst), stride);                          \
-    LASX_ST_W_2(in, idx2, idx3, (pdst) + 2 * stride, stride);             \
-}
-
-#define LASX_ST_W_8(in, idx0, idx1, idx2, idx3, idx4, idx5, idx6, idx7,   \
-                    pdst, stride)                                         \
-{                                                                         \
-    LASX_ST_W_4(in, idx0, idx1, idx2, idx3, (pdst), stride);              \
-    LASX_ST_W_4(in, idx4, idx5, idx6, idx7, (pdst) + 4 * stride, stride); \
-}
-
-/* Description : Store double word elements of vector with stride
- * Arguments   : Inputs  - in   source vector
- *                       - idx, idx0, idx1, ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store double word 'idx0' from 'in' to (pdst)
- *               Store double word 'idx1' from 'in' to (pdst + stride)
- *               Similar for other elements
- * Example     : See LASX_ST_W(in, idx, pdst)
- */
-#define LASX_ST_D(in, idx, pdst)                                         \
-{                                                                        \
-    __lasx_xvstelm_d(in, pdst, 0, idx);                                  \
-}
-
-#define LASX_ST_D_2(in, idx0, idx1, pdst, stride)                        \
-{                                                                        \
-    LASX_ST_D(in, idx0, (pdst));                                         \
-    LASX_ST_D(in, idx1, (pdst) + stride);                                \
-}
-
-#define LASX_ST_D_4(in, idx0, idx1, idx2, idx3, pdst, stride)            \
-{                                                                        \
-    LASX_ST_D_2(in, idx0, idx1, (pdst), stride);                         \
-    LASX_ST_D_2(in, idx2, idx3, (pdst) + 2 * stride, stride);            \
-}
-
-/* Description : Store quad word elements of vector with stride
- * Arguments   : Inputs  - in   source vector
- *                       - idx, idx0, idx1, ~
- *                       - pdst    (destination pointer to store to)
- *                       - stride
- * Details     : Store quad word 'idx0' from 'in' to (pdst)
- *               Store quad word 'idx1' from 'in' to (pdst + stride)
- *               Similar for other elements
- * Example     : See LASX_ST_W(in, idx, pdst)
- */
-#define LASX_ST_Q(in, idx, pdst)                                         \
-{                                                                        \
-    LASX_ST_D(in, (idx << 1), pdst);                                     \
-    LASX_ST_D(in, (( idx << 1) + 1), (char*)(pdst) + 8);                 \
-}
-
-#define LASX_ST_Q_2(in, idx0, idx1, pdst, stride)                        \
-{                                                                        \
-    LASX_ST_Q(in, idx0, (pdst));                                         \
-    LASX_ST_Q(in, idx1, (pdst) + stride);                                \
-}
-
-#define LASX_ST_Q_4(in, idx0, idx1, idx2, idx3, pdst, stride)            \
-{                                                                        \
-    LASX_ST_Q_2(in, idx0, idx1, (pdst), stride);                         \
-    LASX_ST_Q_2(in, idx2, idx3, (pdst) + 2 * stride, stride);            \
-}
-
-/* Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - unsigned halfword
- * Details     : Unsigned byte elements from in0 are iniplied with
- *               unsigned byte elements from in0 producing a result
- *               twice the size of input i.e. unsigned halfword.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added together and stored to the out vector
- *               (2 unsigned halfword results)
- * Example     : see LASX_DP2_W_H
- */
-#define LASX_DP2_H_BU(in0, in1, out0)                   \
-{                                                       \
-    __m256i _tmp0_m ;                                   \
-                                                        \
-    _tmp0_m = __lasx_xvmulwev_h_bu( in0, in1 );         \
-    out0 = __lasx_xvmaddwod_h_bu( _tmp0_m, in0, in1 );  \
-}
-#define LASX_DP2_H_BU_2(in0, in1, in2, in3, out0, out1) \
-{                                                       \
-    LASX_DP2_H_BU(in0, in1, out0);                      \
-    LASX_DP2_H_BU(in2, in3, out1);                      \
-}
-#define LASX_DP2_H_BU_4(in0, in1, in2, in3,             \
-                        in4, in5, in6, in7,             \
-                        out0, out1, out2, out3)         \
-{                                                       \
-    LASX_DP2_H_BU_2(in0, in1, in0, in1, out0, out1);    \
-    LASX_DP2_H_BU_2(in4, in5, in6, in7, out2, out3);    \
-}
-
-/* Description : Dot product of byte vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - signed halfword
- * Details     : Signed byte elements from in0 are iniplied with
- *               signed byte elements from in0 producing a result
- *               twice the size of input i.e. signed halfword.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added together and stored to the out vector
- *               (2 signed halfword results)
- * Example     : see LASX_DP2_W_H
- */
-#define LASX_DP2_H_B(in0, in1, out0)                      \
-{                                                         \
-    __m256i _tmp0_m ;                                     \
-                                                          \
-    _tmp0_m = __lasx_xvmulwev_h_b( in0, in1 );            \
-    out0 = __lasx_xvmaddwod_h_b( _tmp0_m, in0, in1 );     \
-}
-#define LASX_DP2_H_B_2(in0, in1, in2, in3, out0, out1)    \
-{                                                         \
-    LASX_DP2_H_B(in0, in1, out0);                         \
-    LASX_DP2_H_B(in2, in3, out1);                         \
-}
-#define LASX_DP2_H_B_4(in0, in1, in2, in3,                \
-                       in4, in5, in6, in7,                \
-                       out0, out1, out2, out3)            \
-{                                                         \
-    LASX_DP2_H_B_2(in0, in1, in2, in3, out0, out1);       \
-    LASX_DP2_H_B_2(in4, in5, in6, in7, out2, out3);       \
-}
-
-/* Description : Dot product of half word vector elements
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- *               Return Type - signed word
- * Details     : Signed half word elements from in* are iniplied with
- *               signed half word elements from in* producing a result
- *               twice the size of input i.e. signed word.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added together and stored to the out vector.
- * Example     : LASX_DP2_W_H(in0, in1, out0)
- *               in0:   1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
- *               in0:   8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
- *               out0:  22,38,38,22, 22,38,38,22
- */
-#define LASX_DP2_W_H(in0, in1, out0)                   \
-{                                                      \
-    __m256i _tmp0_m ;                                  \
-                                                       \
-    _tmp0_m = __lasx_xvmulwev_w_h( in0, in1 );         \
-    out0 = __lasx_xvmaddwod_w_h( _tmp0_m, in0, in1 );  \
-}
-#define LASX_DP2_W_H_2(in0, in1, in2, in3, out0, out1)             \
-{                                                                  \
-    LASX_DP2_W_H(in0, in1, out0);                                  \
-    LASX_DP2_W_H(in2, in3, out1);                                  \
-}
-#define LASX_DP2_W_H_4(in0, in1, in2, in3,                         \
-                       in4, in5, in6, in7, out0, out1, out2, out3) \
-{                                                                  \
-    LASX_DP2_W_H_2(in0, in1, in2, in3, out0, out1);                \
-    LASX_DP2_W_H_2(in4, in5, in6, in7, out2, out3);                \
-}
-#define LASX_DP2_W_H_8(in0, in1, in2, in3, in4, in5, in6, in7,         \
-                       in8, in9, in10, in11, in12, in13, in14, in15,   \
-                       out0, out1, out2, out3, out4, out5, out6, out7) \
-{                                                                      \
-    LASX_DP2_W_H_4(in0, in1, in2, in3, in4, in5, in6, in7,             \
-                   out0, out1, out2, out3);                            \
-    LASX_DP2_W_H_4(in8, in9, in10, in11, in12, in13, in14, in15,       \
-                   out4, out5, out6, out7);                            \
-}
-
-/* Description : Dot product of word vector elements
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- *               Retrun Type - signed double
- * Details     : Signed word elements from in* are iniplied with
- *               signed word elements from in* producing a result
- *               twice the size of input i.e. signed double word.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added together and stored to the out vector.
- * Example     : see LASX_DP2_W_H
- */
-#define LASX_DP2_D_W(in0, in1, out0)                    \
-{                                                       \
-    __m256i _tmp0_m ;                                   \
-                                                        \
-    _tmp0_m = __lasx_xvmulwev_d_w( in0, in1 );          \
-    out0 = __lasx_xvmaddwod_d_w( _tmp0_m, in0, in1 );   \
-}
-#define LASX_DP2_D_W_2(in0, in1, in2, in3, out0, out1)  \
-{                                                       \
-    LASX_DP2_D_W(in0, in1, out0);                       \
-    LASX_DP2_D_W(in2, in3, out1);                       \
-}
-#define LASX_DP2_D_W_4(in0, in1, in2, in3,                             \
-                       in4, in5, in6, in7, out0, out1, out2, out3)     \
-{                                                                      \
-    LASX_DP2_D_W_2(in0, in1, in2, in3, out0, out1);                    \
-    LASX_DP2_D_W_2(in4, in5, in6, in7, out2, out3);                    \
-}
-#define LASX_DP2_D_W_8(in0, in1, in2, in3, in4, in5, in6, in7,         \
-                       in8, in9, in10, in11, in12, in13, in14, in15,   \
-                       out0, out1, out2, out3, out4, out5, out6, out7) \
-{                                                                      \
-    LASX_DP2_D_W_4(in0, in1, in2, in3, in4, in5, in6, in7,             \
-                   out0, out1, out2, out3);                            \
-    LASX_DP2_D_W_4(in8, in9, in10, in11, in12, in13, in14, in15,       \
-                   out4, out5, out6, out7);                            \
-}
-
-/* Description : Dot product of halfword vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Unsigned halfword elements from 'in0' are iniplied with
- *               halfword elements from 'in0' producing a result
- *               twice the size of input i.e. unsigned word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and written to the 'out0' vector
- */
-#define LASX_DP2_W_HU_H(in0, in1, out0)                   \
-{                                                         \
-    __m256i _tmp0_m;                                      \
-                                                          \
-    _tmp0_m = __lasx_xvmulwev_w_hu_h( in0, in1 );         \
-    out0 = __lasx_xvmaddwod_w_hu_h( _tmp0_m, in0, in1 );  \
-}
-
-#define LASX_DP2_W_HU_H_2(in0, in1, in2, in3, out0, out1) \
-{                                                         \
-    LASX_DP2_W_HU_H(in0, in1, out0);                      \
-    LASX_DP2_W_HU_H(in2, in3, out1);                      \
-}
-
-#define LASX_DP2_W_HU_H_4(in0, in1, in2, in3,             \
-                          in4, in5, in6, in7,             \
-                          out0, out1, out2, out3)         \
-{                                                         \
-    LASX_DP2_W_HU_H_2(in0, in1, in2, in3, out0, out1);    \
-    LASX_DP2_W_HU_H_2(in4, in5, in6, in7, out2, out3);    \
-}
-
-/* Description : Dot product & addition of byte vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Retrun Type - halfword
- * Details     : Signed byte elements from in0 are iniplied with
- *               signed byte elements from in0 producing a result
- *               twice the size of input i.e. signed halfword.
- *               Then this iniplication results of adjacent odd-even elements
- *               are added to the out vector
- *               (2 signed halfword results)
- * Example     : LASX_DP2ADD_H_B(in0, in1, in2, out0)
- *               in0:  1,2,3,4, 1,2,3,4, 1,2,3,4, 1,2,3,4,
- *               in1:  1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
- *                     1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *               in2:  8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
- *                     8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *               out0: 23,40,41,26, 23,40,41,26, 23,40,41,26, 23,40,41,26
- */
-#define LASX_DP2ADD_H_B(in0, in1, in2, out0)                 \
-{                                                            \
-    __m256i _tmp0_m;                                         \
-                                                             \
-    _tmp0_m = __lasx_xvmaddwev_h_b( in0, in1, in2 );         \
-    out0 = __lasx_xvmaddwod_h_b( _tmp0_m, in1, in2 );        \
-}
-#define LASX_DP2ADD_H_B_2(in0, in1, in2, in3, in4, in5, out0, out1)  \
-{                                                                    \
-    LASX_DP2ADD_H_B(in0, in1, in2, out0);                            \
-    LASX_DP2ADD_H_B(in3, in4, in5, out1);                            \
-}
-#define LASX_DP2ADD_H_B_4(in0, in1, in2, in3, in4, in5,                \
-                          in6, in7, in8, in9, in10, in11,              \
-                          out0, out1, out2, out3)                      \
-{                                                                      \
-    LASX_DP2ADD_H_B_2(in0, in1, in2, in3, in4, in5, out0, out1);       \
-    LASX_DP2ADD_H_B_2(in6, in7, in8, in9, in10, in11, out2, out3);     \
-}
-
-/* Description : Dot product of halfword vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Signed halfword elements from 'in0' are iniplied with
- *               signed halfword elements from 'in0' producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and written to the 'out0' vector
- */
-#define LASX_DP2ADD_W_H(in0, in1, in2, out0)                 \
-{                                                            \
-    __m256i _tmp0_m;                                         \
-                                                             \
-    _tmp0_m = __lasx_xvmaddwev_w_h( in0, in1, in2 );         \
-    out0 = __lasx_xvmaddwod_w_h( _tmp0_m, in1, in2 );        \
-}
-#define LASX_DP2ADD_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1 ) \
-{                                                                    \
-    LASX_DP2ADD_W_H(in0, in1, in2, out0);                            \
-    LASX_DP2ADD_W_H(in3, in4, in5, out1);                            \
-}
-#define LASX_DP2ADD_W_H_4(in0, in1, in2, in3, in4, in5,              \
-                          in6, in7, in8, in9, in10, in11,            \
-                          out0, out1, out2, out3)                    \
-{                                                                    \
-    LASX_DP2ADD_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1);     \
-    LASX_DP2ADD_W_H_2(in6, in7, in8, in9, in10, in11, out2, out3);   \
-}
-
-/* Description : Dot product of halfword vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Unsigned halfword elements from 'in0' are iniplied with
- *               unsigned halfword elements from 'in0' producing a result
- *               twice the size of input i.e. unsigned word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and written to the 'out0' vector
- */
-#define LASX_DP2ADD_W_HU(in0, in1, in2, out0)          \
-{                                                      \
-    __m256i _tmp0_m;                                   \
-                                                       \
-    _tmp0_m = __lasx_xvmaddwev_w_hu( in0, in1, in2 );  \
-    out0 = __lasx_xvmaddwod_w_hu( _tmp0_m, in1, in2 ); \
-}
-#define LASX_DP2ADD_W_HU_2(in0, in1, in2, in3, in4, in5, out0, out1) \
-{                                                                    \
-    LASX_DP2ADD_W_HU(in0, in1, in2, out0);                           \
-    LASX_DP2ADD_W_HU(in3, in4, in5, out1);                           \
-}
-#define LASX_DP2ADD_W_HU_4(in0, in1, in2, in3, in4, in5,             \
-                           in6, in7, in8, in9, in10, in11,           \
-                           out0, out1, out2, out3)                   \
-{                                                                    \
-    LASX_DP2ADD_W_HU_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
-    LASX_DP2ADD_W_HU_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
-}
-
-/* Description : Dot product of halfword vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Unsigned halfword elements from 'in0' are iniplied with
- *               halfword elements from 'in0' producing a result
- *               twice the size of input i.e. unsigned word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and written to the 'out0' vector
- */
-#define LASX_DP2ADD_W_HU_H(in0, in1, in2, out0)           \
-{                                                         \
-    __m256i _tmp0_m;                                      \
-                                                          \
-    _tmp0_m = __lasx_xvmaddwev_w_hu_h( in0, in1, in2 );   \
-    out0 = __lasx_xvmaddwod_w_hu_h( _tmp0_m, in1, in2 );  \
-}
-
-#define LASX_DP2ADD_W_HU_H_2(in0, in1, in2, in3, in4, in5, out0, out1) \
-{                                                                      \
-    LASX_DP2ADD_W_HU_H(in0, in1, in2, out0);                           \
-    LASX_DP2ADD_W_HU_H(in3, in4, in5, out1);                           \
-}
-
-#define LASX_DP2ADD_W_HU_H_4(in0, in1, in2, in3, in4, in5,             \
-                             in6, in7, in8, in9, in10, in11,           \
-                             out0, out1, out2, out3)                   \
-{                                                                      \
-    LASX_DP2ADD_W_HU_H_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
-    LASX_DP2ADD_W_HU_H_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
-}
-
-/* Description : Vector Unsigned Dot Product and Subtract.
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Unsigned byte elements from 'in0' are iniplied with
- *               unsigned byte elements from 'in0' producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and subtract from double width elements,
- *               then written to the 'out0' vector.
- */
-#define LASX_DP2SUB_H_BU(in0, in1, in2, out0)             \
-{                                                         \
-    __m256i _tmp0_m;                                      \
-                                                          \
-    _tmp0_m = __lasx_xvmulwev_h_bu( in1, in2 );           \
-    _tmp0_m = __lasx_xvmaddwod_h_bu( _tmp0_m, in1, in2 ); \
-    out0 = __lasx_xvsub_h( in0, _tmp0_m );                \
-}
-
-#define LASX_DP2SUB_H_BU_2(in0, in1, in2, in3, in4, in5, out0, out1) \
-{                                                                    \
-    LASX_DP2SUB_H_BU(in0, in1, in2, out0);                           \
-    LASX_DP2SUB_H_BU(in0, in1, in2, out0);                           \
-}
-
-#define LASX_DP2SUB_H_BU_4(in0, in1, in2, in3, in4, in5,             \
-                           in6, in7, in8, in9, in10, in11,           \
-                           out0, out1, out2, out3)                   \
-{                                                                    \
-    LASX_DP2SUB_H_BU_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
-    LASX_DP2SUB_H_BU_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
-}
-
-/* Description : Vector Signed Dot Product and Subtract.
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Signed halfword elements from 'in0' are iniplied with
- *               signed halfword elements from 'in0' producing a result
- *               twice the size of input i.e. signed word.
- *               Multiplication result of adjacent odd-even elements
- *               are added together and subtract from double width elements,
- *               then written to the 'out0' vector.
- */
-#define LASX_DP2SUB_W_H(in0, in1, in2, out0)             \
-{                                                        \
-    __m256i _tmp0_m;                                     \
-                                                         \
-    _tmp0_m = __lasx_xvmulwev_w_h( in1, in2 );           \
-    _tmp0_m = __lasx_xvmaddwod_w_h( _tmp0_m, in1, in2 ); \
-    out0 = __lasx_xvsub_w( in0, _tmp0_m );               \
-}
-
-#define LASX_DP2SUB_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1) \
-{                                                                   \
-    LASX_DP2SUB_W_H(in0, in1, in2, out0);                           \
-    LASX_DP2SUB_W_H(in3, in4, in5, out1);                           \
-}
-
-#define LASX_DP2SUB_W_H_4(in0, in1, in2, in3, in4, in5,             \
-                          in6, in7, in8, in9, in10, in11,           \
-                          out0, out1, out2, out3)                   \
-{                                                                   \
-    LASX_DP2SUB_W_H_2(in0, in1, in2, in3, in4, in5, out0, out1);    \
-    LASX_DP2SUB_W_H_2(in6, in7, in8, in9, in10, in11, out2, out3);  \
-}
-
-/* Description : Dot product of half word vector elements
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- *               Return Type - signed word
- * Details     : Signed half word elements from in* are iniplied with
- *               signed half word elements from in* producing a result
- *               twice the size of input i.e. signed word.
- *               Then this iniplication results of four adjacent elements
- *               are added together and stored to the out vector.
- * Example     : LASX_DP2_W_H(in0, in0, out0)
- *               in0:   3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1,
- *               in0:   -2,1,1,0, 1,0,0,0, 0,0,1,0, 1,0,0,1,
- *               out0:  -2,0,1,1,
- */
-#define LASX_DP4_D_H(in0, in1, out0)                         \
-{                                                            \
-    __m256i _tmp0_m ;                                        \
-                                                             \
-    _tmp0_m = __lasx_xvmulwev_w_h( in0, in1 );               \
-    _tmp0_m = __lasx_xvmaddwod_w_h( _tmp0_m, in0, in1 );     \
-    out0  = __lasx_xvhaddw_d_w( _tmp0_m, _tmp0_m );          \
-}
-#define LASX_DP4_D_H_2(in0, in1, in2, in3, out0, out1)       \
-{                                                            \
-    LASX_DP4_D_H(in0, in1, out0);                            \
-    LASX_DP4_D_H(in2, in3, out1);                            \
-}
-#define LASX_DP4_D_H_4(in0, in1, in2, in3,                            \
-                       in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                     \
-    LASX_DP4_D_H_2(in0, in1, in2, in3, out0, out1);                   \
-    LASX_DP4_D_H_2(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The high half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               higher half of the two-fold sign extension ( signed byte
- *               to signed half word ) and stored to the out vector.
- * Example     : see LASX_ADDWL_W_H_128SV
- */
-#define LASX_ADDWH_H_B_128SV(in0, in1, out0)                                  \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvilvh_b( in0, in0 );                                    \
-    _tmp1_m = __lasx_xvilvh_b( in1, in1 );                                    \
-    out0 = __lasx_xvaddwev_h_b( _tmp0_m, _tmp1_m );                           \
-}
-#define LASX_ADDWH_H_B_2_128SV(in0, in1, in2, in3, out0, out1)                \
-{                                                                             \
-    LASX_ADDWH_H_B_128SV(in0, in1, out0);                                     \
-    LASX_ADDWH_H_B_128SV(in2, in3, out1);                                     \
-}
-#define LASX_ADDWH_H_B_4_128SV(in0, in1, in2, in3,                            \
-                               in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                             \
-    LASX_ADDWH_H_B_2_128SV(in0, in1, in2, in3, out0, out1);                   \
-    LASX_ADDWH_H_B_2_128SV(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The high half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               higher half of the two-fold sign extension ( signed half word
- *               to signed word ) and stored to the out vector.
- * Example     : see LASX_ADDWL_W_H_128SV
- */
-#define LASX_ADDWH_W_H_128SV(in0, in1, out0)                                  \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvilvh_h( in0, in0 );                                    \
-    _tmp1_m = __lasx_xvilvh_h( in1, in1 );                                    \
-    out0 = __lasx_xvaddwev_w_h( _tmp0_m, _tmp1_m );                           \
-}
-#define LASX_ADDWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1)                \
-{                                                                             \
-    LASX_ADDWH_W_H_128SV(in0, in1, out0);                                     \
-    LASX_ADDWH_W_H_128SV(in2, in3, out1);                                     \
-}
-#define LASX_ADDWH_W_H_4_128SV(in0, in1, in2, in3,                            \
-                               in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                             \
-    LASX_ADDWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                   \
-    LASX_ADDWH_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               lower half of the two-fold sign extension ( signed byte
- *               to signed half word ) and stored to the out vector.
- * Example     : see LASX_ADDWL_W_H_128SV
- */
-#define LASX_ADDWL_H_B_128SV(in0, in1, out0)                                  \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvsllwil_h_b( in0, 0 );                                  \
-    _tmp1_m = __lasx_xvsllwil_h_b( in1, 0 );                                  \
-    out0 = __lasx_xvadd_h( _tmp0_m, _tmp1_m );                                \
-}
-#define LASX_ADDWL_H_B_2_128SV(in0, in1, in2, in3, out0, out1)                \
-{                                                                             \
-    LASX_ADDWL_H_B_128SV(in0, in1, out0);                                     \
-    LASX_ADDWL_H_B_128SV(in2, in3, out1);                                     \
-}
-#define LASX_ADDWL_H_B_4_128SV(in0, in1, in2, in3,                            \
-                               in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                             \
-    LASX_ADDWL_H_B_2_128SV(in0, in1, in2, in3, out0, out1);                   \
-    LASX_ADDWL_H_B_2_128SV(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               lower half of the two-fold sign extension ( signed half word
- *               to signed word ) and stored to the out vector.
- * Example     : LASX_ADDWL_W_H_128SV(in0, in1, out0)
- *               in0   3,0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1,
- *               in1   2,-1,1,2, 1,0,0,0, 1,0,1,0, 1,0,0,1,
- *               out0  5,-1,4,2, 1,0,2,-1,
- */
-#define LASX_ADDWL_W_H_128SV(in0, in1, out0)                                  \
-{                                                                             \
-    __m256i _tmp0_m;                                                          \
-                                                                              \
-    _tmp0_m = __lasx_xvilvl_h(in1, in0);                                      \
-    out0 = __lasx_xvhaddw_w_h( _tmp0_m, _tmp0_m );                            \
-}
-#define LASX_ADDWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1)                \
-{                                                                             \
-    LASX_ADDWL_W_H_128SV(in0, in1, out0);                                     \
-    LASX_ADDWL_W_H_128SV(in2, in3, out1);                                     \
-}
-#define LASX_ADDWL_W_H_4_128SV(in0, in1, in2, in3,                            \
-                               in4, in5, in6, in7, out0, out1, out2, out3)    \
-{                                                                             \
-    LASX_ADDWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                   \
-    LASX_ADDWL_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                   \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in0 vector and the in1 vector are added after the
- *               lower half of the two-fold zero extension ( unsigned byte
- *               to unsigned half word ) and stored to the out vector.
- */
-#define LASX_ADDWL_H_BU_128SV(in0, in1, out0)                                 \
-{                                                                             \
-    __m256i _tmp0_m;                                                          \
-                                                                              \
-    _tmp0_m = __lasx_xvilvl_b(in1, in0);                                      \
-    out0 = __lasx_xvhaddw_hu_bu( _tmp0_m, _tmp0_m );                          \
-}
-#define LASX_ADDWL_H_BU_2_128SV(in0, in1, in2, in3, out0, out1)               \
-{                                                                             \
-    LASX_ADDWL_H_BU_128SV(in0, in1, out0);                                    \
-    LASX_ADDWL_H_BU_128SV(in2, in3, out1);                                    \
-}
-#define LASX_ADDWL_H_BU_4_128SV(in0, in1, in2, in3,                           \
-                                in4, in5, in6, in7, out0, out1, out2, out3)   \
-{                                                                             \
-    LASX_ADDWL_H_BU_2_128SV(in0, in1, in2, in3, out0, out1);                  \
-    LASX_ADDWL_H_BU_2_128SV(in4, in5, in6, in7, out2, out3);                  \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : In1 vector plus in0 vector after double zero extension
- *               ( unsigned byte to half word ),add and stored to the out vector.
- * Example     : reference to LASX_ADDW_W_W_H_128SV(in0, in1, out0)
- */
-#define LASX_ADDW_H_H_BU_128SV(in0, in1, out0)                                \
-{                                                                             \
-    __m256i _tmp1_m;                                                          \
-                                                                              \
-    _tmp1_m = __lasx_xvsllwil_hu_bu( in1, 0 );                                \
-    out0 = __lasx_xvadd_h( in0, _tmp1_m );                                    \
-}
-#define LASX_ADDW_H_H_BU_2_128SV(in0, in1, in2, in3, out0, out1)              \
-{                                                                             \
-    LASX_ADDW_H_H_BU_128SV(in0, in1, out0);                                   \
-    LASX_ADDW_H_H_BU_128SV(in2, in3, out1);                                   \
-}
-#define LASX_ADDW_H_H_BU_4_128SV(in0, in1, in2, in3,                          \
-                                 in4, in5, in6, in7, out0, out1, out2, out3)  \
-{                                                                             \
-    LASX_ADDW_H_H_BU_2_128SV(in0, in1, in2, in3, out0, out1);                 \
-    LASX_ADDW_H_H_BU_2_128SV(in4, in5, in6, in7, out2, out3);                 \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : In1 vector plus in0 vector after double sign extension
- *               ( signed half word to word ),add and stored to the out vector.
- * Example     : LASX_ADDW_W_W_H_128SV(in0, in1, out0)
- *               in0   0,1,0,0, -1,0,0,1,
- *               in1   2,-1,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1,
- *               out0  2,0,1,2, -1,0,1,1,
- */
-#define LASX_ADDW_W_W_H_128SV(in0, in1, out0)                                 \
-{                                                                             \
-    __m256i _tmp1_m;                                                          \
-                                                                              \
-    _tmp1_m = __lasx_xvsllwil_w_h( in1, 0 );                                  \
-    out0 = __lasx_xvadd_w( in0, _tmp1_m );                                    \
-}
-#define LASX_ADDW_W_W_H_2_128SV(in0, in1, in2, in3, out0, out1)               \
-{                                                                             \
-    LASX_ADDW_W_W_H_128SV(in0, in1, out0);                                    \
-    LASX_ADDW_W_W_H_128SV(in2, in3, out1);                                    \
-}
-#define LASX_ADDW_W_W_H_4_128SV(in0, in1, in2, in3,                           \
-                                in4, in5, in6, in7, out0, out1, out2, out3)   \
-{                                                                             \
-    LASX_ADDW_W_W_H_2_128SV(in0, in1, in2, in3, out0, out1);                  \
-    LASX_ADDW_W_W_H_2_128SV(in4, in5, in6, in7, out2, out3);                  \
-}
-
-/* Description : Multiplication and addition calculation after expansion
- *               of the lower half of the vector
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector and the in0 vector are multiplied after
- *               the lower half of the two-fold sign extension ( signed
- *               half word to signed word ) , and the result is added to
- *               the vector in0, the stored to the out vector.
- * Example     : LASX_MADDWL_W_H_128SV(in0, in1, in2, out0)
- *               in0   1,2,3,4, 5,6,7 8
- *               in1   1,2,3,4, 1,2,3,4, 5,6,7,8, 5,6,7,8
- *               in2   200,300,400,500, 2000,3000,4000,5000,
- *                     -200,-300,-400,-500, -2000,-3000,-4000,-5000
- *               out0  5,-1,4,2, 1,0,2,-1,
- */
-#define LASX_MADDWL_W_H_128SV(in0, in1, in2, out0)                            \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvsllwil_w_h( in1, 0 );                                  \
-    _tmp1_m = __lasx_xvsllwil_w_h( in2, 0 );                                  \
-    _tmp0_m = __lasx_xvmul_w( _tmp0_m, _tmp1_m );                             \
-    out0 = __lasx_xvadd_w( _tmp0_m, in0 );                                    \
-}
-#define LASX_MADDWL_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1)     \
-{                                                                             \
-    LASX_MADDWL_W_H_128SV(in0, in1, in2, out0);                               \
-    LASX_MADDWL_W_H_128SV(in3, in4, in5, out1);                               \
-}
-#define LASX_MADDWL_W_H_4_128SV(in0, in1, in2, in3, in4, in5,                \
-                                in6, in7, in8, in9, in10, in11,              \
-                                out0, out1, out2, out3)                      \
-{                                                                            \
-    LASX_MADDWL_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1);       \
-    LASX_MADDWL_W_H_2_128SV(in6, in7, in8, in9, in10, in11, out2, out3);     \
-}
-
-/* Description : Multiplication and addition calculation after expansion
- *               of the higher half of the vector
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector and the in0 vector are multiplied after
- *               the higher half of the two-fold sign extension ( signed
- *               half word to signed word ) , and the result is added to
- *               the vector in0, the stored to the out vector.
- * Example     : see LASX_MADDWL_W_H_128SV
- */
-#define LASX_MADDWH_W_H_128SV(in0, in1, in2, out0)                            \
-{                                                                             \
-    __m256i _tmp0_m, _tmp1_m;                                                 \
-                                                                              \
-    _tmp0_m = __lasx_xvilvh_h( in1, in1 );                                    \
-    _tmp1_m = __lasx_xvilvh_h( in2, in2 );                                    \
-    _tmp0_m = __lasx_xvmulwev_w_h( _tmp0_m, _tmp1_m );                        \
-    out0 = __lasx_xvadd_w( _tmp0_m, in0 );                                    \
-}
-#define LASX_MADDWH_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1)     \
-{                                                                             \
-    LASX_MADDWH_W_H_128SV(in0, in1, in2, out0);                               \
-    LASX_MADDWH_W_H_128SV(in3, in4, in5, out1);                               \
-}
-#define LASX_MADDWH_W_H_4_128SV(in0, in1, in2, in3, in4, in5,                \
-                                in6, in7, in8, in9, in10, in11,              \
-                                out0, out1, out2, out3)                      \
-{                                                                            \
-    LASX_MADDWH_W_H_2_128SV(in0, in1, in2, in3, in4, in5, out0, out1);       \
-    LASX_MADDWH_W_H_2_128SV(in6, in7, in8, in9, in10, in11, out2, out3);     \
-}
-
-/* Description : Multiplication calculation after expansion
- *               of the lower half of the vector
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector and the in0 vector are multiplied after
- *               the lower half of the two-fold sign extension ( signed
- *               half word to signed word ) , the stored to the out vector.
- * Example     : LASX_MULWL_W_H_128SV(in0, in1, out0)
- *               in0   3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1,
- *               in1   2,-1,1,2, 1,0,0,0,  0,0,1,0, 1,0,0,1,
- *               out0  6,1,3,0, 0,0,1,0,
- */
-#define LASX_MULWL_W_H_128SV(in0, in1, out0)                    \
-{                                                               \
-    __m256i _tmp0_m, _tmp1_m;                                   \
-                                                                \
-    _tmp0_m = __lasx_xvsllwil_w_h( in0, 0 );                    \
-    _tmp1_m = __lasx_xvsllwil_w_h( in1, 0 );                    \
-    out0 = __lasx_xvmul_w( _tmp0_m, _tmp1_m );                  \
-}
-#define LASX_MULWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1)  \
-{                                                               \
-    LASX_MULWL_W_H_128SV(in0, in1, out0);                       \
-    LASX_MULWL_W_H_128SV(in2, in3, out1);                       \
-}
-#define LASX_MULWL_W_H_4_128SV(in0, in1, in2, in3,              \
-                               in4, in5, in6, in7,              \
-                               out0, out1, out2, out3)          \
-{                                                               \
-    LASX_MULWL_W_H_2_128SV(in0, in1, in2, in3, out0, out1);     \
-    LASX_MULWL_W_H_2_128SV(in4, in5, in6, in7, out2, out3);     \
-}
-
-/* Description : Multiplication calculation after expansion
- *               of the lower half of the vector
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector and the in0 vector are multiplied after
- *               the lower half of the two-fold sign extension ( signed
- *               half word to signed word ) , the stored to the out vector.
- * Example     : see LASX_MULWL_W_H_128SV
- */
-#define LASX_MULWH_W_H_128SV(in0, in1, out0)                    \
-{                                                               \
-    __m256i _tmp0_m, _tmp1_m;                                   \
-                                                                \
-    _tmp0_m = __lasx_xvilvh_h( in0, in0 );                      \
-    _tmp1_m = __lasx_xvilvh_h( in1, in1 );                      \
-    out0 = __lasx_xvmulwev_w_h( _tmp0_m, _tmp1_m );             \
-}
-#define LASX_MULWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1)  \
-{                                                               \
-    LASX_MULWH_W_H_128SV(in0, in1, out0);                       \
-    LASX_MULWH_W_H_128SV(in2, in3, out1);                       \
-}
-#define LASX_MULWH_W_H_4_128SV(in0, in1, in2, in3,              \
-                               in4, in5, in6, in7,              \
-                               out0, out1, out2, out3)          \
-{                                                               \
-    LASX_MULWH_W_H_2_128SV(in0, in1, in2, in3, out0, out1);     \
-    LASX_MULWH_W_H_2_128SV(in4, in5, in6, in7, out2, out3);     \
-}
-
-/* Description : The low half of the vector elements are expanded and
- *               added after being doubled
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0,  out1,  ~
- * Details     : The in1 vector add the in0 vector after the
- *               lower half of the two-fold zero extension ( unsigned byte
- *               to unsigned half word ) and stored to the out vector.
- */
-#define LASX_SADDW_HU_HU_BU_128SV(in0, in1, out0)                    \
-{                                                                    \
-    __m256i _tmp1_m;                                                 \
-    __m256i _zero_m = { 0 };                                         \
-                                                                     \
-    _tmp1_m = __lasx_xvilvl_b( _zero_m, in1 );                       \
-    out0 = __lasx_xvsadd_hu( in0, _tmp1_m );                         \
-}
-#define LASX_SADDW_HU_HU_BU_2_128SV(in0, in1, in2, in3, out0, out1)  \
-{                                                                    \
-    LASX_SADDW_HU_HU_BU_128SV(in0, in1, out0);                       \
-    LASX_SADDW_HU_HU_BU_128SV(in2, in3, out1);                       \
-}
-#define LASX_SADDW_HU_HU_BU_4_128SV(in0, in1, in2, in3,              \
-                                    in4, in5, in6, in7,              \
-                                    out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_SADDW_HU_HU_BU_2_128SV(in0, in1, in2, in3, out0, out1);     \
-    LASX_SADDW_HU_HU_BU_2_128SV(in4, in5, in6, in7, out2, out3);     \
-}
-
-/* Description : Low 8-bit vector elements unsigned extension to halfword
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low 8-bit elements from in0 unsigned extension to halfword,
- *               written to output vector out0. Similar for in1.
- * Example     : See LASX_UNPCK_L_W_H(in0, out0)
- */
-#define LASX_UNPCK_L_HU_BU(in0, out0)                                          \
-{                                                                              \
-    out0 = __lasx_vext2xv_hu_bu(in0);                                          \
-}
-
-#define LASX_UNPCK_L_HU_BU_2(in0, in1, out0, out1)                             \
-{                                                                              \
-    LASX_UNPCK_L_HU_BU(in0, out0);                                             \
-    LASX_UNPCK_L_HU_BU(in1, out1);                                             \
-}
-
-#define LASX_UNPCK_L_HU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3)       \
-{                                                                              \
-    LASX_UNPCK_L_HU_BU_2(in0, in1, out0, out1);                                \
-    LASX_UNPCK_L_HU_BU_2(in2, in3, out2, out3);                                \
-}
-
-#define LASX_UNPCK_L_HU_BU_8(in0, in1, in2, in3, in4, in5, in6, in7,           \
-                             out0, out1, out2, out3, out4, out5, out6, out7)   \
-{                                                                              \
-    LASX_UNPCK_L_HU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3);          \
-    LASX_UNPCK_L_HU_BU_4(in4, in5, in6, in7, out4, out5, out6, out7);          \
-}
-
-/* Description : Low 8-bit vector elements unsigned extension to word
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low 8-bit elements from in0 unsigned extension to word,
- *               written to output vector out0. Similar for in1.
- * Example     : See LASX_UNPCK_L_W_H(in0, out0)
- */
-#define LASX_UNPCK_L_WU_BU(in0, out0)                                         \
-{                                                                             \
-    out0 = __lasx_vext2xv_wu_bu(in0);                                         \
-}
-
-#define LASX_UNPCK_L_WU_BU_2(in0, in1, out0, out1)                            \
-{                                                                             \
-    LASX_UNPCK_L_WU_BU(in0, out0);                                            \
-    LASX_UNPCK_L_WU_BU(in1, out1);                                            \
-}
-
-#define LASX_UNPCK_L_WU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3)      \
-{                                                                             \
-    LASX_UNPCK_L_WU_BU_2(in0, in1, out0, out1);                               \
-    LASX_UNPCK_L_WU_BU_2(in2, in3, out2, out3);                               \
-}
-
-#define LASX_UNPCK_L_WU_BU_8(in0, in1, in2, in3, in4, in5, in6, in7,          \
-                             out0, out1, out2, out3, out4, out5, out6, out7)  \
-{                                                                             \
-    LASX_UNPCK_L_WU_BU_4(in0, in1, in2, in3, out0, out1, out2, out3);         \
-    LASX_UNPCK_L_WU_BU_4(in4, in5, in6, in7, out4, out5, out6, out7);         \
-}
-
-/* Description : Low 8-bit vector elements signed extension to halfword
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low 8-bit elements from in0 signed extension to halfword,
- *               written to output vector out0. Similar for in1.
- * Example     : See LASX_UNPCK_L_W_H(in0, out0)
- */
-#define LASX_UNPCK_L_H_B(in0, out0)                                          \
-{                                                                            \
-    out0 = __lasx_vext2xv_h_b(in0);                                          \
-}
-
-#define LASX_UNPCK_L_H_B_2(in0, in1, out0, out1)                             \
-{                                                                            \
-    LASX_UNPCK_L_H_B(in0, out0);                                             \
-    LASX_UNPCK_L_H_B(in1, out1);                                             \
-}
-
-#define LASX_UNPCK_L_H_B_4(in0, in1, in2, in3, out0, out1, out2, out3)       \
-{                                                                            \
-    LASX_UNPCK_L_H_B_2(in0, in1, out0, out1);                                \
-    LASX_UNPCK_L_H_B_2(in2, in3, out2, out3);                                \
-}
-
-/* Description : Low halfword vector elements signed extension to word
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low halfword elements from in0 signed extension to
- *               word, written to output vector out0. Similar for in1.
- *               Similar for other pairs.
- * Example     : LASX_UNPCK_L_W_H(in0, out0)
- *         in0 : 3, 0, 3, 0,  0, 0, 0, -1,  0, 0, 1, 1,  0, 0, 0, 1
- *        out0 : 3, 0, 3, 0,  0, 0, 0, -1
- */
-#define LASX_UNPCK_L_W_H(in0, out0)                                         \
-{                                                                           \
-    out0 = __lasx_vext2xv_w_h(in0);                                         \
-}
-
-#define LASX_UNPCK_L_W_H_2(in0, in1, out0, out1)                            \
-{                                                                           \
-    LASX_UNPCK_L_W_H(in0, out0);                                            \
-    LASX_UNPCK_L_W_H(in1, out1);                                            \
-}
-
-#define LASX_UNPCK_L_W_H_4(in0, in1, in2, in3, out0, out1, out2, out3)      \
-{                                                                           \
-    LASX_UNPCK_L_W_H_2(in0, in1, out0, out1);                               \
-    LASX_UNPCK_L_W_H_2(in2, in3, out2, out3);                               \
-}
-
-#define LASX_UNPCK_L_W_H_8(in0, in1, in2, in3, in4, in5, in6, in7,          \
-                           out0, out1, out2, out3, out4, out5, out6, out7)  \
-{                                                                           \
-    LASX_UNPCK_L_W_H_4(in0, in1, in2, in3, out0, out1, out2, out3);         \
-    LASX_UNPCK_L_W_H_4(in4, in5, in6, in7, out4, out5, out6, out7);         \
-}
-
-/* Description : Interleave odd byte elements from vectors.
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out, out0, ~
- * Details     : Odd byte elements of in_h and odd byte
- *               elements of in_l are interleaved and copied to out.
- * Example     : See LASX_ILVOD_W(in_h, in_l, out)
- */
-#define LASX_ILVOD_B(in_h, in_l, out)                                            \
-{                                                                                \
-    out = __lasx_xvpackod_b((in_h, in1_l);                                       \
-}
-
-#define LASX_ILVOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                   \
-{                                                                                \
-    LASX_ILVOD_B(in0_h, in0_l, out0);                                            \
-    LASX_ILVOD_B(in1_h, in1_l, out1);                                            \
-}
-
-#define LASX_ILVOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,   \
-                       out0, out1, out2, out3)                                   \
-{                                                                                \
-    LASX_ILVOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                      \
-    LASX_ILVOD_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                      \
-}
-
-/* Description : Interleave odd half word elements from vectors.
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out, out0, ~
- * Details     : Odd half word elements of in_h and odd half word
- *               elements of in_l are interleaved and copied to out.
- * Example     : See LASX_ILVOD_W(in_h, in_l, out)
- */
-#define LASX_ILVOD_H(in_h, in_l, out)                                           \
-{                                                                               \
-    out = __lasx_xvpackod_h(in_h, in_l);                                        \
-}
-
-#define LASX_ILVOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
-{                                                                               \
-    LASX_ILVOD_H(in0_h, in0_l, out0);                                           \
-    LASX_ILVOD_H(in1_h, in1_l, out1);                                           \
-}
-
-#define LASX_ILVOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                       out0, out1, out2, out3)                                  \
-{                                                                               \
-    LASX_ILVOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
-    LASX_ILVOD_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
-}
-
-/* Description : Interleave odd word elements from vectors.
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out, out0, ~
- * Details     : Odd word elements of in_h and odd word
- *               elements of in_l are interleaved and copied to out.
- * Example     : See LASX_ILVOD_W(in_h, in_l, out)
- *        in_h : 1, 2, 3, 4,   5, 6, 7, 8
- *        in_l : 1, 0, 3, 1,   1, 2, 3, 4
- *         out : 0, 2, 1, 4,   2, 6, 4, 8
- */
-#define LASX_ILVOD_W(in_h, in_l, out)                                           \
-{                                                                               \
-    out = __lasx_xvpackod_w(in_h, in_l);                                        \
-}
-
-#define LASX_ILVOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
-{                                                                               \
-    LASX_ILVOD_W(in0_h, in0_l, out0);                                           \
-    LASX_ILVOD_W(in1_h, in1_l, out1);                                           \
-}
-
-#define LASX_ILVOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                       out0, out1, out2, out3)                                  \
-{                                                                               \
-    LASX_ILVOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
-    LASX_ILVOD_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
-}
-
-/* Description : Interleave odd double word elements from vectors.
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out, out0, ~
- * Details     : Odd double word elements of in_h and odd double word
- *               elements of in_l are interleaved and copied to out.
- * Example     : LASX_ILVOD_W(in_h, in_l, out)
- */
-#define LASX_ILVOD_D(in_h, in_l, out)                                           \
-{                                                                               \
-    out = __lasx_xvpackod_d(in_h, in_l);                                        \
-}
-
-#define LASX_ILVOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                  \
-{                                                                               \
-    LASX_ILVOD_D(in0_h, in0_l, out0);                                           \
-    LASX_ILVOD_D(in1_h, in1_l, out1);                                           \
-}
-
-#define LASX_ILVOD_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                       out0, out1, out2, out3)                                  \
-{                                                                               \
-    LASX_ILVOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                     \
-    LASX_ILVOD_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                     \
-}
-
-/* Description : Interleave right half of byte elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of byte elements of in_l and high half of byte
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : See LASX_ILVL_W(in_h, in_l, out0)
- */
-#define LASX_ILVL_B(in_h, in_l, out0)                                      \
-{                                                                          \
-    __m256i tmp0, tmp1;                                                    \
-    tmp0 = __lasx_xvilvl_b(in_h, in_l);                                    \
-    tmp1 = __lasx_xvilvh_b(in_h, in_l);                                    \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
-}
-
-#define LASX_ILVL_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVL_B(in0_h, in0_l, out0)                                  \
-    LASX_ILVL_B(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVL_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVL_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVL_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVL_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVL_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVL_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of byte elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of byte elements of in_l and low half of byte
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVL_B_128SV(in_h, in_l, out0)                                   \
-{                                                                             \
-    out0 = __lasx_xvilvl_b(in_h, in_l);                                       \
-}
-
-#define LASX_ILVL_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVL_B_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVL_B_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVL_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVL_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVL_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVL_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVL_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVL_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of half word elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of half word elements of in_l and right half of
- *               half word elements of in_h are interleaved and copied to
- *               out0. Similar for other pairs.
- * Example     : See LASX_ILVL_W(in_h, in_l, out0)
- */
-#define LASX_ILVL_H(in_h, in_l, out0)                                      \
-{                                                                          \
-    __m256i tmp0, tmp1;                                                    \
-    tmp0 = __lasx_xvilvl_h(in_h, in_l);                                    \
-    tmp1 = __lasx_xvilvh_h(in_h, in_l);                                    \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
-}
-
-#define LASX_ILVL_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVL_H(in0_h, in0_l, out0)                                  \
-    LASX_ILVL_H(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVL_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVL_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVL_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVL_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVL_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVL_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of half word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of half word elements of in_l and low half of half
- *               word elements of in_h are interleaved and copied to
- *               out0. Similar for other pairs.
- * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVL_H_128SV(in_h, in_l, out0)                                   \
-{                                                                             \
-    out0 = __lasx_xvilvl_h(in_h, in_l);                                       \
-}
-
-#define LASX_ILVL_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVL_H_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVL_H_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVL_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVL_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVL_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVL_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVL_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVL_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of word elements from vectors
- * Arguments   : Inputs  - in0_h, in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of halfword elements of in_l and low half of word
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : LASX_ILVL_W(in_h, in_l, out0)
- *        in_h : 0, 1, 0, 1,  0, 1, 0, 1
- *        in_l : 1, 2, 3, 4,  5, 6, 7, 8
- *        out0 : 1, 0, 2, 1,  3, 0, 4, 1
- */
-#define LASX_ILVL_W(in_h, in_l, out0)                                      \
-{                                                                          \
-    __m256i tmp0, tmp1;                                                    \
-    tmp0 = __lasx_xvilvl_w(in_h, in_l);                                    \
-    tmp1 = __lasx_xvilvh_w(in_h, in_l);                                    \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                             \
-}
-
-#define LASX_ILVL_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVL_W(in0_h, in0_l, out0)                                  \
-    LASX_ILVL_W(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVL_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVL_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVL_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVL_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVL_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVL_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h, in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of halfword elements of in_l and low half of word
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : LASX_ILVL_W_128SV(in_h, in_l, out0)
- *        in_h : 0, 1, 0, 1, 0, 1, 0, 1
- *        in_l : 1, 2, 3, 4, 5, 6, 7, 8
- *        out0 : 1, 0, 2, 1, 5, 0, 6, 1
- */
-#define LASX_ILVL_W_128SV(in_h, in_l, out0)                             \
-{                                                                       \
-    out0 = __lasx_xvilvl_w(in_h, in_l);                                 \
-}
-
-#define LASX_ILVL_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVL_W_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVL_W_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVL_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVL_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVL_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVL_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVL_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVL_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave low half of double word elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Low half of double word elements of in_l and low half of
- *               double word elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs
- * Example     : See LASX_ILVL_W(in_h, in_l, out0)
- */
-#define LASX_ILVL_D(in_h, in_l, out0)                                   \
-{                                                                       \
-    __m256i tmp0, tmp1;                                                 \
-    tmp0 = __lasx_xvilvl_d(in_h, in_l);                                 \
-    tmp1 = __lasx_xvilvh_d(in_h, in_l);                                 \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
-}
-
-#define LASX_ILVL_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVL_D(in0_h, in0_l, out0)                                  \
-    LASX_ILVL_D(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVL_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVL_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVL_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVL_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVL_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVL_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave right half of double word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Right half of double word elements of in_l and right half of
- *               double word elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : See LASX_ILVL_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVL_D_128SV(in_h, in_l, out0)                              \
-{                                                                        \
-    out0 = __lasx_xvilvl_d(in_h, in_l);                                  \
-}
-
-#define LASX_ILVL_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVL_D_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVL_D_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVL_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVL_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVL_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVL_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVL_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVL_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of byte elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of byte elements of in_l and high half of
- *               byte
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W(in_h, in_l, out0)
- */
-#define LASX_ILVH_B(in_h, in_l, out0)                            \
-{                                                                \
-    __m256i tmp0, tmp1;                                          \
-    tmp0 = __lasx_xvilvl_b(in_h, in_l);                          \
-    tmp1 = __lasx_xvilvh_b(in_h, in_l);                          \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
-}
-
-#define LASX_ILVH_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVH_B(in0_h, in0_l, out0)                                  \
-    LASX_ILVH_B(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVH_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVH_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVH_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVH_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of byte elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of  byte elements  of  in_l and high half
- *               of byte elements of in_h are interleaved and copied
- *               to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVH_B_128SV(in_h, in_l, out0)                     \
-{                                                               \
-    out0 = __lasx_xvilvh_b(in_h, in_l);                         \
-}
-
-#define LASX_ILVH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVH_B_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVH_B_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVH_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVH_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVH_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of half word elements from vectors
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of half word elements of in_l and high half of
- *               half word
- *               elements of in_h are interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W(in_h, in_l, out0)
- */
-#define LASX_ILVH_H(in_h, in_l, out0)                           \
-{                                                                \
-    __m256i tmp0, tmp1;                                          \
-    tmp0 = __lasx_xvilvl_h(in_h, in_l);                          \
-    tmp1 = __lasx_xvilvh_h(in_h, in_l);                          \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
-}
-
-#define LASX_ILVH_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVH_H(in0_h, in0_l, out0)                                  \
-    LASX_ILVH_H(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVH_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVH_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVH_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVH_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of half word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of  half word elements  of  in_l and high half
- *               of half word elements of in_h are interleaved and copied
- *               to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVH_H_128SV(in_h, in_l, out0)                     \
-{                                                               \
-    out0 = __lasx_xvilvh_h(in_h, in_l);                         \
-}
-
-#define LASX_ILVH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVH_H_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVH_H_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVH_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVH_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVH_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of word elements from vectors
- * Arguments   : Inputs  - in0_h, in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of word elements of in_l and high half of
- *               word elements of in_h are interleaved and copied to
- *               out0.
- *               Similar for other pairs.
- * Example     : LASX_ILVH_W(in_h, in_l, out0)
- *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
- *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
- *         out0: 5, -5,  6, -6,  7, -7,  8, -8
- */
-#define LASX_ILVH_W(in_h, in_l, out0)                            \
-{                                                                \
-    __m256i tmp0, tmp1;                                          \
-    tmp0 = __lasx_xvilvl_w(in_h, in_l);                          \
-    tmp1 = __lasx_xvilvh_w(in_h, in_l);                          \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                   \
-}
-
-#define LASX_ILVH_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVH_W(in0_h, in0_l, out0)                                  \
-    LASX_ILVH_W(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVH_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVH_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVH_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVH_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h, in0_l, ~
- *               Outputs - out0, out1, ~
- * Details     : High half of word elements of every 128-bit of in_l
- *               and high half of word elements of every 128-bit of
- *               in_h are interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : LASX_ILVH_W_128SV(in_h, in_l, out0)
- *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
- *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
- *         out0: 3, -3,  4, -4,  7, -7,  8, -8*
- */
-#define LASX_ILVH_W_128SV(in_h, in_l, out0)                        \
-{                                                                  \
-    out0 = __lasx_xvilvh_w(in_h, in_l);                            \
-}
-
-#define LASX_ILVH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVH_W_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVH_W_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVH_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVH_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVH_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of double word elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out0, out1, ~
- :* Details    : High half of double word elements of in_l and high half of
- *               double word elements of in_h are interleaved and copied to
- *               out0.
- *               Similar for other pairs.
- * Example    : see LASX_ILVH_W(in_h, in_l, out0)
- */
-#define LASX_ILVH_D(in_h, in_l, out0)                           \
-{                                                               \
-    __m256i tmp0, tmp1;                                         \
-    tmp0 = __lasx_xvilvl_d(in_h, in_l);                         \
-    tmp1 = __lasx_xvilvh_d(in_h, in_l);                         \
-    out0 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                  \
-}
-
-#define LASX_ILVH_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)        \
-{                                                                    \
-    LASX_ILVH_D(in0_h, in0_l, out0)                                  \
-    LASX_ILVH_D(in1_h, in1_l, out1)                                  \
-}
-
-#define LASX_ILVH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                      in3_h, in3_l, out0, out1, out2, out3)          \
-{                                                                    \
-    LASX_ILVH_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)            \
-    LASX_ILVH_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)            \
-}
-
-#define LASX_ILVH_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                      in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                      out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                              \
-    LASX_ILVH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                  out0, out1, out2, out3);                                     \
-    LASX_ILVH_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                  out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave high half of double word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0,  in1,  ~
- *               Outputs - out0, out1, ~
- * Details     : High half of double word elements of every 128-bit in_l and
- *               high half of double word elements of every 128-bit in_h are
- *               interleaved and copied to out0.
- *               Similar for other pairs.
- * Example     : see LASX_ILVH_W_128SV(in_h, in_l, out0)
- */
-#define LASX_ILVH_D_128SV(in_h, in_l, out0)                             \
-{                                                                       \
-    out0 = __lasx_xvilvh_d(in_h, in_l);                                 \
-}
-
-#define LASX_ILVH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)           \
-{                                                                             \
-    LASX_ILVH_D_128SV(in0_h, in0_l, out0);                                    \
-    LASX_ILVH_D_128SV(in1_h, in1_l, out1);                                    \
-}
-
-#define LASX_ILVH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,         \
-                            in3_h, in3_l, out0, out1, out2, out3)             \
-{                                                                             \
-    LASX_ILVH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);              \
-    LASX_ILVH_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);              \
-}
-
-#define LASX_ILVH_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,  \
-                            in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7)          \
-{                                                                                    \
-    LASX_ILVH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,      \
-                        out0, out1, out2, out3);                                     \
-    LASX_ILVH_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,      \
-                        out4, out5, out6, out7);                                     \
-}
-
-/* Description : Interleave byte elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of  byte elements  of in_l and low half of byte
- *               elements  of in_h  are interleaved  and copied  to  out_l.
- *               High half of byte elements of in_l and high half of byte
- *               elements of in_h are interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W(in_h, in_l, out_l, out_h)
- */
-#define LASX_ILVLH_B(in_h, in_l, out_h, out_l)                          \
-{                                                                       \
-    __m256i tmp0, tmp1;                                                 \
-    tmp0  = __lasx_xvilvl_b(in_h, in_l);                                \
-    tmp1  = __lasx_xvilvh_b(in_h, in_l);                                \
-    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                         \
-    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                         \
-}
-
-#define LASX_ILVLH_B_2(in0_h, in0_l, in1_h, in1_l,                      \
-                       out0_h, out0_l, out1_h, out1_l)                  \
-{                                                                       \
-    LASX_ILVLH_B(in0_h, in0_l, out0_h, out0_l);                         \
-    LASX_ILVLH_B(in1_h, in1_l, out1_h, out1_l);                         \
-}
-
-#define LASX_ILVLH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
-{                                                                                       \
-    LASX_ILVLH_B_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
-    LASX_ILVLH_B_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
-}
-
-#define LASX_ILVLH_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                       \
-    LASX_ILVLH_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave byte elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of byte elements of in_l and low half of byte elements
- *               of in_h are interleaved and copied to out_l. High  half  of byte
- *               elements  of in_h  and high half  of byte elements  of in_l  are
- *               interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_B_128SV(in_h, in_l, out_h, out_l)                           \
-{                                                                              \
-    LASX_ILVL_B_128SV(in_h, in_l, out_l);                                      \
-    LASX_ILVH_B_128SV(in_h, in_l, out_h);                                      \
-}
-
-#define LASX_ILVLH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
-{                                                                                         \
-    LASX_ILVLH_B_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
-    LASX_ILVLH_B_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
-}
-
-#define LASX_ILVLH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
-{                                                                                              \
-    LASX_ILVLH_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
-    LASX_ILVLH_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
-}
-
-#define LASX_ILVLH_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                             \
-    LASX_ILVLH_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave half word elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of  half word elements  of in_l and low half of half
- *               word elements of in_h  are  interleaved  and  copied  to out_l.
- *               High half of half word elements of in_l and high half of half
- *               word elements of in_h are interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_H(in_h, in_l, out_h, out_l)                           \
-{                                                                        \
-    __m256i tmp0, tmp1;                                                  \
-    tmp0  = __lasx_xvilvl_h(in_h, in_l);                                 \
-    tmp1  = __lasx_xvilvh_h(in_h, in_l);                                 \
-    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
-    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
-}
-
-#define LASX_ILVLH_H_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
-                       out1_h, out1_l)                                   \
-{                                                                        \
-    LASX_ILVLH_H(in0_h, in0_l, out0_h, out0_l);                          \
-    LASX_ILVLH_H(in1_h, in1_l, out1_h, out1_l);                          \
-}
-
-#define LASX_ILVLH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
-{                                                                                       \
-    LASX_ILVLH_H_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
-    LASX_ILVLH_H_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
-}
-
-#define LASX_ILVLH_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                       \
-    LASX_ILVLH_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave half word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0_h, out0_l, ~
- * Details     : Low half of half word elements  of every 128-bit of in_l and
- *               low half of half word elements  of every 128-bit of in_h are
- *               interleaved and copied to out_l.
- *               High half of half word elements of every 128-bit of in_l and
- *               high half of half word elements of every 128-bit of in_h are
- *               interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_H_128SV(in_h, in_l, out_h, out_l)                            \
-{                                                                               \
-    LASX_ILVL_H_128SV(in_h, in_l, out_l);                                       \
-    LASX_ILVH_H_128SV(in_h, in_l, out_h);                                       \
-}
-
-#define LASX_ILVLH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
-{                                                                                         \
-    LASX_ILVLH_H_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
-    LASX_ILVLH_H_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
-}
-
-#define LASX_ILVLH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
-{                                                                                              \
-    LASX_ILVLH_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
-    LASX_ILVLH_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
-}
-
-#define LASX_ILVLH_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                             \
-    LASX_ILVLH_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave word elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of  word elements  of in_l and low half of word
- *               elements of in_h  are  interleaved  and  copied  to out_l.
- *               High half of word elements of in_l and high half of word
- *               elements of in_h are interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : LASX_ILVLH_W(in_h, in_l, out_h, out_l)
- *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
- *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
- *        out_h: 5, -5,  6, -6,  7, -7,  8, -8
- *        out_l: 1, -1,  2, -2,  3, -3,  4, -4
- */
-#define LASX_ILVLH_W(in_h, in_l, out_h, out_l)                           \
-{                                                                        \
-    __m256i tmp0, tmp1;                                                  \
-    tmp0  = __lasx_xvilvl_w(in_h, in_l);                                 \
-    tmp1  = __lasx_xvilvh_w(in_h, in_l);                                 \
-    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
-    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
-}
-
-#define LASX_ILVLH_W_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
-                       out1_h, out1_l)                                   \
-{                                                                        \
-    LASX_ILVLH_W(in0_h, in0_l, out0_h, out0_l);                          \
-    LASX_ILVLH_W(in1_h, in1_l, out1_h, out1_l);                          \
-}
-
-#define LASX_ILVLH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
-{                                                                                       \
-    LASX_ILVLH_W_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
-    LASX_ILVLH_W_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
-}
-
-#define LASX_ILVLH_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                       \
-    LASX_ILVLH_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in0_h,  in0_l,  ~
- *               Outputs - out0_h, out0_l, ~
- * Details     : Low half of word elements  of every 128-bit of in_l and
- *               low half of word elements  of every 128-bit of in_h are
- *               interleaved and copied to out_l.
- *               High half of word elements of every 128-bit of in_l and
- *               high half of word elements of every 128-bit of in_h are
- *               interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
- *         in_h:-1, -2, -3, -4, -5, -6, -7, -8
- *         in_l: 1,  2,  3,  4,  5,  6,  7,  8
- *        out_h: 3, -3,  4, -4,  7, -7,  8, -8
- *        out_l: 1, -1,  2, -2,  5, -5,  6, -6
- */
-#define LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)                            \
-{                                                                               \
-    LASX_ILVL_W_128SV(in_h, in_l, out_l);                                       \
-    LASX_ILVH_W_128SV(in_h, in_l, out_h);                                       \
-}
-
-#define LASX_ILVLH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
-{                                                                                         \
-    LASX_ILVLH_W_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
-    LASX_ILVLH_W_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
-}
-
-#define LASX_ILVLH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
-{                                                                                              \
-    LASX_ILVLH_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
-    LASX_ILVLH_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
-}
-
-#define LASX_ILVLH_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                             \
-    LASX_ILVLH_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave double word elements from vectors
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of double word  elements  of in_l and low half of
- *               double word elements of in_h are interleaved and copied to
- *               out_l. High half of double word  elements  of in_l and high
- *               half of double word  elements  of in_h are interleaved and
- *               copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_D(in_h, in_l, out_h, out_l)                           \
-{                                                                        \
-    __m256i tmp0, tmp1;                                                  \
-    tmp0  = __lasx_xvilvl_d(in_h, in_l);                                 \
-    tmp1  = __lasx_xvilvh_d(in_h, in_l);                                 \
-    out_l = __lasx_xvpermi_q(tmp0, tmp1, 0x02);                          \
-    out_h = __lasx_xvpermi_q(tmp0, tmp1, 0x13);                          \
-}
-
-#define LASX_ILVLH_D_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l,       \
-                       out1_h, out1_l)                                   \
-{                                                                        \
-    LASX_ILVLH_D(in0_h, in0_l, out0_h, out0_l);                          \
-    LASX_ILVLH_D(in1_h, in1_l, out1_h, out1_l);                          \
-}
-
-#define LASX_ILVLH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)  \
-{                                                                                       \
-    LASX_ILVLH_D_2(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);         \
-    LASX_ILVLH_D_2(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);         \
-}
-
-#define LASX_ILVLH_D_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                       out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                       out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                       \
-    LASX_ILVLH_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                   out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_D_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                   out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Interleave double word elements from vectors
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h,  in_l,  ~
- *               Outputs - out_h, out_l, ~
- * Details     : Low half of double word elements of every 128-bit  of in_l and
- *               low half of double word elements of every 128-bit  of in_h are
- *               interleaved and copied to out_l.
- *               High half of double word elements of every 128-bit of in_l and
- *               high half of double word elements of every 128-bit of in_h are
- *               interleaved and copied to out_h.
- *               Similar for other pairs.
- * Example     : see LASX_ILVLH_W_128SV(in_h, in_l, out_h, out_l)
- */
-#define LASX_ILVLH_D_128SV(in_h, in_l, out_h, out_l)                            \
-{                                                                               \
-    LASX_ILVL_D_128SV(in_h, in_l, out_l);                                       \
-    LASX_ILVH_D_128SV(in_h, in_l, out_h);                                       \
-}
-
-#define LASX_ILVLH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l)  \
-{                                                                                         \
-    LASX_ILVLH_D_128SV(in0_h, in0_l, out0_h, out0_l);                                     \
-    LASX_ILVLH_D_128SV(in1_h, in1_l, out1_h, out1_l);                                     \
-}
-
-#define LASX_ILVLH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,           \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l)   \
-{                                                                                              \
-    LASX_ILVLH_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0_h, out0_l, out1_h, out1_l);          \
-    LASX_ILVLH_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2_h, out2_l, out3_h, out3_l);          \
-}
-
-#define LASX_ILVLH_D_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,          \
-                             in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,          \
-                             out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l,  \
-                             out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l)  \
-{                                                                                             \
-    LASX_ILVLH_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l,              \
-                         out0_h, out0_l, out1_h, out1_l, out2_h, out2_l, out3_h, out3_l);     \
-    LASX_ILVLH_D_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l,              \
-                         out4_h, out4_l, out5_h, out5_l, out6_h, out6_l, out7_h, out7_l);     \
-}
-
-/* Description : Immediate number of columns to slide with zero
- * Arguments   : Inputs  - in0, in1, slide_val, ~
- *               Outputs - out0, out1, ~
- * Details     : Byte elements from every 128-bit of in0 vector
- *               are slide into  out0  by  number  of  elements
- *               specified by slide_val.
- * Example     : LASX_SLDI_B_0_128SV(in0, out0, slide_val)
- *          in0: 1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,
- *               19,20,21,22,23,24,25,26,27,28,29,30,31,32
- *         out0: 4, 5,6,7,8,9,10,11,12,13,14,15,16,0,0,0,20,21,
- *               22,23,24,25,26,27,28,29,30,31,32,0,0,0
- *    slide_val: 3
- */
-#define LASX_SLDI_B_0_128SV(in0, out0, slide_val)                   \
-{                                                                   \
-    out0 = __lasx_xvbsrl_v(in0, slide_val);                         \
-}
-
-#define LASX_SLDI_B_2_0_128SV(in0, in1, out0, out1, slide_val)      \
-{                                                                   \
-    LASX_SLDI_B_0_128SV(in0, out0, slide_val);                      \
-    LASX_SLDI_B_0_128SV(in1, out1, slide_val);                      \
-}
-
-#define LASX_SLDI_B_4_0_128SV(in0, in1, in2, in3,                   \
-                              out0, out1, out2, out3, slide_val)    \
-{                                                                   \
-    LASX_SLDI_B_2_0_128SV(in0, in1, out0, out1, slide_val);         \
-    LASX_SLDI_B_2_0_128SV(in2, in3, out2, out3, slide_val);         \
-}
-
-/* Description : Pack even byte elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even byte elements of in_l are copied to the low half of
- *               out0.  Even byte elements of in_h are copied to the high
- *               half of out0.
- *               Similar for other pairs.
- * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
- */
-#define LASX_PCKEV_B(in_h, in_l, out0)                                  \
-{                                                                       \
-    out0 = __lasx_xvpickev_b(in_h, in_l);                               \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                \
-}
-
-#define LASX_PCKEV_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                       \
-    LASX_PCKEV_B(in0_h, in0_l, out0);                                   \
-    LASX_PCKEV_B(in1_h, in1_l, out1);                                   \
-}
-
-#define LASX_PCKEV_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
-    LASX_PCKEV_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
-}
-
-#define LASX_PCKEV_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKEV_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKEV_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack even byte elements of vector pairs
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even byte elements of in_l are copied to the low half of
- *               out0.  Even byte elements of in_h are copied to the high
- *               half of out0.
- *               Similar for other pairs.
- * Example     : see LASX_PCKEV_W_128SV(in_h, in_l, out0)
- */
-#define LASX_PCKEV_B_128SV(in_h, in_l, out0)                            \
-{                                                                       \
-    out0 = __lasx_xvpickev_b(in_h, in_l);                               \
-}
-
-#define LASX_PCKEV_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)    \
-{                                                                       \
-    LASX_PCKEV_B_128SV(in0_h, in0_l, out0);                             \
-    LASX_PCKEV_B_128SV(in1_h, in1_l, out1);                             \
-}
-
-#define LASX_PCKEV_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
-                             in3_h, in3_l, out0, out1, out2, out3)      \
-{                                                                       \
-    LASX_PCKEV_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);       \
-    LASX_PCKEV_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);       \
-}
-
-#define LASX_PCKEV_B_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
-                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l,  \
-                             in6_h, in6_l, in7_h, in7_l, out0, out1,    \
-                             out2, out3, out4, out5, out6, out7)        \
-{                                                                       \
-    LASX_PCKEV_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                         in3_h, in3_l, out0, out1, out2, out3);         \
-    LASX_PCKEV_B_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,      \
-                         in7_h, in7_l, out4, out5, out6, out7);         \
-}
-
-/* Description : Pack even half word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even half word elements of in_l are copied to the  low
- *               half of out0.  Even  half  word  elements  of in_h are
- *               copied to the high half of out0.
- * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
- */
-#define LASX_PCKEV_H(in_h, in_l, out0)                                 \
-{                                                                      \
-    out0 = __lasx_xvpickev_h(in_h, in_l);                              \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                               \
-}
-
-#define LASX_PCKEV_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                       \
-    LASX_PCKEV_H(in0_h, in0_l, out0);                                   \
-    LASX_PCKEV_H(in1_h, in1_l, out1);                                   \
-}
-
-#define LASX_PCKEV_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
-    LASX_PCKEV_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
-}
-
-#define LASX_PCKEV_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKEV_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKEV_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack even half word elements of vector pairs
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even half word elements of in_l are copied to the  low
- *               half of out0.  Even  half  word  elements  of in_h are
- *               copied to the high half of out0.
- * Example     : see LASX_PCKEV_W_128SV(in_h, in_l, out0)
- */
-#define LASX_PCKEV_H_128SV(in_h, in_l, out0)                            \
-{                                                                       \
-    out0 = __lasx_xvpickev_h(in_h, in_l);                               \
-}
-
-#define LASX_PCKEV_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)    \
-{                                                                       \
-    LASX_PCKEV_H_128SV(in0_h, in0_l, out0);                             \
-    LASX_PCKEV_H_128SV(in1_h, in1_l, out1);                             \
-}
-
-#define LASX_PCKEV_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);       \
-    LASX_PCKEV_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);       \
-}
-
-#define LASX_PCKEV_H_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,  \
-                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l,  \
-                             in6_h, in6_l, in7_h, in7_l, out0, out1,    \
-                             out2, out3, out4, out5, out6, out7)        \
-{                                                                       \
-    LASX_PCKEV_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,      \
-                   in3_h, in3_l, out0, out1, out2, out3);               \
-    LASX_PCKEV_H_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,      \
-                   in7_h, in7_l, out4, out5, out6, out7);               \
-}
-
-/* Description : Pack even word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even word  elements  of  in_l are copied to
- *               the low  half of out0.  Even word elements
- *               of in_h are copied to the high half of out0.
- * Example     : LASX_PCKEV_W(in_h, in_l, out0)
- *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
- *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
- *         out0:  1,  3,  5,  7, -1, -3, -5, -7
- */
-#define LASX_PCKEV_W(in_h, in_l, out0)                    \
-{                                                         \
-    out0 = __lasx_xvpickev_w(in_h, in_l);                 \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                  \
-}
-
-#define LASX_PCKEV_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                       \
-    LASX_PCKEV_W(in0_h, in0_l, out0);                                   \
-    LASX_PCKEV_W(in1_h, in1_l, out1);                                   \
-}
-
-#define LASX_PCKEV_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
-    LASX_PCKEV_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
-}
-
-#define LASX_PCKEV_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKEV_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKEV_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack even word elements of vector pairs
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even word  elements  of  in_l are copied to
- *               the low  half of out0.  Even word elements
- *               of in_h are copied to the high half of out0.
- * Example     : LASX_PCKEV_W_128SV(in_h, in_l, out0)
- *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
- *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
- *         out0:  1,  3, -1, -3,  5,  7, -5, -7
- */
-#define LASX_PCKEV_W_128SV(in_h, in_l, out0)                           \
-{                                                                      \
-    out0 = __lasx_xvpickev_w(in_h, in_l);                              \
-}
-
-#define LASX_PCKEV_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)   \
-{                                                                      \
-    LASX_PCKEV_W_128SV(in0_h, in0_l, out0);                            \
-    LASX_PCKEV_W_128SV(in1_h, in1_l, out1);                            \
-}
-
-#define LASX_PCKEV_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, \
-                             in3_h, in3_l, out0, out1, out2, out3)     \
-{                                                                      \
-    LASX_PCKEV_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1);      \
-    LASX_PCKEV_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3);      \
-}
-
-#define LASX_PCKEV_W_8_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, \
-                             in3_h, in3_l, in4_h, in4_l, in5_h, in5_l, \
-                             in6_h, in6_l, in7_h, in7_l, out0, out1,   \
-                             out2, out3, out4, out5, out6, out7)       \
-{                                                                      \
-    LASX_PCKEV_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,     \
-                         in3_h, in3_l, out0, out1, out2, out3);        \
-    LASX_PCKEV_W_4_128SV(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,     \
-                         in7_h, in7_l, out4, out5, out6, out7);        \
-}
-
-/* Description : Pack even half word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even half word elements of in_l are copied to the  low
- *               half of out0.  Even  half  word  elements  of in_h are
- *               copied to the high half of out0.
- * Example     : See LASX_PCKEV_W(in_h, in_l, out0)
- */
-#define LASX_PCKEV_D(in_h, in_l, out0)                                        \
-{                                                                             \
-    out0 = __lasx_xvpickev_d(in_h, in_l);                                     \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                      \
-}
-
-#define LASX_PCKEV_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                \
-{                                                                             \
-    LASX_PCKEV_D(in0_h, in0_l, out0)                                          \
-    LASX_PCKEV_D(in1_h, in1_l, out1)                                          \
-}
-
-#define LASX_PCKEV_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,              \
-                       in3_h, in3_l, out0, out1, out2, out3)                  \
-{                                                                             \
-    LASX_PCKEV_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                    \
-    LASX_PCKEV_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)                    \
-}
-
-/* Description : Pack even half word elements of vector pairs
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even half word elements of in_l are copied to the  low
- *               half of out0.  Even  half  word  elements  of in_h are
- *               copied to the high half of out0.
- * Example     : LASX_PCKEV_D_128SV(in_h, in_l, out0)
- *        in_h : 1, 2, 3, 4
- *        in_l : 5, 6, 7, 8
- *        out0 : 5, 1, 7, 3
- */
-#define LASX_PCKEV_D_128SV(in_h, in_l, out0)                                  \
-{                                                                             \
-    out0 = __lasx_xvpickev_d(in_h, in_l);                                     \
-}
-
-#define LASX_PCKEV_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                             \
-    LASX_PCKEV_D_128SV(in0_h, in0_l, out0)                                    \
-    LASX_PCKEV_D_128SV(in1_h, in1_l, out1)                                    \
-}
-
-#define LASX_PCKEV_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                             in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                             \
-    LASX_PCKEV_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
-    LASX_PCKEV_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
-}
-
-/* Description : Pack even quad word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Even quad elements of in_l are copied to the low
- *               half of out0. Even  quad  elements  of  in_h are
- *               copied to the high half of out0.
- *               Similar for other pairs.
- * Example     : see LASX_PCKEV_W(in_h, in_l, out0)
- */
-#define LASX_PCKEV_Q(in_h, in_l, out0)                          \
-{                                                               \
-    out0 = __lasx_xvpermi_q(in_h, in_l, 0x20);                  \
-}
-
-#define LASX_PCKEV_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                       \
-    LASX_PCKEV_Q(in0_h, in0_l, out0);                                   \
-    LASX_PCKEV_Q(in1_h, in1_l, out1);                                   \
-}
-
-#define LASX_PCKEV_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                       in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                       \
-    LASX_PCKEV_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1);             \
-    LASX_PCKEV_Q_2(in2_h, in2_l, in3_h, in3_l, out2, out3);             \
-}
-
-#define LASX_PCKEV_Q_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKEV_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKEV_Q_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd byte elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd byte elements of in_l are copied to the low half of
- *               out0. Odd byte elements of in_h are copied to the high
- *               half of out0.
- *               Similar for other pairs.
- * Example     : see LASX_PCKOD_W(in_h, in_l, out0)
- */
-#define LASX_PCKOD_B(in_h, in_l, out0)                                         \
-{                                                                              \
-    out0 = __lasx_xvpickod_b(in_h, in_l);                                      \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
-}
-
-#define LASX_PCKOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
-{                                                                              \
-    LASX_PCKOD_B(in0_h, in0_l, out0);                                          \
-    LASX_PCKOD_B(in1_h, in1_l, out1);                                          \
-}
-
-#define LASX_PCKOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
-                       in3_h, in3_l, out0, out1, out2, out3)                   \
-{                                                                              \
-    LASX_PCKOD_B_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
-    LASX_PCKOD_B_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
-}
-
-#define LASX_PCKOD_B_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKOD_B_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKOD_B_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd half word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd half word elements of in_l are copied to the low
- *               half of out0. Odd half word elements of in_h are copied
- *               to the high half of out0.
- * Example     : see LASX_PCKOD_W(in_h, in_l, out0)
- */
-#define LASX_PCKOD_H(in_h, in_l, out0)                                         \
-{                                                                              \
-    out0 = __lasx_xvpickod_h(in_h, in_l);                                      \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
-}
-
-#define LASX_PCKOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
-{                                                                              \
-    LASX_PCKOD_H(in0_h, in0_l, out0);                                          \
-    LASX_PCKOD_H(in1_h, in1_l, out1);                                          \
-}
-
-#define LASX_PCKOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
-                       in3_h, in3_l, out0, out1, out2, out3)                   \
-{                                                                              \
-    LASX_PCKOD_H_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
-    LASX_PCKOD_H_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
-}
-
-#define LASX_PCKOD_H_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKOD_H_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKOD_H_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd word elements of in_l are copied to the low half of out0.
- *               Odd word elements of in_h are copied to the high half of out0.
- * Example     : LASX_PCKOD_W(in_h, in_l, out0)
- *         in_h: -1, -2, -3, -4, -5, -6, -7, -8
- *         in_l:  1,  2,  3,  4,  5,  6,  7,  8
- *         out0:  2,  4,  6,  8, -2, -4, -6, -8
- */
-#define LASX_PCKOD_W(in_h, in_l, out0)                                         \
-{                                                                              \
-    out0 = __lasx_xvpickod_w(in_h, in_l);                                      \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                       \
-}
-
-#define LASX_PCKOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
-{                                                                              \
-    LASX_PCKOD_W(in0_h, in0_l, out0);                                          \
-    LASX_PCKOD_W(in1_h, in1_l, out1);                                          \
-}
-
-#define LASX_PCKOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
-                       in3_h, in3_l, out0, out1, out2, out3)                   \
-{                                                                              \
-    LASX_PCKOD_W_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
-    LASX_PCKOD_W_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
-}
-
-#define LASX_PCKOD_W_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKOD_W_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKOD_W_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd half word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd half word elements of in_l are copied to the low
- *               half of out0. Odd half word elements of in_h are
- *               copied to the high half of out0.
- * Example     : See LASX_PCKOD_W(in_h, in_l, out0)
- */
-#define LASX_PCKOD_D(in_h, in_l, out0)                                        \
-{                                                                             \
-    out0 = __lasx_xvpickod_d(in_h, in_l);                                     \
-    out0 = __lasx_xvpermi_d(out0, 0xd8);                                      \
-}
-
-#define LASX_PCKOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                \
-{                                                                             \
-    LASX_PCKOD_D(in0_h, in0_l, out0)                                          \
-    LASX_PCKOD_D(in1_h, in1_l, out1)                                          \
-}
-
-#define LASX_PCKOD_D_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,              \
-                       in3_h, in3_l, out0, out1, out2, out3)                  \
-{                                                                             \
-    LASX_PCKOD_D_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                    \
-    LASX_PCKOD_D_2(in2_h, in2_l, in3_h, in3_l, out2, out3)                    \
-}
-
-/* Description : Pack odd quad word elements of vector pairs
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd quad elements of in0_h are copied to the high half of
- *               out0 & odd quad elements of in0_l are copied to the low
- *               half of out0.
- *               Odd quad elements of in1_h are copied to the high half of
- *               out1 & odd quad elements of in1_l are copied to the low
- *               half of out1.
- *               LASX_PCKOD_Q(in_h, in_l, out0)
- *               in_h:   0,0,0,0, 0,0,0,0, 19,10,11,12, 13,14,15,16
- *               in_l:   0,0,0,0, 0,0,0,0, 1,2,3,4, 5,6,7,8
- *               out0:  1,2,3,4, 5,6,7,8, 19,10,11,12, 13,14,15,16
- */
-#define LASX_PCKOD_Q(in_h, in_l, out0)                                         \
-{                                                                              \
-    out0 = __lasx_xvpermi_q(in_h, in_l, 0x31);                                 \
-}
-
-#define LASX_PCKOD_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1)                 \
-{                                                                              \
-    LASX_PCKOD_Q(in0_h, in0_l, out0);                                          \
-    LASX_PCKOD_Q(in1_h, in1_l, out1);                                          \
-}
-
-#define LASX_PCKOD_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,               \
-                       in3_h, in3_l, out0, out1, out2, out3)                   \
-{                                                                              \
-    LASX_PCKOD_Q_2(in0_h, in0_l, in1_h, in1_l, out0, out1);                    \
-    LASX_PCKOD_Q_2(in2_h, in2_l, in3_h, in3_l, out2, out3);                    \
-}
-
-#define LASX_PCKOD_Q_8(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l, in3_h, in3_l, \
-                       in4_h, in4_l, in5_h, in5_l, in6_h, in6_l, in7_h, in7_l, \
-                       out0, out1, out2, out3, out4, out5, out6, out7)         \
-{                                                                              \
-    LASX_PCKOD_Q_4(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,                   \
-                   in3_h, in3_l, out0, out1, out2, out3);                      \
-    LASX_PCKOD_Q_4(in4_h, in4_l, in5_h, in5_l, in6_h, in6_l,                   \
-                   in7_h, in7_l, out4, out5, out6, out7);                      \
-}
-
-/* Description : Pack odd byte elements of vector pairsi
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd byte elements of in_l are copied to the low
- *               half of out0 of . Odd byte elements of in_h are
- *               copied to the high half of out0.
- * Example     : See LASX_PCKOD_D_128SV(in_h, in_l, out0)
- */
-#define LASX_PCKOD_B_128SV(in_h, in_l, out0)                                  \
-{                                                                             \
-    out0 = __lasx_xvpickod_b(in_h, in_l);                                     \
-}
-
-#define LASX_PCKOD_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                             \
-    LASX_PCKOD_B_128SV(in0_h, in0_l, out0)                                    \
-    LASX_PCKOD_B_128SV(in1_h, in1_l, out1)                                    \
-}
-
-#define LASX_PCKOD_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                             in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                             \
-    LASX_PCKOD_B_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
-    LASX_PCKOD_B_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
-}
-
-/* Description : Pack odd half word elements of vector pairsi
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd half word elements of in_l are copied to the low
- *               half of out0 of . Odd half word elements of in_h are
- *               copied to the high half of out0.
- * Example     : See LASX_PCKOD_D_128SV(in_h, in_l, out0)
- */
-#define LASX_PCKOD_H_128SV(in_h, in_l, out0)                                  \
-{                                                                             \
-    out0 = __lasx_xvpickod_h(in_h, in_l);                                     \
-}
-
-#define LASX_PCKOD_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                             \
-    LASX_PCKOD_H_128SV(in0_h, in0_l, out0)                                    \
-    LASX_PCKOD_H_128SV(in1_h, in1_l, out1)                                    \
-}
-
-#define LASX_PCKOD_H_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                             in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                             \
-    LASX_PCKOD_H_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
-    LASX_PCKOD_H_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
-}
-
-/* Description : Pack odd word elements of vector pairsi
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd word elements of in_l are copied to the low
- *               half of out0 of . Odd word elements of in_h are
- *               copied to the high half of out0.
- * Example     : See LASX_PCKOD_D_128SV(in_h, in_l, out0)
- */
-#define LASX_PCKOD_W_128SV(in_h, in_l, out0)                                  \
-{                                                                             \
-    out0 = __lasx_xvpickod_w(in_h, in_l);                                     \
-}
-
-#define LASX_PCKOD_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                             \
-    LASX_PCKOD_W_128SV(in0_h, in0_l, out0)                                    \
-    LASX_PCKOD_W_128SV(in1_h, in1_l, out1)                                    \
-}
-
-#define LASX_PCKOD_W_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                             in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                             \
-    LASX_PCKOD_W_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
-    LASX_PCKOD_W_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
-}
-
-/* Description : Pack odd half word elements of vector pairsi
- *               (128-bit symmetry version)
- * Arguments   : Inputs  - in_h, in_l, ~
- *               Outputs - out0, out1, ~
- * Details     : Odd half word elements of in_l are copied to the low
- *               half of out0 of . Odd half word elements of in_h are
- *               copied to the high half of out0.
- * Example     : LASX_PCKOD_D_128SV(in_h, in_l, out0)
- *        in_h : 1, 2, 3, 4
- *        in_l : 5, 6, 7, 8
- *        out0 : 6, 2, 8, 4
- */
-#define LASX_PCKOD_D_128SV(in_h, in_l, out0)                                  \
-{                                                                             \
-    out0 = __lasx_xvpickod_d(in_h, in_l);                                     \
-}
-
-#define LASX_PCKOD_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)          \
-{                                                                             \
-    LASX_PCKOD_D_128SV(in0_h, in0_l, out0)                                    \
-    LASX_PCKOD_D_128SV(in1_h, in1_l, out1)                                    \
-}
-
-#define LASX_PCKOD_D_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,        \
-                             in3_h, in3_l, out0, out1, out2, out3)            \
-{                                                                             \
-    LASX_PCKOD_D_2_128SV(in0_h, in0_l, in1_h, in1_l, out0, out1)              \
-    LASX_PCKOD_D_2_128SV(in2_h, in2_l, in3_h, in3_l, out2, out3)              \
-}
-
-/* Description : Transposes 8x8 block with half word elements in vectors.
- * Arguments   : Inputs  - in0, in1, ~
- *               Outputs - out0, out1, ~
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : LASX_TRANSPOSE8x8_H_128SV
- *         in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *         in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
- *         in2 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
- *         in3 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *         in4 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
- *         in5 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *         in6 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *         in7 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
- *
- *        out0 : 1,8,8,1, 9,1,1,9, 1,8,8,1, 9,1,1,9
- *        out1 : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
- *        out2 : 3,3,3,3, 3,3,3,3, 3,3,3,3, 3,3,3,3
- *        out3 : 4,4,4,4, 4,4,4,4, 4,4,4,4, 4,4,4,4
- *        out4 : 5,5,5,5, 5,5,5,5, 5,5,5,5, 5,5,5,5
- *        out5 : 6,6,6,6, 6,6,6,6, 6,6,6,6, 6,6,6,6
- *        out6 : 7,7,7,7, 7,7,7,7, 7,7,7,7, 7,7,7,7
- *        out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
- */
-#define LASX_TRANSPOSE8x8_H_128SV(in0, in1, in2, in3, in4, in5, in6, in7,           \
-                                  out0, out1, out2, out3, out4, out5, out6, out7)   \
-{                                                                                   \
-    __m256i s0_m, s1_m;                                                             \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                         \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                         \
-                                                                                    \
-    LASX_ILVL_H_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                            \
-    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp1_m, tmp0_m);                                 \
-    LASX_ILVH_H_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                            \
-    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp3_m, tmp2_m);                                 \
-                                                                                    \
-    LASX_ILVL_H_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                            \
-    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp5_m, tmp4_m);                                 \
-    LASX_ILVH_H_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                            \
-    LASX_ILVLH_H_128SV(s1_m, s0_m, tmp7_m, tmp6_m);                                 \
-                                                                                    \
-    LASX_PCKEV_D_4_128SV(tmp0_m, tmp4_m, tmp1_m, tmp5_m, tmp2_m, tmp6_m,            \
-                         tmp3_m, tmp7_m, out0, out2, out4, out6);                   \
-    LASX_PCKOD_D_4_128SV(tmp0_m, tmp4_m, tmp1_m, tmp5_m, tmp2_m, tmp6_m,            \
-                         tmp3_m, tmp7_m, out1, out3, out5, out7);                   \
-}
-
-/* Description : Transposes 8x8 block with word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * Details     :
- */
-#define LASX_TRANSPOSE8x8_W(in0, in1, in2, in3, in4, in5, in6, in7,         \
-                            out0, out1, out2, out3, out4, out5, out6, out7) \
-{                                                                           \
-    __m256i s0_m, s1_m;                                                     \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                 \
-                                                                            \
-    LASX_ILVL_W_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                    \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp1_m, tmp0_m);                         \
-    LASX_ILVH_W_2_128SV(in2, in0, in3, in1, s0_m, s1_m);                    \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp3_m, tmp2_m);                         \
-                                                                            \
-    LASX_ILVL_W_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                    \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp5_m, tmp4_m);                         \
-    LASX_ILVH_W_2_128SV(in6, in4, in7, in5, s0_m, s1_m);                    \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, tmp7_m, tmp6_m);                         \
-    LASX_PCKEV_Q_4(tmp4_m, tmp0_m, tmp5_m, tmp1_m, tmp6_m, tmp2_m,          \
-                   tmp7_m, tmp3_m, out0, out1, out2, out3);                 \
-    LASX_PCKOD_Q_4(tmp4_m, tmp0_m, tmp5_m, tmp1_m, tmp6_m, tmp2_m,          \
-                   tmp7_m, tmp3_m, out4, out5, out6, out7);                 \
-}
-
-/* Description : Transposes 2x2 block with quad word elements in vectors
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- * Details     :
- */
-#define LASX_TRANSPOSE2x2_Q(in0, in1, out0, out1) \
-{                                                 \
-    __m256i tmp0;                                 \
-    tmp0 = __lasx_xvpermi_q(in1, in0, 0x02);      \
-    out1 = __lasx_xvpermi_q(in1, in0, 0x13);      \
-    out0 = tmp0;                                  \
-}
-
-/* Description : Transposes 4x4 block with double word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * Details     :
- */
-#define LASX_TRANSPOSE4x4_D(in0, in1, in2, in3, out0, out1, out2, out3) \
-{                                                                       \
-    __m256i tmp0, tmp1, tmp2, tmp3;                                     \
-    LASX_ILVLH_D_2_128SV(in1, in0, in3, in2, tmp0, tmp1, tmp2, tmp3);   \
-    out0 = __lasx_xvpermi_q(tmp2, tmp0, 0x20);                          \
-    out2 = __lasx_xvpermi_q(tmp2, tmp0, 0x31);                          \
-    out1 = __lasx_xvpermi_q(tmp3, tmp1, 0x20);                          \
-    out3 = __lasx_xvpermi_q(tmp3, tmp1, 0x31);                          \
-}
-
-/* Description : Transpose 4x4 block with half word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- *               Return Type - signed halfword
- */
-#define LASX_TRANSPOSE4x4_H_128SV(in0, in1, in2, in3, out0, out1, out2, out3) \
-{                                                                             \
-    __m256i s0_m, s1_m;                                                       \
-                                                                              \
-    LASX_ILVL_H_2_128SV(in1, in0, in3, in2, s0_m, s1_m);                      \
-    LASX_ILVLH_W_128SV(s1_m, s0_m, out2, out0);                               \
-    out1 = __lasx_xvilvh_d(out0, out0);                                       \
-    out3 = __lasx_xvilvh_d(out2, out2);                                       \
-}
-
-/* Description : Transposes input 8x8 byte block
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
- *                         (input 8x8 byte block)
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- *                         (output 8x8 byte block)
- * Details     :
- */
-#define LASX_TRANSPOSE8x8_B(in0, in1, in2, in3, in4, in5, in6, in7,         \
-                            out0, out1, out2, out3, out4, out5, out6, out7) \
-{                                                                           \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                 \
-    LASX_ILVL_B_4_128SV(in2, in0, in3, in1, in6, in4, in7, in5,             \
-                       tmp0_m, tmp1_m, tmp2_m, tmp3_m);                     \
-    LASX_ILVLH_B_128SV(tmp1_m, tmp0_m, tmp5_m, tmp4_m);                     \
-    LASX_ILVLH_B_128SV(tmp3_m, tmp2_m, tmp7_m, tmp6_m);                     \
-    LASX_ILVLH_W_128SV(tmp6_m, tmp4_m, out2, out0);                         \
-    LASX_ILVLH_W_128SV(tmp7_m, tmp5_m, out6, out4);                         \
-    LASX_SLDI_B_2_0_128SV(out0, out2, out1, out3, 8);                       \
-    LASX_SLDI_B_2_0_128SV(out4, out6, out5, out7, 8);                       \
-}
-
-/* Description : Transposes input 16x8 byte block
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7,
- *                         in8, in9, in10, in11, in12, in13, in14, in15
- *                         (input 16x8 byte block)
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- *                         (output 8x16 byte block)
- * Details     :
- */
-#define LASX_TRANSPOSE16x8_B(in0, in1, in2, in3, in4, in5, in6, in7,              \
-                             in8, in9, in10, in11, in12, in13, in14, in15,        \
-                             out0, out1, out2, out3, out4, out5, out6, out7)      \
-{                                                                                 \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                       \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                       \
-    __m256i t0, t1, t2, t3, t4, t5, t6, t7;                                       \
-    LASX_ILVL_B_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
-                        in10, in8, in11, in9, in14, in12, in15, in13,             \
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
-    LASX_ILVLH_B_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
-    LASX_ILVLH_B_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
-    LASX_ILVLH_W_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
-    LASX_ILVLH_W_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
-    LASX_ILVLH_D_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out1, out0, out3, out2); \
-    LASX_ILVLH_D_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out5, out4, out7, out6); \
-}
-
-/* Description : Transposes input 16x8 byte block
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7,
- *                         in8, in9, in10, in11, in12, in13, in14, in15
- *                         (input 16x8 byte block)
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- *                         (output 8x16 byte block)
- * Details     : The rows of the matrix become columns, and the columns become rows.
- * Example     : LASX_TRANSPOSE16x8_H
- *         in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in2 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in3 : 4,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in4 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in5 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in6 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in7 : 8,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in8 : 9,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *         in9 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in10 : 0,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in11 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in12 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in13 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in14 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *        in15 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
- *
- *        out0 : 1,2,3,4,5,6,7,8,9,1,0,2,3,7,5,6
- *        out1 : 2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
- *        out2 : 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
- *        out3 : 4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4
- *        out4 : 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
- *        out5 : 6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
- *        out6 : 7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
- *        out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
- */
-#define LASX_TRANSPOSE16x8_H(in0, in1, in2, in3, in4, in5, in6, in7,              \
-                             in8, in9, in10, in11, in12, in13, in14, in15,        \
-                             out0, out1, out2, out3, out4, out5, out6, out7)      \
-{                                                                                 \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                       \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                       \
-    __m256i t0, t1, t2, t3, t4, t5, t6, t7;                                       \
-    LASX_ILVL_H_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
-                        in10, in8, in11, in9, in14, in12, in15, in13,             \
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
-    LASX_ILVLH_H_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
-    LASX_ILVLH_H_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
-    LASX_ILVLH_D_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
-    LASX_ILVLH_D_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
-    LASX_PCKEV_Q_2(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out1);                   \
-    LASX_PCKEV_Q_2(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out2, out3);                   \
-                                                                                  \
-    LASX_ILVH_H_8_128SV(in2, in0, in3, in1, in6, in4, in7, in5,                   \
-                        in10, in8, in11, in9, in14, in12, in15, in13,             \
-                        tmp0_m, tmp1_m, tmp2_m, tmp3_m,                           \
-                        tmp4_m, tmp5_m, tmp6_m, tmp7_m);                          \
-    LASX_ILVLH_H_2_128SV(tmp1_m, tmp0_m, tmp3_m, tmp2_m, t1, t0, t3, t2);         \
-    LASX_ILVLH_H_2_128SV(tmp5_m, tmp4_m, tmp7_m, tmp6_m, t5, t4, t7, t6);         \
-    LASX_ILVLH_D_2_128SV(t2, t0, t3, t1, tmp2_m, tmp0_m, tmp6_m, tmp4_m);         \
-    LASX_ILVLH_D_2_128SV(t6, t4, t7, t5, tmp3_m, tmp1_m, tmp7_m, tmp5_m);         \
-    LASX_PCKEV_Q_2(tmp1_m, tmp0_m, tmp3_m, tmp2_m, out4, out5);                   \
-    LASX_PCKEV_Q_2(tmp5_m, tmp4_m, tmp7_m, tmp6_m, out6, out7);                   \
-}
-
-/* Description : Clips all signed word elements of input vector
- *               between 0 & 255
- * Arguments   : Inputs  - in       (input vector)
- *               Outputs - out_m    (output vector with clipped elements)
- *               Return Type - signed word
- */
-#define LASX_CLIP_W_0_255(in, out_m)        \
-{                                           \
-    out_m = __lasx_xvmaxi_w(in, 0);         \
-    out_m = __lasx_xvsat_wu(out_m, 7);      \
-}
-
-#define LASX_CLIP_W_0_255_2(in0, in1, out0, out1)  \
-{                                                  \
-    LASX_CLIP_W_0_255(in0, out0);                  \
-    LASX_CLIP_W_0_255(in1, out1);                  \
-}
-
-#define LASX_CLIP_W_0_255_4(in0, in1, in2, in3, out0, out1, out2, out3)  \
-{                                                                        \
-    LASX_CLIP_W_0_255_2(in0, in1, out0, out1);                           \
-    LASX_CLIP_W_0_255_2(in2, in3, out2, out3);                           \
-}
-
-/* Description : Clips all signed halfword elements of input vector
- *               between 0 & 255
- * Arguments   : Inputs  - in       (input vector)
- *               Outputs - out_m    (output vector with clipped elements)
- *               Return Type - signed halfword
- */
-#define LASX_CLIP_H_0_255(in, out_m)        \
-{                                           \
-    out_m = __lasx_xvmaxi_h(in, 0);         \
-    out_m = __lasx_xvsat_hu(out_m, 7);      \
-}
-
-#define LASX_CLIP_H_0_255_2(in0, in1, out0, out1)  \
-{                                                  \
-    LASX_CLIP_H_0_255(in0, out0);                  \
-    LASX_CLIP_H_0_255(in1, out1);                  \
-}
-
-#define LASX_CLIP_H_0_255_4(in0, in1, in2, in3, out0, out1, out2, out3)  \
-{                                                                        \
-    LASX_CLIP_H_0_255_2(in0, in1, out0, out1);                           \
-    LASX_CLIP_H_0_255_2(in2, in3, out2, out3);                           \
-}
-
-/* Description : Clips all halfword elements of input vector between min & max
- *               out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
- * Arguments   : Inputs  - in    (input vector)
- *                       - min   (min threshold)
- *                       - max   (max threshold)
- *               Outputs - in    (output vector with clipped elements)
- *               Return Type - signed halfword
- */
-#define LASX_CLIP_H(in, min, max)    \
-{                                    \
-    in = __lasx_xvmax_h(min, in);    \
-    in = __lasx_xvmin_h(max, in);    \
-}
-
-/* Description : Dot product and addition of 3 signed byte input vectors
- * Arguments   : Inputs  - in0, in1, in2, coeff0, coeff1, coeff2
- *               Outputs - out0_m
- *               Return Type - signed halfword
- * Details     : Dot product of 'in0' with 'coeff0'
- *               Dot product of 'in1' with 'coeff1'
- *               Dot product of 'in2' with 'coeff2'
- *               Addition of all the 3 vector results
- *               out0_m = (in0 * coeff0) + (in1 * coeff1) + (in2 * coeff2)
- */
-#define LASX_DP2ADD_H_B_3(in0, in1, in2, out0_m, coeff0, coeff1, coeff2) \
-{                                                                        \
-    LASX_DP2_H_B(in0, coeff0, out0_m);                                   \
-    LASX_DP2ADD_H_B(out0_m, in1, coeff1, out0_m);                        \
-    LASX_DP2ADD_H_B(out0_m, in2, coeff2, out0_m);                        \
-}
-
-/* Description : Each byte element is logically xor'ed with immediate 128
- * Arguments   : Inputs  - in0, in1
- *               Outputs - in0, in1 (in-place)
- * Details     : Each unsigned byte element from input vector 'in0' is
- *               logically xor'ed with 128 and result is in-place stored in
- *               'in0' vector
- *               Each unsigned byte element from input vector 'in1' is
- *               logically xor'ed with 128 and result is in-place stored in
- *               'in1' vector
- *               Similar for other pairs
- * Example     : LASX_XORI_B_128(in0)
- *               in0: 9,10,11,12, 13,14,15,16, 121,122,123,124, 125,126,127,128, 17,18,19,20, 21,22,23,24,
- *               248,249,250,251, 252,253,254,255,
- *               in0: 137,138,139,140, 141,142,143,144, 249,250,251,252, 253,254,255,0, 145,146,147,148,
- *               149,150,151,152, 120,121,122,123, 124,125,126,127
- */
-#define LASX_XORI_B_128(in0)                                 \
-{                                                            \
-    in0 = __lasx_xvxori_b(in0, 128);                         \
-}
-#define LASX_XORI_B_2_128(in0, in1)                          \
-{                                                            \
-    LASX_XORI_B_128(in0);                                    \
-    LASX_XORI_B_128(in1);                                    \
-}
-#define LASX_XORI_B_4_128(in0, in1, in2, in3)                \
-{                                                            \
-    LASX_XORI_B_2_128(in0, in1);                             \
-    LASX_XORI_B_2_128(in2, in3);                             \
-}
-#define LASX_XORI_B_8_128(in0, in1, in2, in3, in4, in5, in6, in7)  \
-{                                                                  \
-    LASX_XORI_B_4_128(in0, in1, in2, in3);                         \
-    LASX_XORI_B_4_128(in4, in5, in6, in7);                         \
-}
-
-/* Description : Indexed halfword element values are replicated to all
- *               elements in output vector. If 'indx0 < 8' use SPLATI_R_*,
- *               if 'indx0 >= 8' use SPLATI_L_*
- * Arguments   : Inputs  - in, idx0, idx1
- *               Outputs - out0, out1
- * Details     : 'idx0' element value from 'in' vector is replicated to all
- *                elements in 'out0' vector
- *                Valid index range for halfword operation is 0-7
- */
-#define LASX_SPLATI_L_H(in, idx0, out0)                        \
-{                                                              \
-    in = __lasx_xvpermi_q(in, in, 0x02);                       \
-    out0 = __lasx_xvrepl128vei_h(in, idx0);                    \
-}
-#define LASX_SPLATI_H_H(in, idx0, out0)                        \
-{                                                              \
-    in = __lasx_xvpermi_q(in, in, 0X13);                       \
-    out0 = __lasx_xvrepl128vei_h(in, idx0 - 8);                \
-}
-#define LASX_SPLATI_L_H_2(in, idx0, idx1, out0, out1)          \
-{                                                              \
-    LASX_SPLATI_L_H(in, idx0, out0);                           \
-    out1 = __lasx_xvrepl128vei_h(in, idx1);                    \
-}
-#define LASX_SPLATI_H_H_2(in, idx0, idx1, out0, out1)          \
-{                                                              \
-    LASX_SPLATI_H_H(in, idx0, out0);                           \
-    out1 = __lasx_xvrepl128vei_h(in, idx1 - 8);                \
-}
-#define LASX_SPLATI_L_H_4(in, idx0, idx1, idx2, idx3,          \
-                          out0, out1, out2, out3)              \
-{                                                              \
-    LASX_SPLATI_L_H_2(in, idx0, idx1, out0, out1);             \
-    out2 = __lasx_xvrepl128vei_h(in, idx2);                    \
-    out3 = __lasx_xvrepl128vei_h(in, idx3);                    \
-}
-#define SPLATI_H_H_4(in, idx0, idx1, idx2, idx3,               \
-                     out0, out1, out2, out3)                   \
-{                                                              \
-    LASX_SPLATI_H_H_2(in, idx0, idx1, out0, out1);             \
-    out2 = __lasx_xvrepl128vei_h(in, idx2 - 8);                \
-    out3 = __lasx_xvrepl128vei_h(in, idx3 - 8);                \
-}
-
-/* Description : Pack even elements of input vectors & xor with 128
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out_m
- * Details     : Signed byte even elements from 'in0' and 'in1' are packed
- *               together in one vector and the resulted vector is xor'ed with
- *               128 to shift the range from signed to unsigned byte
- */
-#define LASX_PICKEV_XORI128_B(in0, in1, out_m)  \
-{                                               \
-    out_m = __lasx_xvpickev_b(in1, in0);        \
-    out_m = __lasx_xvxori_b(out_m, 128);        \
-}
-
-/* Description : Shift right logical all byte elements of vector.
- * Arguments   : Inputs  - in, shift
- *               Outputs - in (in place)
- * Details     : Each element of vector in is shifted right logical by
- *               number of bits respective element holds in vector shift and
- *               result is in place written to in.
- *               Here, shift is a vector passed in.
- * Example     : See LASX_SRL_W(in, shift)
-     */
-#define LASX_SRL_B(in, shift)                                         \
-{                                                                     \
-    in = __lasx_xvsrl_b(in, shift);                                   \
-}
-
-#define LASX_SRL_B_2(in0, in1, shift)                                 \
-{                                                                     \
-    LASX_SRL_B(in0, shift);                                           \
-    LASX_SRL_B(in1, shift);                                           \
-}
-
-#define LASX_SRL_B_4(in0, in1, in2, in3, shift)                       \
-{                                                                     \
-    LASX_SRL_B_2(in0, in1, shift);                                    \
-    LASX_SRL_B_2(in2, in3, shift);                                    \
-}
-
-/* Description : Shift right logical all halfword elements of vector.
- * Arguments   : Inputs  - in, shift
- *               Outputs - in (in place)
- * Details     : Each element of vector in is shifted right logical by
- *               number of bits respective element holds in vector shift and
- *               result is in place written to in.
- *               Here, shift is a vector passed in.
- * Example     : See LASX_SRL_W(in, shift)
- */
-#define LASX_SRL_H(in, shift)                                         \
-{                                                                     \
-    in = __lasx_xvsrl_h(in, shift);                                   \
-}
-
-#define LASX_SRL_H_2(in0, in1, shift)                                 \
-{                                                                     \
-    LASX_SRL_H(in0, shift);                                           \
-    LASX_SRL_H(in1, shift);                                           \
-}
-
-#define LASX_SRL_H_4(in0, in1, in2, in3, shift)                       \
-{                                                                     \
-    LASX_SRL_H_2(in0, in1, shift);                                    \
-    LASX_SRL_H_2(in2, in3, shift);                                    \
-}
-
-/* Description : Shift right logical all word elements of vector.
- * Arguments   : Inputs  - in, shift
- *               Outputs - in (in place)
- * Details     : Each element of vector in is shifted right logical by
- *               number of bits respective element holds in vector shift and
- *               result is in place written to in.
- *               Here, shift is a vector passed in.
- * Example     : LASX_SRL_W(in, shift)
- *          in : 1, 3, 2, -4,      0, -2, 25, 0
- *       shift : 1, 1, 1, 1,       2, 2, 2, 2
- *  in(output) : 0, 1, 1, 32766,   0, 16383, 6, 0
- */
-#define LASX_SRL_W(in, shift)                                         \
-{                                                                     \
-    in = __lasx_xvsrl_w(in, shift);                                   \
-}
-
-#define LASX_SRL_W_2(in0, in1, shift)                                 \
-{                                                                     \
-    LASX_SRL_W(in0, shift);                                           \
-    LASX_SRL_W(in1, shift);                                           \
-}
-
-#define LASX_SRL_W_4(in0, in1, in2, in3, shift)                       \
-{                                                                     \
-    LASX_SRL_W_2(in0, in1, shift);                                    \
-    LASX_SRL_W_2(in2, in3, shift);                                    \
-}
-
-/* Description : Shift right logical all double word elements of vector.
- * Arguments   : Inputs  - in, shift
- *               Outputs - in (in place)
- * Details     : Each element of vector in is shifted right logical by
- *               number of bits respective element holds in vector shift and
- *               result is in place written to in.
- *               Here, shift is a vector passed in.
- * Example     : See LASX_SRL_W(in, shift)
- */
-#define LASX_SRL_D(in, shift)                                         \
-{                                                                     \
-    in = __lasx_xvsrl_d(in, shift);                                   \
-}
-
-#define LASX_SRL_D_2(in0, in1, shift)                                 \
-{                                                                     \
-    LASX_SRL_D(in0, shift);                                           \
-    LASX_SRL_D(in1, shift);                                           \
-}
-
-#define LASX_SRL_D_4(in0, in1, in2, in3, shift)                       \
-{                                                                     \
-    LASX_SRL_D_2(in0, in1, shift);                                    \
-    LASX_SRL_D_2(in2, in3, shift);                                    \
-}
-
-
-/* Description : Shift right arithmetic rounded (immediate)
- * Arguments   : Inputs  - in0, in1, shift
- *               Outputs - in0, in1, (in place)
- * Details     : Each element of vector 'in0' is shifted right arithmetic by
- *               value in 'shift'.
- *               The last discarded bit is added to shifted value for rounding
- *               and the result is in place written to 'in0'
- *               Similar for other pairs
- * Example     : LASX_SRARI_H(in0, out0, shift)
- *               in0:   1,2,3,4, -5,-6,-7,-8, 19,10,11,12, 13,14,15,16
- *               shift: 2
- *               out0:  0,1,1,1, -1,-1,-2,-2, 5,3,3,3, 3,4,4,4
- */
-#define LASX_SRARI_H(in0, out0, shift)                              \
-{                                                                   \
-    out0 = __lasx_xvsrari_h(in0, shift);                            \
-}
-#define LASX_SRARI_H_2(in0, in1, out0, out1, shift)                 \
-{                                                                   \
-    LASX_SRARI_H(in0, out0, shift);                                 \
-    LASX_SRARI_H(in1, out1, shift);                                 \
-}
-#define LASX_SRARI_H_4(in0, in1, in2, in3, out0, out1, out2, out3, shift) \
-{                                                                         \
-    LASX_SRARI_H_2(in0, in1, out0, out1, shift);                          \
-    LASX_SRARI_H_2(in2, in3, out2, out3, shift);                          \
-}
-
-/* Description : Shift right arithmetic (immediate)
- * Arguments   : Inputs  - in0, in1, shift
- *               Outputs - in0, in1, (in place)
- * Details     : Each element of vector 'in0' is shifted right arithmetic by
- *               value in 'shift'.
- *               Similar for other pairs
- * Example     : see LASX_SRARI_H(in0, out0, shift)
- */
-#define LASX_SRAI_W(in0, out0, shift)                                    \
-{                                                                        \
-    out0 = __lasx_xvsrai_w(in0, shift);                                  \
-}
-#define LASX_SRAI_W_2(in0, in1, out0, out1, shift)                       \
-{                                                                        \
-    LASX_SRAI_W(in0, out0, shift);                                       \
-    LASX_SRAI_W(in1, out1, shift);                                       \
-}
-#define LASX_SRAI_W_4(in0, in1, in2, in3, out0, out1, out2, out3, shift) \
-{                                                                        \
-    LASX_SRAI_W_2(in0, in1, out0, out1, shift);                          \
-    LASX_SRAI_W_2(in2, in3, out2, out3, shift);                          \
-}
-#define LASX_SRAI_W_8(in0, in1, in2, in3, in4, in5, in6, in7,                 \
-                      out0, out1, out2, out3, out4, out5, out6, out7, shift)  \
-{                                                                             \
-    LASX_SRAI_W_4(in0, in1, in2, in3, out0, out1, out2, out3, shift);         \
-    LASX_SRAI_W_4(in4, in5, in6, in7, out4, out5, out6, out7, shift);         \
-}
-
-/* Description : Saturate the halfword element values to the max
- *               unsigned value of (sat_val+1 bits)
- *               The element data width remains unchanged
- * Arguments   : Inputs  - in0, in1, in2, in3, sat_val
- *               Outputs - in0, in1, in2, in3 (in place)
- *               Return Type - unsigned halfword
- * Details     : Each unsigned halfword element from 'in0' is saturated to the
- *               value generated with (sat_val+1) bit range
- *               Results are in placed to original vectors
- * Example     : LASX_SAT_H(in0, out0, sat_val)
- *               in0:    1,2,3,4, 5,6,7,8, 19,10,11,12, 13,14,15,16
- *               sat_val:3
- *               out0:   1,2,3,4, 5,6,7,7, 7,7,7,7, 7,7,7,7
- */
-#define LASX_SAT_H(in0, out0, sat_val)                                     \
-{                                                                          \
-    out0 = __lasx_xvsat_h(in0, sat_val);                                   \
-} //some error in xvsat_h built-in function
-#define LASX_SAT_H_2(in0, in1, out0, out1, sat_val)                        \
-{                                                                          \
-    LASX_SAT_H(in0, out0, sat_val);                                        \
-    LASX_SAT_H(in1, out1, sat_val);                                        \
-}
-#define LASX_SAT_H_4(in0, in1, in2, in3, out0, out1, out2, out3, sat_val)  \
-{                                                                          \
-    LASX_SAT_H_2(in0, in1, out0, out1, sat_val);                           \
-    LASX_SAT_H_2(in2, in3, out2, out3, sat_val);                           \
-}
-
-/* Description : Addition of 2 pairs of vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1
- * Details     : Each halfwords element from 2 pairs vectors is added
- *               and 2 results are produced
- * Example     : LASX_ADD_H(in0, in1, out)
- *               in0:  1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
- *               in1:  8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
- *               out:  9,9,9,9, 9,9,9,9, 9,9,9,9, 9,9,9,9
- */
-#define LASX_ADD_H(in0, in1, out)             \
-{                                             \
-    out = __lasx_xvadd_h(in0, in1);           \
-}
-#define LASX_ADD_H_2(in0, in1, in2, in3, out0, out1) \
-{                                                    \
-    LASX_ADD_H(in0, in1, out0);                      \
-    LASX_ADD_H(in2, in3, out1);                      \
-}
-#define LASX_ADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7, out0, out1, out2, out3)      \
-{                                                                                         \
-    LASX_ADD_H_2(in0, in1, in2, in3, out0, out1);                                         \
-    LASX_ADD_H_2(in4, in5, in6, in7, out2, out3);                                         \
-}
-#define LASX_ADD_H_8(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11, in12, \
-                     in13, in14, in15, out0, out1, out2, out3, out4, out5, out6, out7)   \
-{                                                                                        \
-    LASX_ADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7, out0, out1, out2, out3);        \
-    LASX_ADD_H_4(in8, in9, in10, in11, in12, in13, in14, in15, out4, out5, out6, out7);  \
-}
-
-/* Description : Horizontal subtraction of unsigned byte vector elements
- * Arguments   : Inputs  - in0, in1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Each unsigned odd byte element from 'in0' is subtracted from
- *               even unsigned byte element from 'in0' (pairwise) and the
- *               halfword result is written to 'out0'
- */
-#define LASX_HSUB_UB_2(in0, in1, out0, out1)   \
-{                                              \
-    out0 = __lasx_xvhsubw_hu_bu(in0, in0);     \
-    out1 = __lasx_xvhsubw_hu_bu(in1, in1);     \
-}
-
-#define LASX_HSUB_UB_4(in0, in1, in2, in3, out0, out1, out2, out3)    \
-{                                                                     \
-    LASX_HSUB_UB_2(in0, in1, out0, out1);                                   \
-    LASX_HSUB_UB_2(in2, in3, out2, out3);                                   \
-}
-
-/* Description : Shuffle byte vector elements as per mask vector
- * Arguments   : Inputs  - in0, in1, in2, in3, mask0, mask1
- *               Outputs - out0, out1
- *               Return Type - as per RTYPE
- * Details     : Selective byte elements from in0 & in1 are copied to out0 as
- *               per control vector mask0
- *               Selective byte elements from in2 & in3 are copied to out1 as
- *               per control vector mask1
- * Example     : LASX_SHUF_B_128SV(in0, in1,  mask0, out0)
- *               in_h :  9,10,11,12, 13,14,15,16, 0,0,0,0, 0,0,0,0,
- *                      17,18,19,20, 21,22,23,24, 0,0,0,0, 0,0,0,0
- *               in_l :  1, 2, 3, 4,  5, 6, 7, 8, 0,0,0,0, 0,0,0,0,
- *                      25,26,27,28, 29,30,31,32, 0,0,0,0, 0,0,0,0
- *               mask0:  0, 1, 2, 3,  4, 5, 6, 7, 16,17,18,19, 20,21,22,23,
- *                      16,17,18,19, 20,21,22,23,  0, 1, 2, 3,  4, 5, 6, 7
- *               out0 :  1, 2, 3, 4,  5, 6, 7, 8,  9,10,11,12, 13,14,15,16,
- *                      17,18,19,20, 21,22,23,24, 25,26,27,28, 29,30,31,32
- */
-
-#define LASX_SHUF_B_128SV(in_h, in_l,  mask0, out0)                            \
-{                                                                              \
-    out0 = __lasx_xvshuf_b(in_h, in_l, mask0);                                 \
-}
-#define LASX_SHUF_B_2_128SV(in0_h, in0_l, in1_h, in1_l, mask0, mask1,          \
-                            out0, out1)                                        \
-{                                                                              \
-    LASX_SHUF_B_128SV(in0_h, in0_l,  mask0, out0);                             \
-    LASX_SHUF_B_128SV(in1_h, in1_l,  mask1, out1);                             \
-}
-#define LASX_SHUF_B_4_128SV(in0_h, in0_l, in1_h, in1_l, in2_h, in2_l,          \
-                            in3_h, in3_l, mask0, mask1, mask2, mask3,          \
-                            out0, out1, out2, out3)                            \
-{                                                                              \
-    LASX_SHUF_B_2_128SV(in0_h, in0_l, in1_h, in1_l, mask0, mask1, out0, out1); \
-    LASX_SHUF_B_2_128SV(in2_h, in2_l, in3_h, in3_l, mask2, mask3, out2, out3); \
-}
-
-/* Description : Addition of signed halfword elements and signed saturation
- * Arguments   : Inputs  - in0, in1, in2, in3 ~
- *               Outputs - out0, out1 ~
- * Details     : Signed halfword elements from 'in0' are added to signed
- *               halfword elements of 'in1'. The result is then signed saturated
- *               between -32768 to +32767 (as per halfword data type)
- *               Similar for other pairs
- * Example     : LASX_SADD_H(in0, in1, out0)
- *               in0:   1,2,32766,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
- *               in1:   8,7,30586,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
- *               out0:  9,9,32767,9, 9,9,9,9, 9,9,9,9, 9,9,9,9,
- */
-#define LASX_SADD_H(in0, in1, out0)                            \
-{                                                              \
-    out0 = __lasx_xvsadd_h(in0, in1);                          \
-}
-#define LASX_SADD_H_2(in0, in1, in2, in3, out0, out1)          \
-{                                                              \
-    LASX_SADD_H(in0, in1, out0);                               \
-    LASX_SADD_H(in2, in3, out1);                               \
-}
-#define LASX_SADD_H_4(in0, in1, in2, in3, in4, in5, in6, in7,  \
-                      out0, out1, out2, out3)                  \
-{                                                              \
-    LASX_SADD_H_2(in0, in1, in2, in3, out0, out1);             \
-    LASX_SADD_H_2(in4, in5, in6, in7, out2, out3);             \
-}
-
-/* Description : Average with rounding (in0 + in1 + 1) / 2.
- * Arguments   : Inputs  - in0, in1, in2, in3,
- *               Outputs - out0, out1
- * Details     : Each unsigned byte element from 'in0' vector is added with
- *               each unsigned byte element from 'in1' vector.
- *               Average with rounding is calculated and written to 'out0'
- */
-#define LASX_AVER_BU( in0, in1, out0 )   \
-{                                        \
-    out0 = __lasx_xvavgr_bu( in0, in1 ); \
-}
-
-#define LASX_AVER_BU_2( in0, in1, in2, in3, out0, out1 )  \
-{                                                         \
-    LASX_AVER_BU( in0, in1, out0 );                       \
-    LASX_AVER_BU( in2, in3, out1 );                       \
-}
-
-#define LASX_AVER_BU_4( in0, in1, in2, in3, in4, in5, in6, in7,  \
-                        out0, out1, out2, out3 )                 \
-{                                                                \
-    LASX_AVER_BU_2( in0, in1, in2, in3, out0, out1 );            \
-    LASX_AVER_BU_2( in4, in5, in6, in7, out2, out3 );            \
-}
-
-/* Description : Butterfly of 4 input vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * Details     : Butterfly operationuu
- */
-#define LASX_BUTTERFLY_4(RTYPE, in0, in1, in2, in3, out0, out1, out2, out3)  \
-{                                                                            \
-    out0 = (__m256i)( (RTYPE)in0 + (RTYPE)in3 );                             \
-    out1 = (__m256i)( (RTYPE)in1 + (RTYPE)in2 );                             \
-                                                                             \
-    out2 = (__m256i)( (RTYPE)in1 - (RTYPE)in2 );                             \
-    out3 = (__m256i)( (RTYPE)in0 - (RTYPE)in3 );                             \
-}
-
-/* Description : Butterfly of 8 input vectors
- * Arguments   : Inputs  - in0 in1 in2 ~
- *               Outputs - out0 out1 out2 ~
- * Details     : Butterfly operation
- */
-#define LASX_BUTTERFLY_8(RTYPE, in0, in1, in2, in3, in4, in5, in6, in7,    \
-                         out0, out1, out2, out3, out4, out5, out6, out7)   \
-{                                                                          \
-    out0 = (__m256i)( (RTYPE)in0 + (RTYPE)in7 );                           \
-    out1 = (__m256i)( (RTYPE)in1 + (RTYPE)in6 );                           \
-    out2 = (__m256i)( (RTYPE)in2 + (RTYPE)in5 );                           \
-    out3 = (__m256i)( (RTYPE)in3 + (RTYPE)in4 );                           \
-                                                                           \
-    out4 = (__m256i)( (RTYPE)in3 - (RTYPE)in4 );                           \
-    out5 = (__m256i)( (RTYPE)in2 - (RTYPE)in5 );                           \
-    out6 = (__m256i)( (RTYPE)in1 - (RTYPE)in6 );                           \
-    out7 = (__m256i)( (RTYPE)in0 - (RTYPE)in7 );                           \
-}
-
-/*
- ****************************************************************************
- ***************  Non-generic macro definition ******************************
- ****************************************************************************
- */
-
-/* Description : Transpose 8x4 block with half word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- *               Return Type - signed halfword
- */
-#define LASX_TRANSPOSE8X4_H_128SV( in0, in1, in2, in3,                    \
-                                   out0, out1, out2, out3 )               \
-{                                                                         \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                               \
-                                                                          \
-    LASX_ILVL_H_2_128SV( in1, in0, in3, in2, tmp0_m, tmp1_m );            \
-    LASX_ILVH_H_2_128SV( in1, in0, in3, in2, tmp2_m, tmp3_m );            \
-    LASX_ILVL_W_2_128SV( tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out2 );    \
-    LASX_ILVH_W_2_128SV( tmp1_m, tmp0_m, tmp3_m, tmp2_m, out1, out3 );    \
-}
-
-/* Description : Transpose 16x8 block into 8x16 with byte elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7,
- *                         in8, in9, in10, in11, in12, in13, in14, in15
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- *               Return Type - unsigned byte
- */
-#define LASX_TRANSPOSE16x8_B_128SV( in0, in1, in2, in3,                   \
-                                    in4, in5, in6, in7,                   \
-                                    in8, in9, in10, in11,                 \
-                                    in12, in13, in14, in15,               \
-                                    out0, out1, out2, out3,               \
-                                    out4, out5, out6, out7 )              \
-{                                                                         \
-    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                               \
-    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;                               \
-                                                                          \
-    out7 = __lasx_xvpackev_d( in8, in0 );                                 \
-    out6 = __lasx_xvpackev_d( in9, in1 );                                 \
-    out5 = __lasx_xvpackev_d( in10, in2 );                                \
-    out4 = __lasx_xvpackev_d( in11, in3 );                                \
-    out3 = __lasx_xvpackev_d( in12, in4 );                                \
-    out2 = __lasx_xvpackev_d( in13, in5 );                                \
-    out1 = __lasx_xvpackev_d( in14, in6 );                                \
-    out0 = __lasx_xvpackev_d( in15, in7 );                                \
-                                                                          \
-    tmp0_m = __lasx_xvpackev_b( out6, out7 );                             \
-    tmp4_m = __lasx_xvpackod_b( out6, out7 );                             \
-    tmp1_m = __lasx_xvpackev_b( out4, out5 );                             \
-    tmp5_m = __lasx_xvpackod_b( out4, out5 );                             \
-    out5 = __lasx_xvpackev_b( out2, out3 );                               \
-    tmp6_m = __lasx_xvpackod_b( out2, out3 );                             \
-    out7 = __lasx_xvpackev_b( out0, out1 );                               \
-    tmp7_m = __lasx_xvpackod_b( out0, out1 );                             \
-                                                                          \
-    tmp2_m = __lasx_xvpackev_h( tmp1_m, tmp0_m );                         \
-    tmp3_m = __lasx_xvpackev_h( out7, out5 );                             \
-    out0 = __lasx_xvpackev_w( tmp3_m, tmp2_m );                           \
-    out4 = __lasx_xvpackod_w( tmp3_m, tmp2_m );                           \
-                                                                          \
-    tmp2_m = __lasx_xvpackod_h( tmp1_m, tmp0_m );                         \
-    tmp3_m = __lasx_xvpackod_h( out7, out5 );                             \
-    out2 = __lasx_xvpackev_w( tmp3_m, tmp2_m );                           \
-    out6 = __lasx_xvpackod_w( tmp3_m, tmp2_m );                           \
-                                                                          \
-    tmp2_m = __lasx_xvpackev_h( tmp5_m, tmp4_m );                         \
-    tmp3_m = __lasx_xvpackev_h( tmp7_m, tmp6_m );                         \
-    out1 = __lasx_xvpackev_w( tmp3_m, tmp2_m );                           \
-    out5 = __lasx_xvpackod_w( tmp3_m, tmp2_m );                           \
-                                                                          \
-    tmp2_m = __lasx_xvpackod_h( tmp5_m, tmp4_m );                         \
-    tmp2_m = __lasx_xvpackod_h( tmp5_m, tmp4_m );                         \
-    tmp3_m = __lasx_xvpackod_h( tmp7_m, tmp6_m );                         \
-    tmp3_m = __lasx_xvpackod_h( tmp7_m, tmp6_m );                         \
-    out3 = __lasx_xvpackev_w( tmp3_m, tmp2_m );                           \
-    out7 = __lasx_xvpackod_w( tmp3_m, tmp2_m );                           \
-}
-
-/* Description : Horizontal addition of 8 signed word elements of input vector
- * Arguments   : Input  - in       (signed word vector)
- *               Output - sum_m    (s32 sum)
- * Details     : 8 signed word elements of 'in' vector are added together and
- *               the resulting integer sum is returned
- */
-#define LASX_HADD_SW_S32( in )                               \
-( {                                                          \
-    int32_t s_sum_m;                                         \
-    v4i64  out;                                              \
-                                                             \
-    out = __lasx_xvhaddw_d_w( in, in );                      \
-    s_sum_m = out[0] + out[1] + out[2] + out[3];             \
-    s_sum_m;                                                 \
-} )
-
-/* Description : Horizontal addition of 16 half word elements of input vector
- * Arguments   : Input  - in       (half word vector)
- *               Output - sum_m    (i32 sum)
- * Details     : 16 half word elements of 'in' vector are added together and
- *               the resulting integer sum is returned
- */
-#define LASX_HADD_UH_U32( in )                               \
-( {                                                          \
-    uint32_t u_sum_m;                                        \
-    v4u64  out;                                              \
-    __m256i res_m;                                           \
-                                                             \
-    res_m = __lasx_xvhaddw_wu_hu( in, in );                  \
-    out = ( v4u64 )__lasx_xvhaddw_du_wu( res_m, res_m );     \
-    u_sum_m = out[0] + out[1] + out[2] + out[3];             \
-    u_sum_m;                                                 \
-} )
-
-/* Description : Dot product and addition of 3 signed halfword input vectors
- * Arguments   : Inputs  - in0, in1, in2, coeff0, coeff1, coeff2
- *               Output - out0_m
- *               Return Type - signed halfword
- * Details     : Dot product of 'in0' with 'coeff0'
- *               Dot product of 'in1' with 'coeff1'
- *               Dot product of 'in2' with 'coeff2'
- *               Addition of all the 3 vector results
- *               out0_m = (in0 * coeff0) + (in1 * coeff1) + (in2 * coeff2)
- */
-#define LASX_DPADD_SH_3( in0, in1, in2, coeff0, coeff1, coeff2 )     \
-( {                                                                  \
-    __m256i out0_m;                                                  \
-                                                                     \
-    LASX_DP2ADD_H_B_3(in0, in1, in2, out0_m, coeff0, coeff1, coeff2);\
-    out0_m;                                                          \
-} )
-
-/* Description : Sign extend halfword elements from input vector and return
- *               the result in pair of vectors
- * Arguments   : Input  - in            (halfword vector)
- *               Outputs - out0, out1   (sign extended word vectors)
- *               Return Type - signed word
- * Details     : Sign bit of halfword elements from input vector 'in' is
- *               extracted and interleaved right with same vector 'in0' to
- *               generate 4 signed word elements in 'out0'
- *               Then interleaved left with same vector 'in0' to
- *               generate 4 signed word elements in 'out1'
- */
-#define LASX_UNPCK_SH_128SV( in, out0, out1 )   \
-{                                               \
-    __m256i tmp_m;                              \
-                                                \
-    tmp_m = __lasx_xvslti_h( in, 0 );           \
-    LASX_ILVLH_H_128SV( tmp_m, in, out1, out0 );\
-}
-
-/* Description : Vector Bit Move If Not Zero
- * Arguments   : Input  - in0, in1, mask      (vector)
- *               Outputs - out                (vectors)
- * Details     : Copy to destination vector in0 all bits from source vector in1
- *               for which the corresponding bits from target vector mask
- *               are 1 and leaves unchanged all destination bits for which
- *               the corresponding target bits are 0.
- *               out = (in1 AND mask) OR (in0 AND NOT mask)
- */
-#define LASX_BMNZ( in0, in1, mask, out )  \
-{                                         \
-    __m256i tmp0, tmp1;                   \
-                                          \
-    tmp1 = __lasx_xvand_v( in1, mask);    \
-    tmp0 = __lasx_xvandn_v( mask, in0);   \
-    out = __lasx_xvor_v(tmp1, tmp0);      \
-}
-
-#endif /* GENERIC_MACROS_LASX_H */
diff --git a/common/loongarch/loongson_intrinsics.h b/common/loongarch/loongson_intrinsics.h
new file mode 100644
index 00000000..a21a68a3
--- /dev/null
+++ b/common/loongarch/loongson_intrinsics.h
@@ -0,0 +1,1965 @@
+/*****************************************************************************
+ * loongson_intrinsics.h: loongarch macros
+ *****************************************************************************
+ * Copyright (C) 2020 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: Peng Zhou    <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * All rights reserved.
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei Gu   <guxiwei-hf@loongson.cn>
+ *                Lu Wang    <wanglu@loongson.cn>
+ *
+ * This file is maintained in LSOM project, don't change it directly.
+ * You can get the latest version of this header from: ***
+ *
+ */
+
+#ifndef LOONGSON_INTRINSICS_H
+#define LOONGSON_INTRINSICS_H
+
+/**
+ * MAJOR version: Macro usage changes.
+ * MINOR version: Add new functions, or bug fix.
+ * MICRO version: Comment changes or implementation changes.
+ */
+#define LSOM_VERSION_MAJOR 1
+#define LSOM_VERSION_MINOR 0
+#define LSOM_VERSION_MICRO 1
+
+#define DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0); \
+    _OUT1 = _INS(_IN1); \
+}
+
+#define DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0, _IN1); \
+    _OUT1 = _INS(_IN2, _IN3); \
+}
+
+#define DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0, _IN1, _IN2); \
+    _OUT1 = _INS(_IN3, _IN4, _IN5); \
+}
+
+#define DUP4_ARG1(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1); \
+    DUP2_ARG1(_INS, _IN2, _IN3, _OUT2, _OUT3); \
+}
+
+#define DUP4_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
+                  _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1); \
+    DUP2_ARG2(_INS, _IN4, _IN5, _IN6, _IN7, _OUT2, _OUT3); \
+}
+
+#define DUP4_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
+                  _IN8, _IN9, _IN10, _IN11, _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4,  _IN5,  _OUT0, _OUT1); \
+    DUP2_ARG3(_INS, _IN6, _IN7, _IN8, _IN9, _IN10, _IN11, _OUT2, _OUT3); \
+}
+
+/*
+ * =============================================================================
+ * Description : Print out elements in vector.
+ * Arguments   : Inputs  - RTYPE, _element_num, _in0, _enter
+ *               Outputs -
+ * Details     : Print out '_element_num' elements in 'RTYPE' vector '_in0', if
+ *               '_enter' is TRUE, prefix "\nVP:" will be added first.
+ * Example     : VECT_PRINT(v4i32,4,in0,1); // in0: 1,2,3,4
+ *               VP:1,2,3,4,
+ * =============================================================================
+ */
+#define VECT_PRINT(RTYPE, element_num, in0, enter)    \
+{                                                     \
+    RTYPE _tmp0 = (RTYPE)in0;                         \
+    int _i = 0;                                       \
+    if (enter)                                        \
+        printf("\nVP:");                              \
+    for(_i = 0; _i < element_num; _i++)               \
+        printf("%d,",_tmp0[_i]);                      \
+}
+
+#ifdef __loongarch_sx
+#include <lsxintrin.h>
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               Then the results plus to signed half word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_b(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_h_b(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               unsigned byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               The results plus to signed half word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_bu(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_h_bu(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of half word vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - __m128i
+ * Details     : Signed half word elements from in_h are multiplied by
+ *               signed half word elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               Then the results plus to signed word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_w_h(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_b(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_b(in_h, in_l);
+    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               unsigned byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_bu(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_bu(in_h, in_l);
+    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_bu_b(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,-1
+ *         out : 22,38,38,22, 22,38,38,6
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_bu_b(in_h, in_l);
+    out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_w_h(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_w_h(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_w_h(in_h, in_l);
+    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all halfword elements of input vector between min & max
+ *               out = ((_in) < (min)) ? (min) : (((_in) > (max)) ? (max) : (_in))
+ * Arguments   : Inputs  - _in  (input vector)
+ *                       - min  (min threshold)
+ *                       - max  (max threshold)
+ *               Outputs - out  (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : out = __lsx_vclip_h(_in)
+ *         _in : -8,2,280,249, -8,255,280,249
+ *         min : 1,1,1,1, 1,1,1,1
+ *         max : 9,9,9,9, 9,9,9,9
+ *         out : 1,2,9,9, 1,9,9,9
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip_h(__m128i _in, __m128i min, __m128i max)
+{
+    __m128i out;
+
+    out = __lsx_vmax_h(min, _in);
+    out = __lsx_vmin_h(max, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Set each element of vector between 0 and 255
+ * Arguments   : Inputs  - _in
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from _in are clamped between 0 and 255.
+ * Example     : out = __lsx_vclip255_h(_in)
+ *         _in : -8,255,280,249, -8,255,280,249
+ *         out : 0,255,255,249, 0,255,255,249
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip255_h(__m128i _in)
+{
+    __m128i out;
+
+    out = __lsx_vmaxi_h(_in, 0);
+    out = __lsx_vsat_hu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Set each element of vector between 0 and 255
+ * Arguments   : Inputs  - _in
+ *               Outputs - out
+ *               Retrun Type - word
+ * Details     : Signed byte elements from _in are clamped between 0 and 255.
+ * Example     : out = __lsx_vclip255_w(_in)
+ *         _in : -8,255,280,249
+ *         out : 0,255,255,249
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip255_w(__m128i _in)
+{
+    __m128i out;
+
+    out = __lsx_vmaxi_w(_in, 0);
+    out = __lsx_vsat_wu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Swap two variables
+ * Arguments   : Inputs  - _in0, _in1
+ *               Outputs - _in0, _in1 (in-place)
+ * Details     : Swapping of two input variables using xor
+ * Example     : LSX_SWAP(_in0, _in1)
+ *        _in0 : 1,2,3,4
+ *        _in1 : 5,6,7,8
+ *   _in0(out) : 5,6,7,8
+ *   _in1(out) : 1,2,3,4
+ * =============================================================================
+ */
+#define LSX_SWAP(_in0, _in1)                                            \
+{                                                                       \
+    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
+    _in1 = __lsx_vxor_v(_in0, _in1);                                    \
+    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
+}                                                                       \
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     :
+ * Example     :
+ *               1, 2, 3, 4            1, 5, 9,13
+ *               5, 6, 7, 8    to      2, 6,10,14
+ *               9,10,11,12  =====>    3, 7,11,15
+ *              13,14,15,16            4, 8,12,16
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE4x4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                              \
+    __m128i _t0, _t1, _t2, _t3;                                                \
+                                                                               \
+    _t0   = __lsx_vilvl_w(_in1, _in0);                                         \
+    _t1   = __lsx_vilvh_w(_in1, _in0);                                         \
+    _t2   = __lsx_vilvl_w(_in3, _in2);                                         \
+    _t3   = __lsx_vilvh_w(_in3, _in2);                                         \
+    _out0 = __lsx_vilvl_d(_t2, _t0);                                           \
+    _out1 = __lsx_vilvh_d(_t2, _t0);                                           \
+    _out2 = __lsx_vilvl_d(_t3, _t1);                                           \
+    _out3 = __lsx_vilvh_d(_t3, _t1);                                           \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with byte elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LSX_TRANSPOSE8x8_B
+ *        _in0 : 00,01,02,03,04,05,06,07, 00,00,00,00,00,00,00,00
+ *        _in1 : 10,11,12,13,14,15,16,17, 00,00,00,00,00,00,00,00
+ *        _in2 : 20,21,22,23,24,25,26,27, 00,00,00,00,00,00,00,00
+ *        _in3 : 30,31,32,33,34,35,36,37, 00,00,00,00,00,00,00,00
+ *        _in4 : 40,41,42,43,44,45,46,47, 00,00,00,00,00,00,00,00
+ *        _in5 : 50,51,52,53,54,55,56,57, 00,00,00,00,00,00,00,00
+ *        _in6 : 60,61,62,63,64,65,66,67, 00,00,00,00,00,00,00,00
+ *        _in7 : 70,71,72,73,74,75,76,77, 00,00,00,00,00,00,00,00
+ *
+ *      _ out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
+ *      _ out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
+ *      _ out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
+ *      _ out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
+ *      _ out4 : 04,14,24,34,44,54,64,74, 00,00,00,00,00,00,00,00
+ *      _ out5 : 05,15,25,35,45,55,65,75, 00,00,00,00,00,00,00,00
+ *      _ out6 : 06,16,26,36,46,56,66,76, 00,00,00,00,00,00,00,00
+ *      _ out7 : 07,17,27,37,47,57,67,77, 00,00,00,00,00,00,00,00
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+   __m128i zero = {0};                                                            \
+   __m128i shuf8 = {0x0F0E0D0C0B0A0908, 0x1716151413121110};                      \
+   __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
+                                                                                  \
+   _t0 = __lsx_vilvl_b(_in2, _in0);                                               \
+   _t1 = __lsx_vilvl_b(_in3, _in1);                                               \
+   _t2 = __lsx_vilvl_b(_in6, _in4);                                               \
+   _t3 = __lsx_vilvl_b(_in7, _in5);                                               \
+   _t4 = __lsx_vilvl_b(_t1, _t0);                                                 \
+   _t5 = __lsx_vilvh_b(_t1, _t0);                                                 \
+   _t6 = __lsx_vilvl_b(_t3, _t2);                                                 \
+   _t7 = __lsx_vilvh_b(_t3, _t2);                                                 \
+   _out0 = __lsx_vilvl_w(_t6, _t4);                                               \
+   _out2 = __lsx_vilvh_w(_t6, _t4);                                               \
+   _out4 = __lsx_vilvl_w(_t7, _t5);                                               \
+   _out6 = __lsx_vilvh_w(_t7, _t5);                                               \
+   _out1 = __lsx_vshuf_b(zero, _out0, shuf8);                                     \
+   _out3 = __lsx_vshuf_b(zero, _out2, shuf8);                                     \
+   _out5 = __lsx_vshuf_b(zero, _out4, shuf8);                                     \
+   _out7 = __lsx_vshuf_b(zero, _out6, shuf8);                                     \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with half word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ * Example     :
+ *              00,01,02,03,04,05,06,07           00,10,20,30,40,50,60,70
+ *              10,11,12,13,14,15,16,17           01,11,21,31,41,51,61,71
+ *              20,21,22,23,24,25,26,27           02,12,22,32,42,52,62,72
+ *              30,31,32,33,34,35,36,37    to     03,13,23,33,43,53,63,73
+ *              40,41,42,43,44,45,46,47  ======>  04,14,24,34,44,54,64,74
+ *              50,51,52,53,54,55,56,57           05,15,25,35,45,55,65,75
+ *              60,61,62,63,64,65,66,67           06,16,26,36,46,56,66,76
+ *              70,71,72,73,74,75,76,77           07,17,27,37,47,57,67,77
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    __m128i _s0, _s1, _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                     \
+                                                                                  \
+    _s0 = __lsx_vilvl_h(_in6, _in4);                                              \
+    _s1 = __lsx_vilvl_h(_in7, _in5);                                              \
+    _t0 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t1 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvh_h(_in6, _in4);                                              \
+    _s1 = __lsx_vilvh_h(_in7, _in5);                                              \
+    _t2 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t3 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvl_h(_in2, _in0);                                              \
+    _s1 = __lsx_vilvl_h(_in3, _in1);                                              \
+    _t4 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t5 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvh_h(_in2, _in0);                                              \
+    _s1 = __lsx_vilvh_h(_in3, _in1);                                              \
+    _t6 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t7 = __lsx_vilvh_h(_s1, _s0);                                                \
+                                                                                  \
+    _out0 = __lsx_vpickev_d(_t0, _t4);                                            \
+    _out2 = __lsx_vpickev_d(_t1, _t5);                                            \
+    _out4 = __lsx_vpickev_d(_t2, _t6);                                            \
+    _out6 = __lsx_vpickev_d(_t3, _t7);                                            \
+    _out1 = __lsx_vpickod_d(_t0, _t4);                                            \
+    _out3 = __lsx_vpickod_d(_t1, _t5);                                            \
+    _out5 = __lsx_vpickod_d(_t2, _t6);                                            \
+    _out7 = __lsx_vpickod_d(_t3, _t7);                                            \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 8x4 byte block into 4x8
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3      (input 8x4 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3  (output 4x8 byte block)
+ *               Return Type - as per RTYPE
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LSX_TRANSPOSE8x4_B
+ *        _in0 : 00,01,02,03,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in1 : 10,11,12,13,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in2 : 20,21,22,23,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in3 : 30,31,32,33,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in4 : 40,41,42,43,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in5 : 50,51,52,53,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in6 : 60,61,62,63,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in7 : 70,71,72,73,00,00,00,00, 00,00,00,00,00,00,00,00
+ *
+ *       _out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
+ *       _out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
+ *       _out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
+ *       _out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x4_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,       \
+                           _out0, _out1, _out2, _out3)                           \
+{                                                                                \
+    __m128i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                  \
+                                                                                 \
+    _tmp0_m = __lsx_vpackev_w(_in4, _in0);                                       \
+    _tmp1_m = __lsx_vpackev_w(_in5, _in1);                                       \
+    _tmp2_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
+    _tmp0_m = __lsx_vpackev_w(_in6, _in2);                                       \
+    _tmp1_m = __lsx_vpackev_w(_in7, _in3);                                       \
+                                                                                 \
+    _tmp3_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
+    _tmp0_m = __lsx_vilvl_h(_tmp3_m, _tmp2_m);                                   \
+    _tmp1_m = __lsx_vilvh_h(_tmp3_m, _tmp2_m);                                   \
+                                                                                 \
+    _out0 = __lsx_vilvl_w(_tmp1_m, _tmp0_m);                                     \
+    _out2 = __lsx_vilvh_w(_tmp1_m, _tmp0_m);                                     \
+    _out1 = __lsx_vilvh_d(_out2, _out0);                                         \
+    _out3 = __lsx_vilvh_d(_out0, _out2);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 16x8 block with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7, in8
+ *                         in9, in10, in11, in12, in13, in14, in15
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ * Example     :
+ *              000,001,002,003,004,005,006,007
+ *              008,009,010,011,012,013,014,015
+ *              016,017,018,019,020,021,022,023
+ *              024,025,026,027,028,029,030,031
+ *              032,033,034,035,036,037,038,039
+ *              040,041,042,043,044,045,046,047        000,008,...,112,120
+ *              048,049,050,051,052,053,054,055        001,009,...,113,121
+ *              056,057,058,059,060,061,062,063   to   002,010,...,114,122
+ *              064,068,066,067,068,069,070,071 =====> 003,011,...,115,123
+ *              072,073,074,075,076,077,078,079        004,012,...,116,124
+ *              080,081,082,083,084,085,086,087        005,013,...,117,125
+ *              088,089,090,091,092,093,094,095        006,014,...,118,126
+ *              096,097,098,099,100,101,102,103        007,015,...,119,127
+ *              104,105,106,107,108,109,110,111
+ *              112,113,114,115,116,117,118,119
+ *              120,121,122,123,124,125,126,127
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _in8,  \
+                            _in9, _in10, _in11, _in12, _in13, _in14, _in15, _out0, \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)       \
+{                                                                                  \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
+    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
+    DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5,       \
+              _tmp0, _tmp1, _tmp2, _tmp3);                                         \
+    DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,        \
+              _in13, _tmp4, _tmp5, _tmp6, _tmp7);                                  \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);                \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);                \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);                \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);                \
+    DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);                    \
+    DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);                    \
+    DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);                    \
+    DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);                    \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);            \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);            \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);            \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);            \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     : Butterfly operation
+ * Example     :
+ *               out0 = in0 + in3;
+ *               out1 = in1 + in2;
+ *               out2 = in1 - in2;
+ *               out3 = in0 - in3;
+ * =============================================================================
+ */
+#define LSX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_b(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_b(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_b(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_b(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_h(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_h(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_h(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_h(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_w(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_w(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_w(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_w(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_d(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_d(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_d(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_d(_in0, _in3);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     :
+ *              _out0 = _in0 + _in7;
+ *              _out1 = _in1 + _in6;
+ *              _out2 = _in2 + _in5;
+ *              _out3 = _in3 + _in4;
+ *              _out4 = _in3 - _in4;
+ *              _out5 = _in2 - _in5;
+ *              _out6 = _in1 - _in6;
+ *              _out7 = _in0 - _in7;
+ * =============================================================================
+ */
+#define LSX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_b(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_b(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_b(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_b(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_b(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_b(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_b(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_b(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_h(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_h(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_h(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_h(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_h(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_h(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_h(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_h(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_w(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_w(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_w(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_w(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_w(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_w(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_w(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_w(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_d(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_d(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_d(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_d(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_d(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_d(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_d(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_d(_in0, _in7);                                            \
+}
+
+#endif //LSX
+
+#ifdef __loongarch_asx
+#include <lasxintrin.h>
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_bu(in_h, in_l);
+    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Signed byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this iniplication results of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_b(in_h, in_l);
+    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector.
+ * Example     : out = __lasx_xvdp2_w_h(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of word vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Retrun Type - signed double
+ * Details     : Signed word elements from in_h are multiplied with
+ *               signed word elements from in_l producing a result
+ *               twice the size of input i.e. signed double word.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector.
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_d_w(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_d_w(in_h, in_l);
+    out = __lasx_xvmaddwod_d_w(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. unsigned word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_hu_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_h_b(__m256i in_c,__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_h_b(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - per RTYPE
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_h(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               unsigned halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_hu(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_hu(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_hu_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_hu_h(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Vector Unsigned Dot Product and Subtract
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtracted from double width elements
+ *               in_c vector.
+ * Example     : See out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2sub_h_bu(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_bu(in_h, in_l);
+    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+    out = __lasx_xvsub_h(in_c, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Vector Signed Dot Product and Subtract
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               Signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtracted from double width elements
+ *               in_c vector.
+ * Example     : out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
+ *        in_c : 0,0,0,0, 0,0,0,0
+ *        in_h : 3,1,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
+ *        in_l : 2,1,1,0, 1,0,0,0, 0,0,1,0, 1,0,0,1
+ *         out : -7,-3,0,0, 0,-1,0,-1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    out = __lasx_xvsub_w(in_c, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are iniplied with
+ *               signed halfword elements from in_l producing a result
+ *               four times the size of input i.e. signed doubleword.
+ *               Then this iniplication results of four adjacent elements
+ *               are added together and stored to the out vector.
+ * Example     : out = __lasx_xvdp4_d_h(in_h, in_l)
+ *        in_h :  3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1
+ *        in_l : -2,1,1,0, 1,0,0,0, 0,0,1, 0, 1,0,0,1
+ *         out : -2,0,1,1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp4_d_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    out = __lasx_xvhaddw_d_w(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The high half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               higher half of the two-fold sign extension (signed byte
+ *               to signed halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwh_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvh_b(in_h, in_l);
+    out = __lasx_xvhaddw_h_b(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The high half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               higher half of the two-fold sign extension (signed halfword
+ *               to signed word) and stored to the out vector.
+ * Example     : out = __lasx_xvaddwh_w_h(in_h, in_l)
+ *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
+ *         out : 1,0,0,-1, 1,0,0, 2
+ * =============================================================================
+ */
+ static inline __m256i __lasx_xvaddwh_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvh_h(in_h, in_l);
+    out = __lasx_xvhaddw_w_h(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               lower half of the two-fold sign extension (signed byte
+ *               to signed halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_b(in_h, in_l);
+    out = __lasx_xvhaddw_h_b(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               lower half of the two-fold sign extension (signed halfword
+ *               to signed word) and stored to the out vector.
+ * Example     : out = __lasx_xvaddwl_w_h(in_h, in_l)
+ *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
+ *         out : 5,-1,4,2, 1,0,2,-1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_h(in_h, in_l);
+    out = __lasx_xvhaddw_w_h(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The out vector and the out vector are added after the
+ *               lower half of the two-fold zero extension (unsigned byte
+ *               to unsigned halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_b(in_h, in_l);
+    out = __lasx_xvhaddw_hu_bu(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_l vector after double zero extension (unsigned byte to
+ *               signed halfword)，added to the in_h vector.
+ * Example     : See out = __lasx_xvaddw_w_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddw_h_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvsllwil_hu_bu(in_l, 0);
+    out = __lasx_xvadd_h(in_h, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_l vector after double sign extension (signed halfword to
+ *               signed word), added to the in_h vector.
+ * Example     : out = __lasx_xvaddw_w_w_h(in_h, in_l)
+ *        in_h : 0, 1,0,0, -1,0,0,1,
+ *        in_l : 2,-1,1,2,  1,0,0,0, 0,0,1,0, 1,0,0,1,
+ *         out : 2, 0,1,2, -1,0,1,1,
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddw_w_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvsllwil_w_h(in_l, 0);
+    out = __lasx_xvadd_w(in_h, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication and addition calculation after expansion
+ *               of the lower half of the vector.
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed halfword
+ *               to signed word), and the result is added to the vector in_c,
+ *               then stored to the out vector.
+ * Example     : out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 5,6,7,8
+ *        in_h : 1,2,3,4, 1,2,3,4, 5,6,7,8, 5,6,7,8
+ *        in_l : 200, 300, 400, 500,  2000, 3000, 4000, 5000,
+ *              -200,-300,-400,-500, -2000,-3000,-4000,-5000
+ *         out : 201, 602,1203,2004, -995, -1794,-2793,-3992
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmaddwl_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+    tmp0 = __lasx_xvmul_w(tmp0, tmp1);
+    out  = __lasx_xvadd_w(tmp0, in_c);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication and addition calculation after expansion
+ *               of the higher half of the vector.
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the higher half of the two-fold sign extension (signed
+ *               halfword to signed word), and the result is added to
+ *               the vector in_c, then stored to the out vector.
+ * Example     : See out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmaddwh_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvilvh_h(in_h, in_h);
+    tmp1 = __lasx_xvilvh_h(in_l, in_l);
+    tmp0 = __lasx_xvmulwev_w_h(tmp0, tmp1);
+    out  = __lasx_xvadd_w(tmp0, in_c);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication calculation after expansion of the lower
+ *               half of the vector.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed
+ *               halfword to signed word), then stored to the out vector.
+ * Example     : out = __lasx_xvmulwl_w_h(in_h, in_l)
+ *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
+ *         out : 6,1,3,0, 0,0,1,0
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmulwl_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+    out  = __lasx_xvmul_w(tmp0, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication calculation after expansion of the lower
+ *               half of the vector.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed
+ *               halfword to signed word), then stored to the out vector.
+ * Example     : out = __lasx_xvmulwh_w_h(in_h, in_l)
+ *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
+ *         out : 0,0,0,0, 0,0,0,1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmulwh_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvilvh_h(in_h, in_h);
+    tmp1 = __lasx_xvilvh_h(in_l, in_l);
+    out  = __lasx_xvmulwev_w_h(tmp0, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added saturately after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector adds the in_l vector saturately after the lower
+ *               half of the two-fold zero extension (unsigned byte to unsigned
+ *               halfword) and the results are stored to the out vector.
+ * Example     : out = __lasx_xvsaddw_hu_hu_bu(in_h, in_l)
+ *        in_h : 2,65532,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1
+ *        in_l : 3,6,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1, 3,18,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
+ *         out : 5,65535,4,2, 1,0,0,1, 3,18,4,0, 1,0,0,2,
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsaddw_hu_hu_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp1, out;
+    __m256i zero = {0};
+
+    tmp1 = __lasx_xvilvl_b(zero, in_l);
+    out  = __lasx_xvsadd_hu(in_h, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all halfword elements of input vector between min & max
+ *               out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
+ * Arguments   : Inputs  - in    (input vector)
+ *                       - min   (min threshold)
+ *                       - max   (max threshold)
+ *               Outputs - in    (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : out = __lasx_xvclip_h(in, min, max)
+ *          in : -8,2,280,249, -8,255,280,249, 4,4,4,4, 5,5,5,5
+ *         min : 1,1,1,1, 1,1,1,1, 1,1,1,1, 1,1,1,1
+ *         max : 9,9,9,9, 9,9,9,9, 9,9,9,9, 9,9,9,9
+ *         out : 1,2,9,9, 1,9,9,9, 4,4,4,4, 5,5,5,5
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip_h(__m256i in, __m256i min, __m256i max)
+{
+    __m256i out;
+
+    out = __lasx_xvmax_h(min, in);
+    out = __lasx_xvmin_h(max, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all signed halfword elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs  - in   (input vector)
+ *               Outputs - out  (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : See out = __lasx_xvclamp255_w(in)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip255_h(__m256i in)
+{
+    __m256i out;
+
+    out = __lasx_xvmaxi_h(in, 0);
+    out = __lasx_xvsat_hu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all signed word elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs - in   (input vector)
+ *               Output - out  (output vector with clipped elements)
+ *               Return Type - signed word
+ * Example     : out = __lasx_xvclamp255_w(in)
+ *          in : -8,255,280,249, -8,255,280,249
+ *         out :  0,255,255,249,  0,255,255,249
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip255_w(__m256i in)
+{
+    __m256i out;
+
+    out = __lasx_xvmaxi_w(in, 0);
+    out = __lasx_xvsat_wu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
+ *               if 'indx >= 8' use xvsplati_h_*.
+ * Arguments   : Inputs - in, idx
+ *               Output - out
+ * Details     : Idx element value from in vector is replicated to all
+ *               elements in out vector.
+ *               Valid index range for halfword operation is 0-7
+ * Example     : out = __lasx_xvsplati_l_h(in, idx)
+ *          in : 20,10,11,12, 13,14,15,16, 0,0,2,0, 0,0,0,0
+ *         idx : 0x02
+ *         out : 11,11,11,11, 11,11,11,11, 11,11,11,11, 11,11,11,11
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsplati_l_h(__m256i in, int idx)
+{
+    __m256i out;
+
+    out = __lasx_xvpermi_q(in, in, 0x02);
+    out = __lasx_xvreplve_h(out, idx);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
+ *               if 'indx >= 8' use xvsplati_h_*.
+ * Arguments   : Inputs - in, idx
+ *               Output - out
+ * Details     : Idx element value from in vector is replicated to all
+ *               elements in out vector.
+ *               Valid index range for halfword operation is 0-7
+ * Example     : out = __lasx_xvsplati_h_h(in, idx)
+ *          in : 20,10,11,12, 13,14,15,16, 0,2,0,0, 0,0,0,0
+ *         idx : 0x09
+ *         out : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
+{
+    __m256i out;
+
+    out = __lasx_xvpermi_q(in, in, 0x13);
+    out = __lasx_xvreplve_h(out, idx);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with double word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Example     : LASX_TRANSPOSE4x4_D
+ *         _in0 : 1,2,3,4
+ *         _in1 : 1,2,3,4
+ *         _in2 : 1,2,3,4
+ *         _in3 : 1,2,3,4
+ *
+ *        _out0 : 1,1,1,1
+ *        _out1 : 2,2,2,2
+ *        _out2 : 3,3,3,3
+ *        _out3 : 4,4,4,4
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE4x4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                               \
+    __m256i _tmp0, _tmp1, _tmp2, _tmp3;                                         \
+    _tmp0 = __lasx_xvilvl_d(_in1, _in0);                                        \
+    _tmp1 = __lasx_xvilvh_d(_in1, _in0);                                        \
+    _tmp2 = __lasx_xvilvl_d(_in3, _in2);                                        \
+    _tmp3 = __lasx_xvilvh_d(_in3, _in2);                                        \
+    _out0 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x20);                               \
+    _out2 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x31);                               \
+    _out1 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x20);                               \
+    _out3 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x31);                               \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ * Example     : LASX_TRANSPOSE8x8_W
+ *         _in0 : 1,2,3,4,5,6,7,8
+ *         _in1 : 2,2,3,4,5,6,7,8
+ *         _in2 : 3,2,3,4,5,6,7,8
+ *         _in3 : 4,2,3,4,5,6,7,8
+ *         _in4 : 5,2,3,4,5,6,7,8
+ *         _in5 : 6,2,3,4,5,6,7,8
+ *         _in6 : 7,2,3,4,5,6,7,8
+ *         _in7 : 8,2,3,4,5,6,7,8
+ *
+ *        _out0 : 1,2,3,4,5,6,7,8
+ *        _out1 : 2,2,2,2,2,2,2,2
+ *        _out2 : 3,3,3,3,3,3,3,3
+ *        _out3 : 4,4,4,4,4,4,4,4
+ *        _out4 : 5,5,5,5,5,5,5,5
+ *        _out5 : 6,6,6,6,6,6,6,6
+ *        _out6 : 7,7,7,7,7,7,7,7
+ *        _out7 : 8,8,8,8,8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_w(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvl_w(_in3, _in1);                                          \
+    _tmp0_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp1_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_w(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvh_w(_in3, _in1);                                          \
+    _tmp2_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp3_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvl_w(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvl_w(_in7, _in5);                                          \
+    _tmp4_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp5_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_w(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvh_w(_in7, _in5);                                          \
+    _tmp6_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp7_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _out0 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x20);                               \
+    _out1 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x20);                               \
+    _out2 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x20);                               \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x20);                               \
+    _out4 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x31);                               \
+    _out5 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x31);                               \
+    _out6 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x31);                               \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x31);                               \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 16x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
+ *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
+ *                         (input 16x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : See LASX_TRANSPOSE16x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
+                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+{                                                                                    \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvl_b(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvl_b(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvl_b(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvl_b(_in15, _in13);                                         \
+    _out0 = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                       \
+    _out1 = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                       \
+    _out2 = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                       \
+    _out3 = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                       \
+    _out4 = __lasx_xvilvl_b(_tmp5_m, _tmp4_m);                                       \
+    _out5 = __lasx_xvilvh_b(_tmp5_m, _tmp4_m);                                       \
+    _out6 = __lasx_xvilvl_b(_tmp7_m, _tmp6_m);                                       \
+    _out7 = __lasx_xvilvh_b(_tmp7_m, _tmp6_m);                                       \
+    _tmp0_m = __lasx_xvilvl_w(_out2, _out0);                                         \
+    _tmp2_m = __lasx_xvilvh_w(_out2, _out0);                                         \
+    _tmp4_m = __lasx_xvilvl_w(_out3, _out1);                                         \
+    _tmp6_m = __lasx_xvilvh_w(_out3, _out1);                                         \
+    _tmp1_m = __lasx_xvilvl_w(_out6, _out4);                                         \
+    _tmp3_m = __lasx_xvilvh_w(_out6, _out4);                                         \
+    _tmp5_m = __lasx_xvilvl_w(_out7, _out5);                                         \
+    _tmp7_m = __lasx_xvilvh_w(_out7, _out5);                                         \
+    _out0 = __lasx_xvilvl_d(_tmp1_m, _tmp0_m);                                       \
+    _out1 = __lasx_xvilvh_d(_tmp1_m, _tmp0_m);                                       \
+    _out2 = __lasx_xvilvl_d(_tmp3_m, _tmp2_m);                                       \
+    _out3 = __lasx_xvilvh_d(_tmp3_m, _tmp2_m);                                       \
+    _out4 = __lasx_xvilvl_d(_tmp5_m, _tmp4_m);                                       \
+    _out5 = __lasx_xvilvh_d(_tmp5_m, _tmp4_m);                                       \
+    _out6 = __lasx_xvilvl_d(_tmp7_m, _tmp6_m);                                       \
+    _out7 = __lasx_xvilvh_d(_tmp7_m, _tmp6_m);                                       \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 16x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
+ *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
+ *                         (input 16x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LASX_TRANSPOSE16x8_H
+ *        _in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in2 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in3 : 4,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in4 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in5 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in6 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in7 : 8,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in8 : 9,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in9 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in10 : 0,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in11 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in12 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in13 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in14 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in15 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *
+ *       _out0 : 1,2,3,4,5,6,7,8,9,1,0,2,3,7,5,6
+ *       _out1 : 2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
+ *       _out2 : 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
+ *       _out3 : 4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4
+ *       _out4 : 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
+ *       _out5 : 6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
+ *       _out6 : 7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
+ *       _out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE16x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
+                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+   {                                                                                 \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
+    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                  \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvl_h(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvl_h(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvl_h(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvl_h(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvl_h(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvl_h(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvl_h(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvl_h(_in15, _in13);                                         \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
+    _out0 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
+    _out1 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
+    _out2 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvh_h(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvh_h(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvh_h(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvh_h(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvh_h(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvh_h(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvh_h(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvh_h(_in15, _in13);                                         \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
+    _out4 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
+    _out5 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
+    _out6 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with halfword elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ *               Return Type - signed halfword
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : See LASX_TRANSPOSE8x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE4x4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)     \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+                                                                                    \
+    _s0_m = __lasx_xvilvl_h(_in1, _in0);                                            \
+    _s1_m = __lasx_xvilvl_h(_in3, _in2);                                            \
+    _out0 = __lasx_xvilvl_w(_s1_m, _s0_m);                                          \
+    _out2 = __lasx_xvilvh_w(_s1_m, _s0_m);                                          \
+    _out1 = __lasx_xvilvh_d(_out0, _out0);                                          \
+    _out3 = __lasx_xvilvh_d(_out2, _out2);                                          \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 8x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *                         (input 8x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x8 byte block)
+ * Example     : See LASX_TRANSPOSE8x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
+{                                                                                   \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                          \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                          \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                          \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                          \
+    _tmp4_m = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                    \
+    _tmp5_m = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                    \
+    _tmp6_m = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                    \
+    _tmp7_m = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                    \
+    _out0 = __lasx_xvilvl_w(_tmp6_m, _tmp4_m);                                      \
+    _out2 = __lasx_xvilvh_w(_tmp6_m, _tmp4_m);                                      \
+    _out4 = __lasx_xvilvl_w(_tmp7_m, _tmp5_m);                                      \
+    _out6 = __lasx_xvilvh_w(_tmp7_m, _tmp5_m);                                      \
+    _out1 = __lasx_xvbsrl_v(_out0, 8);                                              \
+    _out3 = __lasx_xvbsrl_v(_out2, 8);                                              \
+    _out5 = __lasx_xvbsrl_v(_out4, 8);                                              \
+    _out7 = __lasx_xvbsrl_v(_out6, 8);                                              \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with halfword elements in vectors.
+ * Arguments   : Inputs  - _in0, _in1, ~
+ *               Outputs - _out0, _out1, ~
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LASX_TRANSPOSE8x8_H
+ *        _in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *        _in2 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *        _in3 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in4 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *        _in5 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in6 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in7 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *
+ *       _out0 : 1,8,8,1, 9,1,1,9, 1,8,8,1, 9,1,1,9
+ *       _out1 : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ *       _out2 : 3,3,3,3, 3,3,3,3, 3,3,3,3, 3,3,3,3
+ *       _out3 : 4,4,4,4, 4,4,4,4, 4,4,4,4, 4,4,4,4
+ *       _out4 : 5,5,5,5, 5,5,5,5, 5,5,5,5, 5,5,5,5
+ *       _out5 : 6,6,6,6, 6,6,6,6, 6,6,6,6, 6,6,6,6
+ *       _out6 : 7,7,7,7, 7,7,7,7, 7,7,7,7, 7,7,7,7
+ *       _out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_h(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvl_h(_in7, _in5);                                          \
+    _tmp0_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp1_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_h(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvh_h(_in7, _in5);                                          \
+    _tmp2_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp3_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_h(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvl_h(_in3, _in1);                                          \
+    _tmp4_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp5_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_h(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvh_h(_in3, _in1);                                          \
+    _tmp6_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp7_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+                                                                                    \
+    _out0 = __lasx_xvpickev_d(_tmp0_m, _tmp4_m);                                    \
+    _out2 = __lasx_xvpickev_d(_tmp1_m, _tmp5_m);                                    \
+    _out4 = __lasx_xvpickev_d(_tmp2_m, _tmp6_m);                                    \
+    _out6 = __lasx_xvpickev_d(_tmp3_m, _tmp7_m);                                    \
+    _out1 = __lasx_xvpickod_d(_tmp0_m, _tmp4_m);                                    \
+    _out3 = __lasx_xvpickod_d(_tmp1_m, _tmp5_m);                                    \
+    _out5 = __lasx_xvpickod_d(_tmp2_m, _tmp6_m);                                    \
+    _out7 = __lasx_xvpickod_d(_tmp3_m, _tmp7_m);                                    \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_4
+ *               _out0 = _in0 + _in3;
+ *               _out1 = _in1 + _in2;
+ *               _out2 = _in1 - _in2;
+ *               _out3 = _in0 - _in3;
+ * =============================================================================
+ */
+#define LASX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_b(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_b(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_b(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_b(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_h(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_h(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_h(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_h(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_w(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_w(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_w(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_w(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_d(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_d(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_d(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_d(_in0, _in3);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_8
+ *               _out0 = _in0 + _in7;
+ *               _out1 = _in1 + _in6;
+ *               _out2 = _in2 + _in5;
+ *               _out3 = _in3 + _in4;
+ *               _out4 = _in3 - _in4;
+ *               _out5 = _in2 - _in5;
+ *               _out6 = _in1 - _in6;
+ *               _out7 = _in0 - _in7;
+ * =============================================================================
+ */
+#define LASX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_b(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_b(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_b(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_b(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_b(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_b(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_b(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_b(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_h(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_h(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_h(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_h(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_h(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_h(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_h(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_h(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_w(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_w(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_w(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_w(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_w(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_w(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_w(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_w(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_d(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_d(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_d(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_d(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_d(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_d(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_d(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_d(_in0, _in7);                                           \
+}
+
+/*
+ ****************************************************************************
+ ***************  Non-generic macro definition ******************************
+ ****************************************************************************
+ */
+
+/* Description : Horizontal addition of 8 signed word elements of input vector
+ * Arguments   : Input  - in       (signed word vector)
+ *               Output - sum_m    (s32 sum)
+ * Details     : 8 signed word elements of 'in' vector are added together and
+ *               the resulting integer sum is returned
+ */
+#define LASX_HADD_SW_S32( in )                               \
+( {                                                          \
+    int32_t s_sum_m;                                         \
+    v4i64  out;                                              \
+                                                             \
+    out = __lasx_xvhaddw_d_w( in, in );                      \
+    s_sum_m = out[0] + out[1] + out[2] + out[3];             \
+    s_sum_m;                                                 \
+} )
+
+/* Description : Horizontal addition of 16 half word elements of input vector
+ * Arguments   : Input  - in       (half word vector)
+ *               Output - sum_m    (i32 sum)
+ * Details     : 16 half word elements of 'in' vector are added together and
+ *               the resulting integer sum is returned
+ */
+#define LASX_HADD_UH_U32( in )                               \
+( {                                                          \
+    uint32_t u_sum_m;                                        \
+    v4u64  out;                                              \
+    __m256i res_m;                                           \
+                                                             \
+    res_m = __lasx_xvhaddw_wu_hu( in, in );                  \
+    out = ( v4u64 )__lasx_xvhaddw_du_wu( res_m, res_m );     \
+    u_sum_m = out[0] + out[1] + out[2] + out[3];             \
+    u_sum_m;                                                 \
+} )
+
+/* Description : Sign extend halfword elements from input vector and return
+ *               the result in pair of vectors
+ * Arguments   : Input  - in            (halfword vector)
+ *               Outputs - out0, out1   (sign extended word vectors)
+ *               Return Type - signed word
+ * Details     : Sign bit of halfword elements from input vector 'in' is
+ *               extracted and interleaved right with same vector 'in0' to
+ *               generate 4 signed word elements in 'out0'
+ *               Then interleaved left with same vector 'in0' to
+ *               generate 4 signed word elements in 'out1'
+ */
+#define LASX_UNPCK_SH( in, out0, out1 )         \
+{                                               \
+    __m256i tmp_m;                              \
+                                                \
+    tmp_m = __lasx_xvslti_h( in, 0 );           \
+    out0 = __lasx_xvilvl_h( tmp_m, in );        \
+    out1 = __lasx_xvilvh_h( tmp_m, in );        \
+}
+
+/*
+ * Description : Transpose 8x4 block with half word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Example     : LASX_TRANSPOSE8x4_H
+ *        _in0 : 1,2,3,4, 5,6,7,8, 2,2,2,2, 2,2,2,2
+ *        _in1 : 8,2,3,4, 5,6,7,8, 3,3,3,3, 3,3,3,3
+ *        _in2 : 8,2,3,4, 5,6,7,8, 4,4,4,4, 4,4,4,4
+ *        _in3 : 1,2,3,4, 5,6,7,8, 0,0,0,0, 0,0,0,0
+ *
+ *       _out0 : 1,8,8,1, 2,2,2,2, 2,3,4,0, 2,3,4,0
+ *       _out1 : 3,3,3,3, 4,4,4,4, 2,3,4,0, 2,3,4,0
+ *       _out2 : 5,5,5,5, 6,6,6,6, 2,3,4,0, 2,3,4,0
+ *       _out3 : 7,7,7,7, 8,8,8,8, 2,3,4,0, 2,3,4,0
+ */
+#define LASX_TRANSPOSE8X4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                               \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                     \
+                                                                                \
+    tmp0_m = __lasx_xvilvl_h(_in1, _in0);                                       \
+    tmp1_m = __lasx_xvilvl_h(_in3, _in2);                                       \
+    tmp2_m = __lasx_xvilvh_h(_in1, _in0);                                       \
+    tmp3_m = __lasx_xvilvh_h(_in3, _in2);                                       \
+    _out0 = __lasx_xvilvl_w(tmp1_m, tmp0_m);                                    \
+    _out2 = __lasx_xvilvl_w(tmp3_m, tmp2_m);                                    \
+    _out1 = __lasx_xvilvh_w(tmp1_m, tmp0_m);                                    \
+    _out3 = __lasx_xvilvh_w(tmp3_m, tmp2_m);                                    \
+}
+
+#endif //LASX
+
+#endif /* LOONGSON_INTRINSICS_H */
+
diff --git a/common/loongarch/mc-c.c b/common/loongarch/mc-c.c
index ba04c97b..d442efcd 100644
--- a/common/loongarch/mc-c.c
+++ b/common/loongarch/mc-c.c
@@ -25,7 +25,7 @@
  *****************************************************************************/
 
 #include "common/common.h"
-#include "generic_macros_lasx.h"
+#include "loongson_intrinsics.h"
 #include "mc.h"
 
 #if !HIGH_BIT_DEPTH
@@ -771,13 +771,14 @@ static void avc_biwgt_opscale_4x2_nw_lasx( uint8_t *p_src1,
     src1 = __lasx_xvldrepl_w( p_src2, 0 );
     src0 = __lasx_xvpackev_w( src1, src0 );
 
-    LASX_ILVL_B_128SV( src0, src2, src0);
+    src0 = __lasx_xvilvl_b( src0, src2 );
 
-    LASX_DP2_H_BU( src0, wgt, src0 );
+    src0 = __lasx_xvdp2_h_bu( src0, wgt );
     src0 = __lasx_xvmaxi_h( src0, 0 );
     src0 = __lasx_xvssrln_bu_h(src0, denom);
 
-    LASX_ST_W_2( src0, 0, 1, p_dst, i_dst_stride );
+    __lasx_xvstelm_w(src0, p_dst, 0, 0);
+    __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
 }
 
 static void avc_biwgt_opscale_4x4multiple_nw_lasx( uint8_t *p_src1,
@@ -795,7 +796,9 @@ static void avc_biwgt_opscale_4x4multiple_nw_lasx( uint8_t *p_src1,
     __m256i src1_wgt, src2_wgt, wgt;
     __m256i src0, src1, src2, src3, tmp0;
     __m256i denom;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
     int32_t i_dst_stride_x4 = i_dst_stride << 2;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
 
     src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
     src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
@@ -829,13 +832,16 @@ static void avc_biwgt_opscale_4x4multiple_nw_lasx( uint8_t *p_src1,
         src1 = __lasx_xvpackev_w( src3, src2 );
         src0 = __lasx_xvpermi_q( src0, src1, 0x02 );
 
-        LASX_ILVL_B_128SV( src0, tmp0, src0);
+        src0 = __lasx_xvilvl_b( src0, tmp0 );
 
-        LASX_DP2_H_BU( src0, wgt, src0 );
+        src0 = __lasx_xvdp2_h_bu( src0, wgt );
         src0 = __lasx_xvmaxi_h( src0, 0 );
         src0 = __lasx_xvssrln_bu_h(src0, denom);
 
-        LASX_ST_W_4( src0, 0, 1, 4, 5, p_dst, i_dst_stride );
+        __lasx_xvstelm_w(src0, p_dst, 0, 0);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x2, 0, 4);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x3, 0, 5);
         p_dst += i_dst_stride_x4;
     }
 }
@@ -893,27 +899,28 @@ static void avc_biwgt_opscale_8width_nw_lasx( uint8_t *p_src1,
 
     wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
 
-#define BIWGT_OPSCALE_8W_NW                          \
-    src0 = __lasx_xvldrepl_d( p_src1, 0 );           \
-    p_src1 += i_src1_stride;                         \
-    src1 = __lasx_xvldrepl_d( p_src1, 0 );           \
-    p_src1 += i_src1_stride;                         \
-                                                     \
-    src2 = __lasx_xvldrepl_d( p_src2, 0 );           \
-    p_src2 += i_src2_stride;                         \
-    src3 = __lasx_xvldrepl_d( p_src2, 0 );           \
-    p_src2 += i_src2_stride;                         \
-                                                     \
-    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );     \
-    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );     \
-    LASX_ILVL_B_128SV( src1, src0, src0);            \
-                                                     \
-    LASX_DP2_H_BU( src0, wgt, src0 );                \
-    src0 = __lasx_xvmaxi_h( src0, 0 );               \
-    src0 = __lasx_xvssrln_bu_h(src0, denom);         \
-                                                     \
-    LASX_ST_D_2( src0, 0, 2, p_dst, i_dst_stride );  \
-    p_dst += i_dst_stride_x2;
+#define BIWGT_OPSCALE_8W_NW                              \
+    src0 = __lasx_xvldrepl_d( p_src1, 0 );               \
+    p_src1 += i_src1_stride;                             \
+    src1 = __lasx_xvldrepl_d( p_src1, 0 );               \
+    p_src1 += i_src1_stride;                             \
+                                                         \
+    src2 = __lasx_xvldrepl_d( p_src2, 0 );               \
+    p_src2 += i_src2_stride;                             \
+    src3 = __lasx_xvldrepl_d( p_src2, 0 );               \
+    p_src2 += i_src2_stride;                             \
+                                                         \
+    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );         \
+    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );         \
+    src0 = __lasx_xvilvl_b( src1, src0 );                \
+                                                         \
+    src0 = __lasx_xvdp2_h_bu( src0, wgt );               \
+    src0 = __lasx_xvmaxi_h( src0, 0 );                   \
+    src0 = __lasx_xvssrln_bu_h(src0, denom);             \
+                                                         \
+    __lasx_xvstelm_d(src0, p_dst, 0, 0);                 \
+    __lasx_xvstelm_d(src0, p_dst + i_dst_stride, 0, 2);  \
+    p_dst += i_dst_stride_x2;                            \
 
     for( u_cnt = ( i_height >> 2 ); u_cnt--; )
     {
@@ -939,8 +946,12 @@ static void avc_biwgt_opscale_16width_nw_lasx( uint8_t *p_src1,
     __m256i src1_wgt, src2_wgt, wgt;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i denom;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
     int32_t i_src1_stride_x4 = i_src1_stride << 2;
+    int32_t i_src2_stride_x2 = i_src2_stride << 1;
     int32_t i_src2_stride_x4 = i_src2_stride << 2;
+    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
+    int32_t i_src2_stride_x3 = i_src2_stride_x2 + i_src2_stride;
 
     src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
     src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
@@ -951,22 +962,24 @@ static void avc_biwgt_opscale_16width_nw_lasx( uint8_t *p_src1,
 #define BIWGT_OPSCALE_16W_NW( srcA, srcB )       \
     srcA = __lasx_xvpermi_d( srcA, 0x50 );       \
     srcB = __lasx_xvpermi_d( srcB, 0x50 );       \
-    LASX_ILVL_B_128SV( srcB, srcA, srcA);        \
+    srcA = __lasx_xvilvl_b( srcB, srcA );        \
                                                  \
-    LASX_DP2_H_B( srcA, wgt, srcA );             \
+    srcA = __lasx_xvdp2_h_b( srcA, wgt );        \
     srcA = __lasx_xvmaxi_h( srcA, 0 );           \
     srcA = __lasx_xvssrln_bu_h(srcA, denom);     \
                                                  \
-    LASX_ST_D( srcA, 0, p_dst );                 \
-    LASX_ST_D( srcA, 2, ( p_dst + 8 ) );         \
+    __lasx_xvstelm_d(srcA, p_dst, 0, 0);         \
+    __lasx_xvstelm_d(srcA, p_dst + 8, 0, 2);     \
     p_dst += i_dst_stride;
 
     for( u_cnt = ( i_height >> 2 ); u_cnt--; )
     {
-        LASX_LD_4( p_src1, i_src1_stride, src0, src1, src2, src3 );
+        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                   i_src1_stride_x2, p_src1, i_src1_stride_x3, src0, src1, src2, src3 );
         p_src1 += i_src1_stride_x4;
 
-        LASX_LD_4( p_src2, i_src2_stride, src4, src5, src6, src7 );
+        DUP4_ARG2( __lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, p_src2,
+                   i_src2_stride_x2, p_src2, i_src2_stride_x3, src4, src5, src6, src7 );
         p_src2 += i_src2_stride_x4;
 
         BIWGT_OPSCALE_16W_NW( src0, src4 );
@@ -1012,14 +1025,15 @@ static void avc_biwgt_opscale_4x2_lasx( uint8_t *p_src1,
     src1 = __lasx_xvldrepl_w( p_src2, 0 );
     src0 = __lasx_xvpackev_w( src1, src0 );
 
-    LASX_ILVL_B_128SV( src0, src2, src0);
+    src0 = __lasx_xvilvl_b( src0, src2 );
 
-    LASX_DP2_H_BU( src0, wgt, src0 );
+    src0 = __lasx_xvdp2_h_bu( src0, wgt );
     src0 = __lasx_xvsadd_h( src0, offset );
     src0 = __lasx_xvmaxi_h( src0, 0 );
     src0 = __lasx_xvssrln_bu_h(src0, denom);
 
-    LASX_ST_W_2( src0, 0, 1, p_dst, i_dst_stride );
+    __lasx_xvstelm_w(src0, p_dst, 0, 0);
+    __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
 }
 
 static void avc_biwgt_opscale_4x4multiple_lasx( uint8_t *p_src1,
@@ -1038,7 +1052,9 @@ static void avc_biwgt_opscale_4x4multiple_lasx( uint8_t *p_src1,
     __m256i src1_wgt, src2_wgt, wgt;
     __m256i src0, src1, src2, src3, tmp0;
     __m256i denom, offset;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
     int32_t i_dst_stride_x4 = i_dst_stride << 2;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
 
     i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
 
@@ -1075,14 +1091,17 @@ static void avc_biwgt_opscale_4x4multiple_lasx( uint8_t *p_src1,
         src1 = __lasx_xvpackev_w( src3, src2 );
         src0 = __lasx_xvpermi_q( src0, src1, 0x02 );
 
-        LASX_ILVL_B_128SV( src0, tmp0, src0);
+        src0 = __lasx_xvilvl_b( src0, tmp0 );
 
-        LASX_DP2_H_BU( src0, wgt, src0 );
+        src0 = __lasx_xvdp2_h_bu( src0, wgt );
         src0 = __lasx_xvsadd_h( src0, offset );
         src0 = __lasx_xvmaxi_h( src0, 0 );
         src0 = __lasx_xvssrln_bu_h(src0, denom);
 
-        LASX_ST_W_4( src0, 0, 1, 4, 5, p_dst, i_dst_stride );
+        __lasx_xvstelm_w(src0, p_dst, 0, 0);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x2, 0, 4);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x3, 0, 5);
         p_dst += i_dst_stride_x4;
     }
 }
@@ -1145,27 +1164,28 @@ static void avc_biwgt_opscale_8width_lasx( uint8_t *p_src1,
 
     wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
 
-#define BIWGT_OPSCALE_8W                             \
-    src0 = __lasx_xvldrepl_d( p_src1, 0 );           \
-    p_src1 += i_src1_stride;                         \
-    src1 = __lasx_xvldrepl_d( p_src1, 0 );           \
-    p_src1 += i_src1_stride;                         \
-                                                     \
-    src2 = __lasx_xvldrepl_d( p_src2, 0 );           \
-    p_src2 += i_src2_stride;                         \
-    src3 = __lasx_xvldrepl_d( p_src2, 0 );           \
-    p_src2 += i_src2_stride;                         \
-                                                     \
-    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );     \
-    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );     \
-    LASX_ILVL_B_128SV( src1, src0, src0);            \
-                                                     \
-    LASX_DP2_H_BU( src0, wgt, src0 );                \
-    src0 = __lasx_xvsadd_h( src0, offset );          \
-    src0 = __lasx_xvmaxi_h( src0, 0 );               \
-    src0 = __lasx_xvssrln_bu_h(src0, denom);         \
-                                                     \
-    LASX_ST_D_2( src0, 0, 2, p_dst, i_dst_stride );  \
+#define BIWGT_OPSCALE_8W                                 \
+    src0 = __lasx_xvldrepl_d( p_src1, 0 );               \
+    p_src1 += i_src1_stride;                             \
+    src1 = __lasx_xvldrepl_d( p_src1, 0 );               \
+    p_src1 += i_src1_stride;                             \
+                                                         \
+    src2 = __lasx_xvldrepl_d( p_src2, 0 );               \
+    p_src2 += i_src2_stride;                             \
+    src3 = __lasx_xvldrepl_d( p_src2, 0 );               \
+    p_src2 += i_src2_stride;                             \
+                                                         \
+    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );         \
+    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );         \
+    src0 = __lasx_xvilvl_b( src1, src0 );                \
+                                                         \
+    src0 = __lasx_xvdp2_h_bu( src0, wgt );               \
+    src0 = __lasx_xvsadd_h( src0, offset );              \
+    src0 = __lasx_xvmaxi_h( src0, 0 );                   \
+    src0 = __lasx_xvssrln_bu_h(src0, denom);             \
+                                                         \
+    __lasx_xvstelm_d(src0, p_dst, 0, 0);                 \
+    __lasx_xvstelm_d(src0, p_dst + i_dst_stride, 0, 2);  \
     p_dst += i_dst_stride_x2;
 
     for( u_cnt = ( i_height >> 2 ); u_cnt--; )
@@ -1194,8 +1214,12 @@ static void avc_biwgt_opscale_16width_lasx( uint8_t *p_src1,
     __m256i src1_wgt, src2_wgt, wgt;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i denom, offset;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
     int32_t i_src1_stride_x4 = i_src1_stride << 2;
+    int32_t i_src2_stride_x2 = i_src2_stride << 1;
     int32_t i_src2_stride_x4 = i_src2_stride << 2;
+    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
+    int32_t i_src2_stride_x3 = i_src2_stride_x2 + i_src2_stride;
 
     i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
 
@@ -1209,23 +1233,25 @@ static void avc_biwgt_opscale_16width_lasx( uint8_t *p_src1,
 #define BIWGT_OPSCALE_16W( srcA, srcB )          \
     srcA = __lasx_xvpermi_d( srcA, 0x50 );       \
     srcB = __lasx_xvpermi_d( srcB, 0x50 );       \
-    LASX_ILVL_B_128SV( srcB, srcA, srcA);        \
+    srcA = __lasx_xvilvl_b( srcB, srcA );        \
                                                  \
-    LASX_DP2_H_BU( srcA, wgt, srcA );            \
+    srcA = __lasx_xvdp2_h_bu( srcA, wgt );       \
     srcA = __lasx_xvsadd_h( srcA, offset );      \
     srcA = __lasx_xvmaxi_h( srcA, 0 );           \
     srcA = __lasx_xvssrln_bu_h(srcA, denom);     \
                                                  \
-    LASX_ST_D( srcA, 0, p_dst );                 \
-    LASX_ST_D( srcA, 2, ( p_dst + 8 ) );         \
+    __lasx_xvstelm_d(srcA, p_dst, 0, 0);         \
+    __lasx_xvstelm_d(srcA, p_dst + 8, 0, 2);     \
     p_dst += i_dst_stride;
 
     for( u_cnt = ( i_height >> 2 ); u_cnt--; )
     {
-        LASX_LD_4( p_src1, i_src1_stride, src0, src1, src2, src3 );
+        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                   i_src1_stride_x2, p_src1, i_src1_stride_x3, src0, src1, src2, src3 );
         p_src1 += i_src1_stride_x4;
 
-        LASX_LD_4( p_src2, i_src2_stride, src4, src5, src6, src7 );
+        DUP4_ARG2( __lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, p_src2,
+                   i_src2_stride_x2, p_src2, i_src2_stride_x3, src4, src5, src6, src7 );
         p_src2 += i_src2_stride_x4;
 
         BIWGT_OPSCALE_16W( src0, src4 );
@@ -1251,15 +1277,15 @@ static void avg_src_width4_lasx( uint8_t *p_src1, int32_t i_src1_stride,
 
     for( i_cnt = ( i_height >> 1 ); i_cnt--; )
     {
-        LASX_LD_2( p_src1, i_src1_stride, src0, src1);
+        DUP2_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, src0, src1);
         p_src1 += i_src1_stride_x2;
-        LASX_LD_2( p_src2, i_src2_stride, dst0, dst1);
+        DUP2_ARG2(__lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, dst0, dst1);
         p_src2 += i_src2_stride_x2;
 
-        LASX_AVER_BU_2( src0, dst0, src1, dst1, dst0, dst1 );
-        LASX_ST_W( dst0, 0, p_dst );
+        DUP2_ARG2( __lasx_xvavgr_bu, src0, dst0, src1, dst1, dst0, dst1 );
+        __lasx_xvstelm_w( dst0, p_dst, 0, 0 );
         p_dst += i_dst_stride;
-        LASX_ST_W( dst1, 0, p_dst );
+        __lasx_xvstelm_w( dst1, p_dst, 0, 0 );
         p_dst += i_dst_stride;
     }
 }
@@ -1307,15 +1333,15 @@ static void avg_src_width8_lasx( uint8_t *p_src1, int32_t i_src1_stride,
     "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
     "bnez      %[cnt],           1b                                     \n\t"
     "2:                                                                 \n\t"
-     : [src1]"+&r"( (uint8_t *) p_src1 ),
-       [src2]"+&r"( (uint8_t *) p_src2 ),
+     : [src1]"+&r"(p_src1),
+       [src2]"+&r"(p_src2),
        [src1_stride2]"=&r"(i_src1_stride_x2),
        [src1_stride3]"=&r"(i_src1_stride_x3),
        [src1_stride4]"=&r"(i_src1_stride_x4),
        [src2_stride2]"=&r"(i_src2_stride_x2),
        [src2_stride3]"=&r"(i_src2_stride_x3),
        [src2_stride4]"=&r"(i_src2_stride_x4),
-       [dst]"+&r"( (uint8_t *) p_dst ), [cnt]"+&r"(i_cnt)
+       [dst]"+&r"(p_dst), [cnt]"+&r"(i_cnt)
      : [src1_stride1]"r"(i_src1_stride),
        [src2_stride1]"r"(i_src2_stride),
        [dst_stride1]"r"(i_dst_stride)
@@ -1388,8 +1414,8 @@ static void avg_src_width16_lasx( uint8_t *p_src1, int32_t i_src1_stride,
 
     "bnez      %[cnt],           1b                                     \n\t"
     "2:                                                                 \n\t"
-     : [src1]"+&r"( (uint8_t *) p_src1 ),
-       [src2]"+&r"( (uint8_t *) p_src2 ),
+     : [src1]"+&r"(p_src1),
+       [src2]"+&r"(p_src2),
        [src1_stride2]"=&r"(i_src1_stride_x2),
        [src1_stride3]"=&r"(i_src1_stride_x3),
        [src1_stride4]"=&r"(i_src1_stride_x4),
@@ -1399,7 +1425,7 @@ static void avg_src_width16_lasx( uint8_t *p_src1, int32_t i_src1_stride,
        [dst_stride2]"=&r"(i_dst_stride_x2),
        [dst_stride3]"=&r"(i_dst_stride_x3),
        [dst_stride4]"=&r"(i_dst_stride_x4),
-       [dst]"+&r"( (uint8_t *) p_dst ), [cnt]"+&r"(i_cnt)
+       [dst]"+&r"(p_dst), [cnt]"+&r"(i_cnt)
      : [src1_stride1]"r"(i_src1_stride),
        [src2_stride1]"r"(i_src2_stride),
        [dst_stride1]"r"(i_dst_stride)
@@ -1439,7 +1465,7 @@ static void *x264_memcpy_aligned_lasx(void *dst, const void *src, size_t n)
     "addi.d    %[dst],          %[dst],            64                   \n\t"
     "blt       %[zero],         %[n],              4b                   \n\t"
     "5:                                                                 \n\t"
-    : [dst]"+&r"((void *) dst), [src]"+&r"((void *) src), [n]"+&r"((int64_t) n),
+    : [dst]"+&r"(dst), [src]"+&r"(src), [n]"+&r"(n),
       [d]"=&r"(d)
     : [zero]"r"(zero)
     : "memory"
@@ -1698,14 +1724,14 @@ static inline void avg_src_width16_no_align_lasx( uint8_t *p_src1,
 
     for( i_cnt = i_height; i_cnt--; )
     {
-        src0 = LASX_LD( p_src1 );
+        src0 = __lasx_xvld( p_src1, 0 );
         p_src1 += i_src1_stride;
-        src1 = LASX_LD( p_src2 );
+        src1 = __lasx_xvld( p_src2, 0 );
         p_src2 += i_src2_stride;
 
         src0 = __lasx_xvavgr_bu( src0, src1 );
-        LASX_ST_D( src0, 0, p_dst );
-        LASX_ST_D( src0, 1, ( p_dst + 8 ) );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 0, 1 );
         p_dst += i_dst_stride;
     }
 }
@@ -1723,15 +1749,15 @@ static inline void avg_src_width20_no_align_lasx( uint8_t *p_src1,
 
     for( i_cnt = i_height; i_cnt--; )
     {
-        src0 = LASX_LD( p_src1 );
+        src0 = __lasx_xvld( p_src1, 0 );
         p_src1 += i_src1_stride;
-        src1 = LASX_LD( p_src2 );
+        src1 = __lasx_xvld( p_src2, 0 );
         p_src2 += i_src2_stride;
 
         src0 = __lasx_xvavgr_bu( src0, src1 );
-        LASX_ST_D( src0, 0, p_dst );
-        LASX_ST_D( src0, 1, ( p_dst + 8 ) );
-        LASX_ST_W( src0, 4, ( p_dst + 16 ) );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 8, 1 );
+        __lasx_xvstelm_w( src0, p_dst, 16, 4 );
         p_dst += i_dst_stride;
     }
 }
@@ -1749,14 +1775,14 @@ static inline void avg_src_width12_no_align_lasx( uint8_t *p_src1,
 
     for( i_cnt = i_height; i_cnt--; )
     {
-        src0 = LASX_LD( p_src1 );
+        src0 = __lasx_xvld( p_src1, 0 );
         p_src1 += i_src1_stride;
-        src1 = LASX_LD( p_src2 );
+        src1 = __lasx_xvld( p_src2, 0 );
         p_src2 += i_src2_stride;
 
         src0 = __lasx_xvavgr_bu( src0, src1 );
-        LASX_ST_D( src0, 0, p_dst );
-        LASX_ST_W( src0, 2, ( p_dst + 8 ) );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_w( src0, p_dst, 8, 2 );
         p_dst += i_dst_stride;
     }
 }
@@ -1774,13 +1800,13 @@ static inline void avg_src_width8_no_align_lasx( uint8_t *p_src1,
 
     for( i_cnt = i_height; i_cnt--; )
     {
-        src0 = LASX_LD( p_src1 );
+        src0 = __lasx_xvld( p_src1, 0 );
         p_src1 += i_src1_stride;
-        src1 = LASX_LD( p_src2 );
+        src1 = __lasx_xvld( p_src2, 0 );
         p_src2 += i_src2_stride;
 
         src0 = __lasx_xvavgr_bu( src0, src1 );
-        LASX_ST_D( src0, 0, p_dst );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
         p_dst += i_dst_stride;
     }
 }
@@ -1798,13 +1824,13 @@ static inline void avg_src_width4_no_align_lasx( uint8_t *p_src1,
 
     for( i_cnt = i_height; i_cnt--; )
     {
-        src0 = LASX_LD( p_src1 );
+        src0 = __lasx_xvld( p_src1, 0 );
         p_src1 += i_src1_stride;
-        src1 = LASX_LD( p_src2 );
+        src1 = __lasx_xvld( p_src2, 0 );
         p_src2 += i_src2_stride;
 
         src0 = __lasx_xvavgr_bu( src0, src1 );
-        LASX_ST_W( src0, 0, p_dst );
+        __lasx_xvstelm_w( src0, p_dst, 0, 0 );
         p_dst += i_dst_stride;
     }
 }
@@ -1837,19 +1863,19 @@ static inline void mc_weight_w16_no_align_lasx( uint8_t *p_dst,
 
     for( u_cnt = i_height; u_cnt--; )
     {
-        src = LASX_LD( p_src);
+        src = __lasx_xvld( p_src, 0 );
         p_src += i_src_stride;
 
         src = __lasx_xvpermi_d( src, 0x50 );
-        LASX_ILVL_B_128SV( zero, src, src);
+        src = __lasx_xvilvl_b( zero, src );
 
         src = __lasx_xvmul_h( src, wgt );
         src = __lasx_xvsadd_h( src, offset );
         src = __lasx_xvmaxi_h( src, 0 );
         src = __lasx_xvssrln_bu_h(src, denom);
 
-        LASX_ST_D( src, 0, p_dst );
-        LASX_ST_D( src, 2, ( p_dst + 8 ) );
+        __lasx_xvstelm_d( src, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src, p_dst, 8, 2 );
         p_dst += i_dst_stride;
     }
 }
@@ -1885,14 +1911,14 @@ static inline void mc_weight_w8_no_align_lasx( uint8_t *p_dst,
         src = __lasx_xvldrepl_d( p_src, 0 );
         p_src += i_src_stride;
 
-        LASX_ILVL_B_128SV( zero, src, src);
+        src = __lasx_xvilvl_b( zero, src );
 
         src = __lasx_xvmul_h( src, wgt );
         src = __lasx_xvsadd_h( src, offset );
         src = __lasx_xvmaxi_h( src, 0 );
         src = __lasx_xvssrln_bu_h(src, denom);
 
-        LASX_ST_D( src, 0, p_dst );
+        __lasx_xvstelm_d( src, p_dst, 0, 0 );
         p_dst += i_dst_stride;
     }
 }
@@ -1928,14 +1954,14 @@ static inline void mc_weight_w4_no_align_lasx( uint8_t *p_dst,
         src = __lasx_xvldrepl_w( p_src, 0 );
         p_src += i_src_stride;
 
-        LASX_ILVL_B_128SV( zero, src, src);
+        src = __lasx_xvilvl_b( zero, src );
 
         src = __lasx_xvmul_h( src, wgt );
         src = __lasx_xvsadd_h( src, offset );
         src = __lasx_xvmaxi_h( src, 0 );
         src = __lasx_xvssrln_bu_h(src, denom);
 
-        LASX_ST_W( src, 0, p_dst );
+        __lasx_xvstelm_w( src, p_dst, 0, 0 );
         p_dst += i_dst_stride;
     }
 }
@@ -2227,29 +2253,32 @@ static uint8_t *get_ref_lasx( uint8_t *p_dst, intptr_t *p_dst_stride,
     int32_t i_qpel_idx;
     int32_t i_offset;
     uint8_t *p_src1;
+    int32_t r_vy = m_vy & 3;
+    int32_t r_vx = m_vx & 3;
+    int32_t width = i_width >> 2;
 
-    i_qpel_idx = ( ( m_vy & 3 ) << 2 ) + ( m_vx & 3 );
+    i_qpel_idx = ( r_vy << 2 ) + r_vx;
     i_offset = ( m_vy >> 2 ) * i_src_stride + ( m_vx >> 2 );
     p_src1 = p_src[x264_hpel_ref0[i_qpel_idx]] + i_offset +
-           ( 3 == ( m_vy & 3 ) ) * i_src_stride;
+           ( 3 == r_vy ) * i_src_stride;
 
     if( i_qpel_idx & 5 )
     {
         uint8_t *p_src2 = p_src[x264_hpel_ref1[i_qpel_idx]] +
-                          i_offset + ( 3 == ( m_vx & 3 ) );
-        pixel_avg_wtab_lasx[i_width >> 2](
+                          i_offset + ( 3 == r_vx );
+        pixel_avg_wtab_lasx[width](
                 p_dst, *p_dst_stride, p_src1, i_src_stride,
                 p_src2, i_height );
 
         if( pWeight->weightfn )
         {
-            pWeight->weightfn[i_width>>2](p_dst, *p_dst_stride, p_dst, *p_dst_stride, pWeight, i_height);
+            pWeight->weightfn[width](p_dst, *p_dst_stride, p_dst, *p_dst_stride, pWeight, i_height);
         }
         return p_dst;
     }
     else if ( pWeight->weightfn )
     {
-        pWeight->weightfn[i_width>>2]( p_dst, *p_dst_stride, p_src1, i_src_stride, pWeight, i_height );
+        pWeight->weightfn[width]( p_dst, *p_dst_stride, p_src1, i_src_stride, pWeight, i_height );
         return p_dst;
     }
     else
@@ -2278,26 +2307,26 @@ static void avc_interleaved_chroma_hv_2x2_lasx( uint8_t *p_src,
     __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
     __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
 
-    mask = LASX_LD( &pu_chroma_mask_arr[0] );
+    mask = __lasx_xvld(pu_chroma_mask_arr, 0);
     mask1 = __lasx_xvaddi_bu(mask, 1);
 
-    src0 = LASX_LD( p_src );
-    p_src += i_src_stride;
-
-    LASX_LD_2( p_src, i_src_stride, src1, src2 );
+    src0 = __lasx_xvld( p_src, 0);
+    src1 = __lasx_xvldx( p_src, i_src_stride);
+    src2 = __lasx_xvldx( p_src, (i_src_stride << 1));
 
-    LASX_SHUF_B_2_128SV( src1, src0, src2, src1, mask1, mask1, src3, src4 );
-    LASX_SHUF_B_2_128SV( src1, src0, src2, src1, mask, mask, src0, src1 );
+    DUP2_ARG3( __lasx_xvshuf_b, src1, src0, mask1, src2, src1, mask1, src3, src4 );
+    DUP2_ARG3( __lasx_xvshuf_b, src1, src0, mask, src2, src1, mask, src0, src1 );
 
     src0 = __lasx_xvpermi_q( src0, src3, 0x02 );
     src1 = __lasx_xvpermi_q( src1, src4, 0x02 );
-    LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+    DUP2_ARG2( __lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
     src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-    src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
-    src0 = __lasx_xvadd_h( src0, src1 );
+    src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
     src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-    LASX_ST_H_2( src0, 0, 2, p_dst_u, i_dst_stride );
-    LASX_ST_H_2( src0, 8, 10, p_dst_v, i_dst_stride );
+    __lasx_xvstelm_h( src0, p_dst_u, 0, 0 );
+    __lasx_xvstelm_h( src0, p_dst_u + i_dst_stride, 0, 2 );
+    __lasx_xvstelm_h( src0, p_dst_v, 0, 8 );
+    __lasx_xvstelm_h( src0, p_dst_v + i_dst_stride, 0, 10 );
 }
 
 static void avc_interleaved_chroma_hv_2x4_lasx( uint8_t *p_src,
@@ -2310,6 +2339,12 @@ static void avc_interleaved_chroma_hv_2x4_lasx( uint8_t *p_src,
                                                 uint32_t u_coef_ver0,
                                                 uint32_t u_coef_ver1 )
 {
+    int32_t src_stride2 = i_src_stride << 1;
+    int32_t src_stride3 = i_src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = i_dst_stride << 1;
+    int32_t dst_stride3 = i_dst_stride + dst_stride2;
+    int32_t dst_stride4 = dst_stride2 << 1;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m256i mask, mask1;
 
@@ -2319,37 +2354,43 @@ static void avc_interleaved_chroma_hv_2x4_lasx( uint8_t *p_src,
     __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
     __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
 
-    mask = LASX_LD( &pu_chroma_mask_arr[0] );
+    mask = __lasx_xvld( pu_chroma_mask_arr, 0);
     mask1 = __lasx_xvaddi_bu(mask, 1);
 
-    src0 = LASX_LD( p_src );
-    p_src += i_src_stride;
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvldx( p_src, i_src_stride);
+    src2 = __lasx_xvldx( p_src, src_stride2);
+    src3 = __lasx_xvldx( p_src, src_stride3);
+    src4 = __lasx_xvldx( p_src, src_stride4);
 
-    LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
-
-    LASX_SHUF_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
-                         mask1, mask1, mask1, mask1,
-                         src5, src6, src7, src8 );
-    LASX_SHUF_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
-                         mask, mask, mask, mask, src0, src1, src2, src3 );
+    DUP4_ARG3( __lasx_xvshuf_b, src1, src0, mask1, src2, src1, mask1, src3, src2,
+               mask1, src4, src3, mask1, src5, src6, src7, src8 );
+    DUP4_ARG3( __lasx_xvshuf_b, src1, src0, mask, src2, src1, mask, src3, src2, mask,
+               src4, src3, mask, src0, src1, src2, src3 );
 
     src0 = __lasx_xvpermi_q( src0, src2, 0x02 );
     src1 = __lasx_xvpermi_q( src1, src3, 0x02 );
-    LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
     src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-    src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
-    src0 = __lasx_xvadd_h( src0, src1 );
+    src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
     src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-    LASX_ST_H_4( src0, 0, 2, 8, 10, p_dst_u, i_dst_stride );
+    __lasx_xvstelm_h(src0, p_dst_u, 0, 0);
+    __lasx_xvstelm_h(src0, p_dst_u + i_dst_stride, 0, 2);
+    __lasx_xvstelm_h(src0, p_dst_u + dst_stride2, 0, 8);
+    __lasx_xvstelm_h(src0, p_dst_u + dst_stride3, 0, 10);
+    p_dst_u += dst_stride4;
 
     src0 = __lasx_xvpermi_q( src5, src7, 0x02 );
     src1 = __lasx_xvpermi_q( src6, src8, 0x02 );
-    LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+    DUP2_ARG2(__lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
     src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-    src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
-    src0 = __lasx_xvadd_h( src0, src1 );
+    src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
     src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-    LASX_ST_H_4( src0, 0, 2, 8, 10, p_dst_v, i_dst_stride );
+    __lasx_xvstelm_h(src0, p_dst_v, 0, 0);
+    __lasx_xvstelm_h(src0, p_dst_v + i_dst_stride, 0, 2);
+    __lasx_xvstelm_h(src0, p_dst_v + dst_stride2, 0, 8);
+    __lasx_xvstelm_h(src0, p_dst_v + dst_stride3, 0, 10);
+    p_dst_v += dst_stride4;
 }
 
 static void avc_interleaved_chroma_hv_2w_lasx( uint8_t *p_src,
@@ -2398,26 +2439,26 @@ static void avc_interleaved_chroma_hv_4x2_lasx( uint8_t *p_src,
     __m256i coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
     __m256i coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
 
-    mask = LASX_LD( &pu_chroma_mask_arr[0] );
+    mask = __lasx_xvld( pu_chroma_mask_arr, 0 );
     mask1 = __lasx_xvaddi_bu(mask, 1);
 
-    src0 = LASX_LD( p_src );
-    p_src += i_src_stride;
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvldx( p_src, i_src_stride);
+    src2 = __lasx_xvldx( p_src, (i_src_stride << 1));
 
-    LASX_LD_2( p_src, i_src_stride, src1, src2 );
-
-    LASX_SHUF_B_2_128SV( src1, src0, src2, src1, mask1, mask1, src3, src4 );
-    LASX_SHUF_B_2_128SV( src1, src0, src2, src1, mask, mask, src0, src1 );
+    DUP2_ARG3( __lasx_xvshuf_b, src1, src0, mask1, src2, src1, mask1, src3, src4 );
+    DUP2_ARG3( __lasx_xvshuf_b, src1, src0, mask, src2, src1, mask, src0, src1 );
 
     src0 = __lasx_xvpermi_q( src0, src3, 0x02 );
     src1 = __lasx_xvpermi_q( src1, src4, 0x02 );
-    LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+    DUP2_ARG2( __lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
     src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-    src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
-    src0 = __lasx_xvadd_h( src0, src1 );
+    src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
     src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-    LASX_ST_W_2( src0, 0, 1, p_dst_u, i_dst_stride );
-    LASX_ST_W_2( src0, 4, 5, p_dst_v, i_dst_stride );
+    __lasx_xvstelm_w( src0, p_dst_u, 0, 0 );
+    __lasx_xvstelm_w( src0, p_dst_u + i_dst_stride, 0, 1 );
+    __lasx_xvstelm_w( src0, p_dst_v, 0, 4 );
+    __lasx_xvstelm_w( src0, p_dst_v + i_dst_stride, 0,  5 );
 }
 
 static void avc_interleaved_chroma_hv_4x4mul_lasx( uint8_t *p_src,
@@ -2434,8 +2475,12 @@ static void avc_interleaved_chroma_hv_4x4mul_lasx( uint8_t *p_src,
     uint32_t u_row;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m256i mask, mask1;
-    int32_t i_src_stride_x4 = i_src_stride << 2;
-    int32_t i_dst_stride_x4 = i_dst_stride << 2;
+    int32_t src_stride2 = i_src_stride << 1;
+    int32_t dst_stride2 = i_dst_stride << 1;
+    int32_t src_stride3 = i_src_stride + src_stride2;
+    int32_t dst_stride3 = i_dst_stride + dst_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride4 = dst_stride2 << 1;
     __m256i coeff_hz_vec0, coeff_hz_vec1;
     __m256i coeff_hz_vec;
     __m256i coeff_vt_vec0, coeff_vt_vec1;
@@ -2446,42 +2491,47 @@ static void avc_interleaved_chroma_hv_4x4mul_lasx( uint8_t *p_src,
     coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
     coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
 
-    mask = LASX_LD( &pu_chroma_mask_arr[0] );
+    mask = __lasx_xvld( pu_chroma_mask_arr, 0 );
     mask1 = __lasx_xvaddi_bu(mask, 1);
 
-    src0 = LASX_LD( p_src );
-    p_src += i_src_stride;
+    src0 = __lasx_xvld( p_src, 0 );
 
     for( u_row = ( i_height >> 2 ); u_row--; )
     {
-        LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
-        p_src += i_src_stride_x4;
+        src1 = __lasx_xvldx(p_src, i_src_stride);
+        src2 = __lasx_xvldx(p_src, src_stride2);
+        src3 = __lasx_xvldx(p_src, src_stride3);
+        src4 = __lasx_xvldx(p_src, src_stride4);
+        p_src += src_stride4;
 
-        LASX_SHUF_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
-                             mask1, mask1, mask1, mask1,
-                             src5, src6, src7, src8 );
-        LASX_SHUF_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
-                             mask, mask, mask, mask, src0, src1, src2, src3 );
+        DUP4_ARG3( __lasx_xvshuf_b, src1, src0, mask1, src2, src1, mask1, src3, src2,
+                   mask1, src4, src3, mask1, src5, src6, src7, src8 );
+        DUP4_ARG3( __lasx_xvshuf_b, src1, src0, mask, src2, src1, mask, src3, src2,
+                   mask, src4, src3, mask, src0, src1, src2, src3 );
 
         src0 = __lasx_xvpermi_q( src0, src2, 0x02 );
         src1 = __lasx_xvpermi_q( src1, src3, 0x02 );
-        LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+        DUP2_ARG2( __lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
         src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-        src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
-        src0 = __lasx_xvadd_h( src0, src1 );
+        src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
         src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-        LASX_ST_W_4( src0, 0, 1, 4, 5, p_dst_u, i_dst_stride );
-        p_dst_u += i_dst_stride_x4;
+        __lasx_xvstelm_w(src0, p_dst_u, 0, 0);
+        __lasx_xvstelm_w(src0, p_dst_u + i_dst_stride, 0, 1);
+        __lasx_xvstelm_w(src0, p_dst_u + dst_stride2, 0, 4);
+        __lasx_xvstelm_w(src0, p_dst_u + dst_stride3, 0, 5);
+        p_dst_u += dst_stride4;
 
         src0 = __lasx_xvpermi_q( src5, src7, 0x02 );
         src1 = __lasx_xvpermi_q( src6, src8, 0x02 );
-        LASX_DP2_H_BU_2( src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
+        DUP2_ARG2( __lasx_xvdp2_h_bu, src0, coeff_hz_vec, src1, coeff_hz_vec, src0, src1);
         src0 = __lasx_xvmul_h( src0, coeff_vt_vec1 );
-        src1 = __lasx_xvmul_h( src1, coeff_vt_vec0 );
-        src0 = __lasx_xvadd_h( src0, src1 );
+        src0 = __lasx_xvmadd_h( src0, src1, coeff_vt_vec0 );
         src0 = __lasx_xvssrlrni_bu_h(src0, src0, 6);
-        LASX_ST_W_4( src0, 0, 1, 4, 5, p_dst_v, i_dst_stride );
-        p_dst_v += i_dst_stride_x4;
+        __lasx_xvstelm_w(src0, p_dst_v, 0, 0);
+        __lasx_xvstelm_w(src0, p_dst_v + i_dst_stride, 0, 1);
+        __lasx_xvstelm_w(src0, p_dst_v + dst_stride2, 0, 4);
+        __lasx_xvstelm_w(src0, p_dst_v + dst_stride3, 0, 5);
+        p_dst_v += dst_stride4;
 
         src0 = src4;
     }
@@ -2534,8 +2584,13 @@ static void avc_interleaved_chroma_hv_8w_lasx( uint8_t *p_src,
     __m256i coeff_vt_vec0, coeff_vt_vec1;
     __m256i tmp0, tmp1, tmp2, tmp3;
     __m256i head_u, head_v;
-    int32_t i_src_stride_x4 = i_src_stride << 2;
-    int32_t i_dst_stride_x4 = i_dst_stride << 2;
+
+    int32_t src_stride2 = i_src_stride << 1;
+    int32_t dst_stride2 = i_dst_stride << 1;
+    int32_t src_stride3 = i_src_stride + src_stride2;
+    int32_t dst_stride3 = i_dst_stride + dst_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride4 = dst_stride2 << 1;
 
     coeff_hz_vec0 = __lasx_xvreplgr2vr_b( u_coef_hor0 );
     coeff_hz_vec1 = __lasx_xvreplgr2vr_b( u_coef_hor1 );
@@ -2543,58 +2598,63 @@ static void avc_interleaved_chroma_hv_8w_lasx( uint8_t *p_src,
     coeff_vt_vec0 = __lasx_xvreplgr2vr_h( u_coef_ver0 );
     coeff_vt_vec1 = __lasx_xvreplgr2vr_h( u_coef_ver1 );
 
-    mask = LASX_LD( &pu_chroma_mask_arr1[0] );
+    mask = __lasx_xvld( pu_chroma_mask_arr1, 0 );
     mask1 = __lasx_xvaddi_bu(mask, 1);
 
-    src0 = LASX_LD( p_src );
-    p_src += i_src_stride;
+    src0 = __lasx_xvld( p_src, 0 );
     tmp0 = __lasx_xvpermi_q( src0, src0, 0x11 );
-    LASX_SHUF_B_2_128SV( tmp0, src0, tmp0, src0, mask, mask1, head_u, head_v );
-    LASX_DP2_H_BU_2( head_u, coeff_hz_vec, head_v,
-                     coeff_hz_vec, head_u, head_v );
+    DUP2_ARG3( __lasx_xvshuf_b, tmp0, src0, mask, tmp0, src0, mask1, head_u, head_v );
+    DUP2_ARG2( __lasx_xvdp2_h_bu, head_u, coeff_hz_vec, head_v, coeff_hz_vec,
+               head_u, head_v );
 
     for( u_row = ( i_height >> 2 ); u_row--; )
     {
-        LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
-        p_src += i_src_stride_x4;
+        src1 = __lasx_xvldx(p_src, i_src_stride);
+        src2 = __lasx_xvldx(p_src, src_stride2);
+        src3 = __lasx_xvldx(p_src, src_stride3);
+        src4 = __lasx_xvldx(p_src, src_stride4);
+        p_src += src_stride4;
         src5 = __lasx_xvpermi_q( src1, src2, 0x02 );
         src6 = __lasx_xvpermi_q( src1, src2, 0x13 );
         src7 = __lasx_xvpermi_q( src3, src4, 0x02 );
         src8 = __lasx_xvpermi_q( src3, src4, 0x13 );
 
-        LASX_SHUF_B_2_128SV( src6, src5, src8, src7, mask, mask, tmp0, tmp1 );
-        LASX_DP2_H_BU_2( tmp0, coeff_hz_vec, tmp1, coeff_hz_vec, tmp0, tmp1);
+        DUP2_ARG3( __lasx_xvshuf_b, src6, src5, mask, src8, src7, mask, tmp0, tmp1 );
+        DUP2_ARG2( __lasx_xvdp2_h_bu, tmp0, coeff_hz_vec, tmp1, coeff_hz_vec, tmp0, tmp1);
         tmp2 = __lasx_xvpermi_q( head_u, tmp0, 0x02 );
         tmp3 = __lasx_xvpermi_q( tmp0, tmp1, 0x03 );
         head_u = __lasx_xvpermi_q( tmp1, tmp1, 0x11 );
 
         tmp0 = __lasx_xvmul_h( tmp0, coeff_vt_vec0 );
         tmp1 = __lasx_xvmul_h( tmp1, coeff_vt_vec0 );
-        tmp2 = __lasx_xvmul_h( tmp2, coeff_vt_vec1 );
-        tmp3 = __lasx_xvmul_h( tmp3, coeff_vt_vec1 );
-        tmp0 = __lasx_xvadd_h( tmp0, tmp2 );
-        tmp1 = __lasx_xvadd_h( tmp1, tmp3 );
+        tmp0 = __lasx_xvmadd_h( tmp0, tmp2, coeff_vt_vec1 );
+        tmp1 = __lasx_xvmadd_h( tmp1, tmp3, coeff_vt_vec1 );
 
         tmp0 = __lasx_xvssrlrni_bu_h(tmp1, tmp0, 6);
-        LASX_ST_D_4( tmp0, 0, 2, 1, 3, p_dst_u, i_dst_stride );
-        p_dst_u += i_dst_stride_x4;
 
-        LASX_SHUF_B_2_128SV( src6, src5, src8, src7, mask1, mask1, tmp0, tmp1 );
-        LASX_DP2_H_BU_2( tmp0, coeff_hz_vec, tmp1, coeff_hz_vec, tmp0, tmp1);
+        __lasx_xvstelm_d(tmp0, p_dst_u, 0, 0);
+        __lasx_xvstelm_d(tmp0, p_dst_u + i_dst_stride, 0, 2);
+        __lasx_xvstelm_d(tmp0, p_dst_u + dst_stride2, 0, 1);
+        __lasx_xvstelm_d(tmp0, p_dst_u + dst_stride3, 0, 3);
+        p_dst_u += dst_stride4;
+
+        DUP2_ARG3( __lasx_xvshuf_b, src6, src5, mask1, src8, src7, mask1, tmp0, tmp1 );
+        DUP2_ARG2( __lasx_xvdp2_h_bu, tmp0, coeff_hz_vec, tmp1, coeff_hz_vec, tmp0, tmp1);
         tmp2 = __lasx_xvpermi_q( head_v, tmp0, 0x02 );
         tmp3 = __lasx_xvpermi_q( tmp0, tmp1, 0x03 );
         head_v = __lasx_xvpermi_q( tmp1, tmp1, 0x11 );
 
         tmp0 = __lasx_xvmul_h( tmp0, coeff_vt_vec0 );
         tmp1 = __lasx_xvmul_h( tmp1, coeff_vt_vec0 );
-        tmp2 = __lasx_xvmul_h( tmp2, coeff_vt_vec1 );
-        tmp3 = __lasx_xvmul_h( tmp3, coeff_vt_vec1 );
-        tmp0 = __lasx_xvadd_h( tmp0, tmp2 );
-        tmp1 = __lasx_xvadd_h( tmp1, tmp3 );
+        tmp0 = __lasx_xvmadd_h( tmp0, tmp2, coeff_vt_vec1 );
+        tmp1 = __lasx_xvmadd_h( tmp1, tmp3, coeff_vt_vec1 );
 
         tmp0 = __lasx_xvssrlrni_bu_h(tmp1, tmp0, 6);
-        LASX_ST_D_4( tmp0, 0, 2, 1, 3, p_dst_v, i_dst_stride );
-        p_dst_v += i_dst_stride_x4;
+        __lasx_xvstelm_d(tmp0, p_dst_v, 0, 0);
+        __lasx_xvstelm_d(tmp0, p_dst_v + i_dst_stride, 0, 2);
+        __lasx_xvstelm_d(tmp0, p_dst_v + dst_stride2, 0, 1);
+        __lasx_xvstelm_d(tmp0, p_dst_v + dst_stride3, 0, 3);
+        p_dst_v += dst_stride4;
     }
 }
 
@@ -2653,9 +2713,9 @@ static void copy_width4_lasx( uint8_t *p_src, int32_t i_src_stride,
         src1 = __lasx_xvldrepl_w( p_src, 0 );
         p_src += i_src_stride;
 
-        LASX_ST_W( src0, 0, p_dst );
+        __lasx_xvstelm_w( src0, p_dst, 0, 0 );
         p_dst += i_dst_stride;
-        LASX_ST_W( src1, 0, p_dst );
+        __lasx_xvstelm_w( src1, p_dst, 0, 0 );
         p_dst += i_dst_stride;
     }
 }
@@ -2677,13 +2737,13 @@ static void copy_width8_lasx( uint8_t *p_src, int32_t i_src_stride,
     src3 = __lasx_xvldrepl_d( p_src, 0 );           \
     p_src += i_src_stride;                          \
                                                     \
-    LASX_ST_D( src0, 0, p_dst );                    \
+    __lasx_xvstelm_d( src0, p_dst, 0, 0 );          \
     p_dst += i_dst_stride;                          \
-    LASX_ST_D( src1, 0, p_dst );                    \
+    __lasx_xvstelm_d( src1, p_dst, 0, 0 );          \
     p_dst += i_dst_stride;                          \
-    LASX_ST_D( src2, 0, p_dst );                    \
+    __lasx_xvstelm_d( src2, p_dst, 0, 0 );          \
     p_dst += i_dst_stride;                          \
-    LASX_ST_D( src3, 0, p_dst );                    \
+    __lasx_xvstelm_d( src3, p_dst, 0, 0 );          \
     p_dst += i_dst_stride;
 
     if( 0 == i_height % 12 )
@@ -2723,26 +2783,26 @@ static void copy_width16_lasx( uint8_t *p_src, int32_t i_src_stride,
     __m256i src0, src1, src2, src3;
 
 #define COPY_W16_H4                                 \
-    src0 = LASX_LD(p_src);                          \
+    src0 = __lasx_xvld(p_src, 0);                   \
     p_src += i_src_stride;                          \
-    src1 = LASX_LD(p_src);                          \
+    src1 = __lasx_xvld(p_src, 0);                   \
     p_src += i_src_stride;                          \
-    src2 = LASX_LD(p_src);                          \
+    src2 = __lasx_xvld(p_src, 0);                   \
     p_src += i_src_stride;                          \
-    src3 = LASX_LD(p_src);                          \
+    src3 = __lasx_xvld(p_src, 0);                   \
     p_src += i_src_stride;                          \
                                                     \
-    LASX_ST_D( src0, 0, p_dst );                    \
-    LASX_ST_D( src0, 1, ( p_dst + 8 ) );            \
+    __lasx_xvstelm_d( src0, p_dst, 0, 0 );          \
+    __lasx_xvstelm_d( src0, p_dst, 8, 1 );          \
     p_dst += i_dst_stride;                          \
-    LASX_ST_D( src1, 0, p_dst );                    \
-    LASX_ST_D( src1, 1, ( p_dst + 8 ) );            \
+    __lasx_xvstelm_d( src1, p_dst, 0, 0 );          \
+    __lasx_xvstelm_d( src1, p_dst, 8, 1 );          \
     p_dst += i_dst_stride;                          \
-    LASX_ST_D( src2, 0, p_dst );                    \
-    LASX_ST_D( src2, 1, ( p_dst + 8 ) );            \
+    __lasx_xvstelm_d( src2, p_dst, 0, 0 );          \
+    __lasx_xvstelm_d( src2, p_dst, 8, 1 );          \
     p_dst += i_dst_stride;                          \
-    LASX_ST_D( src3, 0, p_dst );                    \
-    LASX_ST_D( src3, 1, ( p_dst + 8 ) );            \
+    __lasx_xvstelm_d( src3, p_dst, 0, 0 );          \
+    __lasx_xvstelm_d( src3, p_dst, 8, 1 );          \
     p_dst += i_dst_stride;
 
     if( 0 == i_height % 12 )
@@ -2865,25 +2925,34 @@ static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
     __m256i res10_h, res32_h, res10_l, res32_l;
     __m256i tmp10_h, tmp32_h, tmp10_l, tmp32_l;
     __m256i filt0, filt1, filt2;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
     int32_t i_src_stride_x4 = i_src_stride << 2;
     int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
 
     u_h4w = i_height % 4;
     filt0 = __lasx_xvreplgr2vr_h( i_filt_const0 );
     filt1 = __lasx_xvreplgr2vr_h( i_filt_const1 );
     filt2 = __lasx_xvreplgr2vr_h( i_filt_const2 );
 
-    src0 = LASX_LD( p_src );
+    src0 = __lasx_xvld( p_src, 0 );
     p_src += i_src_stride;
-    LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
+               p_src, i_src_stride_x3, src1, src2, src3, src4 );
     p_src += i_src_stride_x4;
 
-    LASX_XORI_B_128( src0 );
-    LASX_XORI_B_4_128( src1, src2, src3, src4 );
-
-    LASX_ILVLH_B_4_128SV( src1, src0, src2, src1, src3, src2, src4, src3,
-                          src10_h, src10_l, src21_h, src21_l,
-                          src32_h, src32_l, src43_h, src43_l );
+    src0 = __lasx_xvxori_b( src0, 128 );
+    DUP4_ARG2( __lasx_xvxori_b, src1, 128, src2, 128, src3, 128, src4, 128,
+               src1, src2, src3, src4 );
+
+    src10_l = __lasx_xvilvl_b( src1, src0 );
+    src10_h = __lasx_xvilvh_b( src1, src0 );
+    src21_l = __lasx_xvilvl_b( src2, src1 );
+    src21_h = __lasx_xvilvh_b( src2, src1 );
+    src32_l = __lasx_xvilvl_b( src3, src2 );
+    src32_h = __lasx_xvilvh_b( src3, src2 );
+    src43_l = __lasx_xvilvl_b( src4, src3 );
+    src43_h = __lasx_xvilvh_b( src4, src3 );
     res10_h = __lasx_xvpermi_q( src21_h, src10_h, 0x20 );
     res32_h = __lasx_xvpermi_q( src43_h, src32_h, 0x20 );
     res10_l = __lasx_xvpermi_q( src21_l, src10_l, 0x20 );
@@ -2891,34 +2960,55 @@ static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
 
     for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
     {
-        LASX_LD_4( p_src, i_src_stride, src5, src6, src7, src8 );
+        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
+                   p_src, i_src_stride_x3, src5, src6, src7, src8 );
         p_src += i_src_stride_x4;
 
-        LASX_XORI_B_4_128( src5, src6, src7, src8 );
-        LASX_ILVLH_B_4_128SV( src5, src4, src6, src5, src7, src6, src8, src7,
-                              src54_h, src54_l, src65_h, src65_l,
-                              src76_h, src76_l, src87_h, src87_l );
+        DUP4_ARG2( __lasx_xvxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5,
+                   src6, src7, src8 );
+        src54_l = __lasx_xvilvl_b( src5, src4 );
+        src54_h = __lasx_xvilvh_b( src5, src4 );
+        src65_l = __lasx_xvilvl_b( src6, src5 );
+        src65_h = __lasx_xvilvh_b( src6, src5 );
+        src76_l = __lasx_xvilvl_b( src7, src6 );
+        src76_h = __lasx_xvilvh_b( src7, src6 );
+        src87_l = __lasx_xvilvl_b( src8, src7 );
+        src87_h = __lasx_xvilvh_b( src8, src7 );
         tmp10_h = __lasx_xvpermi_q( src65_h, src54_h, 0x20 );
         tmp32_h = __lasx_xvpermi_q( src87_h, src76_h, 0x20 );
         tmp10_l = __lasx_xvpermi_q( src65_l, src54_l, 0x20 );
         tmp32_l = __lasx_xvpermi_q( src87_l, src76_l, 0x20 );
 
-        out10_h = LASX_DPADD_SH_3( res10_h, res32_h, tmp10_h,
-                                   filt0, filt1, filt2 );
-        out32_h = LASX_DPADD_SH_3( res32_h, tmp10_h, tmp32_h,
-                                   filt0, filt1, filt2 );
-        out10_l = LASX_DPADD_SH_3( res10_l, res32_l, tmp10_l,
-                                   filt0, filt1, filt2 );
-        out32_l = LASX_DPADD_SH_3( res32_l, tmp10_l, tmp32_l,
-                                   filt0, filt1, filt2 );
+        out10_h = __lasx_xvdp2_h_b( res10_h, filt0 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, res32_h, filt1 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, tmp10_h, filt2 );
+
+
+        out32_h = __lasx_xvdp2_h_b( res32_h, filt0 );
+        out32_h = __lasx_xvdp2add_h_b( out32_h, tmp10_h, filt1 );
+        out32_h = __lasx_xvdp2add_h_b( out32_h, tmp32_h, filt2 );
+
+        out10_l = __lasx_xvdp2_h_b( res10_l, filt0 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, res32_l, filt1 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, tmp10_l, filt2 );
+
+        out32_l = __lasx_xvdp2_h_b( res32_l, filt0 );
+        out32_l = __lasx_xvdp2add_h_b( out32_l, tmp10_l, filt1 );
+        out32_l = __lasx_xvdp2add_h_b( out32_l, tmp32_l, filt2 );
 
         out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
         out32_l = __lasx_xvssrarni_b_h(out32_h, out32_l, 5);
-        LASX_XORI_B_2_128( out10_l, out32_l );
+        DUP2_ARG2( __lasx_xvxori_b, out10_l, 128, out32_l, 128, out10_l, out32_l );
 
-        LASX_ST_Q_2(out10_l, 0, 1, p_dst, i_dst_stride );
+        __lasx_xvstelm_d( out10_l, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out10_l, p_dst, 8, 1 );
+        __lasx_xvstelm_d( out10_l, p_dst + i_dst_stride, 0, 2 );
+        __lasx_xvstelm_d( out10_l, p_dst + i_dst_stride, 8, 3 );
         p_dst += i_dst_stride_x2;
-        LASX_ST_Q_2(out32_l, 0, 1, p_dst, i_dst_stride );
+        __lasx_xvstelm_d( out32_l, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out32_l, p_dst, 8, 1 );
+        __lasx_xvstelm_d( out32_l, p_dst + i_dst_stride, 0, 2 );
+        __lasx_xvstelm_d( out32_l, p_dst + i_dst_stride, 8, 3 );
         p_dst += i_dst_stride_x2;
 
         res10_h = tmp10_h;
@@ -2930,17 +3020,22 @@ static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
 
     for( u_loop_cnt = u_h4w; u_loop_cnt--; )
     {
-        src5 = LASX_LD( p_src );
+        src5 = __lasx_xvld( p_src, 0 );
         p_src += i_src_stride;
-        LASX_XORI_B_128( src5 );
-        LASX_ILVLH_B_128SV( src5, src4, src54_h, src54_l );
-        out10_h = LASX_DPADD_SH_3( src10_h, src32_h, src54_h,
-                                   filt0, filt1, filt2 );
-        out10_l = LASX_DPADD_SH_3( src10_l, src32_l, src54_l,
-                                  filt0, filt1, filt2 );
+        src5 = __lasx_xvxori_b( src5, 128 );
+        src54_h = __lasx_xvilvl_b( src5, src4 );
+        src54_l = __lasx_xvilvh_b( src5, src4 );
+        out10_h = __lasx_xvdp2_h_b( src10_h, filt0 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, src32_h, filt1 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, src54_h, filt2 );
+
+        out10_l = __lasx_xvdp2_h_b( src10_l, filt0 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, src32_l, filt1 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, src54_l, filt2 );
         out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
-        LASX_XORI_B_128( out10_l );
-        LASX_ST_Q( out10_l, 0, p_dst );
+        out10_l = __lasx_xvxori_b( out10_l, 128 );
+        __lasx_xvstelm_d( out10_l, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out10_l, p_dst, 8, 1 );
         p_dst += i_dst_stride;
 
         src10_h = src21_h;
@@ -2961,10 +3056,10 @@ static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
     out0_m = __lasx_xvhaddw_h_b( tmp0_m, tmp0_m );             \
                                                                \
     tmp0_m = __lasx_xvshuf_b(in, in, mask1);                   \
-    LASX_DP2ADD_H_B( out0_m, minus5b, tmp0_m, out0_m );        \
+    out0_m = __lasx_xvdp2add_h_b( out0_m, minus5b, tmp0_m );   \
                                                                \
     tmp1_m = __lasx_xvshuf_b(in, in, mask2);                   \
-    LASX_DP2ADD_H_B( out0_m, plus20b, tmp1_m, out0_m );        \
+    out0_m = __lasx_xvdp2add_h_b( out0_m, plus20b, tmp1_m );   \
                                                                \
     out0_m;                                                    \
 } )
@@ -2974,17 +3069,20 @@ static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
     __m256i tmp0_m, tmp1_m;                                                \
     __m256i out0_m, out1_m, out2_m, out3_m;                                \
                                                                            \
-    LASX_ILVLH_H_128SV( in5, in0, tmp0_m, tmp1_m );                        \
+    tmp0_m = __lasx_xvilvh_h( in5, in0 );                                  \
+    tmp1_m = __lasx_xvilvl_h( in5, in0 );                                  \
                                                                            \
     tmp0_m = __lasx_xvhaddw_w_h( tmp0_m, tmp0_m );                         \
     tmp1_m = __lasx_xvhaddw_w_h( tmp1_m, tmp1_m );                         \
                                                                            \
-    LASX_ILVLH_H_128SV( in1, in4, out0_m, out1_m );                        \
-    LASX_DP2ADD_W_H_2( tmp0_m, out0_m, minus5h, tmp1_m,                    \
-                       out1_m, minus5h, tmp0_m, tmp1_m );                  \
-    LASX_ILVLH_H_128SV( in2, in3, out2_m, out3_m );                        \
-    LASX_DP2ADD_W_H_2( tmp0_m, out2_m, plus20h, tmp1_m,                    \
-                       out3_m, plus20h, tmp0_m, tmp1_m );                  \
+    out0_m = __lasx_xvilvh_h( in1, in4 );                                  \
+    out1_m = __lasx_xvilvl_h( in1, in4 );                                  \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, tmp0_m, out0_m, minus5h, tmp1_m,       \
+               out1_m, minus5h, tmp0_m, tmp1_m );                          \
+    out2_m = __lasx_xvilvh_h( in2, in3 );                                  \
+    out3_m = __lasx_xvilvl_h( in2, in3 );                                  \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, tmp0_m, out2_m, plus20h, tmp1_m,       \
+               out3_m, plus20h, tmp0_m, tmp1_m );                          \
                                                                            \
     out0_m = __lasx_xvssrarni_h_w(tmp0_m, tmp1_m, 10);                     \
     out0_m = __lasx_xvsat_h(out0_m, 7);                                    \
@@ -3004,7 +3102,9 @@ static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
     __m256i dst0, dst1, dst2, dst3;
     __m256i out0, out1;
     __m256i minus5b, plus20b, minus5h, plus20h;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
     int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
 
     minus5b = __lasx_xvreplgr2vr_b( minus );
     plus20b = __lasx_xvreplgr2vr_b( plus );
@@ -3012,13 +3112,14 @@ static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
     plus20h = __lasx_xvreplgr2vr_h( plus );
 
     u_h4w = i_height & 3;
-    mask0 = LASX_LD( &pu_luma_mask_arr[0] );
-    mask1 = LASX_LD( &pu_luma_mask_arr[32] );
-    mask2 = LASX_LD( &pu_luma_mask_arr[64] );
+    mask0 = __lasx_xvld( pu_luma_mask_arr, 0 );
+    mask1 = __lasx_xvld( &pu_luma_mask_arr[32], 0 );
+    mask2 = __lasx_xvld( &pu_luma_mask_arr[64], 0 );
 
-    src0 = LASX_LD( p_src );
+    src0 = __lasx_xvld( p_src, 0 );
     p_src += i_src_stride;
-    LASX_LD_4( p_src, i_src_stride, src1, src2, src3, src4 );
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
+               p_src, i_src_stride_x3, src1, src2, src3, src4 );
     p_src += i_src_stride_x4;
     src0 = __lasx_xvpermi_d( src0, 0x94);
     src1 = __lasx_xvpermi_d( src1, 0x94);
@@ -3026,8 +3127,9 @@ static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
     src3 = __lasx_xvpermi_d( src3, 0x94);
     src4 = __lasx_xvpermi_d( src4, 0x94);
 
-    LASX_XORI_B_128( src0 );
-    LASX_XORI_B_4_128( src1, src2, src3, src4 );
+    src0 = __lasx_xvxori_b( src0, 128 );
+    DUP4_ARG2( __lasx_xvxori_b, src1, 128, src2, 128, src3, 128, src4, 128, src1, src2,
+               src3, src4 );
 
     src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
     src1 = LASX_HORZ_FILTER_SH( src1, mask0, mask1, mask2 );
@@ -3037,14 +3139,16 @@ static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
 
     for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
     {
-        LASX_LD_4( p_src, i_src_stride, src5, src6, src7, src8 );
+        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                   i_src_stride_x2, p_src, i_src_stride_x3, src5, src6, src7, src8 );
         p_src += i_src_stride_x4;
         src5 = __lasx_xvpermi_d( src5, 0x94);
         src6 = __lasx_xvpermi_d( src6, 0x94);
         src7 = __lasx_xvpermi_d( src7, 0x94);
         src8 = __lasx_xvpermi_d( src8, 0x94);
 
-        LASX_XORI_B_4_128( src5, src6, src7, src8 );
+        DUP4_ARG2( __lasx_xvxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
+                   src5, src6, src7, src8 );
 
         src5 = LASX_HORZ_FILTER_SH( src5, mask0, mask1, mask2 );
         src6 = LASX_HORZ_FILTER_SH( src6, mask0, mask1, mask2 );
@@ -3060,14 +3164,18 @@ static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
                                                  src6, src7, src8 );
         out0 = __lasx_xvpickev_b( dst1, dst0 );
         out1 = __lasx_xvpickev_b( dst3, dst2 );
-        LASX_XORI_B_2_128( out0, out1 );
-        LASX_ST_D_2( out0, 0, 2, p_dst, 8);
+        DUP2_ARG2( __lasx_xvxori_b, out0, 128, out1, 128, out0, out1 );
+        __lasx_xvstelm_d( out0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out0, p_dst, 8, 2 );
         p_dst += i_dst_stride;
-        LASX_ST_D_2( out0, 1, 3, p_dst, 8);
+        __lasx_xvstelm_d( out0, p_dst, 0, 1 );
+        __lasx_xvstelm_d( out0, p_dst, 8, 3 );
         p_dst += i_dst_stride;
-        LASX_ST_D_2( out1, 0, 2, p_dst, 8);
+        __lasx_xvstelm_d( out1, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out1, p_dst, 8, 2 );
         p_dst += i_dst_stride;
-        LASX_ST_D_2( out1, 1, 3, p_dst, 8);
+        __lasx_xvstelm_d( out1, p_dst, 0, 1 );
+        __lasx_xvstelm_d( out1, p_dst, 8, 3 );
         p_dst += i_dst_stride;
 
         src3 = src7;
@@ -3080,10 +3188,10 @@ static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
 
     for( u_loop_cnt = u_h4w; u_loop_cnt--; )
     {
-        src5 = LASX_LD( p_src );
+        src5 = __lasx_xvld( p_src, 0 );
         p_src += i_src_stride;
         src5 = __lasx_xvpermi_d( src5, 0x94);
-        LASX_XORI_B_128( src5 );
+        src5 = __lasx_xvxori_b( src5, 128 );
 
         src5 = LASX_HORZ_FILTER_SH( src5, mask0, mask1, mask2 );
         dst0 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src0, src1,
@@ -3091,8 +3199,9 @@ static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
                                                  src4, src5 );
 
         out0 = __lasx_xvpickev_b( dst0, dst0 );
-        LASX_XORI_B_128( out0 );
-        LASX_ST_D_2( out0, 0, 2, p_dst, 8);
+        out0 = __lasx_xvxori_b( out0, 128 );
+        __lasx_xvstelm_d( out0, p_dst, 0, 0);
+        __lasx_xvstelm_d( out0, p_dst, 8, 2);
         p_dst += i_dst_stride;
 
         src0 = src1;
@@ -3112,26 +3221,30 @@ static void avc_luma_hz_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
     __m256i src0, src1, src2, src3;
     __m256i mask0, mask1, mask2;
     __m256i minus5b, plus20b;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
     int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
 
     minus5b = __lasx_xvreplgr2vr_b( minus );
     plus20b = __lasx_xvreplgr2vr_b( plus );
 
     u_h4w = i_height & 3;
-    mask0 = LASX_LD( &pu_luma_mask_arr[0] );
-    mask1 = LASX_LD( &pu_luma_mask_arr[32] );
-    mask2 = LASX_LD( &pu_luma_mask_arr[64] );
+    mask0 = __lasx_xvld( pu_luma_mask_arr, 0 );
+    mask1 = __lasx_xvld( &pu_luma_mask_arr[32], 0 );
+    mask2 = __lasx_xvld( &pu_luma_mask_arr[64], 0 );
 
     for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
     {
-        LASX_LD_4( p_src, i_src_stride, src0, src1, src2, src3 );
+        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
+                   p_src, i_src_stride_x3, src0, src1, src2, src3 );
         p_src += i_src_stride_x4;
         src0 = __lasx_xvpermi_d( src0, 0x94);
         src1 = __lasx_xvpermi_d( src1, 0x94);
         src2 = __lasx_xvpermi_d( src2, 0x94);
         src3 = __lasx_xvpermi_d( src3, 0x94);
 
-        LASX_XORI_B_4_128( src0, src1, src2, src3 );
+        DUP4_ARG2( __lasx_xvxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                   src0, src1, src2, src3 );
 
         src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
         src1 = LASX_HORZ_FILTER_SH( src1, mask0, mask1, mask2 );
@@ -3140,28 +3253,33 @@ static void avc_luma_hz_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
 
         src0 = __lasx_xvssrarni_b_h(src1, src0, 5);
         src1 = __lasx_xvssrarni_b_h(src3, src2, 5);
-        LASX_XORI_B_2_128(src0, src1);
-        LASX_ST_D_2( src0, 0, 2, p_dst, 8);
+        DUP2_ARG2( __lasx_xvxori_b, src0, 128, src1, 128, src0, src1);
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 8, 2 );
         p_dst += i_dst_stride;
-        LASX_ST_D_2( src0, 1, 3, p_dst, 8);
+        __lasx_xvstelm_d( src0, p_dst, 0, 1);
+        __lasx_xvstelm_d( src0, p_dst, 8, 3);
         p_dst += i_dst_stride;
-        LASX_ST_D_2( src1, 0, 2, p_dst, 8);
+        __lasx_xvstelm_d( src1, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src1, p_dst, 8, 2 );
         p_dst += i_dst_stride;
-        LASX_ST_D_2( src1, 1, 3, p_dst, 8);
+        __lasx_xvstelm_d( src1, p_dst, 0, 1 );
+        __lasx_xvstelm_d( src1, p_dst, 8, 3 );
         p_dst += i_dst_stride;
     }
 
     for( u_loop_cnt = u_h4w; u_loop_cnt--; )
     {
-        src0 = LASX_LD( p_src );
+        src0 = __lasx_xvld( p_src, 0 );
         p_src += i_src_stride;
         src0 = __lasx_xvpermi_d( src0, 0x94);
 
-        LASX_XORI_B_128( src0 );
+        src0 = __lasx_xvxori_b( src0, 128 );
         src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
         src0 = __lasx_xvssrarni_b_h(src0, src0, 5);
-        LASX_XORI_B_128( src0 );
-        LASX_ST_D_2( src0, 0, 2, p_dst, 8);
+        src0 = __lasx_xvxori_b( src0, 128 );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 8, 2 );
         p_dst += i_dst_stride;
     }
 }
@@ -3197,6 +3315,7 @@ static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
                                               int32_t i_height )
 {
     int32_t i_loop_width, i_loop_height, i_w16_mul;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
     __m256i sld1_vec0, sld1_vec1, sld1_vec2, sld1_vec3, sld1_vec4, sld1_vec5;
     __m256i pckev_vec0, pckev_vec1, pckev_vec2;
@@ -3204,18 +3323,18 @@ static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
     __m256i tmp0, tmp1, tmp2, tmp3;
     __m256i mask;
 
-    mask = LASX_LD( &pu_core_mask_arr[0] );
+    mask = __lasx_xvld( pu_core_mask_arr, 0 );
 
     i_w16_mul = i_width - i_width % 16;
     for( i_loop_height = i_height; i_loop_height--; )
     {
-        src0  = LASX_LD( p_src );
-        LASX_LD_2( p_src + i_src_stride, i_src_stride, src1, src2 );
+        src0  = __lasx_xvld( p_src, 0 );
+        DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src1, src2 );
         p_src += 16;
         for( i_loop_width = 0; i_loop_width < ( i_w16_mul >> 4 ); i_loop_width++ )
         {
-            src3  = LASX_LD( p_src );
-            LASX_LD_2( p_src + i_src_stride, i_src_stride, src4, src5 );
+            src3  = __lasx_xvld( p_src, 0 );
+            DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src4, src5 );
             src6 = __lasx_xvpermi_q( src3, src3, 0x11 );
             src7 = __lasx_xvpermi_q( src4, src4, 0x11 );
             src8 = __lasx_xvpermi_q( src5, src5, 0x11 );
@@ -3227,28 +3346,32 @@ static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
             pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
             pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
             pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
-            LASX_AVER_BU_4( pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
-                            pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
-                            tmp0, tmp1, tmp2, tmp3 );
-            LASX_AVER_BU_2( tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
-            LASX_ST_Q( tmp0, 0, p_dst0 );
-            LASX_ST_Q( tmp1, 0, p_dst2 );
-
-            LASX_SHUF_B_2_128SV( src3, src0, src4, src1,
-                                 mask, mask, sld1_vec0, sld1_vec1 );
-            LASX_SHUF_B_2_128SV( src5, src2, src6, src3,
-                                 mask, mask, sld1_vec2, sld1_vec3 );
-            LASX_SHUF_B_2_128SV( src7, src4, src8, src5,
-                                 mask, mask, sld1_vec4, sld1_vec5 );
+            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
+                       tmp3 );
+            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            __lasx_xvstelm_d( tmp0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( tmp0, p_dst0, 8, 1 );
+            __lasx_xvstelm_d( tmp1, p_dst2, 0, 0 );
+            __lasx_xvstelm_d( tmp1, p_dst2, 8, 1 );
+
+            DUP2_ARG3( __lasx_xvshuf_b, src3, src0, mask, src4, src1,
+                       mask, sld1_vec0, sld1_vec1 );
+            DUP2_ARG3( __lasx_xvshuf_b, src5, src2, mask, src6, src3,
+                       mask, sld1_vec2, sld1_vec3 );
+            DUP2_ARG3( __lasx_xvshuf_b, src7, src4, mask, src8, src5,
+                       mask, sld1_vec4, sld1_vec5 );
             pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
             pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
             pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
-            LASX_AVER_BU_4( pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
-                            pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
-                            tmp0, tmp1, tmp2, tmp3 );
-            LASX_AVER_BU_2( tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
-            LASX_ST_Q( tmp0, 0, p_dst1 );
-            LASX_ST_Q( tmp1, 0, p_dst3 );
+            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
+                       tmp3 );
+            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            __lasx_xvstelm_d( tmp0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( tmp0, p_dst1, 8, 1 );
+            __lasx_xvstelm_d( tmp1, p_dst3, 0, 0 );
+            __lasx_xvstelm_d( tmp1, p_dst3, 8, 1 );
 
             src0 = src6;
             src1 = src7;
@@ -3262,8 +3385,8 @@ static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
         for( i_loop_width = i_w16_mul; i_loop_width < i_width;
              i_loop_width += 8 )
         {
-            src3  = LASX_LD( p_src );
-            LASX_LD_2( p_src + i_src_stride, i_src_stride, src4, src5 );
+            src3  = __lasx_xvld( p_src, 0 );
+            DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src4, src5 );
             p_src += 16;
 
             pckev_vec0 = __lasx_xvpickev_b( src3, src0 );
@@ -3272,28 +3395,28 @@ static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
             pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
             pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
             pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
-            LASX_AVER_BU_4( pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
-                            pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
-                            tmp0, tmp1, tmp2, tmp3 );
-            LASX_AVER_BU_2( tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
-            LASX_ST_D( tmp0, 0, p_dst0 );
-            LASX_ST_D( tmp1, 0, p_dst2 );
-
-            LASX_SHUF_B_2_128SV( src3, src0, src4, src1,
-                                 mask, mask, sld1_vec0, sld1_vec1 );
-            LASX_SHUF_B_2_128SV( src5, src2, src3, src3,
-                                 mask, mask, sld1_vec2, sld1_vec3 );
-            LASX_SHUF_B_2_128SV( src4, src4, src5, src5,
-                                 mask, mask, sld1_vec4, sld1_vec5 );
+            DUP4_ARG2( __lasx_xvhsubw_hu_bu, pckev_vec1, pckev_vec0, pckod_vec1,
+                       pckod_vec0, pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
+                       tmp0, tmp1, tmp2, tmp3 );
+            DUP2_ARG2( __lasx_xvhsubw_hu_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            __lasx_xvstelm_d( tmp0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( tmp1, p_dst2, 0, 0 );
+
+            DUP2_ARG3( __lasx_xvshuf_b, src3, src0, src4, src1, mask,
+                       mask, sld1_vec0, sld1_vec1 );
+            DUP2_ARG3( __lasx_xvshuf_b, src5, src2, src3, src3, mask,
+                       mask, sld1_vec2, sld1_vec3 );
+            DUP2_ARG3( __lasx_xvshuf_b, src4, src4, src5, src5, mask,
+                       mask, sld1_vec4, sld1_vec5 );
             pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
             pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
             pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
-            LASX_AVER_BU_4( pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
-                            pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
-                            tmp0, tmp1, tmp2, tmp3 );
-            LASX_AVER_BU_2( tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
-            LASX_ST_D( tmp0, 0, p_dst1 );
-            LASX_ST_D( tmp1, 0, p_dst3 );
+            DUP4_ARG2( __lasx_xvhsubw_hu_bu, pckev_vec1, pckev_vec0, pckod_vec1,
+                       pckod_vec0, pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
+                       tmp0, tmp1, tmp2, tmp3 );
+            DUP2_ARG2( __lasx_xvhsubw_hu_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            __lasx_xvstelm_d( tmp0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( tmp1, p_dst3, 0, 0 );
             p_dst0 += 8;
             p_dst1 += 8;
             p_dst2 += 8;
@@ -3335,8 +3458,24 @@ static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
     __m256i vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3;
     __m256i vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7;
     uint8_t *p_dst, *p_dstA, *p_dstB, *p_srcA;
-    int32_t i_dst0_stride_x2 = dst0_stride << 1;
-    int32_t i_dst1_stride_x2 = dst1_stride << 1;
+    int32_t dst0_stride_x2 = dst0_stride << 1;
+    int32_t dst0_stride_x3 = dst0_stride_x2 + dst0_stride;
+    int32_t dst0_stride_x4 = dst0_stride << 2;
+    int32_t dst0_stride_x5 = dst0_stride_x4 + dst0_stride;
+    int32_t dst0_stride_x6 = dst0_stride_x4 + dst0_stride_x2;
+    int32_t dst0_stride_x7 = dst0_stride_x4 + dst0_stride_x3;
+    int32_t dst1_stride_x2 = dst1_stride << 1;
+    int32_t dst1_stride_x3 = dst1_stride_x2 + dst1_stride;
+    int32_t dst1_stride_x4 = dst1_stride << 2;
+    int32_t dst1_stride_x5 = dst1_stride_x4 + dst1_stride;
+    int32_t dst1_stride_x6 = dst1_stride_x4 + dst1_stride_x2;
+    int32_t dst1_stride_x7 = dst1_stride_x4 + dst1_stride_x3;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_src_stride_x5 = i_src_stride_x4 + i_src_stride;
+    int32_t i_src_stride_x6 = i_src_stride_x4 + i_src_stride_x2;
+    int32_t i_src_stride_x7 = i_src_stride_x4 + i_src_stride_x3;
 
     i_w_mul32 = i_width - ( i_width & 31 );
     i_w_mul16 = i_width - ( i_width & 15 );
@@ -3347,21 +3486,25 @@ static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
     {
         for( i_loop_width = ( i_w_mul32 >> 5 ); i_loop_width--; )
         {
-            LASX_LD_8( p_src, i_src_stride,
-                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
             p_src += 32;
-            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-
-            LASX_LD_8( p_src, i_src_stride,
-                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
             p_src += 32;
-            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
-            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
 
             in0 = __lasx_xvpermi_q( vec_pckev0, vec_pckev4, 0x02 );
             in1 = __lasx_xvpermi_q( vec_pckev0, vec_pckev4, 0x13 );
@@ -3372,14 +3515,19 @@ static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
             in6 = __lasx_xvpermi_q( vec_pckev3, vec_pckev7, 0x02 );
             in7 = __lasx_xvpermi_q( vec_pckev3, vec_pckev7, 0x13 );
 
-            LASX_ILVL_D_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                 vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            LASX_ILVH_D_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                 vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
+            DUP4_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvilvh_d, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
 
-            LASX_ST_8( vec_pckev0, vec_pckev4, vec_pckev1, vec_pckev5,
-                       vec_pckev2, vec_pckev6, vec_pckev3, vec_pckev7,
-                       p_dst0, dst0_stride );
+            __lasx_xvst( vec_pckev0, p_dst0, 0 );
+            __lasx_xvstx( vec_pckev4, p_dst0, dst0_stride );
+            __lasx_xvstx( vec_pckev1, p_dst0, dst0_stride_x2 );
+            __lasx_xvstx( vec_pckev5, p_dst0, dst0_stride_x3 );
+            __lasx_xvstx( vec_pckev2, p_dst0, dst0_stride_x4 );
+            __lasx_xvstx( vec_pckev6, p_dst0, dst0_stride_x5 );
+            __lasx_xvstx( vec_pckev3, p_dst0, dst0_stride_x6 );
+            __lasx_xvstx( vec_pckev7, p_dst0, dst0_stride_x7 );
 
             in0 = __lasx_xvpermi_q( vec_pckod0, vec_pckod4, 0x02 );
             in1 = __lasx_xvpermi_q( vec_pckod0, vec_pckod4, 0x13 );
@@ -3390,14 +3538,19 @@ static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
             in6 = __lasx_xvpermi_q( vec_pckod3, vec_pckod7, 0x02 );
             in7 = __lasx_xvpermi_q( vec_pckod3, vec_pckod7, 0x13 );
 
-            LASX_ILVL_D_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                 vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-            LASX_ILVH_D_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                 vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
+            DUP4_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+            DUP4_ARG2( __lasx_xvilvh_d, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
 
-            LASX_ST_8( vec_pckod0, vec_pckod4, vec_pckod1, vec_pckod5,
-                       vec_pckod2, vec_pckod6, vec_pckod3, vec_pckod7,
-                       p_dst1, dst1_stride );
+            __lasx_xvst( vec_pckod0, p_dst1, 0 );
+            __lasx_xvstx( vec_pckod4, p_dst1, dst1_stride );
+            __lasx_xvstx( vec_pckod1, p_dst1, dst1_stride_x2 );
+            __lasx_xvstx( vec_pckod5, p_dst1, dst1_stride_x3 );
+            __lasx_xvstx( vec_pckod2, p_dst1, dst1_stride_x4 );
+            __lasx_xvstx( vec_pckod6, p_dst1, dst1_stride_x5 );
+            __lasx_xvstx( vec_pckod3, p_dst1, dst1_stride_x6 );
+            __lasx_xvstx( vec_pckod7, p_dst1, dst1_stride_x7 );
 
             p_dst0 += 32;
             p_dst1 += 32;
@@ -3405,45 +3558,63 @@ static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
 
         for( i_loop_width = ( ( i_width & 31 ) >> 4 ); i_loop_width--; )
         {
-            LASX_LD_8( p_src, i_src_stride,
-                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
             p_src += 32;
-            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
 
-            LASX_ST_D_2( vec_pckev0, 0, 2, p_dst0, 8 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 8, 2 );
             p_dst = p_dst0 + dst0_stride;
-            LASX_ST_D_2( vec_pckev0, 1, 3, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst, 8, 3 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckev1, 0, 2, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 8, 2 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckev1, 1, 3, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 8, 3 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckev2, 0, 2, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 8, 2 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckev2, 1, 3, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 8, 3 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckev3, 0, 2, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 8, 2 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckev3, 1, 3, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 8, 3 );
 
-            LASX_ST_D_2( vec_pckod0, 0, 2, p_dst1, 8 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 8, 2 );
             p_dst = p_dst1 + dst0_stride;
-            LASX_ST_D_2( vec_pckod0, 1, 3, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst, 8, 3 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckod1, 0, 2, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 8, 2 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckod1, 1, 3, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 8, 3 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckod2, 0, 2, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 8, 2 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckod2, 1, 3, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 8, 3 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckod3, 0, 2, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 8, 2 );
             p_dst = p_dst + dst0_stride;
-            LASX_ST_D_2( vec_pckod3, 1, 3, p_dst, 8 );
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 8, 3 );
 
             p_dst0 += 16;
             p_dst1 += 16;
@@ -3451,29 +3622,39 @@ static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
 
         for( i_loop_width = ( ( i_width & 15 ) >> 3 ); i_loop_width--; )
         {
-            LASX_LD_8( p_src, i_src_stride,
-                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
             p_src += 16;
-            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-
-            LASX_ST_D_2( vec_pckev0, 0, 1, p_dst0, dst0_stride );
-            p_dst = p_dst0 + i_dst0_stride_x2;
-            LASX_ST_D_2( vec_pckev1, 0, 1, p_dst, dst0_stride );
-            p_dst = p_dst + i_dst0_stride_x2;
-            LASX_ST_D_2( vec_pckev2, 0, 1, p_dst, dst0_stride );
-            p_dst = p_dst + i_dst0_stride_x2;
-            LASX_ST_D_2( vec_pckev3, 0, 1, p_dst, dst0_stride );
-
-            LASX_ST_D_2( vec_pckod0, 0, 1, p_dst1, dst0_stride );
-            p_dst = p_dst1 + i_dst1_stride_x2;
-            LASX_ST_D_2( vec_pckod1, 0, 1, p_dst, dst0_stride );
-            p_dst = p_dst + i_dst1_stride_x2;
-            LASX_ST_D_2( vec_pckod2, 0, 1, p_dst, dst0_stride );
-            p_dst = p_dst + i_dst1_stride_x2;
-            LASX_ST_D_2( vec_pckod3, 0, 1, p_dst, dst0_stride );
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0 + dst0_stride, 0, 1 );
+            p_dst = p_dst0 + dst0_stride_x2;
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev1, p_dst + dst0_stride, 0, 1 );
+            p_dst = p_dst + dst0_stride_x2;
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev2, p_dst + dst0_stride, 0, 1 );
+            p_dst = p_dst + dst0_stride_x2;
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev3, p_dst + dst0_stride, 0, 1 );
+
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1 + dst0_stride, 0, 1 );
+            p_dst = p_dst1 + dst1_stride_x2;
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod1, p_dst + dst0_stride, 0, 1 );
+            p_dst = p_dst + dst1_stride_x2;
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod2, p_dst + dst0_stride, 0, 1 );
+            p_dst = p_dst + dst1_stride_x2;
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod3, p_dst + dst0_stride, 0, 1 );
 
             p_dst0 += 8;
             p_dst1 += 8;
@@ -3482,29 +3663,39 @@ static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
 
         for( i_loop_width = ( ( i_width & 7 ) >> 2 ); i_loop_width--; )
         {
-            LASX_LD_8( p_src, i_src_stride,
-                       in0, in1, in2, in3, in4, in5, in6, in7 );
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
             p_src += 8;
-            LASX_PCKEV_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
-            LASX_PCKOD_B_4_128SV( in1, in0, in3, in2, in5, in4, in7, in6,
-                                  vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
-
-            LASX_ST_W_2( vec_pckev0, 0, 2, p_dst0, dst0_stride );
-            p_dst = p_dst0 + i_dst0_stride_x2;
-            LASX_ST_W_2( vec_pckev1, 0, 2, p_dst, dst0_stride );
-            p_dst = p_dst + i_dst0_stride_x2;
-            LASX_ST_W_2( vec_pckev2, 0, 2, p_dst, dst0_stride );
-            p_dst = p_dst + i_dst0_stride_x2;
-            LASX_ST_W_2( vec_pckev3, 0, 2, p_dst, dst0_stride );
-
-            LASX_ST_W_2( vec_pckod0, 0, 2, p_dst1, dst0_stride );
-            p_dst = p_dst1 + i_dst1_stride_x2;
-            LASX_ST_W_2( vec_pckod1, 0, 2, p_dst, dst0_stride );
-            p_dst = p_dst + i_dst1_stride_x2;
-            LASX_ST_W_2( vec_pckod2, 0, 2, p_dst, dst0_stride );
-            p_dst = p_dst + i_dst1_stride_x2;
-            LASX_ST_W_2( vec_pckod3, 0, 2, p_dst, dst0_stride );
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            __lasx_xvstelm_w( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_w( vec_pckev0, p_dst0 + dst0_stride, 0, 2 );
+            p_dst = p_dst0 + dst0_stride_x2;
+            __lasx_xvstelm_w( vec_pckev1, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckev1, p_dst + dst0_stride, 0, 2 );
+            p_dst = p_dst + dst0_stride_x2;
+            __lasx_xvstelm_w( vec_pckev2, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckev2, p_dst + dst0_stride, 0, 2 );
+            p_dst = p_dst + dst0_stride_x2;
+            __lasx_xvstelm_w( vec_pckev3, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckev3, p_dst + dst0_stride, 0, 2 );
+
+            __lasx_xvstelm_w( vec_pckod0, p_dst1, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod0, p_dst1 + dst0_stride, 0, 2 );
+            p_dst = p_dst1 + dst1_stride_x2;
+            __lasx_xvstelm_w( vec_pckod1, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod1, p_dst + dst0_stride, 0, 2 );
+            p_dst = p_dst + dst1_stride_x2;
+            __lasx_xvstelm_w( vec_pckod2, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod2, p_dst + dst0_stride, 0, 2 );
+            p_dst = p_dst + dst1_stride_x2;
+            __lasx_xvstelm_w( vec_pckod3, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod3, p_dst + dst0_stride, 0, 2 );
 
             p_dst0 += 4;
             p_dst1 += 4;
@@ -3571,36 +3762,38 @@ static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
     {
         for( i_loop_width = ( i_w_mul16 >> 4 ); i_loop_width--; )
         {
-            in0 = LASX_LD( p_src );
+            in0 = __lasx_xvld( p_src, 0 );
             p_src += 32;
             vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
             vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
-            LASX_ST_D_2( vec_pckev0, 0, 2, p_dst0, 8 );
-            LASX_ST_D_2( vec_pckod0, 0, 2, p_dst1, 8 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 8, 2 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 8, 2 );
             p_dst0 += 16;
             p_dst1 += 16;
         }
 
         for( i_loop_width = ( ( i_width & 15 ) >> 3 ); i_loop_width--; )
         {
-            in0 = LASX_LD( p_src );
+            in0 = __lasx_xvld( p_src, 0 );
             p_src += 16;
             vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
             vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
-            LASX_ST_D( vec_pckev0, 0, p_dst0 );
-            LASX_ST_D( vec_pckod0, 0, p_dst1 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
             p_dst0 += 8;
             p_dst1 += 8;
         }
 
         for( i_loop_width = ( ( i_width & 7 ) >> 2 ); i_loop_width--; )
         {
-            in0 = LASX_LD( p_src );
+            in0 = __lasx_xvld( p_src, 0 );
             p_src += 8;
             vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
             vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
-            LASX_ST_W( vec_pckev0, 0, p_dst0 );
-            LASX_ST_W( vec_pckod0, 0, p_dst1 );
+            __lasx_xvstelm_w( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod0, p_dst1, 0, 0 );
             p_dst0 += 4;
             p_dst1 += 4;
         }
@@ -3633,6 +3826,12 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
     __m256i vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3;
     __m256i vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3;
     uint8_t *p_dst_t, *p_srcA, *p_srcB;
+    int32_t i_src0_stride_x2 = i_src0_stride << 1;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
+    int32_t i_src0_stride_x3 = i_src0_stride_x2 + i_src0_stride;
+    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
 
     i_w_mul8 = i_width - ( i_width & 7 );
     i_h4w = i_height - ( i_height & 3 );
@@ -3641,12 +3840,15 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
     {
         for( i_loop_width = ( i_width >> 5 ); i_loop_width--; )
         {
-            LASX_LD_4( p_src0, i_src0_stride, src0, src1, src2, src3 );
-            LASX_LD_4( p_src1, i_src1_stride, src4, src5, src6, src7 );
-            LASX_ILVL_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
-                                 vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
-            LASX_ILVH_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
-                                 vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
+            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
+                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
+            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
+
+            DUP4_ARG2( __lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                       vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+            DUP4_ARG2( __lasx_xvilvh_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                       vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
 
             src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
             src1 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x02 );
@@ -3658,8 +3860,14 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
             src6 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x13 );
             src7 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x13 );
 
-            LASX_ST_4( src0, src1, src2, src3, p_dst, i_dst_stride );
-            LASX_ST_4( src4, src5, src6, src7, ( p_dst + 32 ), i_dst_stride );
+            __lasx_xvst( src0, p_dst, 0 );
+            __lasx_xvstx( src1, p_dst, i_dst_stride );
+            __lasx_xvstx( src2, p_dst, i_dst_stride_x2 );
+            __lasx_xvstx( src3, p_dst, i_dst_stride_x3 );
+            __lasx_xvst( src4, p_dst, 32 );
+            __lasx_xvstx( src5, p_dst, 32 + i_dst_stride );
+            __lasx_xvstx( src6, p_dst, 32 + i_dst_stride_x2 );
+            __lasx_xvstx( src7, p_dst, 32 + i_dst_stride_x3 );
 
             p_src0 += 32;
             p_src1 += 32;
@@ -3668,20 +3876,24 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
 
         for( i_loop_width = ( ( i_width & 31 ) >> 4 ); i_loop_width--; )
         {
-            LASX_LD_4( p_src0, i_src0_stride, src0, src1, src2, src3 );
-            LASX_LD_4( p_src1, i_src1_stride, src4, src5, src6, src7 );
-            LASX_ILVL_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
-                                 vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
-            LASX_ILVH_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
-                                 vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
+            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
+                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
+            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
+            DUP4_ARG2(__lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                      vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+            DUP4_ARG2(__lasx_xvilvh_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                      vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
 
             vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
             vec_ilv_l1 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x02 );
             vec_ilv_l2 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x02 );
             vec_ilv_l3 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x02 );
 
-            LASX_ST_4( vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3,
-                       p_dst, i_dst_stride );
+            __lasx_xvst( vec_ilv_l0, p_dst, 0 );
+            __lasx_xvstx( vec_ilv_l1, p_dst, i_dst_stride );
+            __lasx_xvstx( vec_ilv_l2, p_dst, i_dst_stride_x2 );
+            __lasx_xvstx( vec_ilv_l3, p_dst, i_dst_stride_x3 );
 
             p_src0 += 16;
             p_src1 += 16;
@@ -3690,18 +3902,24 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
 
         for( i_loop_width = ( i_width & 15 ) >> 3; i_loop_width--; )
         {
-            LASX_LD_4( p_src0, i_src0_stride, src0, src1, src2, src3 );
-            LASX_LD_4( p_src1, i_src1_stride, src4, src5, src6, src7 );
-            LASX_ILVL_B_4_128SV( src4, src0, src5, src1, src6, src2, src7, src3,
-                                 vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
-
-            LASX_ST_Q( vec_ilv_l0, 0, p_dst );
+            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
+                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
+            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
+            DUP4_ARG2(__lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                      vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+
+            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 8, 1 );
             p_dst_t = p_dst + i_dst_stride;
-            LASX_ST_Q( vec_ilv_l1, 0, p_dst_t );
+            __lasx_xvstelm_d( vec_ilv_l1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l1, p_dst, 8, 1 );
             p_dst_t = p_dst_t + i_dst_stride;
-            LASX_ST_Q( vec_ilv_l2, 0, p_dst_t );
+            __lasx_xvstelm_d( vec_ilv_l2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l2, p_dst, 8, 1 );
             p_dst_t = p_dst_t + i_dst_stride;
-            LASX_ST_Q( vec_ilv_l3, 0, p_dst_t );
+            __lasx_xvstelm_d( vec_ilv_l3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l3, p_dst, 8, 1 );
 
             p_src0 += 8;
             p_src1 += 8;
@@ -3745,13 +3963,15 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
     {
         for( i_loop_width = ( i_width >> 5 ); i_loop_width--; )
         {
-            src0 = LASX_LD( p_src0 );
-            src4 = LASX_LD( p_src1 );
-            LASX_ILVLH_B_128SV( src4, src0, vec_ilv_h0, vec_ilv_l0 );
+            src0 = __lasx_xvld( p_src0, 0 );
+            src4 = __lasx_xvld( p_src1, 0 );
+            vec_ilv_h0 = __lasx_xvilvl_b( src4, src0 );
+            vec_ilv_l0 = __lasx_xvilvh_b( src4, src0 );
 
             src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
             src1 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x13 );
-            LASX_ST_2( src0, src1, p_dst, 32 );
+            __lasx_xvst( src0, p_dst, 0 );
+            __lasx_xvst( src0, p_dst, 32 );
 
             p_src0 += 32;
             p_src1 += 32;
@@ -3760,12 +3980,13 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
 
         for( i_loop_width = ( ( i_width &  31 )  >> 4 ); i_loop_width--; )
         {
-            src0 = LASX_LD( p_src0 );
-            src4 = LASX_LD( p_src1 );
-            LASX_ILVLH_B_128SV( src4, src0, vec_ilv_h0, vec_ilv_l0 );
+            src0 = __lasx_xvld( p_src0, 0 );
+            src4 = __lasx_xvld( p_src1, 0 );
+            vec_ilv_h0 = __lasx_xvilvl_b( src4, src0 );
+            vec_ilv_l0 = __lasx_xvilvh_b( src4, src0 );
 
             vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
-            LASX_ST( vec_ilv_l0, p_dst );
+            __lasx_xvst( vec_ilv_l0, p_dst, 0 );
 
             p_src0 += 16;
             p_src1 += 16;
@@ -3774,10 +3995,11 @@ static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
 
         for( i_loop_width = ( i_width & 15 ) >> 3; i_loop_width--; )
         {
-            src0 = LASX_LD( p_src0 );
-            src4 = LASX_LD( p_src1 );
+            src0 = __lasx_xvld( p_src0, 0 );
+            src4 = __lasx_xvld( p_src1, 0 );
             vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
-            LASX_ST_Q( vec_ilv_l0, 0, p_dst );
+            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 8, 1 );
 
             p_src0 += 8;
             p_src1 += 8;
@@ -3810,39 +4032,50 @@ static void core_store_interleave_chroma_lasx( uint8_t *p_src0,
     int32_t i_loop_height, i_h4w;
     __m256i in0, in1, in2, in3, in4, in5, in6, in7;
     __m256i tmp0, tmp1, tmp2, tmp3;
+    int32_t i_src0_stride_x2 = i_src0_stride << 1;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
     int32_t i_src0_stride_x4 = i_src0_stride << 2;
     int32_t i_src1_stride_x4 = i_src1_stride << 2;
+    int32_t i_src0_stride_x3 = i_src0_stride_x2 + i_src0_stride;
+    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
 
     i_h4w = i_height & 3;
     for( i_loop_height = ( i_height >> 2 ); i_loop_height--; )
     {
-        LASX_LD_4( p_src0, i_src0_stride, in0, in1, in2, in3 );
+        DUP4_ARG2( __lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
+                   i_src0_stride_x2, p_src0, i_src0_stride_x3, in0, in1, in2, in3 );
         p_src0 += i_src0_stride_x4;
-        LASX_LD_4( p_src1, i_src1_stride, in4, in5, in6, in7 );
+        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                   i_src1_stride_x2, p_src1, i_src1_stride_x3, in4, in5, in6, in7 );
         p_src1 += i_src1_stride_x4;
-        LASX_ILVL_B_4_128SV( in4, in0, in5, in1, in6, in2, in7, in3,
-                             tmp0, tmp1, tmp2, tmp3 );
+        DUP4_ARG2( __lasx_xvilvl_b, in4, in0, in5, in1, in6, in2, in7, in3,
+                   tmp0, tmp1, tmp2, tmp3 );
 
-        LASX_ST_Q( tmp0, 0, p_dst );
+        __lasx_xvstelm_d( tmp0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp0, p_dst, 8, 1 );
         p_dst += i_dst_stride;
-        LASX_ST_Q( tmp1, 0, p_dst );
+        __lasx_xvstelm_d( tmp1, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp1, p_dst, 8, 1 );
         p_dst += i_dst_stride;
-        LASX_ST_Q( tmp2, 0, p_dst );
+        __lasx_xvstelm_d( tmp2, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp2, p_dst, 8, 1 );
         p_dst += i_dst_stride;
-        LASX_ST_Q( tmp3, 0, p_dst );
+        __lasx_xvstelm_d( tmp3, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp3, p_dst, 8, 1 );
         p_dst += i_dst_stride;
     }
 
     for( i_loop_height = i_h4w; i_loop_height--; )
     {
-        in0 = LASX_LD( p_src0 );
+        in0 = __lasx_xvld( p_src0, 0 );
         p_src0 += i_src0_stride;
-        in1 = LASX_LD( p_src1 );
+        in1 = __lasx_xvld( p_src1, 0 );
         p_src1 += i_src1_stride;
 
         tmp0 = __lasx_xvilvl_b( in1, in0 );
 
-        LASX_ST_Q( tmp0, 0, p_dst );
+        __lasx_xvstelm_d( tmp0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp0, p_dst, 8, 1 );
         p_dst += i_dst_stride;
     }
 }
@@ -3906,7 +4139,7 @@ static void memzero_aligned_lasx( void *p_dst, size_t n )
 
     for ( i_cnt = i_tot32; i_cnt--; )
     {
-        LASX_ST( zero, p_dst );
+        __lasx_xvst( zero, p_dst, 0 );
         p_dst += 32;
     }
 
diff --git a/common/loongarch/pixel-c.c b/common/loongarch/pixel-c.c
index b09f59b1..99602106 100644
--- a/common/loongarch/pixel-c.c
+++ b/common/loongarch/pixel-c.c
@@ -25,7 +25,7 @@
  *****************************************************************************/
 
 #include "common/common.h"
-#include "generic_macros_lasx.h"
+#include "loongson_intrinsics.h"
 #include "pixel.h"
 #include "predict.h"
 
@@ -283,147 +283,6 @@ static inline int32_t pixel_satd_8width_lasx( uint8_t *p_pix1, int32_t i_stride,
     return ( sum >> 1 );
 }
 
-static inline int32_t pixel_satd_16width_lasx( uint8_t *p_pix1,
-                                               int32_t i_stride,
-                                               uint8_t *p_pix2,
-                                               int32_t i_stride2,
-                                               uint8_t i_height )
-{
-    int32_t sum, i_8 = 8;
-    uint32_t sum1, sum2;
-    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
-
-    __asm__ volatile (
-    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
-    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
-    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
-    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
-    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
-    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
-    "xvldi          $xr16,            0                                            \n\t"
-    "1:                                                                            \n\t"
-    "addi.d         %[i_height],      %[i_height],          -8                     \n\t"
-    "vld            $vr0,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
-    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
-    "vld            $vr4,             %[p_pix1],            0                      \n\t"
-    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
-    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
-    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
-    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
-    "vld            $vr8,             %[p_pix2],            0                      \n\t"
-    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
-    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
-    "vld            $vr12,            %[p_pix2],            0                      \n\t"
-    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
-    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
-    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
-    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
-    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
-    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
-    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
-    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
-    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
-    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
-    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
-    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
-    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
-    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
-    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
-    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
-    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
-    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
-    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
-    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
-
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
-    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
-
-    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
-    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
-    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
-    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
-    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
-    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
-    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
-    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
-
-    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
-    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
-    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
-    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
-    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
-
-    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
-    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
-    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
-    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
-    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
-    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
-    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
-    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
-
-
-    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
-    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
-    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
-    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
-    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
-    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
-    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
-
-    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
-    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
-    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
-    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
-    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
-    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
-    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
-
-    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
-    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
-    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
-
-    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
-    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
-    "xvadd.h        $xr16,            $xr16,                $xr8                   \n\t"
-    "xvadd.h        $xr16,            $xr16,                $xr0                   \n\t"
-    "bge            %[i_height],      %[i_8],               1b                     \n\t"
-    "2:                                                                            \n\t"
-    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
-    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
-    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
-    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
-    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
-      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
-      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2),
-      [i_height]"+&r"(i_height)
-    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2), [i_8]"r"(i_8)
-    : "memory"
-    );
-
-     return ( sum >> 1 );
-}
-
-
 int32_t x264_pixel_satd_4x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
                                   uint8_t *p_pix2, intptr_t i_stride2 )
 {
@@ -588,39 +447,374 @@ int32_t x264_pixel_satd_8x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
 int32_t x264_pixel_satd_16x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
                                    uint8_t *p_pix2, intptr_t i_stride2 )
 {
-    return pixel_satd_16width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 8 );
+    int32_t sum;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+
+    __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
+    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
+    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
+    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
+    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
+    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
+    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
+    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
+    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
+    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
+    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
+
+    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
+    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
+    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
+    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+
+    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
+    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
+    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
+    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
+
+
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
+
+    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
+    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
+    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
+
+    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
+    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
+
+    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr0,                 $xr8                   \n\t"
+    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2)
+    : "memory"
+    );
+
+    return ( sum >> 1 );
 }
 
 int32_t x264_pixel_satd_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
                                     uint8_t *p_pix2, intptr_t i_stride2 )
 {
-    return pixel_satd_16width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
-}
+    int32_t sum;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
 
-static inline void sad_16width_x4d_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                         uint8_t *p_aref[], int32_t i_ref_stride,
-                                         int32_t i_height, uint32_t *pu_sad_array )
-{
-    int32_t i_ht_cnt;
-    uint8_t *p_ref0, *p_ref1, *p_ref2, *p_ref3;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad0 = __lasx_xvldi( 0 );
-    __m256i sad1 = __lasx_xvldi( 0 );
-    __m256i sad2 = __lasx_xvldi( 0 );
-    __m256i sad3 = __lasx_xvldi( 0 );
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+    __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
+    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
+    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
+    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
+    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
+    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
+    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
+    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
+    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
+    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
+    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
+
+    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
+    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
+    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
+    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+
+    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
+    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
+    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
+    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
+
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
+
+    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
+    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
+    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
+
+    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
+    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
+
+    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr0,                 $xr8                   \n\t"
+
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
+    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
+    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
+    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
+    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
+    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
+    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
+    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
+    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
+    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
+    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
+
+    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
+    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
+    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
+    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+
+    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
+    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
+    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
+    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
+
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
+
+    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
+    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
+    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
+
+    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
+    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
+
+    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr16,                $xr8                   \n\t"
+    "xvadd.h        $xr16,            $xr16,                $xr0                   \n\t"
+
+    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2)
+    : "memory"
+    );
+
+    return ( sum >> 1 );
+}
 
-    p_ref0 = p_aref[0];
-    p_ref1 = p_aref[1];
-    p_ref2 = p_aref[2];
-    p_ref3 = p_aref[3];
+#define SAD_LOAD                                                        \
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;             \
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;             \
+    __m256i diff;                                                       \
+    __m256i sad0 = __lasx_xvldi( 0 );                                   \
+    __m256i sad1 = __lasx_xvldi( 0 );                                   \
+    __m256i sad2 = __lasx_xvldi( 0 );                                   \
+    __m256i sad3 = __lasx_xvldi( 0 );                                   \
+    int32_t i_src_stride_x2 = FENC_STRIDE << 1;                         \
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;                        \
+    int32_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;            \
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;           \
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;                     \
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;                     \
 
 #define LOAD_REF_DATA_16W( p_ref, sad)                                        \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
@@ -628,7 +822,6 @@ static inline void sad_16width_x4d_lasx( uint8_t *p_src, int32_t i_src_stride,
     p_ref += i_ref_stride_x4;                                                 \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
                  ref4, ref5, ref6, ref7 );                                    \
-    p_ref += i_ref_stride_x4;                                                 \
     ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
     ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
     ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
@@ -646,69 +839,111 @@ static inline void sad_16width_x4d_lasx( uint8_t *p_src, int32_t i_src_stride,
     diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
     sad  = __lasx_xvadd_h(sad, diff);                                         \
 
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src0, src1, src2, src3 );
-        p_src += i_src_stride_x4;
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src4, src5, src6, src7 );
-        p_src += i_src_stride_x4;
-        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-        src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-        src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-        LOAD_REF_DATA_16W( p_ref0, sad0 );
-        LOAD_REF_DATA_16W( p_ref1, sad1 );
-        LOAD_REF_DATA_16W( p_ref2, sad2 );
-        LOAD_REF_DATA_16W( p_ref3, sad3 );
-   }
-
-#undef LOAD_REF_DATA_16W
-
 #define ST_REF_DATA(sad)                                  \
     sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
     sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
     sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
 
+void x264_pixel_sad_x4_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                   uint8_t *p_ref1, uint8_t *p_ref2,
+                                   uint8_t *p_ref3, intptr_t i_ref_stride,
+                                   int32_t p_sad_array[4] )
+{
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LOAD_REF_DATA_16W( p_ref0, sad0 );
+    LOAD_REF_DATA_16W( p_ref1, sad1 );
+    LOAD_REF_DATA_16W( p_ref2, sad2 );
+    LOAD_REF_DATA_16W( p_ref3, sad3 );
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    p_ref0 += i_ref_stride_x4;
+    p_ref1 += i_ref_stride_x4;
+    p_ref2 += i_ref_stride_x4;
+    p_ref3 += i_ref_stride_x4;
+
+    LOAD_REF_DATA_16W( p_ref0, sad0 );
+    LOAD_REF_DATA_16W( p_ref1, sad1 );
+    LOAD_REF_DATA_16W( p_ref2, sad2 );
+    LOAD_REF_DATA_16W( p_ref3, sad3 );
+
     ST_REF_DATA(sad0);
-    pu_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
     ST_REF_DATA(sad1);
-    pu_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
     ST_REF_DATA(sad2);
-    pu_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
     ST_REF_DATA(sad3);
-    pu_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
-
-#undef ST_REF_DATA
-
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
 }
 
-static inline void sad_8width_x4d_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                        uint8_t *p_aref[], int32_t i_ref_stride,
-                                        int32_t i_height, uint32_t *pu_sad_array )
+void x264_pixel_sad_x4_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] )
 {
-    int32_t i_ht_cnt;
-    uint8_t *p_ref0, *p_ref1, *p_ref2, *p_ref3;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad0 = __lasx_xvldi( 0 );
-    __m256i sad1 = __lasx_xvldi( 0 );
-    __m256i sad2 = __lasx_xvldi( 0 );
-    __m256i sad3 = __lasx_xvldi( 0 );
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
-    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LOAD_REF_DATA_16W( p_ref0, sad0 );
+    LOAD_REF_DATA_16W( p_ref1, sad1 );
+    LOAD_REF_DATA_16W( p_ref2, sad2 );
+    LOAD_REF_DATA_16W( p_ref3, sad3 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    ST_REF_DATA(sad3);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+}
 
-    p_ref0 = p_aref[0];
-    p_ref1 = p_aref[1];
-    p_ref2 = p_aref[2];
-    p_ref3 = p_aref[3];
+#undef LOAD_REF_DATA_16W
 
 #define LOAD_REF_DATA_8W( p_ref, sad)                                             \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,           \
@@ -716,7 +951,6 @@ static inline void sad_8width_x4d_lasx( uint8_t *p_src, int32_t i_src_stride,
     p_ref += i_ref_stride_x4;                                                     \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,           \
                  ref4, ref5, ref6, ref7 );                                        \
-    p_ref += i_ref_stride_x4;                                                     \
     ref0 = __lasx_xvilvl_d( ref1, ref0 );                                         \
     ref1 = __lasx_xvilvl_d( ref3, ref2 );                                         \
     ref2 = __lasx_xvilvl_d( ref5, ref4 );                                         \
@@ -724,82 +958,77 @@ static inline void sad_8width_x4d_lasx( uint8_t *p_src, int32_t i_src_stride,
     ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                                  \
     ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                                  \
     diff = __lasx_xvabsd_bu( src0, ref0 );                                        \
-    sad += __lasx_xvhaddw_hu_bu( diff, diff );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                    \
+    sad  = __lasx_xvadd_h( sad, diff );                                           \
     diff = __lasx_xvabsd_bu( src1, ref1 );                                        \
-    sad += __lasx_xvhaddw_hu_bu( diff, diff );
-
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src0, src1, src2, src3 );
-        p_src += i_src_stride_x4;
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src4, src5, src6, src7 );
-        p_src += i_src_stride_x4;
-        src0 = __lasx_xvilvl_d( src1, src0 );
-        src1 = __lasx_xvilvl_d( src3, src2 );
-        src2 = __lasx_xvilvl_d( src5, src4 );
-        src3 = __lasx_xvilvl_d( src7, src6 );
-        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-
-        LOAD_REF_DATA_8W( p_ref0, sad0 );
-        LOAD_REF_DATA_8W( p_ref1, sad1 );
-        LOAD_REF_DATA_8W( p_ref2, sad2 );
-        LOAD_REF_DATA_8W( p_ref3, sad3 );
-    }
-
-#undef LOAD_REF_DATA_8W
-
-#define ST_REF_DATA(sad)                                  \
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
-    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
-    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                    \
+    sad  = __lasx_xvadd_h( sad, diff );
 
-    ST_REF_DATA(sad0);
-    pu_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
-    ST_REF_DATA(sad1);
-    pu_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
-    ST_REF_DATA(sad2);
-    pu_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-    ST_REF_DATA(sad3);
-    pu_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+void x264_pixel_sad_x4_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] )
+{
+    SAD_LOAD
 
-#undef ST_REF_DATA
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
 
-}
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
 
-void x264_pixel_sad_x4_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                   uint8_t *p_ref1, uint8_t *p_ref2,
-                                   uint8_t *p_ref3, intptr_t i_ref_stride,
-                                   int32_t p_sad_array[4] )
-{
-    uint8_t *p_aref[4] = { p_ref0, p_ref1, p_ref2, p_ref3 };
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+    LOAD_REF_DATA_8W( p_ref3, sad3 );
 
-    sad_16width_x4d_lasx( p_src, FENC_STRIDE, p_aref, i_ref_stride, 16,
-                          ( uint32_t * ) p_sad_array );
-}
+    p_ref0 += i_ref_stride_x4;
+    p_ref1 += i_ref_stride_x4;
+    p_ref2 += i_ref_stride_x4;
+    p_ref3 += i_ref_stride_x4;
 
-void x264_pixel_sad_x4_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                  uint8_t *p_ref1, uint8_t *p_ref2,
-                                  uint8_t *p_ref3, intptr_t i_ref_stride,
-                                  int32_t p_sad_array[4] )
-{
-    uint8_t *p_aref[4] = { p_ref0, p_ref1, p_ref2, p_ref3 };
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
 
-    sad_16width_x4d_lasx( p_src, FENC_STRIDE, p_aref, i_ref_stride, 8,
-                          ( uint32_t * ) p_sad_array );
-}
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
 
-void x264_pixel_sad_x4_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                  uint8_t *p_ref1, uint8_t *p_ref2,
-                                  uint8_t *p_ref3, intptr_t i_ref_stride,
-                                  int32_t p_sad_array[4] )
-{
-    uint8_t *p_aref[4] = { p_ref0, p_ref1, p_ref2, p_ref3 };
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+    LOAD_REF_DATA_8W( p_ref3, sad3 );
 
-    sad_8width_x4d_lasx( p_src, FENC_STRIDE, p_aref, i_ref_stride, 16,
-                         ( uint32_t * ) p_sad_array );
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    ST_REF_DATA(sad3);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
 }
 
 void x264_pixel_sad_x4_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
@@ -807,12 +1036,45 @@ void x264_pixel_sad_x4_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
                                  uint8_t *p_ref3, intptr_t i_ref_stride,
                                  int32_t p_sad_array[4] )
 {
-    uint8_t *p_aref[4] = { p_ref0, p_ref1, p_ref2, p_ref3 };
+    SAD_LOAD
 
-    sad_8width_x4d_lasx( p_src, FENC_STRIDE, p_aref, i_ref_stride, 8,
-                         ( uint32_t * ) p_sad_array );
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+    LOAD_REF_DATA_8W( p_ref3, sad3 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    ST_REF_DATA(sad3);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
 }
 
+#undef SAD_LOAD
+#undef LOAD_REF_DATA_8W
+#undef ST_REF_DATA
+
 void x264_pixel_sad_x4_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
                                  uint8_t *p_ref1, uint8_t *p_ref2,
                                  uint8_t *p_ref3, intptr_t i_ref_stride,
@@ -976,32 +1238,27 @@ void x264_pixel_sad_x4_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
 
 }
 
-static inline void sad_16width_x3d_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                         uint8_t *p_ref0, uint8_t *p_ref1,
-                                         uint8_t *p_ref2, int32_t i_ref_stride,
-                                         int32_t i_height, uint32_t *pu_sad_array )
-{
-    int32_t i_ht_cnt;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad0 = __lasx_xvldi(0);
-    __m256i sad1 = __lasx_xvldi(0);
-    __m256i sad2 = __lasx_xvldi(0);
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+#define SAD_LOAD                                                              \
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;                   \
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;                   \
+    __m256i diff;                                                             \
+    __m256i sad0 = __lasx_xvldi(0);                                           \
+    __m256i sad1 = __lasx_xvldi(0);                                           \
+    __m256i sad2 = __lasx_xvldi(0);                                           \
+    int32_t i_src_stride_x2 = FENC_STRIDE << 1;                               \
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;                              \
+    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;                  \
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;                 \
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;                           \
     int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
 
+
 #define LOAD_REF_DATA_16W( p_ref, sad)                                        \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
                  ref0, ref1, ref2, ref3 );                                    \
     p_ref += i_ref_stride_x4;                                                 \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
                  ref4, ref5, ref6, ref7 );                                    \
-    p_ref += i_ref_stride_x4;                                                 \
     ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
     ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
     ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
@@ -1019,68 +1276,108 @@ static inline void sad_16width_x3d_lasx( uint8_t *p_src, int32_t i_src_stride,
     diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
     sad  = __lasx_xvadd_h(sad, diff);                                         \
 
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src0, src1, src2, src3 );
-        p_src += i_src_stride_x4;
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src4, src5, src6, src7 );
-        p_src += i_src_stride_x4;
-        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-        src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-        src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
-
-        LOAD_REF_DATA_16W( p_ref0, sad0 );
-        LOAD_REF_DATA_16W( p_ref1, sad1 );
-        LOAD_REF_DATA_16W( p_ref2, sad2 );
-   }
-
-#undef LOAD_REF_DATA_16W
 
 #define ST_REF_DATA(sad)                                  \
     sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
     sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
     sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
 
+void x264_pixel_sad_x3_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                   uint8_t *p_ref1, uint8_t *p_ref2,
+                                   intptr_t i_ref_stride,
+                                   int32_t p_sad_array[3] )
+{
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LOAD_REF_DATA_16W( p_ref0, sad0 );
+    LOAD_REF_DATA_16W( p_ref1, sad1 );
+    LOAD_REF_DATA_16W( p_ref2, sad2 );
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+    p_ref0 += i_ref_stride_x4;
+    p_ref1 += i_ref_stride_x4;
+    p_ref2 += i_ref_stride_x4;
+
+    LOAD_REF_DATA_16W( p_ref0, sad0 );
+    LOAD_REF_DATA_16W( p_ref1, sad1 );
+    LOAD_REF_DATA_16W( p_ref2, sad2 );
+
     ST_REF_DATA(sad0);
-    pu_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
     ST_REF_DATA(sad1);
-    pu_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
     ST_REF_DATA(sad2);
-    pu_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-
-#undef ST_REF_DATA
-
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
 }
 
-static inline void sad_8width_x3d_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                        uint8_t *p_ref0, uint8_t *p_ref1,
-                                        uint8_t *p_ref2, int32_t i_ref_stride,
-                                        int32_t i_height, uint32_t *pu_sad_array )
+void x264_pixel_sad_x3_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] )
 {
-    int32_t i_ht_cnt;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad0 = __lasx_xvldi( 0 );
-    __m256i sad1 = __lasx_xvldi( 0 );
-    __m256i sad2 = __lasx_xvldi( 0 );
-    int32_t i_src_stride_x2 = i_src_stride << 1;
-    int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
-    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
-    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
-    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LOAD_REF_DATA_16W( p_ref0, sad0 );
+    LOAD_REF_DATA_16W( p_ref1, sad1 );
+    LOAD_REF_DATA_16W( p_ref2, sad2 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+}
 
-#define LOAD_REF_DATA_8W( p_ref, sad) \
+#undef LOAD_REF_DATA_16W
+
+#define LOAD_REF_DATA_8W( p_ref, sad)                                          \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
                  ref0, ref1, ref2, ref3 );                                     \
     p_ref += i_ref_stride_x4;                                                  \
     LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
                  ref4, ref5, ref6, ref7 );                                     \
-    p_ref += i_ref_stride_x4;                                                  \
     ref0 = __lasx_xvilvl_d( ref1, ref0 );                                      \
     ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
     ref2 = __lasx_xvilvl_d( ref5, ref4 );                                      \
@@ -1094,69 +1391,64 @@ static inline void sad_8width_x3d_lasx( uint8_t *p_src, int32_t i_src_stride,
     diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
     sad  = __lasx_xvadd_h(sad, diff);                                          \
 
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src0, src1, src2, src3 );
-        p_src += i_src_stride_x4;
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src4, src5, src6, src7 );
-        p_src += i_src_stride_x4;
-        src0 = __lasx_xvilvl_d( src1, src0 );
-        src1 = __lasx_xvilvl_d( src3, src2 );
-        src2 = __lasx_xvilvl_d( src5, src4 );
-        src3 = __lasx_xvilvl_d( src7, src6 );
-        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+void x264_pixel_sad_x3_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] )
+{
+    SAD_LOAD
 
-        LOAD_REF_DATA_8W( p_ref0, sad0 );
-        LOAD_REF_DATA_8W( p_ref1, sad1 );
-        LOAD_REF_DATA_8W( p_ref2, sad2 );
-    }
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
 
-#undef LOAD_REF_DATA_8W
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
 
-#define ST_REF_DATA(sad)                                  \
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
-    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
-    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    p_ref0 += i_ref_stride_x4;
+    p_ref1 += i_ref_stride_x4;
+    p_ref2 += i_ref_stride_x4;
+
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
 
     ST_REF_DATA(sad0);
-    pu_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
     ST_REF_DATA(sad1);
-    pu_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
     ST_REF_DATA(sad2);
-    pu_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
-
-#undef ST_REF_DATA
-
-}
-
-void x264_pixel_sad_x3_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                   uint8_t *p_ref1, uint8_t *p_ref2,
-                                   intptr_t i_ref_stride,
-                                   int32_t p_sad_array[3] )
-{
-    sad_16width_x3d_lasx( p_src, FENC_STRIDE, p_ref0, p_ref1, p_ref2,
-                          i_ref_stride, 16, ( uint32_t * ) p_sad_array );
-}
-
-void x264_pixel_sad_x3_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                  uint8_t *p_ref1, uint8_t *p_ref2,
-                                  intptr_t i_ref_stride,
-                                  int32_t p_sad_array[3] )
-{
-    sad_16width_x3d_lasx( p_src, FENC_STRIDE, p_ref0, p_ref1, p_ref2,
-                          i_ref_stride, 8, ( uint32_t * ) p_sad_array );
-}
-
-void x264_pixel_sad_x3_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
-                                  uint8_t *p_ref1, uint8_t *p_ref2,
-                                  intptr_t i_ref_stride,
-                                  int32_t p_sad_array[3] )
-{
-    sad_8width_x3d_lasx( p_src, FENC_STRIDE, p_ref0, p_ref1, p_ref2,
-                         i_ref_stride, 16, ( uint32_t * ) p_sad_array );
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
 }
 
 void x264_pixel_sad_x3_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
@@ -1164,10 +1456,39 @@ void x264_pixel_sad_x3_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
                                  intptr_t i_ref_stride,
                                  int32_t p_sad_array[3] )
 {
-    sad_8width_x3d_lasx( p_src, FENC_STRIDE, p_ref0, p_ref1, p_ref2,
-                         i_ref_stride, 8, ( uint32_t * ) p_sad_array );
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
 }
 
+#undef SAD_LOAD
+#undef LOAD_REF_DATA_8W
+
 void x264_pixel_sad_x3_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
                                  uint8_t *p_ref1, uint8_t *p_ref2,
                                  intptr_t i_ref_stride,
@@ -1205,11 +1526,6 @@ void x264_pixel_sad_x3_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
 
 #undef LOAD_REF_DATA_8W_4H
 
-#define ST_REF_DATA(sad)                                  \
-    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
-    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
-    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
-
     ST_REF_DATA(sad0);
     p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
     ST_REF_DATA(sad1);
@@ -1349,35 +1665,44 @@ void x264_pixel_sad_x3_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
 
 }
 
-static inline uint32_t sad_16width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                         uint8_t *p_ref, int32_t i_ref_stride,
-                                         int32_t i_height )
+static inline uint32_t sad_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                        uint8_t *p_ref, int32_t i_ref_stride,
+                                        int32_t i_height )
 {
     int32_t i_ht_cnt;
     uint32_t result;
+    uint8_t * p_src2;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
     __m256i diff;
     __m256i sad = __lasx_xvldi( 0 );
-    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x2 = FENC_STRIDE << 1;
     int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
     int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
     int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
     int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+    int32_t i_src_stride_x8 = i_src_stride << 3;
 
     for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
     {
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src0, src1, src2, src3 );
-        p_src += i_src_stride_x4;
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src4, src5, src6, src7 );
-        p_src += i_src_stride_x4;
+        src0 = __lasx_xvld( p_src, 0 );
+        src1 = __lasx_xvld( p_src, FENC_STRIDE );
+        src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+        src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+        p_src2 = p_src + i_src_stride_x4;
+        src4 = __lasx_xvld( p_src2, 0 );
+        src5 = __lasx_xvld( p_src2, FENC_STRIDE );
+        src6 = __lasx_xvldx( p_src2, i_src_stride_x2 );
+        src7 = __lasx_xvldx( p_src2, i_src_stride_x3 );
+        p_src += i_src_stride_x8;
+        src0 = __lasx_xvilvl_w( src1, src0 );
+        src1 = __lasx_xvilvl_w( src3, src2 );
+        src2 = __lasx_xvilvl_w( src5, src4 );
+        src3 = __lasx_xvilvl_w( src7, src6 );
+        src0 = __lasx_xvilvl_d( src1, src0 );
+        src1 = __lasx_xvilvl_d( src3, src2 );
         src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
-        src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
-        src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
 
         LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
                      ref0, ref1, ref2, ref3 );
@@ -1385,23 +1710,17 @@ static inline uint32_t sad_16width_lasx( uint8_t *p_src, int32_t i_src_stride,
         LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
                      ref4, ref5, ref6, ref7 );
         p_ref += i_ref_stride_x4;
+        ref0 = __lasx_xvilvl_w( ref1, ref0 );
+        ref1 = __lasx_xvilvl_w( ref3, ref2 );
+        ref2 = __lasx_xvilvl_w( ref5, ref4 );
+        ref3 = __lasx_xvilvl_w( ref7, ref6 );
+        ref0 = __lasx_xvilvl_d( ref1, ref0 );
+        ref1 = __lasx_xvilvl_d( ref3, ref2 );
         ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-        ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
-        ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
-        ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
         diff = __lasx_xvabsd_bu( src0, ref0 );
         diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h(sad, diff);
-        diff = __lasx_xvabsd_bu( src1, ref1 );
-        diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h(sad, diff);
-        diff = __lasx_xvabsd_bu( src2, ref2 );
-        diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h(sad, diff);
-        diff = __lasx_xvabsd_bu( src3, ref3 );
-        diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h(sad, diff);
-   }
+        sad  = __lasx_xvadd_h( sad, diff );
+    }
     sad = __lasx_xvhaddw_wu_hu(sad, sad);
     sad = __lasx_xvhaddw_du_wu(sad, sad);
     sad = __lasx_xvhaddw_qu_du(sad, sad);
@@ -1410,16 +1729,13 @@ static inline uint32_t sad_16width_lasx( uint8_t *p_src, int32_t i_src_stride,
     return ( result );
 }
 
-static inline uint32_t sad_8width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                        uint8_t *p_ref, int32_t i_ref_stride,
-                                        int32_t i_height )
+int32_t x264_pixel_sad_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                   uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    int32_t i_ht_cnt;
     uint32_t result;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad = __lasx_xvldi( 0 );
+    __m256i diff, sad;
     int32_t i_src_stride_x2 = i_src_stride << 1;
     int32_t i_ref_stride_x2 = i_ref_stride << 1;
     int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
@@ -1427,40 +1743,205 @@ static inline uint32_t sad_8width_lasx( uint8_t *p_src, int32_t i_src_stride,
     int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
     int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
 
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src0, src1, src2, src3 );
-        p_src += i_src_stride_x4;
-        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
-                     src4, src5, src6, src7 );
-        p_src += i_src_stride_x4;
-        src0 = __lasx_xvilvl_d( src1, src0 );
-        src1 = __lasx_xvilvl_d( src3, src2 );
-        src2 = __lasx_xvilvl_d( src5, src4 );
-        src3 = __lasx_xvilvl_d( src7, src6 );
-        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
-        src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    p_ref += i_ref_stride_x4;
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src2, ref2 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src3, ref3 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src2, ref2 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src3, ref3 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+    return result;
+}
+
+int32_t x264_pixel_sad_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t result;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff, sad;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src2, ref2 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src3, ref3 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+
+    return ( result );
+}
+
+int32_t x264_pixel_sad_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t result;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff, sad;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    p_ref += i_ref_stride_x4;
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
 
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref0, ref1, ref2, ref3 );
-        p_ref += i_ref_stride_x4;
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref4, ref5, ref6, ref7 );
-        p_ref += i_ref_stride_x4;
-        ref0 = __lasx_xvilvl_d( ref1, ref0 );
-        ref1 = __lasx_xvilvl_d( ref3, ref2 );
-        ref2 = __lasx_xvilvl_d( ref5, ref4 );
-        ref3 = __lasx_xvilvl_d( ref7, ref6 );
-        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-        ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
-        diff = __lasx_xvabsd_bu( src0, ref0 );
-        diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h(sad, diff);
-        diff = __lasx_xvabsd_bu( src1, ref1 );
-        diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h(sad, diff);
-    }
     sad = __lasx_xvhaddw_wu_hu(sad, sad);
     sad = __lasx_xvhaddw_du_wu(sad, sad);
     sad = __lasx_xvhaddw_qu_du(sad, sad);
@@ -1469,62 +1950,48 @@ static inline uint32_t sad_8width_lasx( uint8_t *p_src, int32_t i_src_stride,
     return ( result );
 }
 
-static inline uint32_t sad_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                        uint8_t *p_ref, int32_t i_ref_stride,
-                                        int32_t i_height )
+int32_t x264_pixel_sad_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    int32_t i_ht_cnt;
     uint32_t result;
-    uint8_t * p_src2;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i diff;
-    __m256i sad = __lasx_xvldi( 0 );
-    int32_t i_src_stride_x2 = FENC_STRIDE << 1;
+    __m256i diff, sad;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
     int32_t i_ref_stride_x2 = i_ref_stride << 1;
-    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
     int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
     int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
     int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
-    int32_t i_src_stride_x8 = i_src_stride << 3;
 
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        src0 = __lasx_xvld( p_src, 0 );
-        src1 = __lasx_xvld( p_src, FENC_STRIDE );
-        src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
-        src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
-        p_src2 = p_src + i_src_stride_x4;
-        src4 = __lasx_xvld( p_src2, 0 );
-        src5 = __lasx_xvld( p_src2, FENC_STRIDE );
-        src6 = __lasx_xvldx( p_src2, i_src_stride_x2 );
-        src7 = __lasx_xvldx( p_src2, i_src_stride_x3 );
-        p_src += i_src_stride_x8;
-        src0 = __lasx_xvilvl_w( src1, src0 );
-        src1 = __lasx_xvilvl_w( src3, src2 );
-        src2 = __lasx_xvilvl_w( src5, src4 );
-        src3 = __lasx_xvilvl_w( src7, src6 );
-        src0 = __lasx_xvilvl_d( src1, src0 );
-        src1 = __lasx_xvilvl_d( src3, src2 );
-        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
 
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref0, ref1, ref2, ref3 );
-        p_ref += i_ref_stride_x4;
-        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
-                     ref4, ref5, ref6, ref7 );
-        p_ref += i_ref_stride_x4;
-        ref0 = __lasx_xvilvl_w( ref1, ref0 );
-        ref1 = __lasx_xvilvl_w( ref3, ref2 );
-        ref2 = __lasx_xvilvl_w( ref5, ref4 );
-        ref3 = __lasx_xvilvl_w( ref7, ref6 );
-        ref0 = __lasx_xvilvl_d( ref1, ref0 );
-        ref1 = __lasx_xvilvl_d( ref3, ref2 );
-        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
-        diff = __lasx_xvabsd_bu( src0, ref0 );
-        diff = __lasx_xvhaddw_hu_bu( diff, diff );
-        sad  = __lasx_xvadd_h( sad, diff );
-    }
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
     sad = __lasx_xvhaddw_wu_hu(sad, sad);
     sad = __lasx_xvhaddw_du_wu(sad, sad);
     sad = __lasx_xvhaddw_qu_du(sad, sad);
@@ -1533,30 +2000,6 @@ static inline uint32_t sad_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
     return ( result );
 }
 
-int32_t x264_pixel_sad_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                   uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    return sad_16width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
-}
-
-int32_t x264_pixel_sad_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                  uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    return sad_16width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
-}
-
-int32_t x264_pixel_sad_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                  uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    return sad_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
-}
-
-int32_t x264_pixel_sad_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride )
-{
-    return sad_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
-}
-
 int32_t x264_pixel_sad_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride )
 {
@@ -1647,98 +2090,109 @@ static inline uint64_t pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix,
 {
     uint32_t u_sum4 = 0, u_sum8 = 0, u_dc;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i zero = __lasx_xvldi( 0 );
-    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
-    __m256i sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7;
+    __m256i diff0, diff1, diff2, diff3;
+    __m256i sub0, sub1, sub2, sub3;
     __m256i temp0, temp1, temp2, temp3;
+    int32_t i_stride2 = i_stride << 1;
+    int32_t i_stride3 = i_stride2 + i_stride;
+    int32_t i_stride4 = i_stride2 << 1;
     v16i16  dc;
-    v4u64 out;
 
-    LASX_LD_8( p_pix, i_stride, src0, src1, src2, src3,
-               src4, src5, src6, src7 );
-    LASX_ILVL_B_4_128SV( zero, src0, zero, src1, zero, src2,
-                         zero, src3, diff0, diff1, diff2, diff3 );
-    LASX_ILVL_B_4_128SV( zero, src4, zero, src5, zero, src6,
-                         zero, src7, diff4, diff5, diff6, diff7 );
-    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7,
-                               diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7 );
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
-    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7,
-                               diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7 );
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
-
-    dc = (v16i16 )diff0;
-    u_dc = ( uint16_t ) ( dc[0] + dc[4] );
-    dc = (v16i16 )diff4;
-    u_dc += ( uint16_t ) ( dc[0] + dc[4] );
-
-    sub0 = __lasx_xvadda_h( diff0, diff1 );
-    sub2 = __lasx_xvadda_h( diff2, diff3 );
-    sub4 = __lasx_xvadda_h( diff4, diff5 );
-    sub6 = __lasx_xvadda_h( diff6, diff7 );
-
-    sub0 = __lasx_xvadd_h( sub0, sub2);
-    sub0 = __lasx_xvadd_h( sub0, sub4);
-    sub0 = __lasx_xvadd_h( sub0, sub6);
-    sub0 = __lasx_xvhaddw_wu_hu( sub0, sub0 );
-    out = ( v4u64 ) __lasx_xvhaddw_du_wu( sub0, sub0 );
-    u_sum4 = out[0] + out[1];
-
-    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7,
-                               sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7 );
-
-    LASX_ILVL_D_2_128SV( sub2, sub0, sub6, sub4, diff0, diff1 );
-    LASX_ILVL_D_2_128SV( sub3, sub1, sub7, sub5, diff4, diff6 );
-
-    diff2 = __lasx_xvilvh_d( sub2, sub0 );
-    diff3 = __lasx_xvilvh_d( sub6, sub4 );
-    diff5 = __lasx_xvilvh_d( sub3, sub1 );
-    diff7 = __lasx_xvilvh_d( sub7, sub5 );
-
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
-
-    sub0 = __lasx_xvadda_h( diff0, diff1 );
-    sub2 = __lasx_xvadda_h( diff2, diff3 );
-    sub4 = __lasx_xvadda_h( diff4, diff5 );
-    sub6 = __lasx_xvadda_h( diff6, diff7 );
-
-    sub0 = __lasx_xvadd_h( sub0, sub2);
-    sub0 = __lasx_xvadd_h( sub0, sub4);
-    sub0 = __lasx_xvadd_h( sub0, sub6);
-    sub0 = __lasx_xvhaddw_wu_hu( sub0, sub0 );
-    out = ( v4u64 ) __lasx_xvhaddw_du_wu( sub0, sub0 );
-    u_sum8 = out[0] + out[1];
+    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src0, src1, src2, src3);
+    p_pix += i_stride4;
+    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src4, src5, src6, src7);
+
+    diff0 = __lasx_xvilvl_d(src1, src0);
+    diff1 = __lasx_xvilvl_d(src3, src2);
+    diff2 = __lasx_xvilvl_d(src5, src4);
+    diff3 = __lasx_xvilvl_d(src7, src6);
+    diff0 = __lasx_xvpermi_q(diff0, diff2, 0x02);
+    diff1 = __lasx_xvpermi_q(diff1, diff3, 0x02);
+    diff2 = __lasx_xvpickev_b(diff1, diff0);
+    diff3 = __lasx_xvpickod_b(diff1, diff0);
+    temp0 = __lasx_xvaddwev_h_bu(diff2, diff3);
+    temp1 = __lasx_xvaddwod_h_bu(diff2, diff3);
+    temp2 = __lasx_xvsubwev_h_bu(diff2, diff3);
+    temp3 = __lasx_xvsubwod_h_bu(diff2, diff3);
+
+    diff0 = __lasx_xvadd_h(temp0, temp1);
+    diff1 = __lasx_xvadd_h(temp2, temp3);
+    diff2 = __lasx_xvsub_h(temp0, temp1);
+    diff3 = __lasx_xvsub_h(temp2, temp3);
+
+    temp0 = __lasx_xvilvl_h(diff1, diff0);
+    temp1 = __lasx_xvilvh_h(diff1, diff0);
+    temp2 = __lasx_xvilvl_h(diff3, diff2);
+    temp3 = __lasx_xvilvh_h(diff3, diff2);
+
+    diff0 = __lasx_xvilvl_w(temp2, temp0);
+    diff1 = __lasx_xvilvh_w(temp2, temp0);
+    diff2 = __lasx_xvilvl_w(temp3, temp1);
+    diff3 = __lasx_xvilvh_w(temp3, temp1);
+
+    temp0 = __lasx_xvadd_h(diff0, diff1);
+    temp2 = __lasx_xvadd_h(diff2, diff3);
+    temp1 = __lasx_xvsub_h(diff0, diff1);
+    temp3 = __lasx_xvsub_h(diff2, diff3);
+
+    diff0 = __lasx_xvadd_h(temp0, temp2);
+    diff1 = __lasx_xvadd_h(temp1, temp3);
+    diff2 = __lasx_xvsub_h(temp0, temp2);
+    diff3 = __lasx_xvsub_h(temp1, temp3);
+
+    dc = (v16i16)diff0;
+    u_dc = (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
+
+    sub0 = __lasx_xvadda_h(diff0, diff1);
+    sub1 = __lasx_xvadda_h(diff2, diff3);
+
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub1 = __lasx_xvpermi_d(sub0, 0x4E);
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
+    u_sum4 = __lasx_xvpickve2gr_wu(sub0, 0);
+
+    temp0 = __lasx_xvpackev_h(diff1, diff0);
+    temp1 = __lasx_xvpackev_h(diff3, diff2);
+    temp2 = __lasx_xvpackod_h(diff1, diff0);
+    temp3 = __lasx_xvpackod_h(diff3, diff2);
+
+    sub0 = __lasx_xvilvl_d(temp1, temp0);
+    sub1 = __lasx_xvilvh_d(temp1, temp0);
+    sub2 = __lasx_xvilvl_d(temp3, temp2);
+    sub3 = __lasx_xvilvh_d(temp3, temp2);
+
+    diff0 = __lasx_xvpermi_q(sub0, sub2, 0x02);
+    diff1 = __lasx_xvpermi_q(sub1, sub2, 0x12);
+    diff2 = __lasx_xvpermi_q(sub0, sub3, 0x03);
+    diff3 = __lasx_xvpermi_q(sub1, sub3, 0x13);
+
+    temp0 = __lasx_xvadd_h(diff0, diff1);
+    temp1 = __lasx_xvsub_h(diff0, diff1);
+    temp2 = __lasx_xvadd_h(diff2, diff3);
+    temp3 = __lasx_xvsub_h(diff2, diff3);
+
+    diff0 = __lasx_xvadd_h(temp0, temp2);
+    diff1 = __lasx_xvadd_h(temp1, temp3);
+    diff2 = __lasx_xvsub_h(temp0, temp2);
+    diff3 = __lasx_xvsub_h(temp1, temp3);
+
+    sub0 = __lasx_xvadda_h(diff0, diff1);
+    sub1 = __lasx_xvadda_h(diff2, diff3);
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub1 = __lasx_xvpermi_d(sub0, 0x4E);
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
+    u_sum8 = __lasx_xvpickve2gr_wu(sub0, 0);
 
     u_sum4 = u_sum4 - u_dc;
     u_sum8 = u_sum8 - u_dc;
 
-    return ( ( uint64_t ) u_sum8 << 32 ) + u_sum4;
+    return ((uint64_t) u_sum8 << 32) + u_sum4;
 }
 
 static inline uint64_t pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix,
@@ -1746,250 +2200,161 @@ static inline uint64_t pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix,
 {
     uint32_t u_sum4 = 0, u_sum8 = 0, u_dc;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i zero = __lasx_xvldi( 0 );
     __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
     __m256i sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7;
-    __m256i temp0, temp1, temp2, temp3;
+    int32_t i_stride2 = i_stride << 1;
+    int32_t i_stride3 = i_stride2 + i_stride;
+    int32_t i_stride4 = i_stride2 << 1;
     v16i16  dc;
 
-    LASX_LD_8( p_pix, i_stride, src0, src1, src2, src3,
-               src4, src5, src6, src7 );
-    src0 = __lasx_xvpermi_d(src0, 0x50);
-    src1 = __lasx_xvpermi_d(src1, 0x50);
-    src2 = __lasx_xvpermi_d(src2, 0x50);
-    src3 = __lasx_xvpermi_d(src3, 0x50);
-    src4 = __lasx_xvpermi_d(src4, 0x50);
-    src5 = __lasx_xvpermi_d(src5, 0x50);
-    src6 = __lasx_xvpermi_d(src6, 0x50);
-    src7 = __lasx_xvpermi_d(src7, 0x50);
-
-    LASX_ILVL_B_4_128SV( zero, src0, zero, src1, zero, src2,
-                         zero, src3, diff0, diff1, diff2, diff3 );
-    LASX_ILVL_B_4_128SV( zero, src4, zero, src5, zero, src6,
-                         zero, src7, diff4, diff5, diff6, diff7 );
-    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7,
-                               diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7 );
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
-    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7,
-                               diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7 );
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
-
-    dc = (v16i16 )diff0;
-    u_dc = ( uint16_t ) ( dc[0] + dc[4] + dc[8] + dc[12] );
-    dc = (v16i16 )diff4;
-    u_dc += ( uint16_t ) ( dc[0] + dc[4] + dc[8] + dc[12] );
-
-    sub0 = __lasx_xvadda_h( diff0, diff1 );
-    sub2 = __lasx_xvadda_h( diff2, diff3 );
-    sub4 = __lasx_xvadda_h( diff4, diff5 );
-    sub6 = __lasx_xvadda_h( diff6, diff7 );
-
-    sub0 = __lasx_xvadd_h( sub0, sub2);
-    sub0 = __lasx_xvadd_h( sub0, sub4);
-    sub0 = __lasx_xvadd_h( sub0, sub6);
-    u_sum4 = LASX_HADD_UH_U32( sub0 );
-
-    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7,
-                               sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7 );
-
-    LASX_ILVL_D_2_128SV( sub2, sub0, sub6, sub4, diff0, diff1 );
-    LASX_ILVL_D_2_128SV( sub3, sub1, sub7, sub5, diff4, diff6 );
-
-    diff2 = __lasx_xvilvh_d( sub2, sub0 );
-    diff3 = __lasx_xvilvh_d( sub6, sub4 );
-    diff5 = __lasx_xvilvh_d( sub3, sub1 );
-    diff7 = __lasx_xvilvh_d( sub7, sub5 );
-
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
-
-    sub0 = __lasx_xvadda_h( diff0, diff1 );
-    sub2 = __lasx_xvadda_h( diff2, diff3 );
-    sub4 = __lasx_xvadda_h( diff4, diff5 );
-    sub6 = __lasx_xvadda_h( diff6, diff7 );
-
-    sub0 = __lasx_xvadd_h( sub0, sub2);
-    sub0 = __lasx_xvadd_h( sub0, sub4);
-    sub0 = __lasx_xvadd_h( sub0, sub6);
-    u_sum8 = LASX_HADD_UH_U32( sub0 );
-
+    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src0, src1, src2, src3);
+    p_pix += i_stride4;
+    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src4, src5, src6, src7);
+
+    diff0 = __lasx_xvpermi_q(src0, src4, 0x02);
+    diff1 = __lasx_xvpermi_q(src1, src5, 0x02);
+    diff2 = __lasx_xvpermi_q(src2, src6, 0x02);
+    diff3 = __lasx_xvpermi_q(src3, src7, 0x02);
+
+    diff4 = __lasx_xvpickev_b(diff1, diff0);
+    diff5 = __lasx_xvpickod_b(diff1, diff0);
+    diff6 = __lasx_xvpickev_b(diff3, diff2);
+    diff7 = __lasx_xvpickod_b(diff3, diff2);
+
+    src0 = __lasx_xvaddwev_h_bu(diff4, diff5);
+    src1 = __lasx_xvaddwod_h_bu(diff4, diff5);
+    src2 = __lasx_xvsubwev_h_bu(diff4, diff5);
+    src3 = __lasx_xvsubwod_h_bu(diff4, diff5);
+    src4 = __lasx_xvaddwev_h_bu(diff6, diff7);
+    src5 = __lasx_xvaddwod_h_bu(diff6, diff7);
+    src6 = __lasx_xvsubwev_h_bu(diff6, diff7);
+    src7 = __lasx_xvsubwod_h_bu(diff6, diff7);
+
+    diff0 = __lasx_xvadd_h(src0, src1);
+    diff1 = __lasx_xvadd_h(src2, src3);
+    diff2 = __lasx_xvsub_h(src0, src1);
+    diff3 = __lasx_xvsub_h(src2, src3);
+    diff4 = __lasx_xvadd_h(src4, src5);
+    diff5 = __lasx_xvadd_h(src6, src7);
+    diff6 = __lasx_xvsub_h(src4, src5);
+    diff7 = __lasx_xvsub_h(src6, src7);
+
+    src0 = __lasx_xvilvl_h(diff1, diff0);
+    src1 = __lasx_xvilvh_h(diff1, diff0);
+    src2 = __lasx_xvilvl_h(diff3, diff2);
+    src3 = __lasx_xvilvh_h(diff3, diff2);
+
+    src4 = __lasx_xvilvl_h(diff5, diff4);
+    src5 = __lasx_xvilvh_h(diff5, diff4);
+    src6 = __lasx_xvilvl_h(diff7, diff6);
+    src7 = __lasx_xvilvh_h(diff7, diff6);
+
+    diff0 = __lasx_xvilvl_w(src2, src0);
+    diff1 = __lasx_xvilvh_w(src2, src0);
+    diff2 = __lasx_xvilvl_w(src3, src1);
+    diff3 = __lasx_xvilvh_w(src3, src1);
+
+    diff4 = __lasx_xvilvl_w(src6, src4);
+    diff5 = __lasx_xvilvh_w(src6, src4);
+    diff6 = __lasx_xvilvl_w(src7, src5);
+    diff7 = __lasx_xvilvh_w(src7, src5);
+
+    src0 = __lasx_xvadd_h(diff0, diff2);
+    src4 = __lasx_xvadd_h(diff1, diff3);
+    src2 = __lasx_xvadd_h(diff4, diff6);
+    src6 = __lasx_xvadd_h(diff5, diff7);
+    src1 = __lasx_xvsub_h(diff0, diff2);
+    src5 = __lasx_xvsub_h(diff1, diff3);
+    src3 = __lasx_xvsub_h(diff4, diff6);
+    src7 = __lasx_xvsub_h(diff5, diff7);
+
+    diff0 = __lasx_xvadd_h(src0, src2);
+    diff1 = __lasx_xvadd_h(src1, src3);
+    diff2 = __lasx_xvsub_h(src0, src2);
+    diff3 = __lasx_xvsub_h(src1, src3);
+    diff4 = __lasx_xvadd_h(src4, src6);
+    diff5 = __lasx_xvadd_h(src5, src7);
+    diff6 = __lasx_xvsub_h(src4, src6);
+    diff7 = __lasx_xvsub_h(src5, src7);
+
+    dc = (v16i16)diff0;
+    u_dc = (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
+    dc = (v16i16)diff4;
+    u_dc += (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
+
+    sub0 = __lasx_xvadda_h(diff0, diff1);
+    sub1 = __lasx_xvadda_h(diff2, diff3);
+    sub2 = __lasx_xvadda_h(diff4, diff5);
+    sub3 = __lasx_xvadda_h(diff6, diff7);
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub0 = __lasx_xvadd_h(sub0, sub2);
+    sub0 = __lasx_xvadd_h(sub0, sub3);
+    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
+    u_sum4 = __lasx_xvpickve2gr_wu(sub0, 0) + __lasx_xvpickve2gr_wu(sub0, 4);
+
+    sub0 = __lasx_xvpackev_h(diff1, diff0);
+    sub1 = __lasx_xvpackod_h(diff1, diff0);
+    sub2 = __lasx_xvpackev_h(diff3, diff2);
+    sub3 = __lasx_xvpackod_h(diff3, diff2);
+    sub4 = __lasx_xvpackev_h(diff5, diff4);
+    sub5 = __lasx_xvpackod_h(diff5, diff4);
+    sub6 = __lasx_xvpackev_h(diff7, diff6);
+    sub7 = __lasx_xvpackod_h(diff7, diff6);
+
+    src0 = __lasx_xvilvl_d(sub2, sub0);
+    src1 = __lasx_xvilvh_d(sub2, sub0);
+    src2 = __lasx_xvilvl_d(sub3, sub1);
+    src3 = __lasx_xvilvh_d(sub3, sub1);
+    src4 = __lasx_xvilvl_d(sub6, sub4);
+    src5 = __lasx_xvilvh_d(sub6, sub4);
+    src6 = __lasx_xvilvl_d(sub7, sub5);
+    src7 = __lasx_xvilvh_d(sub7, sub5);
+
+    diff0 = __lasx_xvpermi_q(src0, src4, 0x02);
+    diff1 = __lasx_xvpermi_q(src1, src5, 0x02);
+    diff2 = __lasx_xvpermi_q(src0, src4, 0x13);
+    diff3 = __lasx_xvpermi_q(src1, src5, 0x13);
+    diff4 = __lasx_xvpermi_q(src2, src6, 0x02);
+    diff5 = __lasx_xvpermi_q(src2, src6, 0x13);
+    diff6 = __lasx_xvpermi_q(src3, src7, 0x02);
+    diff7 = __lasx_xvpermi_q(src3, src7, 0x13);
+
+    src0 = __lasx_xvadd_h(diff0, diff1);
+    src1 = __lasx_xvsub_h(diff0, diff1);
+    src2 = __lasx_xvadd_h(diff2, diff3);
+    src3 = __lasx_xvsub_h(diff2, diff3);
+    src4 = __lasx_xvadd_h(diff4, diff5);
+    src5 = __lasx_xvsub_h(diff4, diff5);
+    src6 = __lasx_xvadd_h(diff6, diff7);
+    src7 = __lasx_xvsub_h(diff6, diff7);
+
+    diff0 = __lasx_xvadd_h(src0, src2);
+    diff1 = __lasx_xvadd_h(src1, src3);
+    diff2 = __lasx_xvsub_h(src0, src2);
+    diff3 = __lasx_xvsub_h(src1, src3);
+    diff4 = __lasx_xvadd_h(src4, src6);
+    diff5 = __lasx_xvadd_h(src5, src7);
+    diff6 = __lasx_xvsub_h(src4, src6);
+    diff7 = __lasx_xvsub_h(src5, src7);
+
+    sub0 = __lasx_xvadda_h(diff0, diff1);
+    sub1 = __lasx_xvadda_h(diff2, diff3);
+    sub2 = __lasx_xvadda_h(diff4, diff5);
+    sub3 = __lasx_xvadda_h(diff6, diff7);
+
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub0 = __lasx_xvadd_h(sub0, sub2);
+    sub0 = __lasx_xvadd_h(sub0, sub3);
+
+    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
+    u_sum8 = __lasx_xvpickve2gr_wu(sub0, 0) + __lasx_xvpickve2gr_wu(sub0, 4);
     u_sum4 = u_sum4 - u_dc;
     u_sum8 = u_sum8 - u_dc;
-
-    return ( ( uint64_t ) u_sum8 << 32 ) + u_sum4;
-}
-
-#define LASX_CALC_MSE_B( src, ref, var )                                   \
-{                                                                          \
-    __m256i src_l0_m, src_l1_m;                                            \
-    __m256i res_l0_m, res_l1_m;                                            \
-                                                                           \
-    LASX_ILVLH_B_128SV( src, ref, src_l1_m, src_l0_m );                    \
-    LASX_HSUB_UB_2( src_l0_m, src_l1_m, res_l0_m, res_l1_m );              \
-    LASX_DP2ADD_W_H_2( var, res_l0_m, res_l0_m,                            \
-                       var, res_l1_m, res_l1_m, var, var );                \
-}
-
-static uint32_t sse_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                 uint8_t *p_ref, int32_t i_ref_stride,
-                                 int32_t i_height )
-{
-    int32_t i_ht_cnt;
-    uint32_t u_sse;
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i var = __lasx_xvldi( 0 );
-    v4i64  out;
-
-    for( i_ht_cnt = ( i_height >> 2 ); i_ht_cnt--; )
-    {
-        src0 = __lasx_xvldrepl_w( p_src, 0 );
-        p_src += i_src_stride;
-        src1 = __lasx_xvldrepl_w( p_src, 0 );
-        p_src += i_src_stride;
-        src2 = __lasx_xvldrepl_w( p_src, 0 );
-        p_src += i_src_stride;
-        src3 = __lasx_xvldrepl_w( p_src, 0 );
-        p_src += i_src_stride;
-        src0 = __lasx_xvpackev_w( src1, src0 );
-        src1 = __lasx_xvpackev_w( src3, src2 );
-        src0 = __lasx_xvpackev_d( src1, src0 );
-
-        ref0 = __lasx_xvldrepl_w( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref1 = __lasx_xvldrepl_w( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref2 = __lasx_xvldrepl_w( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref3 = __lasx_xvldrepl_w( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref0 = __lasx_xvpackev_w( ref1, ref0 );
-        ref1 = __lasx_xvpackev_w( ref3, ref2 );
-        ref0 = __lasx_xvpackev_d( ref1, ref0 );
-
-        LASX_CALC_MSE_B( src0, ref0, var );
-    }
-
-    out = __lasx_xvhaddw_d_w( var, var );
-    u_sse = out[0] + out[1];
-
-    return u_sse;
-}
-
-static uint32_t sse_8width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                 uint8_t *p_ref, int32_t i_ref_stride,
-                                 int32_t i_height )
-{
-    int32_t i_ht_cnt;
-    uint32_t u_sse;
-    __m256i src0, src1, src2, src3;
-    __m256i ref0, ref1, ref2, ref3;
-    __m256i var = __lasx_xvldi( 0 );
-
-    for( i_ht_cnt = ( i_height >> 2 ); i_ht_cnt--; )
-    {
-        src0 = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-        src1 = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-        src2 = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-        src3 = __lasx_xvldrepl_d( p_src, 0 );
-        p_src += i_src_stride;
-
-        ref0 = __lasx_xvldrepl_d( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref1 = __lasx_xvldrepl_d( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref2 = __lasx_xvldrepl_d( p_ref, 0 );
-        p_ref += i_ref_stride;
-        ref3 = __lasx_xvldrepl_d( p_ref, 0 );
-        p_ref += i_ref_stride;
-
-        LASX_PCKEV_D_4_128SV( src1, src0, src3, src2, ref1, ref0, ref3, ref2,
-                              src0, src1, ref0, ref1 );
-        src0 = __lasx_xvpermi_q( src1, src0, 0x20 );
-        ref0 = __lasx_xvpermi_q( ref1, ref0, 0x20 );
-        LASX_CALC_MSE_B( src0, ref0, var );
-    }
-
-    u_sse = LASX_HADD_SW_S32( var );
-
-    return u_sse;
-}
-
-static uint32_t sse_16width_lasx( uint8_t *p_src, int32_t i_src_stride,
-                                  uint8_t *p_ref, int32_t i_ref_stride,
-                                  int32_t i_height )
-{
-    int32_t i_ht_cnt;
-    uint32_t u_sse;
-    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    __m256i var = __lasx_xvldi( 0 );
-
-    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
-    {
-        LASX_LD_2( p_src, i_src_stride, src0, src1 );
-        p_src += ( i_src_stride << 1 );
-        LASX_LD_2( p_src, i_src_stride, src2, src3 );
-        p_src += ( i_src_stride << 1 );
-        LASX_LD_2( p_src, i_src_stride, src4, src5 );
-        p_src += ( i_src_stride << 1 );
-        LASX_LD_2( p_src, i_src_stride, src6, src7 );
-        p_src += ( i_src_stride << 1 );
-
-        LASX_LD_2( p_ref, i_ref_stride, ref0, ref1 );
-        p_ref += ( i_ref_stride << 1 );
-        LASX_LD_2( p_ref, i_ref_stride, ref2, ref3 );
-        p_ref += ( i_ref_stride << 1 );
-        LASX_LD_2( p_ref, i_ref_stride, ref4, ref5 );
-        p_ref += ( i_ref_stride << 1 );
-        LASX_LD_2( p_ref, i_ref_stride, ref6, ref7 );
-        p_ref += ( i_ref_stride << 1 );
-
-        src0 = __lasx_xvpermi_q( src1, src0, 0x20 );
-        ref0 = __lasx_xvpermi_q( ref1, ref0, 0x20 );
-        LASX_CALC_MSE_B( src0, ref0, var );
-        src0 = __lasx_xvpermi_q( src3, src2, 0x20 );
-        ref0 = __lasx_xvpermi_q( ref3, ref2, 0x20 );
-        LASX_CALC_MSE_B( src0, ref0, var );
-        src0 = __lasx_xvpermi_q( src5, src4, 0x20 );
-        ref0 = __lasx_xvpermi_q( ref5, ref4, 0x20 );
-        LASX_CALC_MSE_B( src0, ref0, var );
-        src0 = __lasx_xvpermi_q( src7, src6, 0x20 );
-        ref0 = __lasx_xvpermi_q( ref7, ref6, 0x20 );
-        LASX_CALC_MSE_B( src0, ref0, var );
-    }
-
-    u_sse = LASX_HADD_SW_S32( var );
-
-    return u_sse;
+    return ((uint64_t) u_sum8 << 32) + u_sum4;
 }
 
 uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
@@ -2034,55 +2399,55 @@ static int32_t sa8d_8x8_lasx( uint8_t *p_src, int32_t i_src_stride,
                               uint8_t *p_ref, int32_t i_ref_stride )
 {
     uint32_t u_sum = 0;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    int32_t i_ref_stride_x4 = i_ref_stride << 2;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
     __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
     __m256i temp0, temp1, temp2, temp3;
     v4u64 out;
 
-    LASX_LD_8( p_src, i_src_stride, src0, src1, src2, src3,
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
+               i_src_stride_x3, src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
+               i_src_stride_x3, src4, src5, src6, src7 );
+    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
+               p_ref, i_ref_stride_x3, ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
+               p_ref, i_ref_stride_x3, ref4, ref5, ref6, ref7 );
+
+    DUP4_ARG2( __lasx_xvilvl_b, src0, ref0, src1, ref1, src2, ref2, src3, ref3,
+               src0, src1, src2, src3 );
+    DUP4_ARG2( __lasx_xvilvl_b, src4, ref4, src5, ref5, src6, ref6, src7, ref7,
                src4, src5, src6, src7 );
-    LASX_LD_8( p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
-               ref4, ref5, ref6, ref7 );
-
-    LASX_ILVL_B_4_128SV( src0, ref0, src1, ref1,
-                         src2, ref2, src3, ref3,
-                         src0, src1, src2, src3 );
-    LASX_ILVL_B_4_128SV( src4, ref4, src5, ref5,
-                         src6, ref6, src7, ref7,
+    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src0, src0, src1, src1, src2, src2, src3, src3,
+               src0, src1, src2, src3 );
+    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src4, src4, src5, src5, src6, src6, src7, src7,
+               src4, src5, src6, src7 );
+    LASX_TRANSPOSE8x8_H( src0, src1, src2, src3,
+                         src4, src5, src6, src7,
+                         src0, src1, src2, src3,
                          src4, src5, src6, src7 );
-    LASX_HSUB_UB_4( src0, src1, src2, src3,
-                    src0, src1, src2, src3 );
-    LASX_HSUB_UB_4( src4, src5, src6, src7,
-                    src4, src5, src6, src7 );
-    LASX_TRANSPOSE8x8_H_128SV( src0, src1, src2, src3,
-                               src4, src5, src6, src7,
-                               src0, src1, src2, src3,
-                               src4, src5, src6, src7 );
-    LASX_BUTTERFLY_4( v16i16, src0, src2, src3, src1,
-                      diff0, diff1, diff4, diff5 );
-    LASX_BUTTERFLY_4( v16i16, src4, src6, src7, src5,
-                      diff2, diff3, diff7, diff6 );
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
-    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7,
-                               diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7 );
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
+    LASX_BUTTERFLY_4_H( src0, src2, src3, src1, diff0, diff1, diff4, diff5 );
+    LASX_BUTTERFLY_4_H( src4, src6, src7, src5, diff2, diff3, diff7, diff6 );
+    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
+    LASX_TRANSPOSE8x8_H( diff0, diff1, diff2, diff3,
+                         diff4, diff5, diff6, diff7,
+                         diff0, diff1, diff2, diff3,
+                         diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
 
     temp0 = __lasx_xvadd_h( diff0, diff4 );
     temp1 = __lasx_xvadd_h( diff1, diff5 );
@@ -2113,13 +2478,22 @@ static int32_t sa8d_8x16_lasx( uint8_t *p_src, int32_t i_src_stride,
                                uint8_t *p_ref, int32_t i_ref_stride )
 {
     uint32_t u_sum = 0;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    int32_t i_ref_stride_x4 = i_ref_stride << 2;
     __m256i src0, src1, src2, src3, src4, src5, src6, src7;
     __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
     __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
     __m256i temp0, temp1, temp2, temp3;
 
-    LASX_LD_8( p_src, i_src_stride, src0, src1, src2, src3,
-               src4, src5, src6, src7 );
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
+               i_src_stride_x3, src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
+               i_src_stride_x3, src4, src5, src6, src7 );
     src0 = __lasx_xvpermi_d(src0, 0x50);
     src1 = __lasx_xvpermi_d(src1, 0x50);
     src2 = __lasx_xvpermi_d(src2, 0x50);
@@ -2129,8 +2503,11 @@ static int32_t sa8d_8x16_lasx( uint8_t *p_src, int32_t i_src_stride,
     src6 = __lasx_xvpermi_d(src6, 0x50);
     src7 = __lasx_xvpermi_d(src7, 0x50);
 
-    LASX_LD_8( p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
-               ref4, ref5, ref6, ref7 );
+    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
+               p_ref, i_ref_stride_x3, ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
+               p_ref, i_ref_stride_x3, ref4, ref5, ref6, ref7 );
     ref0 = __lasx_xvpermi_d(ref0, 0x50);
     ref1 = __lasx_xvpermi_d(ref1, 0x50);
     ref2 = __lasx_xvpermi_d(ref2, 0x50);
@@ -2140,44 +2517,32 @@ static int32_t sa8d_8x16_lasx( uint8_t *p_src, int32_t i_src_stride,
     ref6 = __lasx_xvpermi_d(ref6, 0x50);
     ref7 = __lasx_xvpermi_d(ref7, 0x50);
 
-    LASX_ILVL_B_4_128SV( src0, ref0, src1, ref1,
-                         src2, ref2, src3, ref3,
-                         src0, src1, src2, src3 );
-    LASX_ILVL_B_4_128SV( src4, ref4, src5, ref5,
-                         src6, ref6, src7, ref7,
+    DUP4_ARG2( __lasx_xvilvl_b, src0, ref0, src1, ref1, src2, ref2, src3, ref3,
+               src0, src1, src2, src3 );
+    DUP4_ARG2( __lasx_xvilvl_b, src4, ref4, src5, ref5, src6, ref6, src7, ref7,
+               src4, src5, src6, src7 );
+    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src0, src0, src1, src1, src2, src2, src3, src3,
+               src0, src1, src2, src3 );
+    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src4, src4, src5, src5, src6, src6, src7, src7,
+               src4, src5, src6, src7 );
+    LASX_TRANSPOSE8x8_H( src0, src1, src2, src3,
+                         src4, src5, src6, src7,
+                         src0, src1, src2, src3,
                          src4, src5, src6, src7 );
-    LASX_HSUB_UB_4( src0, src1, src2, src3,
-                    src0, src1, src2, src3 );
-    LASX_HSUB_UB_4( src4, src5, src6, src7,
-                    src4, src5, src6, src7 );
-    LASX_TRANSPOSE8x8_H_128SV( src0, src1, src2, src3,
-                               src4, src5, src6, src7,
-                               src0, src1, src2, src3,
-                               src4, src5, src6, src7 );
-    LASX_BUTTERFLY_4( v16i16, src0, src2, src3, src1,
-                      diff0, diff1, diff4, diff5 );
-    LASX_BUTTERFLY_4( v16i16, src4, src6, src7, src5,
-                      diff2, diff3, diff7, diff6 );
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
-    LASX_TRANSPOSE8x8_H_128SV( diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7,
-                               diff0, diff1, diff2, diff3,
-                               diff4, diff5, diff6, diff7 );
-    LASX_BUTTERFLY_4( v16i16, diff0, diff2, diff3, diff1,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff0, diff1, diff3, diff2 );
-    LASX_BUTTERFLY_4( v16i16, diff4, diff6, diff7, diff5,
-                      temp0, temp2, temp3, temp1 );
-    LASX_BUTTERFLY_4( v16i16, temp0, temp1, temp3, temp2,
-                      diff4, diff5, diff7, diff6 );
+    LASX_BUTTERFLY_4_H( src0, src2, src3, src1, diff0, diff1, diff4, diff5 );
+    LASX_BUTTERFLY_4_H( src4, src6, src7, src5, diff2, diff3, diff7, diff6 );
+    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
+    LASX_TRANSPOSE8x8_H( diff0, diff1, diff2, diff3,
+                         diff4, diff5, diff6, diff7,
+                         diff0, diff1, diff2, diff3,
+                         diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
 
     temp0 = __lasx_xvadd_h( diff0, diff4 );
     temp1 = __lasx_xvadd_h( diff1, diff5 );
@@ -2352,63 +2717,526 @@ void x264_intra_sad_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                               p_enc, FENC_STRIDE );
 }
 
+#define SSD_LOAD_8(_p_src, _stride, _stride2, _stride3, _stride4,                  \
+                   _src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7)         \
+{                                                                                  \
+    _src0 = __lasx_xvld(_p_src, 0);                                                \
+    _src1 = __lasx_xvldx(_p_src, _stride);                                         \
+    _src2 = __lasx_xvldx(_p_src, _stride2);                                        \
+    _src3 = __lasx_xvldx(_p_src, _stride3);                                        \
+    _p_src += _stride4;                                                            \
+    _src4 = __lasx_xvld(_p_src, 0);                                                \
+    _src5 = __lasx_xvldx(_p_src, _stride);                                         \
+    _src6 = __lasx_xvldx(_p_src, _stride2);                                        \
+    _src7 = __lasx_xvldx(_p_src, _stride3);                                        \
+}
+
+#define SSD_INSERT_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,       \
+                     _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)       \
+{                                                                                  \
+    _src0 = __lasx_xvpermi_q(_src0, _src1, 0x02);                                  \
+    _src2 = __lasx_xvpermi_q(_src2, _src3, 0x02);                                  \
+    _src4 = __lasx_xvpermi_q(_src4, _src5, 0x02);                                  \
+    _src6 = __lasx_xvpermi_q(_src6, _src7, 0x02);                                  \
+                                                                                   \
+    _ref0 = __lasx_xvpermi_q(_ref0, _ref1, 0x02);                                  \
+    _ref2 = __lasx_xvpermi_q(_ref2, _ref3, 0x02);                                  \
+    _ref4 = __lasx_xvpermi_q(_ref4, _ref5, 0x02);                                  \
+    _ref6 = __lasx_xvpermi_q(_ref6, _ref7, 0x02);                                  \
+}
+
+#define SSD_SUB_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,          \
+                  _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)          \
+{                                                                                  \
+    _src1 = __lasx_xvsubwev_h_bu(_src0, _ref0);                                    \
+    _ref1 = __lasx_xvsubwod_h_bu(_src0, _ref0);                                    \
+    _src3 = __lasx_xvsubwev_h_bu(_src2, _ref2);                                    \
+    _ref3 = __lasx_xvsubwod_h_bu(_src2, _ref2);                                    \
+    _src5 = __lasx_xvsubwev_h_bu(_src4, _ref4);                                    \
+    _ref5 = __lasx_xvsubwod_h_bu(_src4, _ref4);                                    \
+    _src7 = __lasx_xvsubwev_h_bu(_src6, _ref6);                                    \
+    _ref7 = __lasx_xvsubwod_h_bu(_src6, _ref6);                                    \
+}
+
+
 int32_t x264_pixel_ssd_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                    uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    return sse_16width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+    uint32_t u_ssd;
+    intptr_t src_stride2 = i_src_stride << 1;
+    intptr_t ref_stride2 = i_ref_stride << 1;
+    intptr_t src_stride3 = i_src_stride + src_stride2;
+    intptr_t ref_stride3 = i_ref_stride + ref_stride2;
+    intptr_t src_stride4 = src_stride2 << 1;
+    intptr_t ref_stride4 = ref_stride2 << 1;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
+
+    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
+               src0, src1, src2, src3, src4, src5, src6, src7);
+    p_src += src_stride4;
+    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
+               src8, src9, src10, src11, src12, src13, src14, src15);
+
+    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
+               ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    p_ref += ref_stride4;
+    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
+               ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
+                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    SSD_SUB_8(src0, src1, src2, src3, src4, src5, src6, src7,
+              ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    SSD_SUB_8(src8, src9, src10, src11, src12, src13, src14, src15,
+              ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    src0 = __lasx_xvmulwev_w_h(src1, src1);
+    src0 = __lasx_xvmaddwod_w_h(src0, src1, src1);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref1, ref1);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref1, ref1);
+    src0 = __lasx_xvmaddwev_w_h(src0, src3, src3);
+    src0 = __lasx_xvmaddwod_w_h(src0, src3, src3);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref3, ref3);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref3, ref3);
+    src0 = __lasx_xvmaddwev_w_h(src0, src5, src5);
+    src0 = __lasx_xvmaddwod_w_h(src0, src5, src5);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref5, ref5);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref5, ref5);
+    src0 = __lasx_xvmaddwev_w_h(src0, src7, src7);
+    src0 = __lasx_xvmaddwod_w_h(src0, src7, src7);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref7, ref7);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref7, ref7);
+
+    src0 = __lasx_xvmaddwev_w_h(src0, src9, src9);
+    src0 = __lasx_xvmaddwod_w_h(src0, src9, src9);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref9, ref9);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref9, ref9);
+    src0 = __lasx_xvmaddwev_w_h(src0, src11, src11);
+    src0 = __lasx_xvmaddwod_w_h(src0, src11, src11);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref11, ref11);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref11, ref11);
+    src0 = __lasx_xvmaddwev_w_h(src0, src13, src13);
+    src0 = __lasx_xvmaddwod_w_h(src0, src13, src13);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref13, ref13);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref13, ref13);
+    src0 = __lasx_xvmaddwev_w_h(src0, src15, src15);
+    src0 = __lasx_xvmaddwod_w_h(src0, src15, src15);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref15, ref15);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref15, ref15);
+
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
 }
 
 int32_t x264_pixel_ssd_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                   uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    return sse_16width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+    uint32_t u_ssd;
+    intptr_t src_stride2 = i_src_stride << 1;
+    intptr_t ref_stride2 = i_ref_stride << 1;
+    intptr_t src_stride3 = i_src_stride + src_stride2;
+    intptr_t ref_stride3 = i_ref_stride + ref_stride2;
+    intptr_t src_stride4 = src_stride2 << 1;
+    intptr_t ref_stride4 = ref_stride2 << 1;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+
+    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
+               src0, src1, src2, src3, src4, src5, src6, src7);
+
+    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
+               ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    SSD_SUB_8(src0, src1, src2, src3, src4, src5, src6, src7,
+              ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    src0 = __lasx_xvmulwev_w_h(src1, src1);
+    src0 = __lasx_xvmaddwod_w_h(src0, src1, src1);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref1, ref1);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref1, ref1);
+    src0 = __lasx_xvmaddwev_w_h(src0, src3, src3);
+    src0 = __lasx_xvmaddwod_w_h(src0, src3, src3);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref3, ref3);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref3, ref3);
+    src0 = __lasx_xvmaddwev_w_h(src0, src5, src5);
+    src0 = __lasx_xvmaddwod_w_h(src0, src5, src5);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref5, ref5);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref5, ref5);
+    src0 = __lasx_xvmaddwev_w_h(src0, src7, src7);
+    src0 = __lasx_xvmaddwod_w_h(src0, src7, src7);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref7, ref7);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref7, ref7);
+
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
+}
+
+#undef SSD_LOAD_8
+#undef SSD_INSERT_8
+#undef SSD_SUB_8
+
+#define SSD_LOAD_8(_p_src, _src_stride, _src0, _src1, _src2,                       \
+                   _src3, _src4, _src5, _src6, _src7)                              \
+{                                                                                  \
+    _src0 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src1 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src2 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src3 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src4 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src5 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src6 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src7 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+}
+
+#define SSD_INSERT_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,       \
+                     _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)       \
+{                                                                                  \
+    _src0 = __lasx_xvilvl_b(_src0, _ref0);                                         \
+    _src1 = __lasx_xvilvl_b(_src1, _ref1);                                         \
+    _src2 = __lasx_xvilvl_b(_src2, _ref2);                                         \
+    _src3 = __lasx_xvilvl_b(_src3, _ref3);                                         \
+    _src4 = __lasx_xvilvl_b(_src4, _ref4);                                         \
+    _src5 = __lasx_xvilvl_b(_src5, _ref5);                                         \
+    _src6 = __lasx_xvilvl_b(_src6, _ref6);                                         \
+    _src7 = __lasx_xvilvl_b(_src7, _ref7);                                         \
 }
 
 int32_t x264_pixel_ssd_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                   uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    return sse_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
+
+    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+    p_src += i_src_stride;
+    SSD_LOAD_8(p_src, i_src_stride, src8, src9, src10, src11,
+               src12, src13, src14, src15);
+    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7);
+    p_ref += i_ref_stride;
+    SSD_LOAD_8(p_ref, i_ref_stride, ref8, ref9, ref10, ref11,
+               ref12, ref13, ref14, ref15);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
+                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
+    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
+    src4 = __lasx_xvpermi_q(src4, src5, 0x02);
+    src6 = __lasx_xvpermi_q(src6, src7, 0x02);
+    src8 = __lasx_xvpermi_q(src8, src9, 0x02);
+    src10 = __lasx_xvpermi_q(src10, src11, 0x02);
+    src12 = __lasx_xvpermi_q(src12, src13, 0x02);
+    src14 = __lasx_xvpermi_q(src14, src15, 0x02);
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
+    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
+    ref6 = __lasx_xvhsubw_hu_bu(src6, src6);
+    ref8 = __lasx_xvhsubw_hu_bu(src8, src8);
+    ref10 = __lasx_xvhsubw_hu_bu(src10, src10);
+    ref12 = __lasx_xvhsubw_hu_bu(src12, src12);
+    ref14 = __lasx_xvhsubw_hu_bu(src14, src14);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref6, ref6);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref6, ref6);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref8, ref8);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref8, ref8);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref10, ref10);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref10, ref10);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref12, ref12);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref12, ref12);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref14, ref14);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref14, ref14);
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
 }
 
 int32_t x264_pixel_ssd_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    return sse_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+
+    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
+    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
+    src4 = __lasx_xvpermi_q(src4, src5, 0x02);
+    src6 = __lasx_xvpermi_q(src6, src7, 0x02);
+
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
+    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
+    ref6 = __lasx_xvhsubw_hu_bu(src6, src6);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref6, ref6);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref6, ref6);
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
 }
 
 int32_t x264_pixel_ssd_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    return sse_8width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 4 );
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+
+    src0 = __lasx_xvldrepl_d( p_src, 0 );
+    p_src += i_src_stride;
+    src1 = __lasx_xvldrepl_d( p_src, 0 );
+    p_src += i_src_stride;
+    src2 = __lasx_xvldrepl_d( p_src, 0 );
+    p_src += i_src_stride;
+    src3 = __lasx_xvldrepl_d( p_src, 0 );
+
+    ref0 = __lasx_xvldrepl_d( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref1 = __lasx_xvldrepl_d( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref2 = __lasx_xvldrepl_d( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref3 = __lasx_xvldrepl_d( p_ref, 0 );
+
+    src0 = __lasx_xvilvl_b(src0, ref0);
+    src1 = __lasx_xvilvl_b(src1, ref1);
+    src2 = __lasx_xvilvl_b(src2, ref2);
+    src3 = __lasx_xvilvl_b(src3, ref3);
+    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
+    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
+}
+
+#undef SSD_LOAD_8
+
+#define SSD_LOAD_8(_p_src, _src_stride, _src0, _src1, _src2,                       \
+                   _src3, _src4, _src5, _src6, _src7)                              \
+{                                                                                  \
+    _src0 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src1 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src2 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src3 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src4 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src5 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src6 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src7 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
 }
 
 int32_t x264_pixel_ssd_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                   uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    return sse_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
+
+    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+    p_src += i_src_stride;
+    SSD_LOAD_8(p_src, i_src_stride, src8, src9, src10, src11,
+               src12, src13, src14, src15);
+    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7);
+    p_ref += i_ref_stride;
+    SSD_LOAD_8(p_ref, i_ref_stride, ref8, ref9, ref10, ref11,
+               ref12, ref13, ref14, ref15);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
+                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src2 = __lasx_xvilvl_d(src3, src2);
+    src4 = __lasx_xvilvl_d(src5, src4);
+    src6 = __lasx_xvilvl_d(src7, src6);
+    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
+    src4 = __lasx_xvpermi_q(src4, src6, 0x02);
+
+    src1 = __lasx_xvilvl_d(src9, src8);
+    src3 = __lasx_xvilvl_d(src11, src10);
+    src5 = __lasx_xvilvl_d(src13, src12);
+    src7 = __lasx_xvilvl_d(src15, src14);
+    src1 = __lasx_xvpermi_q(src1, src3, 0x02);
+    src5 = __lasx_xvpermi_q(src5, src7, 0x02);
+
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    ref0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src4 = __lasx_xvmaddwev_w_h(ref0, ref4, ref4);
+    ref4 = __lasx_xvmaddwod_w_h(src4, ref4, ref4);
+
+    ref1 = __lasx_xvhsubw_hu_bu(src1, src1);
+    ref5 = __lasx_xvhsubw_hu_bu(src5, src5);
+    src1 = __lasx_xvmaddwev_w_h(ref4, ref1, ref1);
+    src1 = __lasx_xvmaddwod_w_h(src1, ref1, ref1);
+    src1 = __lasx_xvmaddwev_w_h(src1, ref5, ref5);
+    src1 = __lasx_xvmaddwod_w_h(src1, ref5, ref5);
+    ref4 = __lasx_xvhaddw_d_w(src1, src1);
+    ref4 = __lasx_xvhaddw_q_d(ref4, ref4);
+    u_ssd = __lasx_xvpickve2gr_w(ref4, 0) + __lasx_xvpickve2gr_w(ref4, 4);
+
+    return u_ssd;
 }
 
 int32_t x264_pixel_ssd_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    return sse_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+
+    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src2 = __lasx_xvilvl_d(src3, src2);
+    src4 = __lasx_xvilvl_d(src5, src4);
+    src6 = __lasx_xvilvl_d(src7, src6);
+    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
+    src4 = __lasx_xvpermi_q(src4, src6, 0x02);
+
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
+    ref4 = __lasx_xvhaddw_d_w(src0, src0);
+    ref4 = __lasx_xvhaddw_q_d(ref4, ref4);
+    u_ssd = __lasx_xvpickve2gr_w(ref4, 0) + __lasx_xvpickve2gr_w(ref4, 4);
+
+    return u_ssd;
 }
 
 int32_t x264_pixel_ssd_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride )
 {
-    return sse_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 4 );
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+
+    src0 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src1 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src2 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src3 = __lasx_xvldrepl_w( p_src, 0 );
+
+    ref0 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref1 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref2 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref3 = __lasx_xvldrepl_w( p_ref, 0 );
+
+    src0 = __lasx_xvilvl_b(src0, ref0);
+    src1 = __lasx_xvilvl_b(src1, ref1);
+    src2 = __lasx_xvilvl_b(src2, ref2);
+    src3 = __lasx_xvilvl_b(src3, ref3);
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src2 = __lasx_xvilvl_d(src3, src2);
+    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
 }
+#undef SSD_LOAD_8
+#undef SSD_INSERT_8
 
 #define LASX_CALC_MSE_AVG_B( src, ref, var, sub )                          \
 {                                                                          \
     __m256i src_l0_m, src_l1_m;                                            \
     __m256i res_l0_m, res_l1_m;                                            \
                                                                            \
-    LASX_ILVLH_B_128SV( src, ref, src_l1_m, src_l0_m );                    \
-    LASX_HSUB_UB_2( src_l0_m, src_l1_m, res_l0_m, res_l1_m );              \
-    LASX_DP2ADD_W_H_2( var, res_l0_m, res_l0_m,                            \
-                       var, res_l1_m, res_l1_m, var, var );                \
+    src_l1_m = __lasx_xvilvl_b( src, ref );                                \
+    src_l0_m = __lasx_xvilvh_b( src, ref );                                \
+    DUP2_ARG2( __lasx_xvhsubw_hu_bu, src_l0_m, src_l0_m, src_l1_m,         \
+               src_l1_m, res_l0_m, res_l1_m );                             \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, var, res_l0_m, res_l0_m,               \
+               var, res_l1_m, res_l1_m, var, var );                        \
                                                                            \
     res_l0_m = __lasx_xvadd_h( res_l0_m, res_l1_m );                       \
     sub = __lasx_xvadd_h( sub, res_l0_m );                                 \
@@ -2451,8 +3279,8 @@ static inline uint32_t sse_diff_8width_lasx( uint8_t *p_src,
         ref3 = __lasx_xvldrepl_d( p_ref, 0 );
         p_ref += i_ref_stride;
 
-        LASX_PCKEV_D_4_128SV( src1, src0, src3, src2, ref1, ref0, ref3, ref2,
-                              src0, src1, ref0, ref1 );
+        DUP4_ARG2( __lasx_xvpickev_d, src1, src0, src3, src2, ref1, ref0, ref3, ref2,
+                   src0, src1, ref0, ref1 );
         src0 = __lasx_xvpermi_q( src1, src0, 0x20 );
         ref0 = __lasx_xvpermi_q( ref1, ref0, 0x20 );
         LASX_CALC_MSE_AVG_B( src0, ref0, var, avg );
@@ -2469,6 +3297,12 @@ static uint64_t avc_pixel_var16width_lasx( uint8_t *p_pix, int32_t i_stride,
                                            uint8_t i_height )
 {
     uint32_t u_sum = 0, u_sqr_out = 0, u_cnt;
+    int32_t i_stride_x2 = i_stride << 1;
+    int32_t i_stride_x3 = i_stride_x2 + i_stride;
+    int32_t i_stride_x4 = i_stride << 2;
+    int32_t i_stride_x5 = i_stride_x4 + i_stride;
+    int32_t i_stride_x6 = i_stride_x4 + i_stride_x2;
+    int32_t i_stride_x7 = i_stride_x4 + i_stride_x3;
     __m256i pix0, pix1, pix2, pix3, pix4, pix5, pix6, pix7;
     __m256i zero = __lasx_xvldi( 0 );
     __m256i add, pix_h, pix_l;
@@ -2478,14 +3312,17 @@ static uint64_t avc_pixel_var16width_lasx( uint8_t *p_pix, int32_t i_stride,
     src0 = __lasx_xvpermi_q( src1, src0, 0x20 );               \
     add = __lasx_xvhaddw_hu_bu( src0, src0 );                  \
     u_sum += LASX_HADD_UH_U32( add );                          \
-    LASX_ILVLH_B_128SV( zero, src0, pix_h, pix_l );            \
-    LASX_DP2ADD_W_H_2( sqr, pix_h, pix_h,                      \
-                       sqr, pix_l, pix_l, sqr, sqr );
+    pix_h =__lasx_xvilvl_b( zero, src0 );                      \
+    pix_l =__lasx_xvilvh_b( zero, src0 );                      \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, sqr, pix_h, pix_h,         \
+               sqr, pix_l, pix_l, sqr, sqr );
 
     for( u_cnt = ( i_height >> 3 ); u_cnt--; )
     {
-        LASX_LD_8( p_pix, i_stride, pix0, pix1, pix2, pix3,
-                   pix4, pix5, pix6, pix7 );
+        DUP4_ARG2( __lasx_xvldx, p_pix, 0, p_pix, i_stride, p_pix, i_stride_x2, p_pix,
+                   i_stride_x3, pix0, pix1, pix2, pix3 );
+        DUP4_ARG2( __lasx_xvldx, p_pix, i_stride_x4, p_pix, i_stride_x5, p_pix,
+                   i_stride_x6, p_pix, i_stride_x7, pix4, pix5, pix6, pix7 );
         p_pix += ( i_stride << 3 );
 
         LASX_PIXEL_VAR_16W( pix0, pix1 );
@@ -2511,14 +3348,15 @@ static uint64_t avc_pixel_var8width_lasx( uint8_t *p_pix, int32_t i_stride,
     __m256i sqr = __lasx_xvldi( 0 );
 
 #define LASX_PIXEL_VAR_8W( src0, src1, src2, src3 )            \
-    LASX_PCKEV_D_128SV( src1, src0, src0 );                    \
-    LASX_PCKEV_D_128SV( src3, src2, src1 );                    \
+    src0 = __lasx_xvpickev_d( src1, src0 );                    \
+    src1 = __lasx_xvpickev_d( src3, src2 );                    \
     src0 = __lasx_xvpermi_q( src1, src0, 0x20 );               \
     add = __lasx_xvhaddw_hu_bu( src0, src0 );                  \
     u_sum += LASX_HADD_UH_U32( add );                          \
-    LASX_ILVLH_B_128SV( zero, src0, pix_h, pix_l );            \
-    LASX_DP2ADD_W_H_2( sqr, pix_h, pix_h,                      \
-                       sqr, pix_l, pix_l, sqr, sqr );
+    pix_h = __lasx_xvilvl_b( zero, src0 );                     \
+    pix_l = __lasx_xvilvh_b( zero, src0 );                     \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, sqr, pix_h, pix_h,         \
+               sqr, pix_l, pix_l, sqr, sqr );
 
     for( u_cnt = ( i_height >> 3 ); u_cnt--; )
     {
diff --git a/common/loongarch/predict-c.c b/common/loongarch/predict-c.c
index d372235d..d28e3e62 100644
--- a/common/loongarch/predict-c.c
+++ b/common/loongarch/predict-c.c
@@ -25,7 +25,7 @@
  *****************************************************************************/
 
 #include "common/common.h"
-#include "generic_macros_lasx.h"
+#include "loongson_intrinsics.h"
 #include "predict.h"
 
 #if !HIGH_BIT_DEPTH
@@ -34,11 +34,12 @@ static inline void intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src,
                                                    int32_t i_stride )
 {
     uint32_t u_mask = 0x01010101;
+    int32_t i_stride_x4 = i_stride << 2;
     uint8_t *p_src1, *p_src2;
     __m256i sum, mask;
     v8u32 out;
 
-    sum = LASX_LD( p_src - i_stride );
+    sum = __lasx_xvldx( p_src, -i_stride );
     sum = __lasx_xvhaddw_hu_bu( sum, sum );
     out = ( v8u32 ) __lasx_xvhaddw_wu_hu( sum, sum );
     mask = __lasx_xvreplgr2vr_w( u_mask );
@@ -70,20 +71,20 @@ static inline void intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src,
 
     out = ( v8u32 ) __lasx_xvmul_w( ( __m256i ) out, mask );
 
-    LASX_ST_D( out, 0, p_src );
-    LASX_ST_D( out, 1, ( p_src + ( i_stride << 2 ) ) );
+    __lasx_xvstelm_d( out, p_src, 0, 0 );
+    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
     p_src += i_stride;
 
-    LASX_ST_D( out, 0, p_src );
-    LASX_ST_D( out, 1, ( p_src + ( i_stride << 2 ) ) );
+    __lasx_xvstelm_d( out, p_src, 0, 0 );
+    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
     p_src += i_stride;
 
-    LASX_ST_D( out, 0, p_src );
-    LASX_ST_D( out, 1, ( p_src + ( i_stride << 2 ) ) );
+    __lasx_xvstelm_d( out, p_src, 0, 0 );
+    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
     p_src += i_stride;
 
-    LASX_ST_D( out, 0, p_src );
-    LASX_ST_D( out, 1, ( p_src + ( i_stride << 2 ) ) );
+    __lasx_xvstelm_d( out, p_src, 0, 0 );
+    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
     p_src += i_stride;
 }
 
@@ -97,12 +98,14 @@ static inline void intra_predict_dc_4x4_lasx( uint8_t *p_src_top,
 {
     uint32_t u_row;
     uint32_t u_addition = 0;
+    int32_t i_dst_stride_x2 =  i_dst_stride << 1;
+    int32_t i_dst_stride_x3 =  i_dst_stride_x2 + i_dst_stride;
     __m256i src, store;
     v8u32 sum;
 
     if( is_left && is_above )
     {
-        src  = LASX_LD( p_src_top );
+        src  = __lasx_xvld( p_src_top, 0 );
         src = __lasx_xvhaddw_hu_bu( src, src );
         sum = ( v8u32 ) __lasx_xvhaddw_wu_hu( src, src );
         u_addition = sum[0];
@@ -127,7 +130,7 @@ static inline void intra_predict_dc_4x4_lasx( uint8_t *p_src_top,
     }
     else if( is_above )
     {
-        src  = LASX_LD( p_src_top );
+        src  = __lasx_xvld( p_src_top, 0 );
         src = __lasx_xvhaddw_hu_bu( src, src );
         src = __lasx_xvhaddw_wu_hu( src, src );
         src = __lasx_xvsrari_w( src, 2 );
@@ -141,7 +144,10 @@ static inline void intra_predict_dc_4x4_lasx( uint8_t *p_src_top,
         store = __lasx_xvreplgr2vr_b( u_addition );
     }
 
-    LASX_ST_W_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_w( store, p_dst, 0, 0 );
+    __lasx_xvstelm_w( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_w( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_w( store, p_dst + i_dst_stride_x3, 0, 0 );
 }
 
 static inline void intra_predict_dc_8x8_lasx( uint8_t *p_src_top,
@@ -150,10 +156,12 @@ static inline void intra_predict_dc_8x8_lasx( uint8_t *p_src_top,
                                               int32_t i_dst_stride )
 {
     __m256i src0, src1, store;
+    int32_t i_dst_stride_x2 =  i_dst_stride << 1;
+    int32_t i_dst_stride_x3 =  i_dst_stride_x2 + i_dst_stride;
 
     src0 = __lasx_xvldrepl_d( p_src_top, 0 );
     src1 = __lasx_xvldrepl_d( p_src_left, 0 );
-    LASX_PCKEV_D_128SV( src1, src0, src0 );
+    src0 = __lasx_xvpickev_d( src1, src0 );
 
     src0 = __lasx_xvhaddw_hu_bu( src0, src0 );
     src0 = __lasx_xvhaddw_wu_hu( src0, src0 );
@@ -163,9 +171,15 @@ static inline void intra_predict_dc_8x8_lasx( uint8_t *p_src_top,
     src0 = __lasx_xvsrari_w( src0, 4 );
     store = __lasx_xvrepl128vei_b( src0, 0 );
 
-    LASX_ST_D_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
     p_dst += ( i_dst_stride  << 2);
-    LASX_ST_D_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
 }
 
 static inline void intra_predict_dc_16x16_lasx( uint8_t *p_src_top,
@@ -179,12 +193,14 @@ static inline void intra_predict_dc_16x16_lasx( uint8_t *p_src_top,
     uint32_t u_row;
     int32_t i_index = 0;
     uint32_t u_addition = 0;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
     __m256i src, store;
     v4u64 sum;
 
     if( is_left && is_above )
     {
-        src  = LASX_LD( p_src_top );
+        src  = __lasx_xvld( p_src_top, 0 );
         src = __lasx_xvhaddw_hu_bu( src, src );
         src = __lasx_xvhaddw_wu_hu( src, src );
         src = __lasx_xvhaddw_du_wu( src, src );
@@ -226,7 +242,7 @@ static inline void intra_predict_dc_16x16_lasx( uint8_t *p_src_top,
     }
     else if( is_above )
     {
-        src  = LASX_LD( p_src_top );
+        src  = __lasx_xvld( p_src_top, 0 );
         src = __lasx_xvhaddw_hu_bu( src, src );
         src = __lasx_xvhaddw_wu_hu( src, src );
         src = __lasx_xvhaddw_du_wu( src, src );
@@ -243,13 +259,41 @@ static inline void intra_predict_dc_16x16_lasx( uint8_t *p_src_top,
         store = __lasx_xvreplgr2vr_b( u_addition );
     }
 
-    LASX_ST_Q_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
     p_dst += ( i_dst_stride  << 2);
-    LASX_ST_Q_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
     p_dst += ( i_dst_stride  << 2);
-    LASX_ST_Q_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
     p_dst += ( i_dst_stride  << 2);
-    LASX_ST_Q_4( store, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
 }
 
 static inline void intra_predict_horiz_16x16_lasx( uint8_t *p_src,
@@ -277,13 +321,17 @@ static inline void intra_predict_horiz_16x16_lasx( uint8_t *p_src,
         src2 = __lasx_xvreplgr2vr_b( u_inp2 );
         src3 = __lasx_xvreplgr2vr_b( u_inp3 );
 
-        LASX_ST_Q( src0, 0, p_dst );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 8, 1 );
         p_dst += i_dst_stride;
-        LASX_ST_Q( src1, 0, p_dst );
+        __lasx_xvstelm_d( src1, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src1, p_dst, 8, 1 );
         p_dst += i_dst_stride;
-        LASX_ST_Q( src2, 0, p_dst );
+        __lasx_xvstelm_d( src2, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src2, p_dst, 8, 1 );
         p_dst += i_dst_stride;
-        LASX_ST_Q( src3, 0, p_dst );
+        __lasx_xvstelm_d( src3, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src3, p_dst, 8, 1 );
         p_dst += i_dst_stride;
     }
 }
@@ -310,13 +358,13 @@ static inline void intra_predict_horiz_8x8_lasx( uint8_t *p_src,
     src2 = __lasx_xvreplgr2vr_b( u_inp2 );
     src3 = __lasx_xvreplgr2vr_b( u_inp3 );
 
-    LASX_ST_D( src0, 0, p_dst );
+    __lasx_xvstelm_d( src0, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_D( src1, 0, p_dst );
+    __lasx_xvstelm_d( src1, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_D( src2, 0, p_dst );
+    __lasx_xvstelm_d( src2, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_D( src3, 0, p_dst );
+    __lasx_xvstelm_d( src3, p_dst, 0, 0 );
     p_dst += i_dst_stride;
 
     u_inp0 = p_src[0];
@@ -333,13 +381,13 @@ static inline void intra_predict_horiz_8x8_lasx( uint8_t *p_src,
     src2 = __lasx_xvreplgr2vr_b( u_inp2 );
     src3 = __lasx_xvreplgr2vr_b( u_inp3 );
 
-    LASX_ST_D( src0, 0, p_dst );
+    __lasx_xvstelm_d( src0, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_D( src1, 0, p_dst );
+    __lasx_xvstelm_d( src1, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_D( src2, 0, p_dst );
+    __lasx_xvstelm_d( src2, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_D( src3, 0, p_dst );
+    __lasx_xvstelm_d( src3, p_dst, 0, 0 );
     p_dst += i_dst_stride;
 }
 
@@ -365,13 +413,13 @@ static inline void intra_predict_horiz_4x4_lasx( uint8_t *p_src,
     src2 = __lasx_xvreplgr2vr_b( u_inp2 );
     src3 = __lasx_xvreplgr2vr_b( u_inp3 );
 
-    LASX_ST_W( src0, 0, p_dst );
+    __lasx_xvstelm_w( src0, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_W( src1, 0, p_dst );
+    __lasx_xvstelm_w( src1, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_W( src2, 0, p_dst );
+    __lasx_xvstelm_w( src2, p_dst, 0, 0 );
     p_dst += i_dst_stride;
-    LASX_ST_W( src3, 0, p_dst );
+    __lasx_xvstelm_w( src3, p_dst, 0, 0 );
     p_dst += i_dst_stride;
 }
 
@@ -380,15 +428,45 @@ static inline void intra_predict_vert_16x16_lasx( uint8_t *p_src,
                                                   int32_t i_dst_stride )
 {
     __m256i src;
-    src  = LASX_LD( p_src );
-
-    LASX_ST_Q_4( src, 0, 0, 0, 0, p_dst, i_dst_stride );
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
+    src  = __lasx_xvld( p_src, 0 );
+
+    __lasx_xvstelm_d( src, p_dst, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
     p_dst += ( i_dst_stride  << 2);
-    LASX_ST_Q_4( src, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( src, p_dst, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
     p_dst += ( i_dst_stride  << 2);
-    LASX_ST_Q_4( src, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( src, p_dst, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
     p_dst += ( i_dst_stride  << 2);
-    LASX_ST_Q_4( src, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( src, p_dst, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
 }
 
 static inline void intra_predict_vert_8x8_lasx( uint8_t *p_src,
@@ -396,12 +474,20 @@ static inline void intra_predict_vert_8x8_lasx( uint8_t *p_src,
                                                 int32_t i_dst_stride )
 {
     __m256i out;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
 
     out = __lasx_xvldrepl_d( p_src, 0 );
 
-    LASX_ST_D_4( out, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( out, p_dst, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x3, 0, 0 );
     p_dst += ( i_dst_stride << 2 );
-    LASX_ST_D_4( out, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_d( out, p_dst, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x3, 0, 0 );
 }
 
 static inline void intra_predict_vert_4x4_lasx( uint8_t *p_src,
@@ -409,10 +495,15 @@ static inline void intra_predict_vert_4x4_lasx( uint8_t *p_src,
                                                 int32_t i_dst_stride )
 {
     __m256i out;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
 
     out = __lasx_xvldrepl_w( p_src, 0 );
 
-    LASX_ST_W_4( out, 0, 0, 0, 0, p_dst, i_dst_stride );
+    __lasx_xvstelm_w( out, p_dst, 0, 0 );
+    __lasx_xvstelm_w( out, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_w( out, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_w( out, p_dst + i_dst_stride_x3, 0, 0 );
 }
 
 void x264_intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src )
diff --git a/common/loongarch/quant-c.c b/common/loongarch/quant-c.c
index 6167672a..e590c71e 100644
--- a/common/loongarch/quant-c.c
+++ b/common/loongarch/quant-c.c
@@ -25,80 +25,11 @@
  *****************************************************************************/
 
 #include "common/common.h"
-#include "generic_macros_lasx.h"
+#include "loongson_intrinsics.h"
 #include "quant.h"
 
 #if !HIGH_BIT_DEPTH
 
-static inline int32_t avc_coeff_last64_lasx( int16_t *p_src )
-{
-    __m256i src0, src1, src2, src3;
-    __m256i tmp0, tmp1, tmp2, tmp3;
-    v16i16 result;
-    v32u8 mask = { 1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128,
-                   1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128 };
-
-    LASX_LD_4( p_src, 16, src0, src1, src2, src3 );
-
-    tmp0 = __lasx_xvseqi_h( src0, 0 );
-    tmp1 = __lasx_xvseqi_h( src1, 0 );
-    tmp2 = __lasx_xvseqi_h( src2, 0 );
-    tmp3 = __lasx_xvseqi_h( src3, 0 );
-
-    tmp0 = __lasx_xvpickev_b( tmp1, tmp0 );
-    tmp1 = __lasx_xvpickev_b( tmp3, tmp2 );
-
-    tmp0 = tmp0 & (__m256i)mask;
-    tmp1 = tmp1 & (__m256i)mask;
-
-    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
-
-    tmp1 = __lasx_xvhaddw_h_b( tmp1, tmp1 );
-    tmp1 = __lasx_xvpickev_b( tmp1, tmp1 );
-    tmp1 = __lasx_xvhaddw_h_b( tmp1, tmp1 );
-    tmp1 = __lasx_xvpickev_b( tmp1, tmp1 );
-    tmp1 = __lasx_xvhaddw_h_b( tmp1, tmp1 );
-    tmp1 = __lasx_xvpickev_b( tmp1, tmp1 );
-
-    tmp2 = __lasx_xvpermi_q( tmp0, tmp0, 0x11 );
-    tmp0 = __lasx_xvilvl_b( tmp2, tmp0 );
-    tmp3 = __lasx_xvpermi_q( tmp1, tmp1, 0x11 );
-    tmp1 = __lasx_xvilvl_b( tmp3, tmp1 );
-    tmp0 = __lasx_xvpackev_w( tmp1, tmp0 );
-    result = ( v16i16 ) __lasx_xvclo_d( tmp0 );
-
-    return ( 63 - result[0] );
-}
-
-static inline int32_t avc_coeff_last16_lasx( int16_t *p_src )
-{
-    __m256i src0, tmp0, out0, out1;
-    v16i16 result;
-    v32u8 mask = { 1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128,
-                   1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128 };
-
-    src0 = LASX_LD( p_src );
-    out0 = __lasx_xvseqi_h( src0, 0 );
-    out1 = __lasx_xvpermi_q(out0, out0, 0x11);
-    tmp0 = __lasx_xvpickev_b( out1, out0 );
-    tmp0 = tmp0 & (__m256i)mask;
-
-    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvhaddw_h_b( tmp0, tmp0 );
-    tmp0 = __lasx_xvpickev_b( tmp0, tmp0 );
-    result = ( v16i16 ) __lasx_xvclo_h( tmp0 );
-
-    return ( 15 - result[0] );
-}
-
 static inline int32_t avc_quant_4x4_lasx( int16_t *p_dct,
                                           uint16_t *p_mf,
                                           uint16_t *p_bias )
@@ -111,30 +42,33 @@ static inline int32_t avc_quant_4x4_lasx( int16_t *p_dct,
     __m256i bias, bias0, bias1;
     __m256i tmp;
 
-    dct = LASX_LD( p_dct );
-    bias = LASX_LD( p_bias );
-    mf = LASX_LD( p_mf );
+    dct = __lasx_xvld( p_dct, 0 );
+    bias = __lasx_xvld( p_bias, 0 );
+    mf = __lasx_xvld( p_mf, 0 );
 
     dct_mask = __lasx_xvslei_h( dct, 0 );
 
-    LASX_UNPCK_SH_128SV( dct, dct0, dct1 );
-    LASX_ILVLH_H_128SV( zero, bias, bias1, bias0 );
-    LASX_ILVLH_H_128SV( zero, mf, mf1, mf0 );
+    LASX_UNPCK_SH( dct, dct0, dct1 );
+    bias0 = __lasx_xvilvl_h( zero, bias );
+    bias1 = __lasx_xvilvh_h( zero, bias );
+    mf0 = __lasx_xvilvl_h( zero, mf );
+    mf1 = __lasx_xvilvh_h( zero, mf );
 
     dct0 = __lasx_xvadda_w( dct0, bias0 );
     dct1 = __lasx_xvadda_w( dct1, bias1 );
     dct0 = __lasx_xvmul_w(dct0, mf0);
     dct1 = __lasx_xvmul_w(dct1, mf1);
 
-    LASX_SRAI_W_2( dct0, dct1, dct0, dct1, 16);
-    dct = __lasx_xvpickev_h( dct1, dct0 );
+    dct = __lasx_xvsrani_h_w(dct1, dct0, 16);
+
     tmp = __lasx_xvhaddw_w_h( dct, dct );
-    non_zero = LASX_HADD_SW_S32( tmp );
+    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
+    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
+    non_zero = __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
 
     dct0 = __lasx_xvsub_h( zero, dct );
-    LASX_BMNZ( dct, dct0, dct_mask, dct );
-
-    LASX_ST( dct, p_dct );
+    dct = __lasx_xvbitsel_v( dct, dct0, dct_mask );
+    __lasx_xvst( dct, p_dct, 0 );
 
     return !!non_zero;
 }
@@ -151,19 +85,26 @@ static inline int32_t avc_quant_8x8_lasx( int16_t *p_dct,
     __m256i bias0, bias1, bias0_0, bias0_1, bias1_0, bias1_1;
     __m256i tmp;
 
-    LASX_LD_2( p_dct, 16, dct0, dct1 );
-    LASX_LD_2( p_bias, 16, bias0, bias1 );
-    LASX_LD_2( p_mf, 16, mf0, mf1 );
+    dct0 = __lasx_xvld( p_dct, 0 );
+    dct1 = __lasx_xvld( p_dct, 32 );
+    bias0 = __lasx_xvld( p_bias, 0 );
+    bias1 = __lasx_xvld( p_bias, 32 );
+    mf0 = __lasx_xvld( p_mf, 0 );
+    mf1 = __lasx_xvld( p_mf, 32 );
 
     dct_mask0 = __lasx_xvslei_h( dct0, 0 );
     dct_mask1 = __lasx_xvslei_h( dct1, 0 );
 
-    LASX_UNPCK_SH_128SV( dct0, dct0_0, dct0_1 );
-    LASX_UNPCK_SH_128SV( dct1, dct1_0, dct1_1 );
-    LASX_ILVLH_H_2_128SV( zero, bias0, zero, bias1,
-                          bias0_1, bias0_0, bias1_1, bias1_0 );
-    LASX_ILVLH_H_2_128SV( zero, mf0, zero, mf1,
-                          mf0_1, mf0_0, mf1_1, mf1_0 );
+    LASX_UNPCK_SH( dct0, dct0_0, dct0_1 );
+    LASX_UNPCK_SH( dct1, dct1_0, dct1_1 );
+    bias0_0 = __lasx_xvilvl_h( zero, bias0 );
+    bias0_1 = __lasx_xvilvh_h( zero, bias0 );
+    bias1_0 = __lasx_xvilvl_h( zero, bias1 );
+    bias1_1 = __lasx_xvilvh_h( zero, bias1 );
+    mf0_0 = __lasx_xvilvl_h( zero, mf0 );
+    mf0_1 = __lasx_xvilvh_h( zero, mf0 );
+    mf1_0 = __lasx_xvilvl_h( zero, mf1 );
+    mf1_1 = __lasx_xvilvh_h( zero, mf1 );
 
     dct0_0 = __lasx_xvadda_w( dct0_0, bias0_0 );
     dct0_1 = __lasx_xvadda_w( dct0_1, bias0_1 );
@@ -175,36 +116,44 @@ static inline int32_t avc_quant_8x8_lasx( int16_t *p_dct,
     dct1_0 = __lasx_xvmul_w( dct1_0, mf1_0 );
     dct1_1 = __lasx_xvmul_w( dct1_1, mf1_1 );
 
-    LASX_SRAI_W_4( dct0_0, dct0_1, dct1_0, dct1_1,
-                   dct0_0, dct0_1, dct1_0, dct1_1, 16);
-    dct0 = __lasx_xvpickev_h( dct0_1, dct0_0 );
-    dct1 = __lasx_xvpickev_h( dct1_1, dct1_0 );
+    dct0 = __lasx_xvsrani_h_w( dct0_1, dct0_0, 16 );
+    dct1 = __lasx_xvsrani_h_w( dct1_1, dct1_0, 16 );
 
     tmp = __lasx_xvadd_h( dct0, dct1 );
     tmp = __lasx_xvhaddw_w_h( tmp, tmp );
-    non_zero = LASX_HADD_SW_S32( tmp );
+    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
+    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
+    non_zero = __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
 
     dct0_0 = __lasx_xvsub_h( zero, dct0 );
     dct1_0 = __lasx_xvsub_h( zero, dct1 );
-    LASX_BMNZ( dct0, dct0_0, dct_mask0, dct0 );
-    LASX_BMNZ( dct1, dct1_0, dct_mask1, dct1 );
+    dct0 = __lasx_xvbitsel_v( dct0, dct0_0, dct_mask0 );
+    dct1 = __lasx_xvbitsel_v( dct1, dct1_0, dct_mask1 );
 
-    LASX_ST_2( dct0, dct1, p_dct, 16 );
+    __lasx_xvst( dct0, p_dct, 0 );
+    __lasx_xvst( dct1, p_dct, 32 );
 
     /* next part */
-    LASX_LD_2( p_dct + 32, 16, dct0, dct1 );
-    LASX_LD_2( p_bias + 32, 16, bias0, bias1 );
-    LASX_LD_2( p_mf + 32, 16, mf0, mf1 );
+    dct0 = __lasx_xvld( p_dct, 64 );
+    dct1 = __lasx_xvld( p_dct, 96 );
+    bias0 = __lasx_xvld( p_bias, 64 );
+    bias1 = __lasx_xvld( p_bias, 96 );
+    mf0 = __lasx_xvld( p_mf, 64 );
+    mf1 = __lasx_xvld( p_mf, 96 );
 
     dct_mask0 = __lasx_xvslei_h( dct0, 0 );
     dct_mask1 = __lasx_xvslei_h( dct1, 0 );
 
-    LASX_UNPCK_SH_128SV( dct0, dct0_0, dct0_1 );
-    LASX_UNPCK_SH_128SV( dct1, dct1_0, dct1_1 );
-    LASX_ILVLH_H_2_128SV( zero, bias0, zero, bias1,
-                          bias0_1, bias0_0, bias1_1, bias1_0 );
-    LASX_ILVLH_H_2_128SV( zero, mf0, zero, mf1,
-                          mf0_1, mf0_0, mf1_1, mf1_0 );
+    LASX_UNPCK_SH( dct0, dct0_0, dct0_1 );
+    LASX_UNPCK_SH( dct1, dct1_0, dct1_1 );
+    bias0_0 = __lasx_xvilvl_h( zero, bias0 );
+    bias0_1 = __lasx_xvilvh_h( zero, bias0 );
+    bias1_0 = __lasx_xvilvl_h( zero, bias1 );
+    bias1_1 = __lasx_xvilvh_h( zero, bias1 );
+    mf0_0 = __lasx_xvilvl_h( zero, mf0 );
+    mf0_1 = __lasx_xvilvh_h( zero, mf0 );
+    mf1_0 = __lasx_xvilvl_h( zero, mf1 );
+    mf1_1 = __lasx_xvilvh_h( zero, mf1 );
 
     dct0_0 = __lasx_xvadda_w( dct0_0, bias0_0 );
     dct0_1 = __lasx_xvadda_w( dct0_1, bias0_1 );
@@ -216,33 +165,91 @@ static inline int32_t avc_quant_8x8_lasx( int16_t *p_dct,
     dct1_0 = __lasx_xvmul_w( dct1_0, mf1_0 );
     dct1_1 = __lasx_xvmul_w( dct1_1, mf1_1 );
 
-    LASX_SRAI_W_4( dct0_0, dct0_1, dct1_0, dct1_1,
-                   dct0_0, dct0_1, dct1_0, dct1_1, 16);
-    dct0 = __lasx_xvpickev_h( dct0_1, dct0_0 );
-    dct1 = __lasx_xvpickev_h( dct1_1, dct1_0 );
+    dct0 = __lasx_xvsrani_h_w( dct0_1, dct0_0, 16 );
+    dct1 = __lasx_xvsrani_h_w( dct1_1, dct1_0, 16 );
 
     tmp = __lasx_xvadd_h( dct0, dct1 );
     tmp = __lasx_xvhaddw_w_h( tmp, tmp );
-    non_zero += LASX_HADD_SW_S32( tmp );
+    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
+    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
+    non_zero += __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
 
     dct0_0 = __lasx_xvsub_h( zero, dct0 );
     dct1_0 = __lasx_xvsub_h( zero, dct1 );
-    LASX_BMNZ( dct0, dct0_0, dct_mask0, dct0 );
-    LASX_BMNZ( dct1, dct1_0, dct_mask1, dct1 );
+    dct0 = __lasx_xvbitsel_v( dct0, dct0_0, dct_mask0 );
+    dct1 = __lasx_xvbitsel_v( dct1, dct1_0, dct_mask1 );
 
-    LASX_ST_2( dct0, dct1, p_dct + 32, 16 );
+    __lasx_xvst( dct0, p_dct, 64 );
+    __lasx_xvst( dct1, p_dct, 96 );
 
     return !!non_zero;
 }
 
 int32_t x264_coeff_last16_lasx( int16_t *p_src )
 {
-    return avc_coeff_last16_lasx( p_src );
+    __m256i src0, tmp0, tmp1;
+    __m256i one = __lasx_xvldi(1);
+    int32_t result;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    tmp0 = __lasx_xvssrlni_bu_h(src0, src0, 0);
+    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
+    tmp1 = __lasx_xvsle_bu(one, tmp0);
+    tmp0 = __lasx_xvssrlni_bu_h(tmp1, tmp1, 4);
+    tmp1 = __lasx_xvclz_d(tmp0);
+    result = __lasx_xvpickve2gr_w(tmp1, 0);
+
+    return 15 - (result >> 2);
+}
+
+int32_t x264_coeff_last15_lasx( int16_t *psrc )
+{
+    __m256i src0, tmp0, tmp1;
+    __m256i one = __lasx_xvldi(1);
+    int32_t result;
+
+    src0 = __lasx_xvld( psrc, -2 );
+    tmp0 = __lasx_xvssrlni_bu_h(src0, src0, 0);
+    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
+    tmp1 = __lasx_xvsle_bu(one, tmp0);
+    tmp0 = __lasx_xvssrlni_bu_h(tmp1, tmp1, 4);
+    tmp1 = __lasx_xvclz_d(tmp0);
+    result = __lasx_xvpickve2gr_w(tmp1, 0);
+
+    return 14 - (result >> 2);
 }
 
 int32_t x264_coeff_last64_lasx( int16_t *p_src )
 {
-    return avc_coeff_last64_lasx( p_src );
+    int32_t result;
+    __m256i src0, src1, src2, src3;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    __m256i one = __lasx_xvldi(1);
+    __m256i const_8 = __lasx_xvldi(0x408);
+    __m256i const_1 = __lasx_xvldi(0x401);
+    __m256i shift = {0x0000000400000000, 0x0000000500000001,
+                     0x0000000600000002, 0x0000000700000003};
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, 32);
+    src2 = __lasx_xvld( p_src, 64);
+    src3 = __lasx_xvld( p_src, 96);
+
+    tmp0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
+    tmp1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
+    tmp2 = __lasx_xvsle_bu(one, tmp0);
+    tmp3 = __lasx_xvsle_bu(one, tmp1);
+    tmp0 = __lasx_xvssrlni_bu_h(tmp3, tmp2, 4);
+    tmp0 = __lasx_xvperm_w(tmp0, shift);
+    tmp1 = __lasx_xvclz_w(tmp0);
+    tmp0 = __lasx_xvssrlni_hu_w(tmp1, tmp1, 2);
+    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
+
+    tmp1 = __lasx_xvsub_h(const_8, tmp0);
+    tmp0 = __lasx_xvsll_h(const_1, tmp1);
+    tmp0 = __lasx_xvssrlni_bu_h(tmp0, tmp0, 1);
+    tmp1 = __lasx_xvclz_d(tmp0);
+    result = __lasx_xvpickve2gr_w(tmp1, 0);
+    return 63 - result;
 }
 
 int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
diff --git a/common/loongarch/quant.h b/common/loongarch/quant.h
index da11ed5e..50a2fee9 100644
--- a/common/loongarch/quant.h
+++ b/common/loongarch/quant.h
@@ -31,6 +31,8 @@
 int32_t x264_coeff_last64_lasx( int16_t *p_src );
 #define x264_coeff_last16_lasx x264_template(coeff_last16_lasx)
 int32_t x264_coeff_last16_lasx( int16_t *p_src );
+#define x264_coeff_last15_lasx x264_template(coeff_last15_lasx)
+int32_t x264_coeff_last15_lasx( int16_t *p_src );
 #define x264_quant_4x4_lasx x264_template(quant_4x4_lasx)
 int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
 #define x264_quant_4x4x4_lasx x264_template(quant_4x4x4_lasx)
diff --git a/common/quant.c b/common/quant.c
index 324f654e..4949144a 100644
--- a/common/quant.c
+++ b/common/quant.c
@@ -814,6 +814,7 @@ void x264_quant_init( x264_t *h, int cpu, x264_quant_function_t *pf )
         pf->quant_4x4      = x264_quant_4x4_lasx;
         pf->quant_4x4x4    = x264_quant_4x4x4_lasx;
         pf->quant_8x8      = x264_quant_8x8_lasx;
+        pf->coeff_last[ DCT_LUMA_AC] = x264_coeff_last15_lasx;
         pf->coeff_last[DCT_LUMA_4x4] = x264_coeff_last16_lasx;
         pf->coeff_last[DCT_LUMA_8x8] = x264_coeff_last64_lasx;
     }
