diff --git a/Makefile b/Makefile
index c1a9c5b0..9b27df65 100644
--- a/Makefile
+++ b/Makefile
@@ -207,11 +207,11 @@ SRCASM_X += common/loongarch/mc-a.S \
            common/loongarch/pixel-a.S \
            common/loongarch/dct-a.S \
            common/loongarch/quant-a.S \
-           common/loongarch/predict-a.S
+           common/loongarch/predict-a.S \
+           common/loongarch/deblock-a.S
 
 SRCS_X += common/loongarch/pixel-c.c \
           common/loongarch/predict-c.c \
-          common/loongarch/quant-c.c \
           common/loongarch/mc-c.c \
           common/loongarch/deblock-c.c
 
diff --git a/common/deblock.c b/common/deblock.c
index a7985730..152c55ad 100644
--- a/common/deblock.c
+++ b/common/deblock.c
@@ -811,6 +811,8 @@ void x264_deblock_init( int cpu, x264_deblock_function_t *pf, int b_mbaff )
     {
         pf->deblock_luma[1] = x264_deblock_v_luma_lasx;
         pf->deblock_luma[0] = x264_deblock_h_luma_lasx;
+        pf->deblock_luma_intra[1] = x264_deblock_v_luma_intra_lasx;
+        pf->deblock_luma_intra[0] = x264_deblock_h_luma_intra_lasx;
         pf->deblock_strength = x264_deblock_strength_lasx;
     }
 #endif
diff --git a/common/loongarch/deblock-a.S b/common/loongarch/deblock-a.S
new file mode 100644
index 00000000..44df8b21
--- /dev/null
+++ b/common/loongarch/deblock-a.S
@@ -0,0 +1,670 @@
+/*****************************************************************************
+ * deblock-a.S: loongarch deblock functions
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+
+#if !HIGH_BIT_DEPTH
+
+/*Transpose 16 * 6 block with byte elements in vectors*/
+.macro LASX_TRANSPOSE in0,  in1,  in2,  in3,  in4,  in5,  in6,  in7, \
+                      in8,  in9,  in10, in11, in12, in13, in14, in15,\
+                      tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,\
+                      out0, out1, out2, out3, out4, out5
+    xvilvl.b   \tmp0,    \in1,     \in0
+    xvilvl.b   \tmp1,    \in3,     \in2
+    xvilvl.b   \tmp2,    \in5,     \in4
+    xvilvl.b   \tmp3,    \in7,     \in6
+    xvilvl.b   \tmp4,    \in9,     \in8
+    xvilvl.b   \tmp5,    \in11,    \in10
+    xvilvl.b   \tmp6,    \in13,    \in12
+    xvilvl.b   \tmp7,    \in15,    \in14
+    xvpermi.d  \tmp0,    \tmp0,    0xD8
+    xvpermi.d  \tmp1,    \tmp1,    0xD8
+    xvpermi.d  \tmp2,    \tmp2,    0xD8
+    xvpermi.d  \tmp3,    \tmp3,    0xD8
+    xvpermi.d  \tmp4,    \tmp4,    0xD8
+    xvpermi.d  \tmp5,    \tmp5,    0xD8
+    xvpermi.d  \tmp6,    \tmp6,    0xD8
+    xvpermi.d  \tmp7,    \tmp7,    0xD8
+    xvilvl.h   \out0,    \tmp1,    \tmp0
+    xvilvl.h   \out1,    \tmp3,    \tmp2
+    xvilvl.h   \out2,    \tmp5,    \tmp4
+    xvilvl.h   \out3,    \tmp7,    \tmp6
+    xvilvl.w   \tmp0,    \out1,    \out0
+    xvilvh.w   \tmp1,    \out1,    \out0
+    xvilvl.w   \tmp2,    \out3,    \out2
+    xvilvh.w   \tmp3,    \out3,    \out2
+    xvilvl.d   \out0,    \tmp2,    \tmp0
+    xvilvh.d   \out1,    \tmp2,    \tmp0
+    xvilvl.d   \out2,    \tmp3,    \tmp1
+    xvilvh.d   \out3,    \tmp3,    \tmp1
+    xvpermi.d  \out4,    \out0,    0x4E
+    xvpermi.d  \out5,    \out1,    0x4E
+.endm
+
+/*
+ * void deblock_h_luma_lasx(Pixel *pix, intptr_t stride, int alpha,
+ *                          int beta, int8_t *tc0)
+ */
+function deblock_h_luma_lasx
+    slli.d          t0,    a1,    1
+    slli.d          t2,    a1,    2
+
+    xvldrepl.w      xr1,   a4,    0
+    add.d           t1,    t0,    a1
+    xvreplgr2vr.b   xr2,   a3
+    xvilvl.b        xr1,   xr1,   xr1
+
+    // Store registers to the stack
+    addi.d          sp,    sp,    -64
+    fst.d           f24,   sp,    0
+    fst.d           f25,   sp,    8
+    fst.d           f26,   sp,    16
+    fst.d           f27,   sp,    24
+    fst.d           f28,   sp,    32
+    fst.d           f29,   sp,    40
+    fst.d           f30,   sp,    48
+    fst.d           f31,   sp,    56
+
+    // Load data from pix
+    addi.d          t4,    a0,    -3
+    FLDD_LOADX_4    t4,    a1,    t0,   t1,   f10, f11, f12, f13
+    add.d           t5,    t4,    t2
+    FLDD_LOADX_4    t5,    a1,    t0,   t1,   f14, f15, f16, f17
+    add.d           t5,    t5,    t2
+    FLDD_LOADX_4    t5,    a1,    t0,   t1,   f20, f21, f22, f23
+    add.d           t6,    t5,    t2
+    FLDD_LOADX_4    t6,    a1,    t0,   t1,   f24, f25, f26, f27
+
+    LASX_TRANSPOSE  xr10, xr11, xr12, xr13, xr14, xr15, xr16, xr17,  \
+                    xr20, xr21, xr22, xr23, xr24, xr25, xr26, xr27,  \
+                    xr8,  xr9,  xr18, xr19, xr28, xr29, xr30, xr31,  \
+                    xr10, xr11, xr12, xr13, xr14, xr15
+
+    xvilvl.h        xr1,   xr1,   xr1
+    vext2xv.hu.bu   xr20,  xr10
+    vext2xv.hu.bu   xr21,  xr11
+    vext2xv.hu.bu   xr22,  xr12
+    vext2xv.hu.bu   xr23,  xr13
+    vext2xv.hu.bu   xr24,  xr14
+    vext2xv.hu.bu   xr25,  xr15
+    vext2xv.h.b     xr3,   xr1
+
+    xvadd.h         xr26,  xr22,  xr23
+    xvsrari.h       xr26,  xr26,  1
+    xvneg.h         xr4,   xr3
+    xvadd.h         xr27,  xr20,  xr26
+    xvadd.h         xr28,  xr25,  xr26
+    xvsub.h         xr29,  xr23,  xr22
+    xvsrai.h        xr27,  xr27,  1
+    xvsrai.h        xr28,  xr28,  1
+    xvslli.h        xr29,  xr29,  2
+    xvsub.h         xr30,  xr21,  xr24
+    xvsub.h         xr27,  xr27,  xr21
+    xvsub.h         xr28,  xr28,  xr24
+    xvadd.h         xr29,  xr29,  xr30
+    xvclip.h        xr27,  xr27,  xr4,   xr3
+    xvclip.h        xr28,  xr28,  xr4,   xr3
+
+    xvpickev.b      xr16,  xr25,  xr20
+    xvpickev.b      xr17,  xr23,  xr22
+    xvabsd.bu       xr5,   xr16,  xr17
+    xvaddi.hu       xr6,   xr3,   1
+    xvslt.bu        xr5,   xr5,   xr2
+    xvilvl.b        xr30,  xr5,   xr5
+    xvilvh.b        xr31,  xr5,   xr5
+    xvbitsel.v      xr3,   xr3,   xr6,   xr30
+
+    xvsrari.h       xr29,  xr29,  3
+    xvaddi.hu       xr6,   xr3,   1
+    xvbitsel.v      xr3,   xr3,   xr6,   xr31
+    xvneg.h         xr4,   xr3
+
+    xvclip.h        xr29,  xr29,  xr4,   xr3
+    xvadd.h         xr30,  xr21,  xr27
+    xvadd.h         xr18,  xr24,  xr28
+    xvadd.h         xr19,  xr22,  xr29
+    xvsub.h         xr26,  xr23,  xr29
+    xvssrarni.bu.h  xr26,  xr19,  0
+
+    xvpickev.b      xr25,  xr18,  xr30
+    xvpickev.b      xr27,  xr24,  xr21
+    xvpickev.b      xr28,  xr23,  xr22
+    xvpickev.b      xr18,  xr22,  xr21
+
+    xvabsd.bu       xr19,  xr18,  xr17
+    xvreplgr2vr.b   xr30,  a2
+    xvilvl.d        xr31,  xr30,  xr2
+    xvabsd.bu       xr20,  xr14,  xr13
+    xvslt.bu        xr19,  xr19,  xr31
+    xvslt.bu        xr20,  xr20,  xr2
+
+    xvbitsel.v      xr25,  xr27,  xr25,  xr5
+    xvpermi.d       xr20,  xr20,  0x50
+    xvand.v         xr21,  xr20,  xr19
+    xvpermi.d       xr7,   xr21,  0xB1
+    xvand.v         xr21,  xr21,  xr7
+    xvbitsel.v      xr25,  xr27,  xr25,  xr21
+    xvpermi.d       xr1,   xr1,   0x50
+    xvbitsel.v      xr26,  xr28,  xr26,  xr21
+    xvslti.b        xr30,  xr1,   0
+    xvbitsel.v      xr25,  xr25,  xr27,  xr30
+    xvbitsel.v      xr26,  xr26,  xr28,  xr30
+
+    xvilvl.b        xr10,  xr26,  xr25
+    xvilvh.b        xr20,  xr25,  xr26
+    xvilvl.h        xr21,  xr20,  xr10
+    xvilvh.h        xr22,  xr20,  xr10
+
+    // Store data to pix
+    addi.d          t5,    a0,    -2
+    xvstelm.w       xr21,  t5,    0,     0
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr21,  t5,    0,     1
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr21,  t5,    0,     2
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr21,  t5,    0,     3
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr22,  t5,    0,     0
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr22,  t5,    0,     1
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr22,  t5,    0,     2
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr22,  t5,    0,     3
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr21,  t5,    0,     4
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr21,  t5,    0,     5
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr21,  t5,    0,     6
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr21,  t5,    0,     7
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr22,  t5,    0,     4
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr22,  t5,    0,     5
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr22,  t5,    0,     6
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr22,  t5,    0,     7
+    fld.d           f24,   sp,    0
+    fld.d           f25,   sp,    8
+    fld.d           f26,   sp,    16
+    fld.d           f27,   sp,    24
+    fld.d           f28,   sp,    32
+    fld.d           f29,   sp,    40
+    fld.d           f30,   sp,    48
+    fld.d           f31,   sp,    56
+    addi.d          sp,    sp,    64
+endfunc
+
+/*
+ * void deblock_v_luma_lasx(Pixel *pix, intptr_t stride,
+ *                          int alpha, int beta, int8_t *tc0)
+ */
+function deblock_v_luma_lasx
+    slli.d          t0,    a1,    1
+
+    // Load data from tc0
+    xvldrepl.w      xr1,   a4,    0
+    add.d           t1,    t0,    a1
+    xvreplgr2vr.b   xr2,   a3
+    xvilvl.b        xr1,   xr1,   xr1
+
+    // Load data from pix
+    sub.d           t5,    a0,    t1
+    vld             vr10,  t5,    0
+    vldx            vr11,  t5,    a1
+    vldx            vr12,  t5,    t0
+    vld             vr13,  a0,    0
+    vldx            vr14,  a0,    a1
+    vldx            vr15,  a0,    t0
+
+    // Store registers to the stack
+    addi.d          sp,    sp,    -64
+    fst.d           f24,   sp,    0
+    fst.d           f25,   sp,    8
+    fst.d           f26,   sp,    16
+    fst.d           f27,   sp,    24
+    fst.d           f28,   sp,    32
+    fst.d           f29,   sp,    40
+    fst.d           f30,   sp,    48
+    fst.d           f31,   sp,    56
+    xvilvl.h        xr1,   xr1,   xr1
+    vext2xv.hu.bu   xr20,  xr10
+    vext2xv.hu.bu   xr21,  xr11
+    vext2xv.hu.bu   xr22,  xr12
+    vext2xv.hu.bu   xr23,  xr13
+    vext2xv.hu.bu   xr24,  xr14
+    vext2xv.hu.bu   xr25,  xr15
+    vext2xv.h.b     xr3,   xr1
+
+    xvadd.h         xr26,  xr22,  xr23
+    xvsrari.h       xr26,  xr26,  1
+    xvneg.h         xr4,   xr3
+    xvadd.h         xr27,  xr20,  xr26
+    xvadd.h         xr28,  xr25,  xr26
+    xvsub.h         xr29,  xr23,  xr22
+    xvsrai.h        xr27,  xr27,  1
+    xvsrai.h        xr28,  xr28,  1
+    xvslli.h        xr29,  xr29,  2
+    xvsub.h         xr30,  xr21,  xr24
+    xvsub.h         xr27,  xr27,  xr21
+    xvsub.h         xr28,  xr28,  xr24
+    xvadd.h         xr29,  xr29,  xr30
+    xvclip.h        xr27,  xr27,  xr4,   xr3
+    xvclip.h        xr28,  xr28,  xr4,   xr3
+
+    xvpickev.b      xr16,  xr25,  xr20
+    xvpickev.b      xr17,  xr23,  xr22
+    xvabsd.bu       xr5,   xr16,  xr17
+    xvaddi.hu       xr6,   xr3,   1
+    xvslt.bu        xr5,   xr5,   xr2
+    xvilvl.b        xr30,  xr5,   xr5
+    xvilvh.b        xr31,  xr5,   xr5
+    xvbitsel.v      xr3,   xr3,   xr6,   xr30
+
+    xvsrari.h       xr29,  xr29,  3
+    xvaddi.hu       xr6,   xr3,   1
+    xvbitsel.v      xr3,   xr3,   xr6,   xr31
+    xvneg.h         xr4,   xr3
+
+    xvclip.h        xr29,  xr29,  xr4,   xr3
+    xvadd.h         xr30,  xr21,  xr27
+    xvadd.h         xr18,  xr24,  xr28
+    xvadd.h         xr19,  xr22,  xr29
+    xvsub.h         xr26,  xr23,  xr29
+    xvssrarni.bu.h  xr26,  xr19,  0
+
+    xvpickev.b      xr25,  xr18,  xr30
+    xvpickev.b      xr27,  xr24,  xr21
+    xvpickev.b      xr28,  xr23,  xr22
+    xvpickev.b      xr18,  xr22,  xr21
+
+    xvabsd.bu       xr19,  xr18,  xr17
+    xvreplgr2vr.b   xr30,  a2
+    xvilvl.d        xr31,  xr30,  xr2
+    xvabsd.bu       xr20,  xr14,  xr13
+    xvslt.bu        xr19,  xr19,  xr31
+    xvslt.bu        xr20,  xr20,  xr2
+
+    xvbitsel.v      xr25,  xr27,  xr25,  xr5
+    xvpermi.d       xr20,  xr20,  0x50
+    xvand.v         xr21,  xr20,  xr19
+    xvpermi.d       xr7,   xr21,  0xB1
+    xvand.v         xr21,  xr21,  xr7
+    xvbitsel.v      xr25,  xr27,  xr25,  xr21
+    xvpermi.d       xr1,   xr1,   0x50
+    xvbitsel.v      xr26,  xr28,  xr26,  xr21
+    xvslti.b        xr30,  xr1,   0
+    xvbitsel.v      xr25,  xr25,  xr27,  xr30
+    xvbitsel.v      xr26,  xr26,  xr28,  xr30
+
+    sub.d           t5,    a0,    t0
+    xvpermi.d       xr0,   xr25,  0xd8
+    xvpermi.d       xr1,   xr26,  0xd8
+    xvpermi.d       xr2,   xr26,  0x8D
+    xvpermi.d       xr3,   xr25,  0x8D
+
+    // Store data to pix
+    vst             vr0,   t5,    0
+    vstx            vr1,   t5,    a1
+    vst             vr2,   a0,    0
+    vstx            vr3,   a0,    a1
+    fld.d           f24,   sp,    0
+    fld.d           f25,   sp,    8
+    fld.d           f26,   sp,    16
+    fld.d           f27,   sp,    24
+    fld.d           f28,   sp,    32
+    fld.d           f29,   sp,    40
+    fld.d           f30,   sp,    48
+    fld.d           f31,   sp,    56
+    addi.d          sp,    sp,    64
+endfunc
+
+/*
+ * void deblock_v_luma_intra_lasx(Pixel *pix, intptr_t stride,
+ *                                int alpha, int beta)
+ */
+function deblock_v_luma_intra_lasx
+    slli.d          t0,    a1,    1
+    slli.d          t2,    a1,    2
+    add.d           t1,    t0,    a1
+
+    // Load data from pix
+    sub.d           t5,    a0,    t2
+    vld             vr9,   t5,    0
+    vldx            vr10,  t5,    a1
+    vldx            vr11,  t5,    t0
+    vldx            vr12,  t5,    t1
+    vld             vr13,  a0,    0
+    vldx            vr14,  a0,    a1
+    vldx            vr15,  a0,    t0
+    vldx            vr16,  a0,    t1
+
+    // Store registers to the stack
+    addi.d          sp,    sp,    -64
+    fst.d           f24,   sp,    0
+    fst.d           f25,   sp,    8
+    fst.d           f26,   sp,    16
+    fst.d           f27,   sp,    24
+    fst.d           f28,   sp,    32
+    fst.d           f29,   sp,    40
+    fst.d           f30,   sp,    48
+    fst.d           f31,   sp,    56
+    xvreplgr2vr.b   xr1,   a2
+    xvreplgr2vr.b   xr2,   a3
+
+    vext2xv.hu.bu   xr19,  xr9
+    vext2xv.hu.bu   xr20,  xr10
+    vext2xv.hu.bu   xr21,  xr11
+    vext2xv.hu.bu   xr22,  xr12
+    vext2xv.hu.bu   xr23,  xr13
+    vext2xv.hu.bu   xr24,  xr14
+    vext2xv.hu.bu   xr25,  xr15
+    vext2xv.hu.bu   xr26,  xr16
+
+    xvadd.h         xr27,  xr21,  xr22
+    xvadd.h         xr29,  xr19,  xr20
+    xvadd.h         xr3,   xr27,  xr23
+    xvadd.h         xr6,   xr27,  xr24
+    xvadd.h         xr4,   xr3,   xr20
+
+    xvslli.h        xr29,  xr29,  1
+    xvadd.h         xr5,   xr6,   xr4
+    xvadd.h         xr6,   xr6,   xr21
+    xvadd.h         xr5,   xr5,   xr23
+    xvadd.h         xr7,   xr29,  xr4
+
+    xvsrari.h       xr3,   xr4,   2
+    xvsrari.h       xr6,   xr6,   2
+    xvsrari.h       xr4,   xr5,   3
+    xvadd.h         xr27,  xr24,  xr23
+    xvadd.h         xr28,  xr26,  xr25
+    xvsrari.h       xr5,   xr7,   3
+
+    xvadd.h         xr29,  xr22,  xr27
+    xvslli.h        xr28,  xr28,  1
+    xvadd.h         xr7,   xr29,  xr25
+    xvadd.h         xr17,  xr27,  xr21
+    xvadd.h         xr8,   xr7,   xr28
+    xvadd.h         xr18,  xr17,  xr7
+    xvadd.h         xr17,  xr17,  xr24
+    xvadd.h         xr18,  xr18,  xr22
+
+    xvsrari.h       xr7,   xr7,   2
+    xvsrari.h       xr8,   xr8,   3
+    xvsrari.h       xr18,  xr18,  3
+    xvsrari.h       xr17,  xr17,  2
+
+    xvpickev.b      xr27,  xr25,  xr20
+    xvpickev.b      xr28,  xr24,  xr21
+    xvpickev.b      xr29,  xr23,  xr22
+
+    xvpickev.b      xr9,   xr8,   xr5
+    xvpickev.b      xr16,  xr7,   xr3
+    xvabsd.bu       xr30,  xr27,  xr29
+    xvpickev.b      xr19,  xr18,  xr4
+    xvpickev.b      xr26,  xr17,  xr6
+
+    xvslt.bu        xr31,  xr30,  xr2
+    xvabsd.bu       xr20,  xr12,  xr13
+    xvabsd.bu       xr21,  xr11,  xr12
+    xvabsd.bu       xr22,  xr14,  xr13
+    xvsrli.b        xr0,   xr1,   2
+    xvbitsel.v      xr19,  xr26,  xr19,  xr31
+    xvbitsel.v      xr9,   xr27,  xr9,   xr31
+    xvbitsel.v      xr16,  xr28,  xr16,  xr31
+    xvaddi.bu       xr0,   xr0,   2
+    xvpermi.d       xr20,  xr20,  0x50
+    xvpermi.d       xr21,  xr21,  0x50
+    xvpermi.d       xr22,  xr22,  0x50
+    xvslt.bu        xr10,  xr20,  xr0
+    xvslt.bu        xr11,  xr20,  xr1
+    xvslt.bu        xr12,  xr21,  xr2
+    xvslt.bu        xr13,  xr22,  xr2
+    xvand.v         xr30,  xr11,  xr12
+    xvand.v         xr30,  xr30,  xr13
+    xvbitsel.v      xr9,   xr27,  xr9,   xr10
+    xvbitsel.v      xr16,  xr28,  xr16,  xr10
+    xvbitsel.v      xr19,  xr26,  xr19,  xr10
+    xvbitsel.v      xr9,   xr27,  xr9,   xr30
+    xvbitsel.v      xr16,  xr28,  xr16,  xr30
+    xvbitsel.v      xr19,  xr29,  xr19,  xr30
+    xvpermi.d       xr1,   xr9,   0xD8
+    xvpermi.d       xr2,   xr16,  0xD8
+    xvpermi.d       xr3,   xr19,  0xD8
+    xvpermi.d       xr4,   xr19,  0x8D
+    xvpermi.d       xr5,   xr16,  0x8D
+    xvpermi.d       xr6,   xr9,   0x8D
+
+    // Store data to pix
+    vstx            vr1,   t5,    a1
+    vstx            vr2,   t5,    t0
+    vstx            vr3,   t5,    t1
+    vst             vr4,   a0,    0
+    vstx            vr5,   a0,    a1
+    vstx            vr6,   a0,    t0
+
+    // Restore register values
+    fld.d           f24,   sp,    0
+    fld.d           f25,   sp,    8
+    fld.d           f26,   sp,    16
+    fld.d           f27,   sp,    24
+    fld.d           f28,   sp,    32
+    fld.d           f29,   sp,    40
+    fld.d           f30,   sp,    48
+    fld.d           f31,   sp,    56
+    addi.d          sp,    sp,    64
+endfunc
+
+/*
+ * void deblock_h_luma_intra_lasx(Pixel *pix, intptr_t stride,
+ *                                int alpha, int beta)
+ */
+function deblock_h_luma_intra_lasx
+    slli.d          t0,    a1,    1
+    slli.d          t2,    a1,    2
+    addi.d          t5,    a0,    -4
+    add.d           t1,    t0,    a1
+
+    // Store registers to the stack
+    addi.d          sp,    sp,    -64
+    fst.d           f24,   sp,    0
+    fst.d           f25,   sp,    8
+    fst.d           f26,   sp,    16
+    fst.d           f27,   sp,    24
+    fst.d           f28,   sp,    32
+    fst.d           f29,   sp,    40
+    fst.d           f30,   sp,    48
+    fst.d           f31,   sp,    56
+
+    // Load data from pix
+    FLDD_LOADX_4    t5,    a1,    t0,    t1,    f10, f11, f12, f13
+    add.d           t5,    t5,    t2
+    FLDD_LOADX_4    t5,    a1,    t0,    t1,    f14, f15, f16, f17
+    add.d           t5,    t5,    t2
+    FLDD_LOADX_4    t5,    a1,    t0,    t1,    f20, f21, f22, f23
+    add.d           t5,    t5,    t2
+    FLDD_LOADX_4    t5,    a1,    t0,    t1,    f24, f25, f26, f27
+
+    LASX_TRANSPOSE16X8_B   xr10, xr11, xr12, xr13, xr14, xr15, xr16, xr17, \
+                           xr20, xr21, xr22, xr23, xr24, xr25, xr26, xr27, \
+                           xr9,  xr10, xr11, xr12, xr13, xr14, xr15, xr16, \
+                           xr0,  xr1,  xr2,  xr3,  xr4,  xr5,  xr6,  xr7
+
+    xvreplgr2vr.b   xr1,   a2
+    xvreplgr2vr.b   xr2,   a3
+    vext2xv.hu.bu   xr19,  xr9
+    vext2xv.hu.bu   xr20,  xr10
+    vext2xv.hu.bu   xr21,  xr11
+    vext2xv.hu.bu   xr22,  xr12
+    vext2xv.hu.bu   xr23,  xr13
+    vext2xv.hu.bu   xr24,  xr14
+    vext2xv.hu.bu   xr25,  xr15
+    vext2xv.hu.bu   xr26,  xr16
+
+    xvadd.h         xr27,  xr21,  xr22
+    xvadd.h         xr29,  xr19,  xr20
+    xvadd.h         xr3,   xr27,  xr23
+    xvadd.h         xr6,   xr27,  xr24
+    xvadd.h         xr4,   xr3,   xr20
+
+    xvslli.h        xr29,  xr29,  1
+    xvadd.h         xr5,   xr6,   xr4
+    xvadd.h         xr6,   xr6,   xr21
+    xvadd.h         xr5,   xr5,   xr23
+    xvadd.h         xr7,   xr29,  xr4
+
+    xvsrari.h       xr3,   xr4,   2
+    xvsrari.h       xr6,   xr6,   2
+    xvsrari.h       xr4,   xr5,   3
+    xvadd.h         xr27,  xr24,  xr23
+    xvadd.h         xr28,  xr26,  xr25
+    xvsrari.h       xr5,   xr7,   3
+
+    xvadd.h         xr29,  xr22,  xr27
+    xvslli.h        xr28,  xr28,  1
+    xvadd.h         xr7,   xr29,  xr25
+    xvadd.h         xr17,  xr27,  xr21
+    xvadd.h         xr8,   xr7,   xr28
+    xvadd.h         xr18,  xr17,  xr7
+    xvadd.h         xr17,  xr17,  xr24
+    xvadd.h         xr18,  xr18,  xr22
+
+    xvsrari.h       xr7,   xr7,   2
+    xvsrari.h       xr8,   xr8,   3
+    xvsrari.h       xr18,  xr18,  3
+    xvsrari.h       xr17,  xr17,  2
+
+    xvpickev.b      xr27,  xr25,  xr20
+    xvpickev.b      xr28,  xr24,  xr21
+    xvpickev.b      xr29,  xr23,  xr22
+
+    xvpickev.b      xr9,   xr8,   xr5
+    xvpickev.b      xr16,  xr7,   xr3
+    xvabsd.bu       xr30,  xr27,  xr29
+    xvpickev.b      xr19,  xr18,  xr4
+    xvpickev.b      xr26,  xr17,  xr6
+
+    xvslt.bu        xr31,  xr30,  xr2
+    xvabsd.bu       xr20,  xr12,  xr13
+    xvabsd.bu       xr21,  xr11,  xr12
+    xvabsd.bu       xr22,  xr14,  xr13
+    xvsrli.b        xr0,   xr1,   2
+    xvbitsel.v      xr19,  xr26,  xr19,  xr31
+    xvbitsel.v      xr9,   xr27,  xr9,   xr31
+    xvbitsel.v      xr16,  xr28,  xr16,  xr31
+    xvaddi.bu       xr0,   xr0,   2
+    xvpermi.d       xr20,  xr20,  0x50
+    xvpermi.d       xr21,  xr21,  0x50
+    xvpermi.d       xr22,  xr22,  0x50
+    xvslt.bu        xr10,  xr20,  xr0
+    xvslt.bu        xr11,  xr20,  xr1
+    xvslt.bu        xr12,  xr21,  xr2
+    xvslt.bu        xr13,  xr22,  xr2
+    xvand.v         xr30,  xr11,  xr12
+    xvand.v         xr30,  xr30,  xr13
+    xvbitsel.v      xr9,   xr27,  xr9,   xr10
+    xvbitsel.v      xr16,  xr28,  xr16,  xr10
+    xvbitsel.v      xr19,  xr26,  xr19,  xr10
+
+    xvbitsel.v      xr9,   xr27,  xr9,   xr30
+    xvbitsel.v      xr16,  xr28,  xr16,  xr30
+    xvbitsel.v      xr19,  xr29,  xr19,  xr30
+
+    xvilvl.b        xr0,   xr16,  xr9
+    xvpermi.d       xr18,  xr19,  0xB1
+    xvilvh.b        xr1,   xr9,   xr16
+    xvilvl.b        xr2,   xr18,  xr19
+    addi.d          t5,    a0,    -3
+    xvilvl.h        xr3,   xr2,   xr0
+    xvilvh.h        xr4,   xr2,   xr0
+
+    // Store data to pix
+    xvstelm.w       xr3,   t5,    0,     0
+    xvstelm.h       xr1,   t5,    4,     0
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr3,   t5,    0,     1
+    xvstelm.h       xr1,   t5,    4,     1
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr3,   t5,    0,     2
+    xvstelm.h       xr1,   t5,    4,     2
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr3,   t5,    0,     3
+    xvstelm.h       xr1,   t5,    4,     3
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr4,   t5,    0,     0
+    xvstelm.h       xr1,   t5,    4,     4
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr4,   t5,    0,     1
+    xvstelm.h       xr1,   t5,    4,     5
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr4,   t5,    0,     2
+    xvstelm.h       xr1,   t5,    4,     6
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr4,   t5,    0,     3
+    xvstelm.h       xr1,   t5,    4,     7
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr3,   t5,    0,     4
+    xvstelm.h       xr1,   t5,    4,     8
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr3,   t5,    0,     5
+    xvstelm.h       xr1,   t5,    4,     9
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr3,   t5,    0,     6
+    xvstelm.h       xr1,   t5,    4,     10
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr3,   t5,    0,     7
+    xvstelm.h       xr1,   t5,    4,     11
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr4,   t5,    0,     4
+    xvstelm.h       xr1,   t5,    4,     12
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr4,   t5,    0,     5
+    xvstelm.h       xr1,   t5,    4,     13
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr4,   t5,    0,     6
+    xvstelm.h       xr1,   t5,    4,     14
+    add.d           t5,    t5,    a1
+    xvstelm.w       xr4,   t5,    0,     7
+    xvstelm.h       xr1,   t5,    4,     15
+    fld.d           f24,   sp,    0
+    fld.d           f25,   sp,    8
+    fld.d           f26,   sp,    16
+    fld.d           f27,   sp,    24
+    fld.d           f28,   sp,    32
+    fld.d           f29,   sp,    40
+    fld.d           f30,   sp,    48
+    fld.d           f31,   sp,    56
+    addi.d          sp,    sp,    64
+endfunc
+
+.end     deblock-a.S
+
+#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/deblock-c.c b/common/loongarch/deblock-c.c
index b6912662..5748f70e 100644
--- a/common/loongarch/deblock-c.c
+++ b/common/loongarch/deblock-c.c
@@ -30,497 +30,6 @@
 
 #if !HIGH_BIT_DEPTH
 
-#define LASX_LPF_P1_OR_Q1( p0_or_q0_org_in, q0_or_p0_org_in,         \
-                           p1_or_q1_org_in, p2_or_q2_org_in,         \
-                           negate_tc_in, tc_in, p1_or_q1_out )       \
-{                                                                    \
-    __m256i clip0, temp;                                             \
-                                                                     \
-    clip0 = __lasx_xvavgr_hu( p0_or_q0_org_in, q0_or_p0_org_in );    \
-    temp = __lasx_xvslli_h( p1_or_q1_org_in, 1 );                    \
-    clip0 = __lasx_xvsub_h( clip0, temp );                           \
-    clip0 = __lasx_xvavg_h( p2_or_q2_org_in, clip0 );                \
-    clip0 = __lasx_xvclip_h( clip0, negate_tc_in, tc_in );           \
-    p1_or_q1_out = __lasx_xvadd_h( p1_or_q1_org_in, clip0 );         \
-}
-
-#define LASX_LPF_P0Q0( q0_or_p0_org_in, p0_or_q0_org_in,             \
-                       p1_or_q1_org_in, q1_or_p1_org_in,             \
-                       negate_threshold_in, threshold_in,            \
-                       p0_or_q0_out, q0_or_p0_out )                  \
-{                                                                    \
-    __m256i q0_sub_p0, p1_sub_q1, delta;                             \
-                                                                     \
-    q0_sub_p0 = __lasx_xvsub_h( q0_or_p0_org_in, p0_or_q0_org_in );  \
-    p1_sub_q1 = __lasx_xvsub_h( p1_or_q1_org_in, q1_or_p1_org_in );  \
-    q0_sub_p0 = __lasx_xvslli_h( q0_sub_p0, 2 );                     \
-    p1_sub_q1 = __lasx_xvaddi_hu( p1_sub_q1, 4 );                    \
-    delta = __lasx_xvadd_h( q0_sub_p0, p1_sub_q1 );                  \
-    delta = __lasx_xvsrai_h( delta, 3 );                             \
-                                                                     \
-    delta = __lasx_xvclip_h(delta, negate_threshold_in,              \
-            threshold_in);                                           \
-                                                                     \
-    p0_or_q0_out = __lasx_xvadd_h( p0_or_q0_org_in, delta );         \
-    q0_or_p0_out = __lasx_xvsub_h( q0_or_p0_org_in, delta );         \
-                                                                     \
-    DUP2_ARG1( __lasx_xvclip255_h, p0_or_q0_out, q0_or_p0_out,       \
-               p0_or_q0_out, q0_or_p0_out );                         \
-}
-
-void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
-                               int32_t i_alpha, int32_t i_beta, int8_t *p_tc0 )
-{
-    uint8_t *p_src;
-    intptr_t i_stride_2x = ( i_stride << 1 );
-    intptr_t i_stride_4x = ( i_stride << 2 );
-    intptr_t i_stride_3x = i_stride_2x + i_stride;
-    __m256i beta, bs, tc;
-    __m256i zero = __lasx_xvldi( 0 );
-
-    tc = __lasx_xvld( p_tc0, 0 );
-    tc = __lasx_xvilvl_b( tc, tc );
-    tc = __lasx_xvilvl_h( tc, tc );
-
-    beta = __lasx_xvsle_w( zero, tc );
-    bs = __lasx_xvandi_b( beta, 0x01 );
-
-    if( !__lasx_xbz_v( bs ) )
-    {
-        __m256i is_less_than, is_less_than_beta, is_bs_greater_than0;
-        __m256i src0, src1, src2, src3, src4, src5, src6, src7;
-        __m256i p3_org, p2_org, p1_org, p0_org, q0_org, q1_org, q2_org, q3_org;
-        __m256i p2_org_l, p1_org_l, p0_org_l, q0_org_l, q1_org_l, q2_org_l;
-        __m256i p2_org_h, p1_org_h, p0_org_h, q0_org_h, q1_org_h, q2_org_h;
-        __m256i tc_l, tc_h;
-        __m256i mask_l = { 0, 2, 0, 2 };
-        __m256i mask_h = { 3, 0, 3, 0 };
-
-        is_bs_greater_than0 = __lasx_xvslt_bu( zero, bs );
-
-        {
-            p_src = p_pix - 4;
-            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
-                       i_stride_3x, src0, src1, src2, src3 );
-            p_src += i_stride_4x;
-            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
-                       i_stride_3x, src4, src5, src6, src7 );
-            p_src += i_stride_4x;
-            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
-                       i_stride_3x, p2_org_l, p1_org_l, p0_org_l, q0_org_l );
-            p_src += i_stride_4x;
-            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
-                       i_stride_3x, q1_org_l, q2_org_l, p2_org_h, p1_org_h );
-            p_src -= i_stride_4x;
-
-            LASX_TRANSPOSE16x8_B( src0, src1, src2, src3,
-                                  src4, src5, src6, src7,
-                                  p2_org_l, p1_org_l, p0_org_l, q0_org_l,
-                                  q1_org_l, q2_org_l, p2_org_h, p1_org_h,
-                                  p3_org, p2_org, p1_org, p0_org,
-                                  q0_org, q1_org, q2_org, q3_org );
-        }
-        {
-            src0 = __lasx_xvabsd_bu( p0_org, q0_org );
-            src1 = __lasx_xvabsd_bu( p1_org, p0_org );
-            src2 = __lasx_xvabsd_bu( q1_org, q0_org );
-
-            src3 = __lasx_xvreplgr2vr_b( i_alpha );
-            beta = __lasx_xvreplgr2vr_b( i_beta );
-
-            src4 = __lasx_xvslt_bu( src0, src3 );
-            is_less_than_beta = __lasx_xvslt_bu( src1, beta );
-            is_less_than = __lasx_xvand_v( is_less_than_beta, src4 );
-            is_less_than_beta = __lasx_xvslt_bu( src2, beta );
-            is_less_than = __lasx_xvand_v( is_less_than_beta,
-                                           is_less_than );
-            is_less_than = __lasx_xvand_v( is_less_than,
-                                           is_bs_greater_than0 );
-        }
-        if( !__lasx_xbz_v( is_less_than ) )
-        {
-            __m256i negate_tc, sign_negate_tc;
-            __m256i negate_tc_l, i16_negatetc_h;
-
-            negate_tc = __lasx_xvsub_b( zero, tc );
-            sign_negate_tc = __lasx_xvslti_b( negate_tc, 0 );
-
-            negate_tc_l = __lasx_xvilvl_b( sign_negate_tc, negate_tc );
-            i16_negatetc_h = __lasx_xvilvh_b( sign_negate_tc, negate_tc );
-
-            tc_l = __lasx_xvilvl_b( zero, tc );
-            tc_h = __lasx_xvilvh_b( zero, tc );
-            p1_org_l = __lasx_xvilvl_b( zero, p1_org );
-            p1_org_h = __lasx_xvilvh_b( zero, p1_org );
-            p0_org_l = __lasx_xvilvl_b( zero, p0_org );
-            p0_org_h = __lasx_xvilvh_b( zero, p0_org );
-            q0_org_l = __lasx_xvilvl_b( zero, q0_org );
-            q0_org_h = __lasx_xvilvh_b( zero, q0_org );
-
-            {
-                __m256i p2_asub_p0;
-                __m256i is_less_than_beta_l, is_less_than_beta_h;
-
-                p2_asub_p0 = __lasx_xvabsd_bu( p2_org, p0_org );
-                is_less_than_beta = __lasx_xvslt_bu( p2_asub_p0, beta );
-                is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
-                                                    is_less_than );
-
-                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
-                                                       zero );
-                if( !__lasx_xbz_v( is_less_than_beta_l ) )
-                {
-                    p2_org_l = __lasx_xvilvl_b( zero, p2_org );
-
-                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, p1_org_l, p2_org_l,
-                                       negate_tc_l, tc_l, src2 );
-                }
-
-                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
-                                                       zero );
-                if( !__lasx_xbz_v( is_less_than_beta_h ) )
-                {
-                    p2_org_h = __lasx_xvilvh_b( zero, p2_org );
-
-                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, p1_org_h, p2_org_h,
-                                       i16_negatetc_h, tc_h, src6 );
-                }
-            }
-
-            if( !__lasx_xbz_v( is_less_than_beta ) )
-            {
-                src6 = __lasx_xvpickev_b( src6, src2 );
-                p1_org = __lasx_xvbitsel_v( p1_org, src6, is_less_than_beta );
-
-                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
-                tc = __lasx_xvadd_b( tc, is_less_than_beta );
-            }
-
-            {
-                __m256i u8_q2asub_q0;
-                __m256i is_less_than_beta_h, is_less_than_beta_l;
-
-                u8_q2asub_q0 = __lasx_xvabsd_bu( q2_org, q0_org );
-                is_less_than_beta = __lasx_xvslt_bu( u8_q2asub_q0, beta );
-                is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
-                                                    is_less_than );
-
-                q1_org_l = __lasx_xvilvl_b( zero, q1_org );
-
-                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
-                                                       zero );
-                if( !__lasx_xbz_v( is_less_than_beta_l ) )
-                {
-                    q2_org_l = __lasx_xvilvl_b( zero, q2_org );
-                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, q1_org_l, q2_org_l,
-                                       negate_tc_l, tc_l, src3 );
-                }
-
-                q1_org_h = __lasx_xvilvh_b( zero, q1_org );
-
-                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
-                                                       zero );
-                if( !__lasx_xbz_v( is_less_than_beta_h ) )
-                {
-                    q2_org_h = __lasx_xvilvh_b( zero, q2_org );
-                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, q1_org_h, q2_org_h,
-                                       i16_negatetc_h, tc_h, src7 );
-                }
-            }
-
-            if( !__lasx_xbz_v( is_less_than_beta ) )
-            {
-                src7 = __lasx_xvpickev_b( src7, src3 );
-                q1_org = __lasx_xvbitsel_v( q1_org, src7, is_less_than_beta );
-
-                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
-                tc = __lasx_xvadd_b( tc, is_less_than_beta );
-            }
-
-            {
-                __m256i threshold_l, negate_thresh_l;
-                __m256i threshold_h, negate_thresh_h;
-                __m256i negate_thresh, sign_negate_thresh;
-
-                negate_thresh = __lasx_xvsub_b( zero, tc );
-                sign_negate_thresh = __lasx_xvslti_b( negate_thresh, 0 );
-
-                DUP2_ARG2( __lasx_xvilvl_b, zero, tc, sign_negate_thresh, negate_thresh,
-                           threshold_l, negate_thresh_l );
-
-                LASX_LPF_P0Q0( q0_org_l, p0_org_l, p1_org_l, q1_org_l,
-                               negate_thresh_l, threshold_l, src0, src1 );
-
-                threshold_h = __lasx_xvilvh_b( zero, tc );
-                negate_thresh_h = __lasx_xvilvh_b( sign_negate_thresh,
-                                                   negate_thresh );
-
-                LASX_LPF_P0Q0( q0_org_h, p0_org_h, p1_org_h, q1_org_h,
-                               negate_thresh_h, threshold_h, src4, src5 );
-            }
-
-            src4 = __lasx_xvpickev_b( src4, src0 );
-            src5 = __lasx_xvpickev_b( src5, src1 );
-
-            p0_org = __lasx_xvbitsel_v( p0_org, src4, is_less_than );
-            q0_org = __lasx_xvbitsel_v( q0_org, src5, is_less_than );
-        }
-        {
-            p_src = p_pix - 3;
-
-            src0 = __lasx_xvilvl_b( p1_org, p2_org );
-            src2 = __lasx_xvilvh_b( p1_org, p2_org );
-            src1 = __lasx_xvilvl_b( q0_org, p0_org );
-            src3 = __lasx_xvilvh_b( q0_org, p0_org );
-            src4 = __lasx_xvilvl_b( q2_org, q1_org );
-            src5 = __lasx_xvilvh_b( q2_org, q1_org );
-
-            src6 = __lasx_xvilvl_h( src1, src0 );
-            src7 = __lasx_xvilvh_h( src1, src0 );
-            src0 = __lasx_xvilvl_h( src3, src2 );
-            src1 = __lasx_xvilvh_h( src3, src2 );
-
-            __lasx_xvstelm_w( src6, p_src, 0, 0 );
-            __lasx_xvstelm_h( src4, p_src, 4, 0 );
-            p_src += i_stride;
-            __lasx_xvstelm_w( src6, p_src, 0, 1 );
-            __lasx_xvstelm_h( src4, p_src, 4, 1 );
-
-            p_src += i_stride;
-            __lasx_xvstelm_w( src6, p_src, 0, 2 );
-            __lasx_xvstelm_h( src4, p_src, 4, 2 );
-            p_src += i_stride;
-            __lasx_xvstelm_w( src6, p_src, 0, 3 );
-            __lasx_xvstelm_h( src4, p_src, 4, 3 );
-
-            p_src += i_stride;
-            __lasx_xvstelm_w( src7, p_src, 0, 0 );
-            __lasx_xvstelm_h( src4, p_src, 4, 4 );
-            p_src += i_stride;
-            __lasx_xvstelm_w( src7, p_src, 0, 1 );
-            __lasx_xvstelm_h( src4, p_src, 4, 5 );
-
-            p_src += i_stride;
-            __lasx_xvstelm_w( src7, p_src, 0, 2 );
-            __lasx_xvstelm_h( src4, p_src, 4, 6 );
-            p_src += i_stride;
-            __lasx_xvstelm_w( src7, p_src, 0, 3 );
-            __lasx_xvstelm_h( src4, p_src, 4, 7 );
-
-            p_src += i_stride;
-            __lasx_xvstelm_w( src0, p_src, 0, 0 );
-            __lasx_xvstelm_h( src5, p_src, 4, 0 );
-            p_src += i_stride;
-            __lasx_xvstelm_w( src0, p_src, 0, 1 );
-            __lasx_xvstelm_h( src5, p_src, 4, 1 );
-
-            p_src += i_stride;
-            __lasx_xvstelm_w( src0, p_src, 0, 2 );
-            __lasx_xvstelm_h( src5, p_src, 4, 2 );
-            p_src += i_stride;
-            __lasx_xvstelm_w( src0, p_src, 0, 3 );
-            __lasx_xvstelm_h( src5, p_src, 4, 3 );
-
-            p_src += i_stride;
-            __lasx_xvstelm_w( src1, p_src, 0, 0 );
-            __lasx_xvstelm_h( src5, p_src, 4, 4 );
-            p_src += i_stride;
-            __lasx_xvstelm_w( src1, p_src, 0, 1 );
-            __lasx_xvstelm_h( src5, p_src, 4, 5 );
-
-            p_src += i_stride;
-            __lasx_xvstelm_w( src1, p_src, 0, 2 );
-            __lasx_xvstelm_h( src5, p_src, 4, 6 );
-            p_src += i_stride;
-            __lasx_xvstelm_w( src1, p_src, 0, 3 );
-            __lasx_xvstelm_h( src5, p_src, 4, 7 );
-        }
-    }
-}
-
-void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
-                               int32_t i_alpha, int32_t i_beta, int8_t *p_tc0 )
-{
-    __m256i bs, tc, beta;
-    __m256i zero = __lasx_xvldi( 0 );
-    intptr_t i_stride_2x = ( i_stride << 1 );
-    intptr_t i_stride_3x = i_stride_2x + i_stride;
-
-    tc = __lasx_xvld( p_tc0, 0 );
-    tc = __lasx_xvilvl_b( tc, tc );
-    tc = __lasx_xvilvl_h( tc, tc );
-
-    beta = __lasx_xvsle_w( zero, tc );
-    bs = __lasx_xvandi_b( beta, 0x01 );
-
-    if( !__lasx_xbz_v( bs ) )
-    {
-        __m256i p2_asub_p0, u8_q2asub_q0;
-        __m256i alpha, is_less_than, is_less_than_beta;
-        __m256i src0, src1, src2, src3, src6, src4, src5, src7;
-        __m256i p2_org, p1_org, p0_org, q0_org, q1_org, q2_org;
-        __m256i p2_org_l, p1_org_l, p0_org_l, q0_org_l, q1_org_l, q2_org_l;
-        __m256i p2_org_h, p1_org_h, p0_org_h, q0_org_h, q1_org_h, q2_org_h;
-        __m256i mask_l = { 0, 2, 0, 2 };
-        __m256i mask_h = { 3, 0, 3, 0 };
-
-        alpha = __lasx_xvreplgr2vr_b( i_alpha );
-        beta = __lasx_xvreplgr2vr_b( i_beta );
-
-        p2_org = __lasx_xvldx( p_pix , -i_stride_3x );
-        p_pix -= i_stride_2x;
-        DUP4_ARG2(__lasx_xvldx, p_pix, 0, p_pix, i_stride, p_pix, i_stride_2x, p_pix,
-                  i_stride_3x, p1_org, p0_org, q0_org, q1_org );
-        p_pix += i_stride_2x;
-        {
-            src5 = __lasx_xvslt_bu( zero, bs );
-            src0 = __lasx_xvabsd_bu( p0_org, q0_org );
-            src1 = __lasx_xvabsd_bu( p1_org, p0_org );
-            src2 = __lasx_xvabsd_bu( q1_org, q0_org );
-
-            src4 = __lasx_xvslt_bu( src0, alpha );
-            is_less_than_beta = __lasx_xvslt_bu( src1, beta );
-            is_less_than = __lasx_xvand_v( is_less_than_beta,
-                                           src4 );
-            is_less_than_beta = __lasx_xvslt_bu( src2, beta );
-            is_less_than = __lasx_xvand_v( is_less_than_beta,
-                                           is_less_than );
-            is_less_than = __lasx_xvand_v( is_less_than, src5 );
-        }
-
-        if( !__lasx_xbz_v( is_less_than ) )
-        {
-            __m256i sign_negate_tc, negate_tc;
-            __m256i negate_tc_l, i16_negatetc_h, tc_h, tc_l;
-
-            q2_org = __lasx_xvldx( p_pix, i_stride_2x );
-            negate_tc = __lasx_xvsub_b( zero, tc );
-            sign_negate_tc = __lasx_xvslti_b( negate_tc, 0 );
-
-            negate_tc_l = __lasx_xvilvl_b( sign_negate_tc, negate_tc );
-            i16_negatetc_h = __lasx_xvilvh_b( sign_negate_tc, negate_tc );
-
-            tc_l = __lasx_xvilvl_b( zero, tc );
-            tc_h = __lasx_xvilvh_b( zero, tc );
-            p1_org_l = __lasx_xvilvl_b( zero, p1_org );
-            p1_org_h = __lasx_xvilvh_b( zero, p1_org );
-            p0_org_l = __lasx_xvilvl_b( zero, p0_org );
-            p0_org_h = __lasx_xvilvh_b( zero, p0_org );
-            q0_org_l = __lasx_xvilvl_b( zero, q0_org );
-            q0_org_h = __lasx_xvilvh_b( zero, q0_org );
-
-            p2_asub_p0 = __lasx_xvabsd_bu( p2_org, p0_org );
-            is_less_than_beta = __lasx_xvslt_bu( p2_asub_p0, beta );
-            is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
-                                                is_less_than );
-            {
-                __m256i is_less_than_beta_l, is_less_than_beta_h;
-
-                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
-                                                       zero );
-                if( !__lasx_xbz_v( is_less_than_beta_l ) )
-                {
-                    p2_org_l = __lasx_xvilvl_b( zero, p2_org );
-
-                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, p1_org_l, p2_org_l,
-                                       negate_tc_l, tc_l, src2 );
-                }
-
-                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
-                                                       zero );
-                if( !__lasx_xbz_v( is_less_than_beta_h ) )
-                {
-                    p2_org_h = __lasx_xvilvh_b( zero, p2_org );
-
-                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, p1_org_h, p2_org_h,
-                                       i16_negatetc_h, tc_h, src6 );
-                }
-            }
-            if( !__lasx_xbz_v( is_less_than_beta ) )
-            {
-                src6 = __lasx_xvpickev_b( src6, src2 );
-                p1_org = __lasx_xvbitsel_v( p1_org, src6, is_less_than_beta );
-                __lasx_xvstelm_d( p1_org, p_pix - i_stride_2x, 0, 0 );
-                __lasx_xvstelm_d( p1_org, p_pix - i_stride_2x, 8, 1 );
-
-                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
-                tc = __lasx_xvadd_b( tc, is_less_than_beta );
-            }
-
-            u8_q2asub_q0 = __lasx_xvabsd_bu( q2_org, q0_org );
-            is_less_than_beta = __lasx_xvslt_bu( u8_q2asub_q0, beta );
-            is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
-                                                is_less_than );
-
-            {
-                __m256i is_less_than_beta_l, is_less_than_beta_h;
-                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
-                                                       zero );
-
-                q1_org_l = __lasx_xvilvl_b( zero, q1_org );
-                if( !__lasx_xbz_v( is_less_than_beta_l ) )
-                {
-                    q2_org_l = __lasx_xvilvl_b( zero, q2_org );
-
-                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, q1_org_l, q2_org_l,
-                                       negate_tc_l, tc_l, src3 );
-                }
-                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
-                                                       zero );
-
-                q1_org_h = __lasx_xvilvh_b( zero, q1_org );
-                if( !__lasx_xbz_v( is_less_than_beta_h ) )
-                {
-                    q2_org_h = __lasx_xvilvh_b( zero, q2_org );
-
-                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, q1_org_h, q2_org_h,
-                                       i16_negatetc_h, tc_h, src7 );
-                }
-            }
-            if( !__lasx_xbz_v( is_less_than_beta ) )
-            {
-                src7 = __lasx_xvpickev_b( src7, src3 );
-                q1_org = __lasx_xvbitsel_v( q1_org, src7, is_less_than_beta );
-                __lasx_xvstelm_d( q1_org, p_pix + i_stride, 0, 0 );
-                __lasx_xvstelm_d( q1_org, p_pix + i_stride, 8, 1 );
-
-                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
-                tc = __lasx_xvadd_b( tc, is_less_than_beta );
-            }
-            {
-                __m256i negate_thresh, sign_negate_thresh;
-                __m256i threshold_l, threshold_h;
-                __m256i negate_thresh_h, negate_thresh_l;
-
-                negate_thresh = __lasx_xvsub_b( zero, tc );
-                sign_negate_thresh = __lasx_xvslti_b( negate_thresh, 0 );
-
-                DUP2_ARG2( __lasx_xvilvl_b, zero, tc, sign_negate_thresh, negate_thresh,
-                           threshold_l, negate_thresh_l );
-                LASX_LPF_P0Q0( q0_org_l, p0_org_l, p1_org_l, q1_org_l,
-                               negate_thresh_l, threshold_l, src0, src1 );
-
-                threshold_h = __lasx_xvilvh_b( zero, tc );
-                negate_thresh_h = __lasx_xvilvh_b( sign_negate_thresh,
-                                                   negate_thresh );
-                LASX_LPF_P0Q0( q0_org_h, p0_org_h, p1_org_h, q1_org_h,
-                               negate_thresh_h, threshold_h, src4, src5 );
-            }
-
-            src4 = __lasx_xvpickev_b( src4, src0 );
-            src5 = __lasx_xvpickev_b( src5, src1 );
-
-            p0_org = __lasx_xvbitsel_v( p0_org, src4, is_less_than );
-            q0_org = __lasx_xvbitsel_v( q0_org, src5, is_less_than );
-
-            __lasx_xvstelm_d( p0_org, p_pix - i_stride, 0, 0 );
-            __lasx_xvstelm_d( p0_org, p_pix - i_stride, 8, 1 );
-            __lasx_xvstelm_d( q0_org, p_pix, 0, 0 );
-            __lasx_xvstelm_d( q0_org, p_pix, 8, 1 );
-        }
-    }
-}
-
 static void avc_deblock_strength_lasx( uint8_t *nnz,
                                        int8_t pi_lef[2][X264_SCAN8_LUMA_SIZE],
                                        int16_t pi_mv[2][X264_SCAN8_LUMA_SIZE][2],
diff --git a/common/loongarch/deblock.h b/common/loongarch/deblock.h
index 4af3a4e8..a0bbc464 100644
--- a/common/loongarch/deblock.h
+++ b/common/loongarch/deblock.h
@@ -32,6 +32,10 @@
 void x264_deblock_v_luma_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta, int8_t *tc0 );
 #define x264_deblock_h_luma_lasx x264_template(deblock_h_luma_lasx)
 void x264_deblock_h_luma_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta, int8_t *tc0 );
+#define x264_deblock_v_luma_intra_lasx x264_template(deblock_v_luma_intra_lasx)
+void x264_deblock_v_luma_intra_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta );
+#define x264_deblock_h_luma_intra_lasx x264_template(deblock_h_luma_intra_lasx)
+void x264_deblock_h_luma_intra_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta );
 #define x264_deblock_strength_lasx x264_template(deblock_strength_lasx)
 void x264_deblock_strength_lasx( uint8_t nnz[X264_SCAN8_SIZE], int8_t ref[2][X264_SCAN8_LUMA_SIZE],
                                  int16_t mv[2][X264_SCAN8_LUMA_SIZE][2], uint8_t bs[2][8][4], int mvy_limit,
diff --git a/common/loongarch/mc-a.S b/common/loongarch/mc-a.S
index 4a5ef444..7a373ffa 100644
--- a/common/loongarch/mc-a.S
+++ b/common/loongarch/mc-a.S
@@ -112,15 +112,16 @@ endconst
 #if !HIGH_BIT_DEPTH
 
 .macro  MC_CHROMA_START
-    srai.d  t0,  a5,  3
-    srai.d  t1,  a6,  3
-    slli.d  t0,  t0,  1
-    mul.d   t1,  t1,  a4
-    add.d   t1,  t1,  t0
-    add.d   a3,  a3,  t1 /* src += (m_vy >> 3) * i_src_stride + (m_vx >> 3) * 2 */
+    srai.d         t0,      a5,    3
+    srai.d         t1,      a6,    3
+    slli.d         t0,      t0,    1
+    mul.d          t1,      t1,    a4
+    add.d          t1,      t1,    t0
+    add.d          a3,      a3,    t1 /* src += (m_vy >> 3) * i_src_stride + (m_vx >> 3) * 2 */
 .endm
 
-/* void mc_chroma( uint8_t *p_dst_u, uint8_t *p_dst_v,
+/*
+ * void mc_chroma( uint8_t *p_dst_u, uint8_t *p_dst_v,
  *                 intptr_t i_dst_stride,
  *                 uint8_t *p_src, intptr_t i_src_stride,
  *                 int32_t m_vx, int32_t m_vy,
@@ -128,403 +129,751 @@ endconst
  */
 function mc_chroma_lasx
     MC_CHROMA_START
-    andi    a5,    a5,    0x07    /* m_vx & 0x07 */
-    andi    a6,    a6,    0x07    /* m_vy & 0x07 */
-    move    t0,    a5
-    slli.d  t0,    t0,    8
-    sub.d   t0,    t0,    a5
-    li.d    a5,    8
-    addi.d  t0,    t0,    8
-    sub.d   a5,    a5,    a6
-    mul.d   a6,    a6,    t0      /* (x * 255 + 8) * y */
-    mul.d   a5,    a5,    t0      /* (x * 255 + 8) * (8 - y) */
-    xvreplgr2vr.h  xr6,   a6      /* cD cC ... cD cC */
-    xvreplgr2vr.h  xr7,   a5      /* cB cA ... cB cA */
-    la.local t0,   ch_shuf
-    xvld    xr5,   t0,    0
-    addi.d  t0,    a7,    -4
-    ldptr.w a7,    sp,    0       /* a7 = i_height */
-    slli.d  t1,    a4,    1
-    blt     zero,  t0,    .L_WIDTH8
+    andi           a5,      a5,    0x07    /* m_vx & 0x07 */
+    andi           a6,      a6,    0x07    /* m_vy & 0x07 */
+    move           t0,      a5
+    slli.d         t0,      t0,    8
+    sub.d          t0,      t0,    a5
+    li.d           a5,      8
+    addi.d         t0,      t0,    8
+    sub.d          a5,      a5,    a6
+    mul.d          a6,      a6,    t0      /* (x * 255 + 8) * y */
+    mul.d          a5,      a5,    t0      /* (x * 255 + 8) * (8 - y) */
+    xvreplgr2vr.h  xr6,     a6      /* cD cC ... cD cC */
+    xvreplgr2vr.h  xr7,     a5      /* cB cA ... cB cA */
+    la.local       t0,      ch_shuf
+    xvld           xr5,     t0,    0
+    addi.d         t0,      a7,    -4
+    ldptr.w        a7,      sp,    0       /* a7 = i_height */
+    slli.d         t1,      a4,    1
+    blt            zero,    t0,    .L_WIDTH8
 .L_LOOP4:
-    vld       vr0,    a3,   0
-    vldx      vr1,    a3,   a4
-    vldx      vr2,    a3,   t1
-    xvpermi.q xr0,   xr1,  0x02
-    xvpermi.q xr1,   xr2,  0x02
-    xvshuf.b  xr0,   xr0,  xr0,   xr5
-    xvshuf.b  xr1,   xr1,  xr1,   xr5
-    xvdp2.h.bu xr2,  xr0,  xr7
-    xvdp2.h.bu xr3,  xr1,  xr6
-    xvadd.h   xr0,   xr2,  xr3
-    xvssrlrni.bu.h   xr0,  xr0,   6
-    xvstelm.w xr0,   a0,   0,     0
-    xvstelm.w xr0,   a1,   0,     1
-    add.d     a0,    a0,   a2
-    add.d     a1,    a1,   a2
-    xvstelm.w xr0,   a0,   0,     4
-    xvstelm.w xr0,   a1,   0,     5
-    add.d     a0,    a0,   a2
-    add.d     a1,    a1,   a2
-    add.d     a3,    a3,   t1
-    addi.d    a7,    a7,   -2
-    blt       zero,  a7,   .L_LOOP4
-    b         .ENDFUNC
+    vld            vr0,     a3,    0
+    vldx           vr1,     a3,    a4
+    vldx           vr2,     a3,    t1
+    xvpermi.q      xr0,     xr1,   0x02
+    xvpermi.q      xr1,     xr2,   0x02
+    xvshuf.b       xr0,     xr0,   xr0,   xr5
+    xvshuf.b       xr1,     xr1,   xr1,   xr5
+    xvdp2.h.bu     xr2,     xr0,   xr7
+    xvdp2.h.bu     xr3,     xr1,   xr6
+    xvadd.h        xr0,     xr2,   xr3
+    xvssrlrni.bu.h xr0,     xr0,   6
+    xvstelm.w      xr0,     a0,    0,     0
+    xvstelm.w      xr0,     a1,    0,     1
+    add.d          a0,      a0,    a2
+    add.d          a1,      a1,    a2
+    xvstelm.w      xr0,     a0,    0,     4
+    xvstelm.w      xr0,     a1,    0,     5
+    add.d          a0,      a0,    a2
+    add.d          a1,      a1,    a2
+    add.d          a3,      a3,    t1
+    addi.d         a7,      a7,    -2
+    blt            zero,    a7,    .L_LOOP4
+    b              .ENDFUNC
 .L_WIDTH8:
-    xvld      xr0,   a3,    0
-    xvpermi.d xr0,   xr0,   0x94
-    xvshuf.b  xr0,   xr0,   xr0,   xr5
+    xvld           xr0,     a3,    0
+    xvpermi.d      xr0,     xr0,   0x94
+    xvshuf.b       xr0,     xr0,   xr0,   xr5
 .L_LOOP8:
-    xvldx     xr3,   a3,    a4
-    xvpermi.d xr3,   xr3,   0x94
-    xvshuf.b  xr3,   xr3,   xr3,   xr5
-    xvdp2.h.bu xr1,  xr0,   xr7
-    xvdp2.h.bu xr2,  xr3,   xr6
-    xvdp2.h.bu xr8,  xr3,   xr7
-
-    xvldx     xr0,   a3,    t1
-    xvpermi.d xr0,   xr0,   0x94
-    xvshuf.b  xr0,   xr0,   xr0,   xr5
-    xvdp2.h.bu xr4,  xr0,   xr6
-    xvadd.h   xr1,   xr1,   xr2
-    xvadd.h   xr3,   xr8,   xr4
-
-    xvssrlrni.bu.h   xr3,   xr1,    6
-
-    xvpermi.q   xr4,   xr3,    0x01
-    xvpackev.w  xr8,   xr4,    xr3
-    xvpackod.w  xr9,   xr4,    xr3
-    vstelm.d    vr8,   a0,     0,    0
-    vstelm.d    vr9,   a1,     0,    0
-    add.d       a0,    a0,     a2
-    add.d       a1,    a1,     a2
-    vstelm.d    vr8,   a0,     0,    1
-    vstelm.d    vr9,   a1,     0,    1
-
-    addi.d      a7,    a7,     -2
-    add.d       a0,    a0,     a2
-    add.d       a1,    a1,     a2
-    add.d       a3,    a3,     t1
-    blt         zero,  a7,     .L_LOOP8
+    xvldx          xr3,     a3,    a4
+    xvpermi.d      xr3,     xr3,   0x94
+    xvshuf.b       xr3,     xr3,   xr3,   xr5
+    xvdp2.h.bu     xr1,     xr0,   xr7
+    xvdp2.h.bu     xr2,     xr3,   xr6
+    xvdp2.h.bu     xr8,     xr3,   xr7
+
+    xvldx          xr0,     a3,    t1
+    xvpermi.d      xr0,     xr0,   0x94
+    xvshuf.b       xr0,     xr0,   xr0,   xr5
+    xvdp2.h.bu     xr4,     xr0,   xr6
+    xvadd.h        xr1,     xr1,   xr2
+    xvadd.h        xr3,     xr8,   xr4
+
+    xvssrlrni.bu.h xr3,     xr1,   6
+
+    xvpermi.q      xr4,     xr3,   0x01
+    xvpackev.w     xr8,     xr4,   xr3
+    xvpackod.w     xr9,     xr4,   xr3
+    vstelm.d       vr8,     a0,    0,     0
+    vstelm.d       vr9,     a1,    0,     0
+    add.d          a0,      a0,    a2
+    add.d          a1,      a1,    a2
+    vstelm.d       vr8,     a0,    0,     1
+    vstelm.d       vr9,     a1,    0,     1
+
+    addi.d         a7,      a7,    -2
+    add.d          a0,      a0,    a2
+    add.d          a1,      a1,    a2
+    add.d          a3,      a3,    t1
+    blt            zero,    a7,    .L_LOOP8
 .ENDFUNC:
-    endfunc
+endfunc
 
 .macro PIXEL_AVG_START
-    slli.d  t0,  a3,  1
-    add.w   t1,  t0,  a3
-    slli.d  t2,  a3,  2
-    slli.d  t3,  a5,  1
-    add.w   t4,  t3,  a5
-    slli.d  t5,  a5,  2
-    slli.d  t6,  a1,  1
-    add.w   t7,  t6,  a1
-    slli.d  t8,  a1,  2
+    slli.d         t0,      a3,    1
+    add.w          t1,      t0,    a3
+    slli.d         t2,      a3,    2
+    slli.d         t3,      a5,    1
+    add.w          t4,      t3,    a5
+    slli.d         t5,      a5,    2
+    slli.d         t6,      a1,    1
+    add.w          t7,      t6,    a1
+    slli.d         t8,      a1,    2
 .endm
 
 .macro BIWEIGHT_AVG_START
-    addi.d          t0,    zero,  64
-    sub.d           t0,    t0,    a6
-    xvreplgr2vr.b   xr0,   a6
-    xvreplgr2vr.b   xr1,   t0
-    xvpackev.b      xr8,   xr1,   xr0
-    xvxor.v         xr9,   xr9,   xr9
-    xvaddi.hu       xr9,   xr9,   6
+    addi.d         t0,      zero,  64
+    sub.d          t0,      t0,    a6
+    xvreplgr2vr.b  xr0,     a6
+    xvreplgr2vr.b  xr1,     t0
+    xvpackev.b     xr8,     xr1,   xr0
+    xvxor.v        xr9,     xr9,   xr9
+    xvaddi.hu      xr9,     xr9,   6
 .endm
 
 .macro BIWEIGHT_AVG_CORE a, b
-    xvpermi.d \a,   \a,  0x50
-    xvpermi.d \b,   \b,  0x50
-    xvilvl.b  \a,   \b,  \a
-    xvmulwev.h.bu.b  \b,  \a,  xr8
-    xvmaddwod.h.bu.b \b,  \a,  xr8
-    xvmaxi.h  \b,   \b,  0
-    xvssrlrn.bu.h    \b,  \b,  xr9
-    xvpermi.d        \b,  \b,  0x08
+    xvpermi.d        \a,      \a,    0x50
+    xvpermi.d        \b,      \b,    0x50
+    xvilvl.b         \a,      \b,    \a
+    xvmulwev.h.bu.b  \b,      \a,    xr8
+    xvmaddwod.h.bu.b \b,      \a,    xr8
+    xvmaxi.h         \b,      \b,    0
+    xvssrlrn.bu.h    \b,      \b,    xr9
+    xvpermi.d        \b,      \b,    0x08
+.endm
+
+.macro PIXEL_AVG_START_W8
+    slli.d           t0,      a3,    1
+    add.w            t1,      t0,    a3
+    slli.d           t3,      a5,    1
+    add.w            t4,      t3,    a5
 .endm
 
+function pixel_avg_weight_w4_lasx export=0
+    addi.d           t0,      zero,  64
+    sub.d            t0,      t0,    a6
+    vreplgr2vr.b     vr0,     a6
+    vreplgr2vr.b     vr1,     t0
+    vpackev.b        vr8,     vr1,   vr0
+.LOOP_HEIGHT_W4_1:
+    fld.s            f0,      a2,    0
+    fldx.s           f1,      a2,    a3
+    fld.s            f2,      a4,    0
+    fldx.s           f3,      a4,    a5
+    vilvl.w          vr0,     vr1,   vr0
+    vilvl.w          vr2,     vr3,   vr2
+    vilvl.b          vr0,     vr2,   vr0
+    vmulwev.h.bu.b   vr1,     vr0,   vr8
+    vmaddwod.h.bu.b  vr1,     vr0,   vr8
+    vssrarni.bu.h    vr1,     vr1,   6
+    fst.s            f1,      a0,    0
+    add.d            a0,      a0,    a1
+    vstelm.w         vr1,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a5,    a4,   1
+    addi.w           a7,      a7,    -2
+    bnez             a7,      .LOOP_HEIGHT_W4_1
+endfunc
+
+function pixel_avg_w4_lasx export=0
+.LOOP_HEIGHT_W4:
+    fld.s            f0,      a2,    0
+    fldx.s           f1,      a2,    a3
+    fld.s            f4,      a4,    0
+    fldx.s           f5,      a4,    a5
+    vilvl.w          vr0,     vr1,   vr0
+    vilvl.w          vr4,     vr5,   vr4
+    vavgr.bu         vr0,     vr0,   vr4
+    fst.s            f0,      a0,    0
+    add.d            a0,      a0,    a1
+    vstelm.w         vr0,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a5,    a4,   1
+    addi.w           a7,      a7,    -2
+    bnez             a7,      .LOOP_HEIGHT_W4
+endfunc
+
+function pixel_avg_weight_w8_lasx export=0
+    addi.d           t0,      zero,  64
+    sub.d            t0,      t0,    a6
+    xvreplgr2vr.b    xr0,     a6
+    xvreplgr2vr.b    xr1,     t0
+    xvpackev.b       xr8,     xr1,   xr0
+    PIXEL_AVG_START_W8
+.LOOP_HEIGHT_W8_1:
+    fld.d            f0,      a2,    0
+    fldx.d           f1,      a2,    a3
+    fldx.d           f2,      a2,    t0
+    fldx.d           f3,      a2,    t1
+    fld.d            f4,      a4,    0
+    fldx.d           f5,      a4,    a5
+    fldx.d           f6,      a4,    t3
+    fldx.d           f7,      a4,    t4
+    vilvl.b          vr0,     vr4,   vr0
+    vilvl.b          vr1,     vr5,   vr1
+    vilvl.b          vr2,     vr6,   vr2
+    vilvl.b          vr3,     vr7,   vr3
+    xvpermi.q        xr1,     xr0,   0x20
+    xvpermi.q        xr3,     xr2,   0x20
+    xvmulwev.h.bu.b  xr2,     xr1,   xr8
+    xvmaddwod.h.bu.b xr2,     xr1,   xr8
+    xvmulwev.h.bu.b  xr4,     xr3,   xr8
+    xvmaddwod.h.bu.b xr4,     xr3,   xr8
+    xvssrarni.bu.h   xr4,     xr2,   6
+    fst.d            f4,      a0,    0
+    add.d            a0,      a0,    a1
+    xvstelm.d        xr4,     a0,    0,    2
+    add.d            a0,      a0,    a1
+    xvstelm.d        xr4,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    xvstelm.d        xr4,     a0,    0,    3
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   2
+    alsl.d           a4,      a5,    a4,   2
+    addi.w           a7,      a7,    -4
+    bnez             a7,      .LOOP_HEIGHT_W8_1
+endfunc
+
+function pixel_avg_w8_lasx export=0
+    PIXEL_AVG_START_W8
+.LOOP_HEIGHT_W8:
+    fld.d            f0,      a2,    0
+    fldx.d           f1,      a2,    a3
+    fldx.d           f2,      a2,    t0
+    fldx.d           f3,      a2,    t1
+    fld.d            f4,      a4,    0
+    fldx.d           f5,      a4,    a5
+    fldx.d           f6,      a4,    t3
+    fldx.d           f7,      a4,    t4
+    vilvl.d          vr0,     vr1,   vr0
+    vilvl.d          vr2,     vr3,   vr2
+    vilvl.d          vr4,     vr5,   vr4
+    vilvl.d          vr6,     vr7,   vr6
+    vavgr.bu         vr0,     vr0,   vr4
+    vavgr.bu         vr2,     vr2,   vr6
+    fst.d            f0,      a0,    0
+    add.d            a0,      a0,    a1
+    vstelm.d         vr0,     a0,    0,    1
+    fstx.d           f2,      a0,    a1
+    alsl.d           a0,      a1,    a0,   1
+    vstelm.d         vr2,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   2
+    alsl.d           a4,      a5,    a4,   2
+    addi.w           a7,      a7,    -4
+    bnez             a7,      .LOOP_HEIGHT_W8
+endfunc
+
 function pixel_avg_weight_w16_lasx export=0
     BIWEIGHT_AVG_START
     PIXEL_AVG_START
 .L_HEIGHT_LOOP_T:
-    LSX_LOADX_4  a2,  a3, t0, t1, vr0, vr1, vr2, vr3
-    LSX_LOADX_4  a4,  a5, t3, t4, vr4, vr5, vr6, vr7
+    LSX_LOADX_4 a2, a3, t0, t1, vr0, vr1, vr2, vr3
+    LSX_LOADX_4 a4, a5, t3, t4, vr4, vr5, vr6, vr7
     BIWEIGHT_AVG_CORE xr0, xr4
     BIWEIGHT_AVG_CORE xr1, xr5
-    vst              vr4,  a0,   0
-    vstx             vr5,  a0,   a1
+    vst              vr4,     a0,    0
+    vstx             vr5,     a0,    a1
     BIWEIGHT_AVG_CORE xr2, xr6
     BIWEIGHT_AVG_CORE xr3, xr7
-    vstx             vr6,  a0,   t6
-    vstx             vr7,  a0,   t7
-    add.d     a2,    a2,   t2
-    add.d     a4,    a4,   t5
-    add.d     a0,    a0,   t8
-    addi.d    a7,    a7,   -4
-    bnez      a7,    .L_HEIGHT_LOOP_T
+    vstx             vr6,     a0,    t6
+    vstx             vr7,     a0,    t7
+    add.d            a2,      a2,    t2
+    add.d            a4,      a4,    t5
+    add.d            a0,      a0,    t8
+    addi.d           a7,      a7,    -4
+    bnez             a7,      .L_HEIGHT_LOOP_T
 endfunc
 
 function pixel_avg_w16_lasx export=0
     PIXEL_AVG_START
 .L_HEIGHT_LOOP:
-    vld       vr0,   a2,   0
-    vldx      vr1,   a2,   a3
-    vldx      vr2,   a2,   t0
-    vldx      vr3,   a2,   t1
-    vld       vr4,   a4,   0
-    vldx      vr5,   a4,   a5
-    vldx      vr6,   a4,   t3
-    vldx      vr7,   a4,   t4
-    vavgr.bu  vr0,   vr0,  vr4
-    vavgr.bu  vr1,   vr1,  vr5
-    vavgr.bu  vr2,   vr2,  vr6
-    vavgr.bu  vr3,   vr3,  vr7
-    vst       vr0,   a0,   0
-    vstx      vr1,   a0,   a1
-    vstx      vr2,   a0,   t6
-    vstx      vr3,   a0,   t7
-    add.d     a0,    a0,   t8
-    add.d     a2,    a2,   t2
-    add.d     a4,    a4,   t5
-
-    vld       vr0,   a2,   0
-    vldx      vr1,   a2,   a3
-    vldx      vr2,   a2,   t0
-    vldx      vr3,   a2,   t1
-    vld       vr4,   a4,   0
-    vldx      vr5,   a4,   a5
-    vldx      vr6,   a4,   t3
-    vldx      vr7,   a4,   t4
-    vavgr.bu  vr0,   vr0,  vr4
-    vavgr.bu  vr1,   vr1,  vr5
-    vavgr.bu  vr2,   vr2,  vr6
-    vavgr.bu  vr3,   vr3,  vr7
-    vst       vr0,   a0,   0
-    vstx      vr1,   a0,   a1
-    vstx      vr2,   a0,   t6
-    vstx      vr3,   a0,   t7
-    add.d     a2,    a2,   t2
-    add.d     a4,    a4,   t5
-    add.d     a0,    a0,   t8
-    addi.d    a7,    a7,   -8
-    bnez      a7,    .L_HEIGHT_LOOP
+    vld              vr0,     a2,    0
+    vldx             vr1,     a2,    a3
+    vldx             vr2,     a2,    t0
+    vldx             vr3,     a2,    t1
+    vld              vr4,     a4,    0
+    vldx             vr5,     a4,    a5
+    vldx             vr6,     a4,    t3
+    vldx             vr7,     a4,    t4
+    vavgr.bu         vr0,     vr0,   vr4
+    vavgr.bu         vr1,     vr1,   vr5
+    vavgr.bu         vr2,     vr2,   vr6
+    vavgr.bu         vr3,     vr3,   vr7
+    vst              vr0,     a0,    0
+    vstx             vr1,     a0,    a1
+    vstx             vr2,     a0,    t6
+    vstx             vr3,     a0,    t7
+    add.d            a0,      a0,    t8
+    add.d            a2,      a2,    t2
+    add.d            a4,      a4,    t5
+
+    vld              vr0,     a2,    0
+    vldx             vr1,     a2,    a3
+    vldx             vr2,     a2,    t0
+    vldx             vr3,     a2,    t1
+    vld              vr4,     a4,    0
+    vldx             vr5,     a4,    a5
+    vldx             vr6,     a4,    t3
+    vldx             vr7,     a4,    t4
+    vavgr.bu         vr0,     vr0,   vr4
+    vavgr.bu         vr1,     vr1,   vr5
+    vavgr.bu         vr2,     vr2,   vr6
+    vavgr.bu         vr3,     vr3,   vr7
+    vst              vr0,     a0,    0
+    vstx             vr1,     a0,    a1
+    vstx             vr2,     a0,    t6
+    vstx             vr3,     a0,    t7
+    add.d            a2,      a2,    t2
+    add.d            a4,      a4,    t5
+    add.d            a0,      a0,    t8
+    addi.d           a7,      a7,    -8
+    bnez             a7,      .L_HEIGHT_LOOP
 endfunc
 
 .macro FILT_PACK_LASX s1, s2, s3
-    xvmulwev.w.h    xr16,  \s1,  \s3
-    xvmulwev.w.h    xr17,  \s2,  \s3
-    xvsrarni.h.w   xr17,  xr16, 15
-    xvmaxi.h        xr17,  xr17, 0
-    xvsat.hu        xr17,  xr17, 7
-    xvmulwod.w.h    xr18,  \s1,  \s3
-    xvmulwod.w.h    xr19,  \s2,  \s3
-    xvsrarni.h.w   xr19,  xr18,  15
-    xvmaxi.h        xr19,  xr19,  0
-    xvsat.hu        xr19,  xr19,  7
-    xvpackev.b      \s1,   xr19,  xr17
+    xvmulwev.w.h     xr16,    \s1,   \s3
+    xvmulwev.w.h     xr17,    \s2,   \s3
+    xvsrarni.h.w     xr17,    xr16,  15
+    xvmaxi.h         xr17,    xr17,  0
+    xvsat.hu         xr17,    xr17,  7
+    xvmulwod.w.h     xr18,    \s1,   \s3
+    xvmulwod.w.h     xr19,    \s2,   \s3
+    xvsrarni.h.w     xr19,    xr18,  15
+    xvmaxi.h         xr19,    xr19,  0
+    xvsat.hu         xr19,    xr19,  7
+    xvpackev.b       \s1,     xr19,  xr17
 .endm
 
 /* s3: temp, s4: UNUSED, s5: imm */
 .macro DO_FILT_V_LASX s1, s2, s3, s4, s5
-    alsl.d    t1,  a2,  a1,  1  /* t1 = a1 + 2 * a2 */
-    alsl.d    t2,  a2,  a3,  1  /* t2 = a3 + 2 * a2 */
-    //preld     0,   t1,  32
-    xvld      xr1, a3,  0
-    xvldx     xr2, a3,  a2
-    xvld      \s3, t2,  0
-    xvld      xr3, a1,  0
-    xvldx     \s1, a1,  a2
-    xvld      \s2, t1,  0
-    xvilvh.b  xr16, xr2, xr1
-    xvilvl.b  xr17, xr2, xr1
-    xvilvh.b  xr18, \s2, \s1
-    xvilvl.b  xr19, \s2, \s1
-    xvilvh.b  xr20, \s3, xr3
-    xvilvl.b  xr21, \s3, xr3
-    xvdp2.h.bu.b    xr1,  xr17,  xr12
-    xvdp2.h.bu.b    xr4,  xr16,  xr12
-    xvdp2.h.bu.b    \s1,  xr19,  xr0
-    xvdp2.h.bu.b    xr2,  xr18,  xr0
-    xvdp2.h.bu.b    xr3,  xr21,  xr14
-    xvdp2.h.bu.b    \s2,  xr20,  xr14
-    xvadd.h   xr1,  xr1,  \s1
-    xvadd.h   xr4,  xr4,  xr2
-    xvadd.h   xr1,  xr1,  xr3
-    xvadd.h   xr4,  xr4,  \s2
-    xmov      \s1,  xr1
-    xmov      \s2,  xr1
-    addi.d    a3,   a3,   32
-    addi.d    a1,   a1,   32
-    xvpermi.q \s1,  xr4,  0x2
-    xvpermi.q \s2,  xr4,  0x13
+    alsl.d           t1,      a2,    a1,  1  /* t1 = a1 + 2 * a2 */
+    alsl.d           t2,      a2,    a3,  1  /* t2 = a3 + 2 * a2 */
+    xvld             xr1,     a3,    0
+    xvldx            xr2,     a3,    a2
+    xvld             \s3,     t2,    0
+    xvld             xr3,     a1,    0
+    xvldx            \s1,     a1,    a2
+    xvld             \s2,     t1,    0
+    xvilvh.b         xr16,    xr2,   xr1
+    xvilvl.b         xr17,    xr2,   xr1
+    xvilvh.b         xr18,    \s2,   \s1
+    xvilvl.b         xr19,    \s2,   \s1
+    xvilvh.b         xr20,    \s3,   xr3
+    xvilvl.b         xr21,    \s3,   xr3
+    xvdp2.h.bu.b     xr1,     xr17,  xr12
+    xvdp2.h.bu.b     xr4,     xr16,  xr12
+    xvdp2.h.bu.b     \s1,     xr19,  xr0
+    xvdp2.h.bu.b     xr2,     xr18,  xr0
+    xvdp2.h.bu.b     xr3,     xr21,  xr14
+    xvdp2.h.bu.b     \s2,     xr20,  xr14
+    xvadd.h          xr1,     xr1,   \s1
+    xvadd.h          xr4,     xr4,   xr2
+    xvadd.h          xr1,     xr1,   xr3
+    xvadd.h          xr4,     xr4,   \s2
+    xmov             \s1,     xr1
+    xmov             \s2,     xr1
+    addi.d           a3,      a3,    32
+    addi.d           a1,      a1,    32
+    xvpermi.q        \s1,     xr4,   0x2
+    xvpermi.q        \s2,     xr4,   0x13
     FILT_PACK_LASX  xr1,  xr4,  xr15
-    addi.d    t1,   a4,   \s5
-    xvstx     xr1,  t0,   t1
+    addi.d           t1,      a4,    \s5
+    xvstx            xr1,     t0,    t1
 .endm
 
 .macro FILT_H s1, s2, s3
-    xvsub.h    \s1,  \s1,  \s2
-    xvsrai.h   \s1,  \s1,  2
-    xvsub.h    \s1,  \s1,  \s2
-    xvadd.h    \s1,  \s1,  \s3
-    xvsrai.h   \s1,  \s1,  2
-    xvadd.h    \s1,  \s1,  \s3
+    xvsub.h          \s1,     \s1,   \s2
+    xvsrai.h         \s1,     \s1,   2
+    xvsub.h          \s1,     \s1,   \s2
+    xvadd.h          \s1,     \s1,   \s3
+    xvsrai.h         \s1,     \s1,   2
+    xvadd.h          \s1,     \s1,   \s3
 .endm
 
 .macro FILT_C s1, s2, s3
-    xmov    xr3,  \s1
-    xvpermi.q   xr3,  \s2,  0x03
-    xvshuf.b    xr1,  \s2,  xr3,  xr23
-    xvshuf.b    xr2,  \s2,  xr3,  xr24
-    xmov    \s1,  \s2
-    xvpermi.q   \s1,  \s3,  0x03
-    xvshuf.b   xr3,  \s1,  \s2,  xr29
-    xvshuf.b   xr4,  \s1,  \s2,  xr27
-    xvadd.h    xr3,  xr2,  xr3
-    xmov       xr2,  \s1
-    xmov       \s1,  \s3
-    xvshuf.b   \s3,  xr2,  \s2,  xr30
-    xvadd.h    xr4,  xr4,  \s2
-    xvadd.h    \s3,  \s3,  xr1
-    FILT_H     \s3,  xr3,  xr4
+    xmov             xr3,     \s1
+    xvpermi.q        xr3,     \s2,   0x03
+    xvshuf.b         xr1,     \s2,   xr3,  xr23
+    xvshuf.b         xr2,     \s2,   xr3,  xr24
+    xmov             \s1,     \s2
+    xvpermi.q        \s1,     \s3,   0x03
+    xvshuf.b         xr3,     \s1,   \s2,  xr29
+    xvshuf.b         xr4,     \s1,   \s2,  xr27
+    xvadd.h          xr3,     xr2,   xr3
+    xmov             xr2,     \s1
+    xmov             \s1,     \s3
+    xvshuf.b         \s3,     xr2,   \s2,  xr30
+    xvadd.h          xr4,     xr4,   \s2
+    xvadd.h          \s3,     \s3,   xr1
+    FILT_H \s3, xr3, xr4
 .endm
 
 .macro DO_FILT_C_LASX s1, s2, s3, s4
     FILT_C \s1, \s2, \s3
     FILT_C \s2, \s1, \s4
     FILT_PACK_LASX \s3, \s4, xr15
-    xvpermi.d   \s3, \s3, 0xd8
-    xvstx  \s3, a5,  a4
+    xvpermi.d        \s3,     \s3,   0xd8
+    xvstx            \s3,     a5,    a4
 .endm
 
 .macro DO_FILT_H_LASX s1, s2, s3
-    xmov    xr3,  \s1
-    xvpermi.q     xr3,  \s2,  0x03
-    xvshuf.b      xr1,  \s2,  xr3,  xr24
-    xvshuf.b      xr2,  \s2,  xr3,  xr25
-    xmov    xr3,  \s2
-    xvpermi.q     xr3,  \s3,  0x03
-    xvshuf.b      xr4,  xr3,  \s2,  xr26
-    xvshuf.b      xr5,  xr3,  \s2,  xr27
-    xvshuf.b      xr6,  xr3,  \s2,  xr28
-    xmov    \s1,  \s2
-    xvdp2.h.bu.b  xr16, xr1,  xr12
-    xvdp2.h.bu.b  xr17, xr2,  xr12
-    xvdp2.h.bu.b  xr18, \s2,  xr14
-    xvdp2.h.bu.b  xr19, xr4,  xr14
-    xvdp2.h.bu.b  xr20, xr5,  xr0
-    xvdp2.h.bu.b  xr21, xr6,  xr0
-    xvadd.h       xr1,  xr16, xr18
-    xvadd.h       xr2,  xr17, xr19
-    xvadd.h       xr1,  xr1,  xr20
-    xvadd.h       xr2,  xr2,  xr21
-    FILT_PACK_LASX  xr1, xr2, xr15
-    xvshuf.b        xr1, xr1, xr1,  xr22
-    xvstx    xr1,  a0,  a4
-    xmov     \s2,  \s3
+    xmov             xr3,     \s1
+    xvpermi.q        xr3,     \s2,   0x03
+    xvshuf.b         xr1,     \s2,   xr3,  xr24
+    xvshuf.b         xr2,     \s2,   xr3,  xr25
+    xmov             xr3,     \s2
+    xvpermi.q        xr3,     \s3,   0x03
+    xvshuf.b         xr4,     xr3,   \s2,  xr26
+    xvshuf.b         xr5,     xr3,   \s2,  xr27
+    xvshuf.b         xr6,     xr3,   \s2,  xr28
+    xmov             \s1,     \s2
+    xvdp2.h.bu.b     xr16,    xr1,   xr12
+    xvdp2.h.bu.b     xr17,    xr2,   xr12
+    xvdp2.h.bu.b     xr18,    \s2,   xr14
+    xvdp2.h.bu.b     xr19,    xr4,   xr14
+    xvdp2.h.bu.b     xr20,    xr5,   xr0
+    xvdp2.h.bu.b     xr21,    xr6,   xr0
+    xvadd.h          xr1,     xr16,  xr18
+    xvadd.h          xr2,     xr17,  xr19
+    xvadd.h          xr1,     xr1,   xr20
+    xvadd.h          xr2,     xr2,   xr21
+    FILT_PACK_LASX xr1, xr2, xr15
+    xvshuf.b         xr1,     xr1,   xr1,  xr22
+    xvstx            xr1,     a0,    a4
+    xmov             \s2,     \s3
 .endm
 
-//-----------------------------------------------------------------------------
-// void hpel_filter( uint8_t *dsth, uint8_t *dstv, uint8_t *dstc,
-//                   uint8_t *src, intptr_t stride, int width, int height )
-//-----------------------------------------------------------------------------
+/*
+ * void hpel_filter( uint8_t *dsth, uint8_t *dstv, uint8_t *dstc,
+ *                  uint8_t *src, intptr_t stride, int width, int height )
+ */
 function hpel_filter_lasx
-    addi.d sp,  sp, -56
-    fst.d  f24, sp, 0
-    fst.d  f25, sp, 8
-    fst.d  f26, sp, 16
-    fst.d  f27, sp, 24
-    fst.d  f28, sp, 32
-    fst.d  f29, sp, 40
-    fst.d  f30, sp, 48
-
-    move   a7,  a3
-    addi.d a5,  a5, -32
-    move   t0,  a1
-    andi   a7,  a7,  31
-    sub.d  a3,  a3,  a7
-    add.d  a0,  a0,  a5
-    add.d  t0,  t0,  a5
-    add.d  a7,  a7,  a5
-    add.d  a5,  a5,  a2
-    move   a2,  a4
-    sub.d  a7,  zero,  a7
-    add.d  a1,  a3,  a2
-    sub.d  a3,  a3,  a2
-    sub.d  a3,  a3,  a2
-    move   a4,  a7
-    la.local    t1,  filt_mul51
-    xvld  xr0,  t1,  0
-    la.local    t2,  filt_mul15
-    xvld  xr12, t2,  0
-    la.local    t3,  filt_mul20
-    xvld  xr14, t3,  0
-    la.local    t4,  pw_1024
-    xvld  xr15, t4,  0
-    la.local    t1,  hpel_shuf
-    xvld  xr22, t1,  0
-    la.local    t2,  shuf_12
-    xvld  xr23, t2,  0
-    la.local    t3,  shuf_1
-    xvld  xr26, t3,  0
-    xvaddi.bu   xr24,  xr23,  2 /* shuf_14  */
-    xvaddi.bu   xr25,  xr23,  3 /* shuf_15  */
-    xvaddi.bu   xr27,  xr26,  1 /* shuf_2   */
-    xvaddi.bu   xr28,  xr26,  2 /* shuf_3   */
-    xvaddi.bu   xr29,  xr26,  3 /* shuf_4   */
-    xvaddi.bu   xr30,  xr26,  5 /* shuf_6   */
-    xvxor.v     xr9,   xr9,   xr9
-    xvxor.v     xr10,  xr10,  xr10
+    addi.d           sp,      sp,    -56
+    fst.d            f24,     sp,    0
+    fst.d            f25,     sp,    8
+    fst.d            f26,     sp,    16
+    fst.d            f27,     sp,    24
+    fst.d            f28,     sp,    32
+    fst.d            f29,     sp,    40
+    fst.d            f30,     sp,    48
+
+    move             a7,      a3
+    addi.d           a5,      a5,    -32
+    move             t0,      a1
+    andi             a7,      a7,    31
+    sub.d            a3,      a3,    a7
+    add.d            a0,      a0,    a5
+    add.d            t0,      t0,    a5
+    add.d            a7,      a7,    a5
+    add.d            a5,      a5,    a2
+    move             a2,      a4
+    sub.d            a7,      zero,  a7
+    add.d            a1,      a3,    a2
+    sub.d            a3,      a3,    a2
+    sub.d            a3,      a3,    a2
+    move             a4,      a7
+    la.local         t1,      filt_mul51
+    xvld             xr0,     t1,    0
+    la.local         t2,      filt_mul15
+    xvld             xr12,    t2,    0
+    la.local         t3,      filt_mul20
+    xvld             xr14,    t3,    0
+    la.local         t4,      pw_1024
+    xvld             xr15,    t4,    0
+    la.local         t1,      hpel_shuf
+    xvld             xr22,    t1,    0
+    la.local         t2,      shuf_12
+    xvld             xr23,    t2,    0
+    la.local         t3,      shuf_1
+    xvld             xr26,    t3,    0
+    xvaddi.bu        xr24,    xr23,  2 /* shuf_14  */
+    xvaddi.bu        xr25,    xr23,  3 /* shuf_15  */
+    xvaddi.bu        xr27,    xr26,  1 /* shuf_2   */
+    xvaddi.bu        xr28,    xr26,  2 /* shuf_3   */
+    xvaddi.bu        xr29,    xr26,  3 /* shuf_4   */
+    xvaddi.bu        xr30,    xr26,  5 /* shuf_6   */
+    xvxor.v          xr9,     xr9,   xr9
+    xvxor.v          xr10,    xr10,  xr10
 .LOOPY:
     DO_FILT_V_LASX xr8, xr7, xr13, xr12, 0
 .LOOPX:
     DO_FILT_V_LASX xr6, xr5, xr11, xr12, 32
 .LASTX:
-    xvsrli.h  xr15, xr15, 1
-    DO_FILT_C_LASX  xr9,  xr8,  xr7,  xr6
-    xvadd.h   xr15, xr15, xr15
-    xmov      xr7,  xr5
-    DO_FILT_H_LASX  xr10, xr13, xr11
-    addi.d    a4,   a4,   32
-    blt       a4,   zero, .LOOPX
-    addi.d    t1,   a4,   -32
-    blt       t1,   zero, .LASTX
+    xvsrli.h         xr15,    xr15,  1
+    DO_FILT_C_LASX xr9, xr8, xr7, xr6
+    xvadd.h          xr15,    xr15,  xr15
+    xmov             xr7,     xr5
+    DO_FILT_H_LASX xr10, xr13, xr11
+    addi.d           a4,      a4,    32
+    blt              a4,      zero,  .LOOPX
+    addi.d           t1,      a4,    -32
+    blt              t1,      zero,  .LASTX
     //setup regs for next y
-    sub.d     a4,   a4,   a7
-    sub.d     a4,   a4,   a2
-    sub.d     a1,   a1,   a4
-    sub.d     a3,   a3,   a4
-    add.d     a0,   a0,   a2
-    add.d     t0,   t0,   a2
-    add.d     a5,   a5,   a2
-    move      a4,   a7
-    addi.d    a6,   a6,   -1
-    blt       zero, a6,   .LOOPY
-    fld.d  f24, sp, 0
-    fld.d  f25, sp, 8
-    fld.d  f26, sp, 16
-    fld.d  f27, sp, 24
-    fld.d  f28, sp, 32
-    fld.d  f29, sp, 40
-    fld.d  f30, sp, 48
-    addi.d sp,  sp, 56
+    sub.d            a4,      a4,    a7
+    sub.d            a4,      a4,    a2
+    sub.d            a1,      a1,    a4
+    sub.d            a3,      a3,    a4
+    add.d            a0,      a0,    a2
+    add.d            t0,      t0,    a2
+    add.d            a5,      a5,    a2
+    move             a4,      a7
+    addi.d           a6,      a6,    -1
+    blt              zero,    a6,    .LOOPY
+    fld.d            f24,     sp,    0
+    fld.d            f25,     sp,    8
+    fld.d            f26,     sp,    16
+    fld.d            f27,     sp,    24
+    fld.d            f28,     sp,    32
+    fld.d            f29,     sp,    40
+    fld.d            f30,     sp,    48
+    addi.d           sp,      sp,    56
 endfunc
 
-//-----------------------------------------------------------------------------
-// void pixel_avg_wxh( pixel *dst, intptr_t dst_stride, pixel *src1, intptr_t src1_stride,
-//                     pixel *src2, intptr_t src2_stride, int weight );
-//-----------------------------------------------------------------------------
+/*
+ * void pixel_avg_wxh( pixel *dst, intptr_t dst_stride, pixel *src1, intptr_t src1_stride,
+ *                     pixel *src2, intptr_t src2_stride, int weight );
+ */
 .macro PIXEL_AVG w, h
 function pixel_avg_\w\()x\h\()_lasx
-    addi.d  t0,  a6,   -32
-    addi.d  a7,  zero, \h
-    bne     t0,  zero,  pixel_avg_weight_w16_lasx
-    b       pixel_avg_w16_lasx
+    addi.d           t0,      a6,    -32
+    addi.d           a7,      zero,  \h
+    bne              t0,      zero,  pixel_avg_weight_w\w\()_lasx
+    b                pixel_avg_w\w\()_lasx
 endfunc
 .endm
 
 PIXEL_AVG 16, 16
 PIXEL_AVG 16,  8
+PIXEL_AVG  8, 16
+PIXEL_AVG  8,  8
+PIXEL_AVG  8,  4
+PIXEL_AVG  4, 16
+PIXEL_AVG  4,  8
+PIXEL_AVG  4,  4
+PIXEL_AVG  4,  2
+
+function mc_weight_w20_noden_lasx
+    xvldrepl.h       xr1,     a4,    40   // offset
+    xvldrepl.b       xr0,     a4,    36   // scale
+.LOOP_WEIGHTW20_NODEN:
+    xvld             xr3,     a2,    0
+    xvldx            xr4,     a2,    a3
+    xvmulwev.h.bu.b  xr7,     xr3,   xr0
+    xvmulwev.h.bu.b  xr8,     xr4,   xr0
+    xvmulwod.h.bu.b  xr3,     xr3,   xr0
+    xvmulwod.h.bu.b  xr4,     xr4,   xr0
+    xvsadd.h         xr7,     xr7,   xr1
+    xvsadd.h         xr8,     xr8,   xr1
+    xvsadd.h         xr3,     xr3,   xr1
+    xvsadd.h         xr4,     xr4,   xr1
+    xvmaxi.h         xr7,     xr7,   0
+    xvmaxi.h         xr8,     xr8,   0
+    xvmaxi.h         xr3,     xr3,   0
+    xvmaxi.h         xr4,     xr4,   0
+    xvssrlni.bu.h    xr8,     xr7,   0
+    xvssrlni.bu.h    xr4,     xr3,   0
+    xvilvl.b         xr3,     xr4,   xr8
+    xvilvh.b         xr4,     xr4,   xr8
+    vst              vr3,     a0,    0
+    xvstelm.w        xr3,     a0,    16,   4
+    add.d            a0,      a0,    a1
+    vst              vr4,     a0,    0
+    xvstelm.w        xr4,     a0,    16,   4
+    alsl.d           a2,      a3,    a2,   1
+    add.d            a0,      a0,    a1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHTW20_NODEN
+endfunc
+
+function mc_weight_w16_noden_lasx
+    xvldrepl.h       xr1,     a4,    40   // offset
+    xvldrepl.h       xr0,     a4,    36   // scale
+.LOOP_WEIGHTW16_NODEN:
+    vld              vr3,     a2,    0
+    vldx             vr4,     a2,    a3
+    vext2xv.hu.bu    xr3,     xr3
+    vext2xv.hu.bu    xr4,     xr4
+    xvmul.h          xr3,     xr3,   xr0
+    xvmul.h          xr4,     xr4,   xr0
+    xvsadd.h         xr3,     xr3,   xr1
+    xvsadd.h         xr4,     xr4,   xr1
+    xvmaxi.h         xr3,     xr3,   0
+    xvmaxi.h         xr4,     xr4,   0
+    xvssrlni.bu.h    xr4,     xr3,   0
+    xvpermi.d        xr3,     xr4,   8
+    xvpermi.d        xr4,     xr4,   13
+    vst              vr3,     a0,    0
+    vstx             vr4,     a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a0,      a1,    a0,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHTW16_NODEN
+endfunc
 
-.end    mc-a.S
+function mc_weight_w8_noden_lasx
+    xvldrepl.h       xr1,     a4,    40   // offset
+    xvldrepl.h       xr0,     a4,    36   // scale
+.LOOP_WEIGHTW8_NODEN:
+    fld.d            f3,      a2,    0
+    fldx.d           f4,      a2,    a3
+    vilvl.d          vr3,     vr4,   vr3
+    vext2xv.hu.bu    xr3,     xr3
+    xvmul.h          xr3,     xr3,   xr0
+    xvsadd.h         xr3,     xr3,   xr1
+    xvmaxi.h         xr3,     xr3,   0
+    xvssrlni.bu.h    xr3,     xr3,   0
+    xvstelm.d        xr3,     a0,    0,    0
+    add.d            a0,      a0,    a1
+    xvstelm.d        xr3,     a0,    0,    2
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHTW8_NODEN
+endfunc
 
+function mc_weight_w4_noden_lasx
+    xvldrepl.h       xr1,     a4,    40   // offset
+    xvldrepl.h       xr0,     a4,    36   // scale
+.LOOP_WEIGHTW4_NODEN:
+    fld.s            f3,      a2,    0
+    fldx.s           f4,      a2,    a3
+    vilvl.w          vr3,     vr4,   vr3
+    vext2xv.hu.bu    xr3,     xr3
+    xvmul.h          xr3,     xr3,   xr0
+    xvsadd.h         xr3,     xr3,   xr1
+    xvmaxi.h         xr3,     xr3,   0
+    xvssrlni.bu.h    xr3,     xr3,   0
+    xvstelm.w        xr3,     a0,    0,    0
+    add.d            a0,      a0,    a1
+    xvstelm.w        xr3,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHTW4_NODEN
+endfunc
+
+function mc_weight_w20_lasx
+    xvldrepl.h       xr1,     a4,    40   // offset
+    xvldrepl.b       xr0,     a4,    36   // scale
+    xvldrepl.h       xr2,     a4,    32   // denom
+    xvsll.h          xr1,     xr1,   xr2
+.LOOP_WEIGHTW20:
+    xvld             xr3,     a2,    0
+    xvldx            xr4,     a2,    a3
+    xvmulwev.h.bu.b  xr7,     xr3,   xr0
+    xvmulwev.h.bu.b  xr8,     xr4,   xr0
+    xvmulwod.h.bu.b  xr3,     xr3,   xr0
+    xvmulwod.h.bu.b  xr4,     xr4,   xr0
+    xvsadd.h         xr7,     xr7,   xr1
+    xvsadd.h         xr8,     xr8,   xr1
+    xvsadd.h         xr3,     xr3,   xr1
+    xvsadd.h         xr4,     xr4,   xr1
+    xvmaxi.h         xr7,     xr7,   0
+    xvmaxi.h         xr8,     xr8,   0
+    xvmaxi.h         xr3,     xr3,   0
+    xvmaxi.h         xr4,     xr4,   0
+    xvssrlrn.bu.h    xr7,     xr7,   xr2
+    xvssrlrn.bu.h    xr8,     xr8,   xr2
+    xvssrlrn.bu.h    xr3,     xr3,   xr2
+    xvssrlrn.bu.h    xr4,     xr4,   xr2
+    xvilvl.b         xr3,     xr3,   xr7
+    xvilvl.b         xr4,     xr4,   xr8
+    vst              vr3,     a0,    0
+    xvstelm.w        xr3,     a0,    16,   4
+    add.d            a0,      a0,    a1
+    vst              vr4,     a0,    0
+    xvstelm.w        xr4,     a0,    16,   4
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHTW20
+endfunc
+
+function mc_weight_w16_lasx
+    xvldrepl.h       xr1,     a4,    40   // offset
+    xvldrepl.h       xr0,     a4,    36   // scale
+    xvldrepl.h       xr2,     a4,    32   // denom
+    xvsll.h          xr1,     xr1,   xr2
+.LOOP_WEIGHTW16:
+    vld              vr3,     a2,    0
+    vldx             vr4,     a2,    a3
+    vext2xv.hu.bu    xr3,     xr3
+    vext2xv.hu.bu    xr4,     xr4
+    xvmul.h          xr3,     xr3,   xr0
+    xvmul.h          xr4,     xr4,   xr0
+    xvsadd.h         xr3,     xr3,   xr1
+    xvsadd.h         xr4,     xr4,   xr1
+    xvmaxi.h         xr3,     xr3,   0
+    xvmaxi.h         xr4,     xr4,   0
+    xvssrlrn.bu.h    xr3,     xr3,   xr2
+    xvssrlrn.bu.h    xr4,     xr4,   xr2
+    xvpermi.d        xr3,     xr3,   8
+    xvpermi.d        xr4,     xr4,   8
+    vst              vr3,     a0,    0
+    vstx             vr4,     a0,    a1
+    alsl.d           a0,      a1,    a0,   1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHTW16
+endfunc
+
+function mc_weight_w8_lasx
+    xvldrepl.h       xr1,     a4,    40   // offset
+    xvldrepl.h       xr0,     a4,    36   // scale
+    xvldrepl.h       xr2,     a4,    32   // denom
+    xvsll.h          xr1,     xr1,   xr2
+    xvxor.v          xr10,    xr10,  xr10
+    xvaddi.hu        xr10,    xr10,  1
+    xvsub.h        xr9,     xr2,   xr10
+    xvsll.h          xr11,    xr10,  xr9
+    xvadd.h          xr1,     xr1,   xr11
+.LOOP_WEIGHTW8:
+    fld.d            f3,      a2,    0
+    fldx.d           f4,      a2,    a3
+    vilvl.d          vr3,     vr4,   vr3
+    vext2xv.hu.bu    xr3,     xr3
+    xvmul.h          xr3,     xr3,   xr0
+    xvsadd.h         xr3,     xr3,   xr1
+    xvmaxi.h         xr3,     xr3,   0
+    xvssrln.bu.h     xr3,     xr3,   xr2
+    xvstelm.d        xr3,     a0,    0,    0
+    add.d            a0,      a0,    a1
+    xvstelm.d        xr3,     a0,    0,    2
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHTW8
+endfunc
+
+function mc_weight_w4_lasx
+    xvldrepl.h       xr1,     a4,    40   // offset
+    xvldrepl.h       xr0,     a4,    36   // scale
+    xvldrepl.h       xr2,     a4,    32   // denom
+    xvsll.h          xr1,     xr1,   xr2
+    xvxor.v          xr10,    xr10,  xr10
+    xvaddi.hu        xr10,    xr10,  1
+    xvsub.h          xr9,     xr2,   xr10
+    xvsll.h          xr11,    xr10,  xr9
+    xvadd.h          xr1,     xr1,   xr11
+.LOOP_WEIGHTW4:
+    fld.s            f3,      a2,    0
+    fldx.s           f4,      a2,    a3
+    vilvl.w          vr3,     vr4,   vr3
+    vext2xv.hu.bu    xr3,     xr3
+    xvmul.h          xr3,     xr3,   xr0
+    xvsadd.h         xr3,     xr3,   xr1
+    xvmaxi.h         xr3,     xr3,   0
+    xvssrln.bu.h     xr3,     xr3,   xr2
+    xvstelm.w        xr3,     a0,    0,    0
+    add.d            a0,      a0,    a1
+    xvstelm.w        xr3,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHTW4
+endfunc
 #endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/mc-c.c b/common/loongarch/mc-c.c
index 8be3f1d1..8933b8c2 100644
--- a/common/loongarch/mc-c.c
+++ b/common/loongarch/mc-c.c
@@ -36,663 +36,15 @@ static const uint8_t pu_core_mask_arr[16 * 2] =
     1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
 };
 
-static void mc_weight_w20_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
-                                const x264_weight_t *weight, int height )
-{
-    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
-    int zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
-
-    i_offset <<= i_denom;
-
-
-    __asm__ volatile(
-    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
-    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
-    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
-    "xvreplgr2vr.b    $xr0,             %[i_scale]                                   \n\t"
-    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
-    "1:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -4                     \n\t"
-    "xvld             $xr3,             %[src],               0                      \n\t"
-    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
-    "xvldx            $xr5,             %[src],               %[src_stride2]         \n\t"
-    "xvldx            $xr6,             %[src],               %[src_stride3]         \n\t"
-    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
-    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
-    "xvmulwev.h.bu.b  $xr9,             $xr5,                 $xr0                   \n\t"
-    "xvmulwev.h.bu.b  $xr10,            $xr6,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr5,             $xr5,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr6,             $xr6,                 $xr0                   \n\t"
-    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
-    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
-    "xvsadd.h         $xr9,             $xr9,                 $xr1                   \n\t"
-    "xvsadd.h         $xr10,            $xr10,                $xr1                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
-    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
-    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
-    "xvmaxi.h         $xr9,             $xr9,                 0                      \n\t"
-    "xvmaxi.h         $xr10,            $xr10,                0                      \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
-    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
-    "xvssrlrn.bu.h    $xr7,             $xr7,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr8,             $xr8,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr9,             $xr9,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr10,            $xr10,                $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr5,             $xr5,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr6,             $xr6,                 $xr2                   \n\t"
-    "xvilvl.b         $xr3,             $xr3,                 $xr7                   \n\t"
-    "xvilvl.b         $xr4,             $xr4,                 $xr8                   \n\t"
-    "xvilvl.b         $xr5,             $xr5,                 $xr9                   \n\t"
-    "xvilvl.b         $xr6,             $xr6,                 $xr10                  \n\t"
-    "vst              $vr3,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "vst              $vr4,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "vst              $vr5,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr5,             %[dst],               16,          4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "vst              $vr6,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr6,             %[dst],               16,          4         \n\t"
-    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "bge              %[height],        %[i_4],               1b                     \n\t"
-    "beqz             %[height],        3f                                           \n\t"
-    "2:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -2                     \n\t"
-    "xvld             $xr3,             %[src],               0                      \n\t"
-    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
-    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
-    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
-    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
-    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvssrlrn.bu.h    $xr7,             $xr7,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr8,             $xr8,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
-    "xvilvl.b         $xr3,             $xr3,                 $xr7                   \n\t"
-    "xvilvl.b         $xr4,             $xr4,                 $xr8                   \n\t"
-    "vst              $vr3,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "vst              $vr4,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
-    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "blt              %[zero],          %[height],            2b                     \n\t"
-    "3:                                                                              \n\t"
-    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
-      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
-    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
-      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
-    : "memory"
-    );
-}
-
-static void mc_weight_w16_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
-                                const x264_weight_t *weight, int height )
-{
-    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset, i_4 = 4;
-    int zero = 0, src_stride2, src_stride3, src_stride4, dst_stride2, dst_stride3, dst_stride4;
-
-    i_offset <<= i_denom;
-
-    __asm__ volatile(
-    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
-    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
-    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "slli.d           %[dst_stride2],   %[dst_stride],        1                      \n\t"
-    "add.d            %[dst_stride3],   %[dst_stride2],       %[dst_stride]          \n\t"
-    "slli.d           %[dst_stride4],   %[dst_stride2],       1                      \n\t"
-    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
-    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
-    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
-    "1:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -4                     \n\t"
-    "vld              $vr3,             %[src],               0                      \n\t"
-    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
-    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
-    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
-    "vext2xv.hu.bu    $xr5,             $xr5                                         \n\t"
-    "vext2xv.hu.bu    $xr6,             $xr6                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvmul.h          $xr5,             $xr5,                 $xr0                   \n\t"
-    "xvmul.h          $xr6,             $xr6,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
-    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
-    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
-    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr5,             $xr5,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr6,             $xr6,                 $xr2                   \n\t"
-    "xvpermi.d        $xr3,             $xr3,                 8                      \n\n"
-    "xvpermi.d        $xr4,             $xr4,                 8                      \n\n"
-    "xvpermi.d        $xr5,             $xr5,                 8                      \n\n"
-    "xvpermi.d        $xr6,             $xr6,                 8                      \n\n"
-    "vst              $vr3,             %[dst],               0                      \n\t"
-    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
-    "vstx             $vr5,             %[dst],               %[dst_stride2]         \n\t"
-    "vstx             $vr6,             %[dst],               %[dst_stride3]         \n\t"
-    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride4]         \n\t"
-    "bge              %[height],        %[i_4],               1b                     \n\t"
-    "beqz             %[height],        3f                                           \n\t"
-    "2:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -2                     \n\t"
-    "vld              $vr3,             %[src],               0                      \n\t"
-    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
-    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
-    "xvpermi.d        $xr3,             $xr3,                 8                      \n\n"
-    "xvpermi.d        $xr4,             $xr4,                 8                      \n\n"
-    "vst              $vr3,             %[dst],               0                      \n\t"
-    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
-    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride2]         \n\t"
-    "blt              %[zero],          %[height],            2b                     \n\t"
-    "3:                                                                              \n\t"
-    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
-      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4), [dst_stride2]"=&r"(dst_stride2),
-      [dst_stride3]"=&r"(dst_stride3), [dst_stride4]"=&r"(dst_stride4)
-    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
-      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
-    : "memory"
-    );
-}
-
-static void mc_weight_w8_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
-                               const x264_weight_t *weight, int height )
-{
-    int i_4 = 4;
-    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
-    int zero = 0, src_stride2, src_stride3, src_stride4;
-
-    i_offset <<= i_denom;
-    i_offset += (1 << ( i_denom -1 ));
-
-    __asm__ volatile(
-    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
-    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
-    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
-    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
-    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
-    "1:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -4                     \n\t"
-    "vld              $vr3,             %[src],               0                      \n\t"
-    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
-    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
-    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
-    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
-    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vilvl.d          $vr4,             $vr6,                 $vr5                   \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
-    "xvssrln.bu.h     $xr4,             $xr4,                 $xr2                   \n\t"
-    "xvstelm.d        $xr3,             %[dst],               0,            0        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.d        $xr3,             %[dst],               0,            2        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.d        $xr4,             %[dst],               0,            0        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.d        $xr4,             %[dst],               0,            2        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "bge              %[height],        %[i_4],               1b                     \n\t"
-    "beqz             %[height],        3f                                           \n\t"
-    "2:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -2                     \n\t"
-    "vld              $vr3,             %[src],               0                      \n\t"
-    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
-    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
-    "xvstelm.d        $xr3,             %[dst],               0,           0         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.d        $xr3,             %[dst],               0,           2         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
-    "blt              %[zero],          %[height],            2b                     \n\t"
-    "3:                                                                              \n\t"
-    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
-      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
-    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
-      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
-    : "memory"
-    );
-}
-
-static void mc_weight_w4_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
-                               const x264_weight_t *weight, int height )
-{
-    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
-    int zero = 0, i_4 = 4;
-
-    i_offset <<= i_denom;
-    i_offset += (1 << ( i_denom -1 ));
-
-    __asm__ volatile(
-    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
-    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
-    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
-    "1:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -4                     \n\t"
-    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vldrepl.w        $vr5,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vldrepl.w        $vr6,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vilvl.w          $vr4,             $vr6,                 $vr5                   \n\t"
-    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           5         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "bge              %[height],        %[i_4],               1b                     \n\t"
-    "beqz             %[height],        3f                                           \n\t"
-    "2:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -2                     \n\t"
-    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "blt              %[zero],          %[height],            2b                     \n\t"
-    "3:                                                                              \n\t"
-    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst)
-    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
-      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
-    : "memory"
-    );
-}
-
-static void mc_weight_w4_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
-                                     const x264_weight_t *weight, int height )
-{
-    int i_scale = weight->i_scale, i_offset = weight->i_offset;
-    int zero = 0, i_4 = 4;
-
-    __asm__ volatile(
-    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
-    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
-    "1:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -4                     \n\t"
-    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vldrepl.w        $vr5,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vldrepl.w        $vr6,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vilvl.w          $vr4,             $vr6,                 $vr5                   \n\t"
-    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           5         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "bge              %[height],        %[i_4],               1b                     \n\t"
-    "beqz             %[height],        3f                                           \n\t"
-    "2:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -2                     \n\t"
-    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
-    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
-    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "blt              %[zero],          %[height],            2b                     \n\t"
-    "3:                                                                              \n\t"
-    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst)
-    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
-      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
-    : "memory"
-    );
-}
-
-static void mc_weight_w8_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
-                                     const x264_weight_t *weight, int height )
-{
-    int i_4 = 4;
-    int i_scale = weight->i_scale, i_offset = weight->i_offset;
-    int zero = 0, src_stride2, src_stride3, src_stride4;
-
-    __asm__ volatile(
-    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
-    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
-    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
-    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
-    "1:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -4                     \n\t"
-    "vld              $vr3,             %[src],               0                      \n\t"
-    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
-    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
-    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
-    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
-    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vilvl.d          $vr4,             $vr6,                 $vr5                   \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
-    "xvstelm.d        $xr4,             %[dst],               0,            0        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.d        $xr4,             %[dst],               0,            2        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.d        $xr4,             %[dst],               0,            1        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.d        $xr4,             %[dst],               0,            3        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "bge              %[height],        %[i_4],               1b                     \n\t"
-    "beqz             %[height],        3f                                           \n\t"
-    "2:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -2                     \n\t"
-    "vld              $vr3,             %[src],               0                      \n\t"
-    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
-    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
-    "xvstelm.d        $xr3,             %[dst],               0,            0        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "xvstelm.d        $xr3,             %[dst],               0,            2        \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
-    "blt              %[zero],          %[height],            2b                     \n\t"
-    "3:                                                                              \n\t"
-    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
-      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
-    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
-      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
-    : "memory"
-    );
-}
-
-static void mc_weight_w16_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
-                                      const x264_weight_t *weight, int height )
-{
-    int i_4 = 4;
-    int i_scale = weight->i_scale, i_offset = weight->i_offset;
-    int zero = 0, src_stride2, src_stride3, src_stride4, dst_stride2, dst_stride3, dst_stride4;
-
-    __asm__ volatile(
-    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
-    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
-    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "slli.d           %[dst_stride2],   %[dst_stride],        1                      \n\t"
-    "add.d            %[dst_stride3],   %[dst_stride2],       %[dst_stride]          \n\t"
-    "slli.d           %[dst_stride4],   %[dst_stride2],       1                      \n\t"
-    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
-    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
-    "1:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -4                     \n\t"
-    "vld              $vr3,             %[src],               0                      \n\t"
-    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
-    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
-    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
-    "vext2xv.hu.bu    $xr5,             $xr5                                         \n\t"
-    "vext2xv.hu.bu    $xr6,             $xr6                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvmul.h          $xr5,             $xr5,                 $xr0                   \n\t"
-    "xvmul.h          $xr6,             $xr6,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
-    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
-    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr6,             $xr5,                 0                      \n\t"
-    "xvpermi.d        $xr3,             $xr4,                 8                      \n\t"
-    "xvpermi.d        $xr4,             $xr4,                 13                     \n\t"
-    "xvpermi.d        $xr5,             $xr6,                 8                      \n\t"
-    "xvpermi.d        $xr6,             $xr6,                 13                     \n\t"
-    "vst              $vr3,             %[dst],               0                      \n\t"
-    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
-    "vstx             $vr5,             %[dst],               %[dst_stride2]         \n\t"
-    "vstx             $vr6,             %[dst],               %[dst_stride3]         \n\t"
-    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride4]         \n\t"
-    "bge              %[height],        %[i_4],               1b                     \n\t"
-    "beqz             %[height],        3f                                           \n\t"
-    "2:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -2                     \n\t"
-    "vld              $vr3,             %[src],               0                      \n\t"
-    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
-    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
-    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
-    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
-    "xvpermi.d        $xr3,             $xr4,                 8                      \n\t"
-    "xvpermi.d        $xr4,             $xr4,                 13                     \n\t"
-    "vst              $vr3,             %[dst],               0                      \n\t"
-    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
-    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride2]         \n\t"
-    "blt              %[zero],          %[height],            2b                     \n\t"
-    "3:                                                                              \n\t"
-    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
-      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4), [dst_stride2]"=&r"(dst_stride2),
-      [dst_stride3]"=&r"(dst_stride3), [dst_stride4]"=&r"(dst_stride4)
-    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
-      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
-    : "memory"
-    );
-}
-
-static void mc_weight_w20_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
-                                      const x264_weight_t *weight, int height )
-{
-    int i_scale = weight->i_scale, i_offset = weight->i_offset;
-    int zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
-
-    __asm__ volatile(
-    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
-    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
-    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
-    "xvreplgr2vr.b    $xr0,             %[i_scale]                                   \n\t"
-    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
-    "1:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -4                     \n\t"
-    "xvld             $xr3,             %[src],               0                      \n\t"
-    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
-    "xvldx            $xr5,             %[src],               %[src_stride2]         \n\t"
-    "xvldx            $xr6,             %[src],               %[src_stride3]         \n\t"
-    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
-    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
-    "xvmulwev.h.bu.b  $xr9,             $xr5,                 $xr0                   \n\t"
-    "xvmulwev.h.bu.b  $xr10,            $xr6,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr5,             $xr5,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr6,             $xr6,                 $xr0                   \n\t"
-    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
-    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
-    "xvsadd.h         $xr9,             $xr9,                 $xr1                   \n\t"
-    "xvsadd.h         $xr10,            $xr10,                $xr1                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
-    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
-    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
-    "xvmaxi.h         $xr9,             $xr9,                 0                      \n\t"
-    "xvmaxi.h         $xr10,            $xr10,                0                      \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
-    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr8,             $xr7,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr10,            $xr9,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr6,             $xr5,                 0                      \n\t"
-    "xvilvl.b         $xr3,             $xr4,                 $xr8                   \n\t"
-    "xvilvh.b         $xr4,             $xr4,                 $xr8                   \n\t"
-    "xvilvl.b         $xr5,             $xr6,                 $xr10                  \n\t"
-    "xvilvh.b         $xr6,             $xr6,                 $xr10                  \n\t"
-    "vst              $vr3,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "vst              $vr4,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "vst              $vr5,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr5,             %[dst],               16,          4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "vst              $vr6,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr6,             %[dst],               16,          4         \n\t"
-    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "bge              %[height],        %[i_4],               1b                     \n\t"
-    "beqz             %[height],        3f                                           \n\t"
-    "2:                                                                              \n\t"
-    "addi.d           %[height],        %[height],            -2                     \n\t"
-    "xvld             $xr3,             %[src],               0                      \n\t"
-    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
-    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
-    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
-    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
-    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
-    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
-    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
-    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
-    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
-    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
-    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
-    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr8,             $xr7,                 0                      \n\t"
-    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
-    "xvilvl.b         $xr3,             $xr4,                 $xr8                   \n\t"
-    "xvilvh.b         $xr4,             $xr4,                 $xr8                   \n\t"
-    "vst              $vr3,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "vst              $vr4,             %[dst],               0                      \n\t"
-    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
-    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
-    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
-    "blt              %[zero],          %[height],            2b                     \n\t"
-    "3:                                                                              \n\t"
-    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
-      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
-    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
-      [zero]"r"(zero), [i_4]"r"(i_4), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
-    : "memory"
-    );
-}
-
 #define MC_WEIGHT(func)                                                                                             \
 static void (* mc##func##_wtab_lasx[6])( uint8_t *, intptr_t, uint8_t *, intptr_t, const x264_weight_t *, int ) =   \
 {                                                                                                                   \
-    mc_weight_w4##func##_lasx,                                                                                 \
-    mc_weight_w4##func##_lasx,                                                                                 \
-    mc_weight_w8##func##_lasx,                                                                                 \
-    mc_weight_w16##func##_lasx,                                                                                \
-    mc_weight_w16##func##_lasx,                                                                                \
-    mc_weight_w20##func##_lasx,                                                                                \
+    x264_mc_weight_w4##func##_lasx,                                                                                 \
+    x264_mc_weight_w4##func##_lasx,                                                                                 \
+    x264_mc_weight_w8##func##_lasx,                                                                                 \
+    x264_mc_weight_w16##func##_lasx,                                                                           \
+    x264_mc_weight_w16##func##_lasx,                                                                           \
+    x264_mc_weight_w20##func##_lasx,                                                                           \
 };
 
 #if !HIGH_BIT_DEPTH
@@ -712,498 +64,14 @@ static void weight_cache_lasx( x264_t *h, x264_weight_t *w )
 
 static weight_fn_t mc_weight_wtab_lasx[6] =
 {
-    mc_weight_w4_lasx,
-    mc_weight_w4_lasx,
-    mc_weight_w8_lasx,
-    mc_weight_w16_lasx,
-    mc_weight_w16_lasx,
-    mc_weight_w20_lasx,
+    x264_mc_weight_w4_lasx,
+    x264_mc_weight_w4_lasx,
+    x264_mc_weight_w8_lasx,
+    x264_mc_weight_w16_lasx,
+    x264_mc_weight_w16_lasx,
+    x264_mc_weight_w20_lasx,
 };
 
-static void avc_biwgt_opscale_4x2_nw_lasx( uint8_t *p_src1,
-                                           int32_t i_src1_stride,
-                                           uint8_t *p_src2,
-                                           int32_t i_src2_stride,
-                                           uint8_t *p_dst, int32_t i_dst_stride,
-                                           int32_t i_log2_denom,
-                                           int32_t i_src1_weight,
-                                           int32_t i_src2_weight )
-{
-    __m256i src1_wgt, src2_wgt, wgt;
-    __m256i src0, src1, src2;
-    __m256i denom;
-
-    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
-    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
-
-    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
-
-    src0 = __lasx_xvldrepl_w( p_src1, 0 );
-    p_src1 += i_src1_stride;
-    src1 = __lasx_xvldrepl_w( p_src1, 0 );
-    src2 = __lasx_xvpackev_w( src1, src0 );
-
-    src0 = __lasx_xvldrepl_w( p_src2, 0 );
-    p_src2 += i_src2_stride;
-    src1 = __lasx_xvldrepl_w( p_src2, 0 );
-    src0 = __lasx_xvpackev_w( src1, src0 );
-
-    src0 = __lasx_xvilvl_b( src0, src2 );
-
-    src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);
-    src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);
-    src0 = __lasx_xvmaxi_h( src0, 0 );
-    src0 = __lasx_xvssrlrn_bu_h(src0, denom);
-
-    __lasx_xvstelm_w(src0, p_dst, 0, 0);
-    __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
-}
-
-static void avc_biwgt_opscale_4x4multiple_nw_lasx( uint8_t *p_src1,
-                                                   int32_t i_src1_stride,
-                                                   uint8_t *p_src2,
-                                                   int32_t i_src2_stride,
-                                                   uint8_t *p_dst,
-                                                   int32_t i_dst_stride,
-                                                   int32_t i_height,
-                                                   int32_t i_log2_denom,
-                                                   int32_t i_src1_weight,
-                                                   int32_t i_src2_weight )
-{
-    uint8_t u_cnt;
-    __m256i src1_wgt, src2_wgt, wgt;
-    __m256i src0, src1, src2, src3, tmp0;
-    __m256i denom;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-    int32_t i_dst_stride_x4 = i_dst_stride << 2;
-    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
-
-    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
-    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
-
-    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
-
-    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
-    {
-        src0 = __lasx_xvldrepl_w( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src1 = __lasx_xvldrepl_w( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src2 = __lasx_xvldrepl_w( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src3 = __lasx_xvldrepl_w( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src0 = __lasx_xvpackev_w( src1, src0 );
-        src1 = __lasx_xvpackev_w( src3, src2 );
-        tmp0 = __lasx_xvpermi_q( src0, src1, 0x02 );
-
-        src0 = __lasx_xvldrepl_w( p_src2, 0 );
-        p_src2 += i_src2_stride;
-        src1 = __lasx_xvldrepl_w( p_src2, 0 );
-        p_src2 += i_src2_stride;
-        src2 = __lasx_xvldrepl_w( p_src2, 0 );
-        p_src2 += i_src2_stride;
-        src3 = __lasx_xvldrepl_w( p_src2, 0 );
-        p_src2 += i_src2_stride;
-        src0 = __lasx_xvpackev_w( src1, src0 );
-        src1 = __lasx_xvpackev_w( src3, src2 );
-        src0 = __lasx_xvpermi_q( src0, src1, 0x02 );
-
-        src0 = __lasx_xvilvl_b( src0, tmp0 );
-
-        src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);
-        src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);
-        src0 = __lasx_xvmaxi_h( src0, 0 );
-        src0 = __lasx_xvssrlrn_bu_h(src0, denom);
-
-        __lasx_xvstelm_w(src0, p_dst, 0, 0);
-        __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
-        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x2, 0, 4);
-        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x3, 0, 5);
-        p_dst += i_dst_stride_x4;
-    }
-}
-
-static void avc_biwgt_opscale_4width_nw_lasx( uint8_t *p_src1,
-                                              int32_t i_src1_stride,
-                                              uint8_t *p_src2,
-                                              int32_t i_src2_stride,
-                                              uint8_t *p_dst,
-                                              int32_t i_dst_stride,
-                                              int32_t i_height,
-                                              int32_t i_log2_denom,
-                                              int32_t i_src1_weight,
-                                              int32_t i_src2_weight )
-{
-    if( 2 == i_height )
-    {
-        avc_biwgt_opscale_4x2_nw_lasx( p_src1, i_src1_stride,
-                                       p_src2, i_src2_stride,
-                                       p_dst, i_dst_stride,
-                                       i_log2_denom, i_src1_weight,
-                                       i_src2_weight );
-    }
-    else
-    {
-        avc_biwgt_opscale_4x4multiple_nw_lasx( p_src1, i_src1_stride,
-                                               p_src2, i_src2_stride,
-                                               p_dst, i_dst_stride,
-                                               i_height, i_log2_denom,
-                                               i_src1_weight,
-                                               i_src2_weight );
-    }
-}
-
-static void avc_biwgt_opscale_8width_nw_lasx( uint8_t *p_src1,
-                                              int32_t i_src1_stride,
-                                              uint8_t *p_src2,
-                                              int32_t i_src2_stride,
-                                              uint8_t *p_dst,
-                                              int32_t i_dst_stride,
-                                              int32_t i_height,
-                                              int32_t i_log2_denom,
-                                              int32_t i_src1_weight,
-                                              int32_t i_src2_weight )
-{
-    uint8_t u_cnt;
-    __m256i src1_wgt, src2_wgt, wgt;
-    __m256i src0, src1, src2, src3;
-    __m256i denom;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-
-    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
-    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
-
-    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
-
-#define BIWGT_OPSCALE_8W_NW                              \
-    src0 = __lasx_xvldrepl_d( p_src1, 0 );               \
-    p_src1 += i_src1_stride;                             \
-    src1 = __lasx_xvldrepl_d( p_src1, 0 );               \
-    p_src1 += i_src1_stride;                             \
-                                                         \
-    src2 = __lasx_xvldrepl_d( p_src2, 0 );               \
-    p_src2 += i_src2_stride;                             \
-    src3 = __lasx_xvldrepl_d( p_src2, 0 );               \
-    p_src2 += i_src2_stride;                             \
-                                                         \
-    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );         \
-    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );         \
-    src0 = __lasx_xvilvl_b( src1, src0 );                \
-                                                         \
-    src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);            \
-    src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);     \
-    src0 = __lasx_xvmaxi_h( src0, 0 );                   \
-    src0 = __lasx_xvssrlrn_bu_h(src0, denom);            \
-                                                         \
-    __lasx_xvstelm_d(src0, p_dst, 0, 0);                 \
-    __lasx_xvstelm_d(src0, p_dst + i_dst_stride, 0, 2);  \
-    p_dst += i_dst_stride_x2;                            \
-
-    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
-    {
-        BIWGT_OPSCALE_8W_NW;
-        BIWGT_OPSCALE_8W_NW;
-    }
-
-#undef BIWGT_OPSCALE_8W_NW
-
-}
-
-static void avc_biwgt_opscale_4x2_lasx( uint8_t *p_src1,
-                                        int32_t i_src1_stride,
-                                        uint8_t *p_src2,
-                                        int32_t i_src2_stride,
-                                        uint8_t *p_dst, int32_t i_dst_stride,
-                                        int32_t i_log2_denom,
-                                        int32_t i_src1_weight,
-                                        int32_t i_src2_weight,
-                                        int32_t i_offset_in )
-{
-    __m256i src1_wgt, src2_wgt, wgt;
-    __m256i src0, src1, src2;
-    __m256i denom, offset;
-
-    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
-
-    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
-    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
-    offset = __lasx_xvreplgr2vr_h( i_offset_in );
-
-    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
-
-    src0 = __lasx_xvldrepl_w( p_src1, 0 );
-    p_src1 += i_src1_stride;
-    src1 = __lasx_xvldrepl_w( p_src1, 0 );
-    src2 = __lasx_xvpackev_w( src1, src0 );
-
-    src0 = __lasx_xvldrepl_w( p_src2, 0 );
-    p_src2 += i_src2_stride;
-    src1 = __lasx_xvldrepl_w( p_src2, 0 );
-    src0 = __lasx_xvpackev_w( src1, src0 );
-
-    src0 = __lasx_xvilvl_b( src0, src2 );
-
-    src0 = __lasx_xvdp2_h_bu( src0, wgt );
-    src0 = __lasx_xvsadd_h( src0, offset );
-    src0 = __lasx_xvmaxi_h( src0, 0 );
-    src0 = __lasx_xvssrln_bu_h(src0, denom);
-
-    __lasx_xvstelm_w(src0, p_dst, 0, 0);
-    __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
-}
-
-static void avc_biwgt_opscale_4x4multiple_lasx( uint8_t *p_src1,
-                                                int32_t i_src1_stride,
-                                                uint8_t *p_src2,
-                                                int32_t i_src2_stride,
-                                                uint8_t *p_dst,
-                                                int32_t i_dst_stride,
-                                                int32_t i_height,
-                                                int32_t i_log2_denom,
-                                                int32_t i_src1_weight,
-                                                int32_t i_src2_weight,
-                                                int32_t i_offset_in )
-{
-    uint8_t u_cnt;
-    __m256i src1_wgt, src2_wgt, wgt;
-    __m256i src0, src1, src2, src3, tmp0;
-    __m256i denom, offset;
-    int32_t i_dst_stride_x2 = i_dst_stride << 1;
-    int32_t i_dst_stride_x4 = i_dst_stride << 2;
-    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
-
-    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
-
-    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
-    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
-    offset = __lasx_xvreplgr2vr_h( i_offset_in );
-
-    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
-
-    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
-    {
-        src0 = __lasx_xvldrepl_w( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src1 = __lasx_xvldrepl_w( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src2 = __lasx_xvldrepl_w( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src3 = __lasx_xvldrepl_w( p_src1, 0 );
-        p_src1 += i_src1_stride;
-        src0 = __lasx_xvpackev_w( src1, src0 );
-        src1 = __lasx_xvpackev_w( src3, src2 );
-        tmp0 = __lasx_xvpermi_q( src0, src1, 0x02 );
-
-        src0 = __lasx_xvldrepl_w( p_src2, 0 );
-        p_src2 += i_src2_stride;
-        src1 = __lasx_xvldrepl_w( p_src2, 0 );
-        p_src2 += i_src2_stride;
-        src2 = __lasx_xvldrepl_w( p_src2, 0 );
-        p_src2 += i_src2_stride;
-        src3 = __lasx_xvldrepl_w( p_src2, 0 );
-        p_src2 += i_src2_stride;
-        src0 = __lasx_xvpackev_w( src1, src0 );
-        src1 = __lasx_xvpackev_w( src3, src2 );
-        src0 = __lasx_xvpermi_q( src0, src1, 0x02 );
-
-        src0 = __lasx_xvilvl_b( src0, tmp0 );
-
-        src0 = __lasx_xvdp2_h_bu( src0, wgt );
-        src0 = __lasx_xvsadd_h( src0, offset );
-        src0 = __lasx_xvmaxi_h( src0, 0 );
-        src0 = __lasx_xvssrln_bu_h(src0, denom);
-
-        __lasx_xvstelm_w(src0, p_dst, 0, 0);
-        __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
-        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x2, 0, 4);
-        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x3, 0, 5);
-        p_dst += i_dst_stride_x4;
-    }
-}
-
-static void avc_biwgt_opscale_4width_lasx( uint8_t *p_src1,
-                                           int32_t i_src1_stride,
-                                           uint8_t *p_src2,
-                                           int32_t i_src2_stride,
-                                           uint8_t *p_dst,
-                                           int32_t i_dst_stride,
-                                           int32_t i_height,
-                                           int32_t i_log2_denom,
-                                           int32_t i_src1_weight,
-                                           int32_t i_src2_weight,
-                                           int32_t i_offset_in )
-{
-    if( 2 == i_height )
-    {
-        avc_biwgt_opscale_4x2_lasx( p_src1, i_src1_stride,
-                                    p_src2, i_src2_stride,
-                                    p_dst, i_dst_stride,
-                                    i_log2_denom, i_src1_weight,
-                                    i_src2_weight, i_offset_in );
-    }
-    else
-    {
-        avc_biwgt_opscale_4x4multiple_lasx( p_src1, i_src1_stride,
-                                            p_src2, i_src2_stride,
-                                            p_dst, i_dst_stride,
-                                            i_height, i_log2_denom,
-                                            i_src1_weight,
-                                            i_src2_weight, i_offset_in );
-    }
-}
-
-static void avc_biwgt_opscale_8width_lasx( uint8_t *p_src1,
-                                           int32_t i_src1_stride,
-                                           uint8_t *p_src2,
-                                           int32_t i_src2_stride,
-                                           uint8_t *p_dst,
-                                           int32_t i_dst_stride,
-                                           int32_t i_height,
-                                           int32_t i_log2_denom,
-                                           int32_t i_src1_weight,
-                                           int32_t i_src2_weight,
-                                           int32_t i_offset_in )
-{
-    uint8_t u_cnt;
-    __m256i src1_wgt, src2_wgt, wgt;
-    __m256i src0, src1, src2, src3;
-    __m256i denom, offset;
-    int32_t i_dst_stride_x2 = ( i_dst_stride << 1 );
-
-    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
-
-    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
-    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
-    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
-    offset = __lasx_xvreplgr2vr_h( i_offset_in );
-
-    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
-
-#define BIWGT_OPSCALE_8W                                 \
-    src0 = __lasx_xvldrepl_d( p_src1, 0 );               \
-    p_src1 += i_src1_stride;                             \
-    src1 = __lasx_xvldrepl_d( p_src1, 0 );               \
-    p_src1 += i_src1_stride;                             \
-                                                         \
-    src2 = __lasx_xvldrepl_d( p_src2, 0 );               \
-    p_src2 += i_src2_stride;                             \
-    src3 = __lasx_xvldrepl_d( p_src2, 0 );               \
-    p_src2 += i_src2_stride;                             \
-                                                         \
-    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );         \
-    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );         \
-    src0 = __lasx_xvilvl_b( src1, src0 );                \
-                                                         \
-    src0 = __lasx_xvdp2_h_bu( src0, wgt );               \
-    src0 = __lasx_xvsadd_h( src0, offset );              \
-    src0 = __lasx_xvmaxi_h( src0, 0 );                   \
-    src0 = __lasx_xvssrln_bu_h(src0, denom);             \
-                                                         \
-    __lasx_xvstelm_d(src0, p_dst, 0, 0);                 \
-    __lasx_xvstelm_d(src0, p_dst + i_dst_stride, 0, 2);  \
-    p_dst += i_dst_stride_x2;
-
-    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
-    {
-        BIWGT_OPSCALE_8W;
-        BIWGT_OPSCALE_8W;
-    }
-
-#undef BIWGT_OPSCALE_8W
-
-}
-
-static void avg_src_width4_lasx( uint8_t *p_src1, int32_t i_src1_stride,
-                                 uint8_t *p_src2, int32_t i_src2_stride,
-                                 uint8_t *p_dst, int32_t i_dst_stride,
-                                 int32_t i_height )
-{
-    int32_t i_cnt;
-    __m256i src0, src1;
-    __m256i dst0, dst1;
-    int32_t i_src1_stride_x2 = i_src1_stride << 1;
-    int32_t i_src2_stride_x2 = i_src2_stride << 1;
-
-    for( i_cnt = ( i_height >> 1 ); i_cnt--; )
-    {
-        DUP2_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, src0, src1);
-        p_src1 += i_src1_stride_x2;
-        DUP2_ARG2(__lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, dst0, dst1);
-        p_src2 += i_src2_stride_x2;
-
-        DUP2_ARG2( __lasx_xvavgr_bu, src0, dst0, src1, dst1, dst0, dst1 );
-        __lasx_xvstelm_w( dst0, p_dst, 0, 0 );
-        p_dst += i_dst_stride;
-        __lasx_xvstelm_w( dst1, p_dst, 0, 0 );
-        p_dst += i_dst_stride;
-    }
-}
-
-static void avg_src_width8_lasx( uint8_t *p_src1, int32_t i_src1_stride,
-                                 uint8_t *p_src2, int32_t i_src2_stride,
-                                 uint8_t *p_dst, int32_t i_dst_stride,
-                                 int32_t i_height )
-{
-    int32_t i_cnt = i_height >> 2;
-    int32_t i_src1_stride_x2, i_src1_stride_x3, i_src1_stride_x4;
-    int32_t i_src2_stride_x2, i_src2_stride_x3, i_src2_stride_x4;
-
-    __asm__ volatile(
-    "slli.w    %[src1_stride2],  %[src1_stride1],  1                    \n\t"
-    "add.w     %[src1_stride3],  %[src1_stride2],  %[src1_stride1]      \n\t"
-    "slli.w    %[src1_stride4],  %[src1_stride1],  2                    \n\t"
-    "slli.w    %[src2_stride2],  %[src2_stride1],  1                    \n\t"
-    "add.w     %[src2_stride3],  %[src2_stride2],  %[src2_stride1]      \n\t"
-    "slli.w    %[src2_stride4],  %[src2_stride1],  2                    \n\t"
-    "beqz      %[cnt],           2f                                     \n\t"
-    "1:                                                                 \n\t"
-    "addi.w    %[cnt],           %[cnt],           -1                   \n\t"
-    "vld       $vr0,             %[src1],          0                    \n\t"
-    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
-    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
-    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
-    "vld       $vr4,             %[src2],          0                    \n\t"
-    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
-    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
-    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
-    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
-    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
-    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
-    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
-    "vstelm.d  $vr0,             %[dst],           0,              0    \n\t"
-    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
-    "vstelm.d  $vr1,             %[dst],           0,              0    \n\t"
-    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
-    "vstelm.d  $vr2,             %[dst],           0,              0    \n\t"
-    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
-    "vstelm.d  $vr3,             %[dst],           0,              0    \n\t"
-    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
-    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
-    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
-    "bnez      %[cnt],           1b                                     \n\t"
-    "2:                                                                 \n\t"
-     : [src1]"+&r"(p_src1),
-       [src2]"+&r"(p_src2),
-       [src1_stride2]"=&r"(i_src1_stride_x2),
-       [src1_stride3]"=&r"(i_src1_stride_x3),
-       [src1_stride4]"=&r"(i_src1_stride_x4),
-       [src2_stride2]"=&r"(i_src2_stride_x2),
-       [src2_stride3]"=&r"(i_src2_stride_x3),
-       [src2_stride4]"=&r"(i_src2_stride_x4),
-       [dst]"+&r"(p_dst), [cnt]"+&r"(i_cnt)
-     : [src1_stride1]"r"(i_src1_stride),
-       [src2_stride1]"r"(i_src2_stride),
-       [dst_stride1]"r"(i_dst_stride)
-     : "memory"
-    );
-}
-
 static void *x264_memcpy_aligned_lasx(void *dst, const void *src, size_t n)
 {
     int64_t zero = 0, d;
@@ -1244,188 +112,6 @@ static void *x264_memcpy_aligned_lasx(void *dst, const void *src, size_t n)
     return NULL;
 }
 
-static void pixel_avg_8x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                 uint8_t *p_pix2, intptr_t pix2_stride,
-                                 uint8_t *p_pix3, intptr_t pix3_stride,
-                                 int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                             p_pix1, pix1_stride, 16 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
-                                          p_pix3, pix3_stride,
-                                          p_pix1, pix1_stride, 16, 5, i_weight,
-                                          ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
-                                       p_pix3, pix3_stride,
-                                       p_pix1, pix1_stride, 16, 5, i_weight,
-                                       ( 64 - i_weight ), 0 );
-    }
-}
-
-static void pixel_avg_8x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                uint8_t *p_pix2, intptr_t pix2_stride,
-                                uint8_t *p_pix3, intptr_t pix3_stride,
-                                int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                             p_pix1, pix1_stride, 8 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
-                                          p_pix3, pix3_stride,
-                                          p_pix1, pix1_stride, 8, 5, i_weight,
-                                          ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
-                                       p_pix3, pix3_stride,
-                                       p_pix1, pix1_stride, 8, 5, i_weight,
-                                       ( 64 - i_weight ), 0 );
-    }
-}
-
-static void pixel_avg_8x4_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                uint8_t *p_pix2, intptr_t pix2_stride,
-                                uint8_t *p_pix3, intptr_t pix3_stride,
-                                int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                             p_pix1, pix1_stride, 4 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
-                                          p_pix3, pix3_stride,
-                                          p_pix1, pix1_stride, 4, 5, i_weight,
-                                          ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
-                                       p_pix3, pix3_stride,
-                                       p_pix1, pix1_stride, 4, 5, i_weight,
-                                       ( 64 - i_weight ), 0 );
-    }
-}
-
-static void pixel_avg_4x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                 uint8_t *p_pix2, intptr_t pix2_stride,
-                                 uint8_t *p_pix3, intptr_t pix3_stride,
-                                 int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                             p_pix1, pix1_stride, 16 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
-                                          p_pix3, pix3_stride,
-                                          p_pix1, pix1_stride, 16, 5, i_weight,
-                                          ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
-                                       p_pix3, pix3_stride,
-                                       p_pix1, pix1_stride, 16, 5, i_weight,
-                                       ( 64 - i_weight ), 0 );
-    }
-}
-
-static void pixel_avg_4x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                uint8_t *p_pix2, intptr_t pix2_stride,
-                                uint8_t *p_pix3, intptr_t pix3_stride,
-                                int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                             p_pix1, pix1_stride, 8 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
-                                          p_pix3, pix3_stride,
-                                          p_pix1, pix1_stride, 8, 5, i_weight,
-                                          ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
-                                       p_pix3, pix3_stride,
-                                       p_pix1, pix1_stride, 8, 5, i_weight,
-                                       ( 64 - i_weight ), 0 );
-    }
-}
-
-static void pixel_avg_4x4_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                uint8_t *p_pix2, intptr_t pix2_stride,
-                                uint8_t *p_pix3, intptr_t pix3_stride,
-                                int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                             p_pix1, pix1_stride, 4 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
-                                          p_pix3, pix3_stride,
-                                          p_pix1, pix1_stride, 4, 5, i_weight,
-                                          ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
-                                       p_pix3, pix3_stride,
-                                       p_pix1, pix1_stride, 4, 5, i_weight,
-                                       ( 64 - i_weight ), 0 );
-    }
-}
-
-static void pixel_avg_4x2_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
-                                uint8_t *p_pix2, intptr_t pix2_stride,
-                                uint8_t *p_pix3, intptr_t pix3_stride,
-                                int32_t i_weight )
-{
-    if( 32 == i_weight )
-    {
-        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
-                             p_pix1, pix1_stride, 2 );
-    }
-    else if( i_weight < 0 || i_weight > 63 )
-    {
-        avc_biwgt_opscale_4x2_nw_lasx( p_pix2, pix2_stride,
-                                       p_pix3, pix3_stride,
-                                       p_pix1, pix1_stride, 5, i_weight,
-                                       ( 64 - i_weight ) );
-    }
-    else
-    {
-        avc_biwgt_opscale_4x2_lasx( p_pix2, pix2_stride,
-                                    p_pix3, pix3_stride,
-                                    p_pix1, pix1_stride, 5, i_weight,
-                                    ( 64 - i_weight ), 0 );
-    }
-}
-
 static inline void avg_src_width16_no_align_lasx( uint8_t *p_src1,
                                                   int32_t i_src1_stride,
                                                   uint8_t *p_src2,
@@ -3242,13 +1928,13 @@ void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
 
         pf->avg[PIXEL_16x16]= x264_pixel_avg_16x16_lasx;
         pf->avg[PIXEL_16x8] = x264_pixel_avg_16x8_lasx;
-        pf->avg[PIXEL_8x16] = pixel_avg_8x16_lasx;
-        pf->avg[PIXEL_8x8] = pixel_avg_8x8_lasx;
-        pf->avg[PIXEL_8x4] = pixel_avg_8x4_lasx;
-        pf->avg[PIXEL_4x16] = pixel_avg_4x16_lasx;
-        pf->avg[PIXEL_4x8] = pixel_avg_4x8_lasx;
-        pf->avg[PIXEL_4x4] = pixel_avg_4x4_lasx;
-        pf->avg[PIXEL_4x2] = pixel_avg_4x2_lasx;
+        pf->avg[PIXEL_8x16] = x264_pixel_avg_8x16_lasx;
+        pf->avg[PIXEL_8x8] = x264_pixel_avg_8x8_lasx;
+        pf->avg[PIXEL_8x4] = x264_pixel_avg_8x4_lasx;
+        pf->avg[PIXEL_4x16] = x264_pixel_avg_4x16_lasx;
+        pf->avg[PIXEL_4x8] = x264_pixel_avg_4x8_lasx;
+        pf->avg[PIXEL_4x4] = x264_pixel_avg_4x4_lasx;
+        pf->avg[PIXEL_4x2] = x264_pixel_avg_4x2_lasx;
 
         pf->weight = mc_weight_wtab_lasx;
         pf->offsetadd = mc_weight_wtab_lasx;
diff --git a/common/loongarch/mc.h b/common/loongarch/mc.h
index 29b35250..ed91585b 100644
--- a/common/loongarch/mc.h
+++ b/common/loongarch/mc.h
@@ -34,8 +34,39 @@ void x264_mc_init_loongarch( int cpu, x264_mc_functions_t *pf );
 void x264_pixel_avg_16x16_lasx( pixel *, intptr_t, pixel *, intptr_t, pixel *, intptr_t, int );
 #define x264_pixel_avg_16x8_lasx x264_template(pixel_avg_16x8_lasx)
 void x264_pixel_avg_16x8_lasx( pixel *, intptr_t, pixel *, intptr_t, pixel *, intptr_t, int );
+#define x264_pixel_avg_8x16_lasx x264_template(pixel_avg_8x16_lasx)
+void x264_pixel_avg_8x16_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_8x8_lasx x264_template(pixel_avg_8x8_lasx)
+void x264_pixel_avg_8x8_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_8x4_lasx x264_template(pixel_avg_8x4_lasx)
+void x264_pixel_avg_8x4_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_4x16_lasx x264_template(pixel_avg_4x16_lasx)
+void x264_pixel_avg_4x16_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_4x8_lasx x264_template(pixel_avg_4x8_lasx)
+void x264_pixel_avg_4x8_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_4x4_lasx x264_template(pixel_avg_4x4_lasx)
+void x264_pixel_avg_4x4_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_4x2_lasx x264_template(pixel_avg_4x2_lasx)
+void x264_pixel_avg_4x2_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+
+#define x264_mc_weight_w20_lasx x264_template(mc_weight_w20_lasx)
+void x264_mc_weight_w20_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w20_noden_lasx x264_template(mc_weight_w20_noden_lasx)
+void x264_mc_weight_w20_noden_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w16_lasx x264_template(mc_weight_w16_lasx)
+void x264_mc_weight_w16_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w16_noden_lasx x264_template(mc_weight_w16_noden_lasx)
+void x264_mc_weight_w16_noden_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w8_lasx x264_template(mc_weight_w8_lasx)
+void x264_mc_weight_w8_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w8_noden_lasx x264_template(mc_weight_w8_noden_lasx)
+void x264_mc_weight_w8_noden_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w4_lasx x264_template(mc_weight_w4_lasx)
+void x264_mc_weight_w4_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w4_noden_lasx x264_template(mc_weight_w4_noden_lasx)
+void x264_mc_weight_w4_noden_lasx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+
 #define x264_hpel_filter_lasx x264_template(hpel_filter_lasx)
 void x264_hpel_filter_lasx( pixel *, pixel *, pixel *, pixel *, intptr_t, int, int, int16_t * );
 
-
 #endif
diff --git a/common/loongarch/quant-a.S b/common/loongarch/quant-a.S
index 49dcb7a8..196546bf 100644
--- a/common/loongarch/quant-a.S
+++ b/common/loongarch/quant-a.S
@@ -26,35 +26,673 @@
 
 #include "asm.S"
 
+const last64_shuf
+.int 0, 4, 1, 5, 2, 6, 3, 7
+endconst
+
+const decimate_table4
+.byte 3, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+endconst
+
+const decimate_table8
+.byte 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1
+.byte 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0
+.byte 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+.byte 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+endconst
+
 /*
  * int quant_4x4x4( dctcoef dct[4][16], udctcoef mf[16], udctcoef bias[16] )
  */
 .macro QUANT_ONE_LASX s1, s2, s3, s4
-    xvld        xr1,  \s1,  0  /* Load dctcoef */
-    xvadda.h    \s4,  xr1, \s3
-    xvmuh.hu    \s4,  \s4, \s2
-    xvsigncov.h \s4,  xr1, \s4
-    xvst        \s4,  \s1,  0
+    xvld           xr1,     \s1,   0  /* Load dctcoef */
+    xvadda.h       \s4,     xr1,   \s3
+    xvmuh.hu       \s4,     \s4,   \s2
+    xvsigncov.h    \s4,     xr1,   \s4
+    xvst           \s4,     \s1,   0
 .endm
 
 function quant_4x4x4_lasx
-    xvld    xr2,  a1,  0
-    xvld    xr3,  a2,  0
-    QUANT_ONE_LASX   a0,  xr2,  xr3,  xr4
-    addi.d  a0,   a0,  32
-    QUANT_ONE_LASX   a0,  xr2,  xr3,  xr0
-    xvssrlni.h.w  xr0, xr4, 0
-    addi.d  a0,   a0,  32
-    QUANT_ONE_LASX   a0,  xr2,  xr3,  xr4
-    addi.d  a0,   a0,  32
-    QUANT_ONE_LASX   a0,  xr2,  xr3,  xr5
-    xvssrlni.h.w  xr5, xr4, 0
-    xvssrlni.h.w  xr5, xr0, 0
-    xvseqi.w      xr5, xr5, 0
-    xvmskltz.w    xr5, xr5
-    xvpickve2gr.w t0,  xr5, 0
-    xvpickve2gr.w t1,  xr5, 4
-    alsl.d      t0,  t1,  t0,  4
-    and         t0,  t0,  t1
-    xori        a0,  t0,  0xf
+    xvld           xr2,     a1,    0
+    xvld           xr3,     a2,    0
+    QUANT_ONE_LASX a0, xr2, xr3, xr4
+    addi.d         a0,      a0,    32
+    QUANT_ONE_LASX a0, xr2, xr3, xr0
+    xvssrlni.h.w   xr0,     xr4,   0
+    addi.d         a0,      a0,    32
+    QUANT_ONE_LASX a0, xr2, xr3, xr4
+    addi.d         a0,      a0,    32
+    QUANT_ONE_LASX a0, xr2, xr3, xr5
+    xvssrlni.h.w   xr5,     xr4,   0
+    xvssrlni.h.w   xr5,     xr0,   0
+    xvseqi.w       xr5,     xr5,   0
+    xvmskltz.w     xr5,     xr5
+    xvpickve2gr.w  t0,      xr5,   0
+    xvpickve2gr.w  t1,      xr5,   4
+    alsl.d         t0,      t1,    t0,    4
+    and            t0,      t0,    t1
+    xori           a0,      t0,    0xf
+endfunc
+
+/*
+ * int quant_8x8( dctcoef dct[64], udctcoef mf[64], udctcoef bias[64] )
+ */
+function quant_8x8_lasx
+    xvld           xr1,     a0,    0
+    vld            vr2,     a1,    0
+    vld            vr3,     a2,    0
+    xvslei.h       xr19,    xr1,   0
+
+    vext2xv.w.h    xr4,     xr1
+    vext2xv.wu.hu  xr5,     xr2
+    vext2xv.wu.hu  xr6,     xr3
+    xvadda.w       xr7,     xr6,   xr4
+    xvmul.w        xr8,     xr7,   xr5
+
+    xvpermi.q      xr1,     xr1,   0x01
+    vld            vr2,     a1,    16
+    vld            vr3,     a2,    16
+    vext2xv.w.h    xr4,     xr1
+    vext2xv.wu.hu  xr5,     xr2
+    vext2xv.wu.hu  xr6,     xr3
+    xvadda.w       xr9,     xr6,   xr4
+    xvmul.w        xr10,    xr9,   xr5
+    xvsrani.h.w    xr10,    xr8,   16
+    xvpermi.d      xr11,    xr10,  0xd8
+    xvneg.h        xr12,    xr11
+    xvbitsel.v     xr15,    xr11,  xr12,    xr19
+    xvst           xr15,    a0,    0
+
+    xvld           xr1,     a0,    32
+    vld            vr2,     a1,    32
+    vld            vr3,     a2,    32
+    xvslei.h       xr19,    xr1,   0
+
+    vext2xv.w.h    xr4,     xr1
+    vext2xv.wu.hu  xr5,     xr2
+    vext2xv.wu.hu  xr6,     xr3
+    xvadda.w       xr7,     xr6,   xr4
+    xvmul.w        xr8,     xr7,   xr5
+
+    xvpermi.q      xr1,     xr1,   0x01
+    vld            vr2,     a1,    48
+    vld            vr3,     a2,    48
+    vext2xv.w.h    xr4,     xr1
+    vext2xv.wu.hu  xr5,     xr2
+    vext2xv.wu.hu  xr6,     xr3
+    xvadda.w       xr9,     xr6,   xr4
+    xvmul.w        xr10,    xr9,   xr5
+    xvsrani.h.w    xr10,    xr8,   16
+    xvpermi.d      xr11,    xr10,  0xd8
+    xvneg.h        xr12,    xr11
+    xvbitsel.v     xr16,    xr11,  xr12,    xr19
+    xvst           xr16,    a0,    32
+
+    xvld           xr1,     a0,    64
+    vld            vr2,     a1,    64
+    vld            vr3,     a2,    64
+    xvslei.h       xr19,    xr1,   0
+
+    vext2xv.w.h    xr4,     xr1
+    vext2xv.wu.hu  xr5,     xr2
+    vext2xv.wu.hu  xr6,     xr3
+    xvadda.w       xr7,     xr6,   xr4
+    xvmul.w        xr8,     xr7,   xr5
+
+    xvpermi.q      xr1,     xr1,   0x01
+    vld            vr2,     a1,    80
+    vld            vr3,     a2,    80
+    vext2xv.w.h    xr4,     xr1
+    vext2xv.wu.hu  xr5,     xr2
+    vext2xv.wu.hu  xr6,     xr3
+    xvadda.w       xr9,     xr6,   xr4
+    xvmul.w        xr10,    xr9,   xr5
+    xvsrani.h.w    xr10,    xr8,   16
+    xvpermi.d      xr11,    xr10,  0xd8
+    xvneg.h        xr12,    xr11
+    xvbitsel.v     xr17,    xr11,  xr12,    xr19
+    xvst           xr17,    a0,    64
+
+    xvld           xr1,     a0,    96
+    vld            vr2,     a1,    96
+    vld            vr3,     a2,    96
+    xvslei.h       xr19,    xr1,   0
+
+    vext2xv.w.h    xr4,     xr1
+    vext2xv.wu.hu  xr5,     xr2
+    vext2xv.wu.hu  xr6,     xr3
+    xvadda.w       xr7,     xr6,   xr4
+    xvmul.w        xr8,     xr7,   xr5
+
+    xvpermi.q      xr1,     xr1,   0x01
+    vld            vr2,     a1,    112
+    vld            vr3,     a2,    112
+    vext2xv.w.h    xr4,     xr1
+    vext2xv.wu.hu  xr5,     xr2
+    vext2xv.wu.hu  xr6,     xr3
+    xvadda.w       xr9,     xr6,   xr4
+    xvmul.w        xr10,    xr9,   xr5
+    xvsrani.h.w    xr10,    xr8,   16
+    xvpermi.d      xr11,    xr10,  0xd8
+    xvneg.h        xr12,    xr11
+    xvbitsel.v     xr18,    xr11,  xr12,    xr19
+    xvst           xr18,    a0,    96
+
+    // xr15 xr16 xr17 xr18
+    xvor.v          xr15,   xr15,  xr16
+    xvor.v          xr17,   xr17,  xr18
+    xvor.v          xr15,   xr15,  xr17
+    xvpermi.q       xr16,   xr15,  0x01
+    vor.v           vr11,   vr16,  vr15
+    vpickve2gr.d    t0,     vr11,  0
+    vpickve2gr.d    t1,     vr11,  1
+    or              t2,     t0,    t1
+    addi.w          t3,     zero,  1
+    maskeqz         a0,     t3,    t2
+endfunc
+
+/*
+ * int quant_4x4( dctcoef dct[16], udctcoef mf[16], udctcoef bias[16] )
+ */
+function quant_4x4_lasx
+    xvld            xr1,    a0,    0
+    vld             vr2,    a1,    0
+    vld             vr3,    a2,    0
+    xvslei.h        xr19,   xr1,   0
+
+    vext2xv.w.h     xr4,    xr1
+    vext2xv.wu.hu   xr5,    xr2
+    vext2xv.wu.hu   xr6,    xr3
+    xvadda.w        xr7,    xr6,   xr4
+    xvmul.w         xr8,    xr7,   xr5
+
+    xvpermi.q       xr1,    xr1,   0x01
+    vld             vr2,    a1,    16
+    vld             vr3,    a2,    16
+    vext2xv.w.h     xr4,    xr1
+    vext2xv.wu.hu   xr5,    xr2
+    vext2xv.wu.hu   xr6,    xr3
+    xvadda.w        xr9,    xr6,   xr4
+    xvmul.w         xr10,   xr9,   xr5
+    xvsrani.h.w     xr10,   xr8,   16
+    xvpermi.d       xr11,   xr10,  0xd8
+    xvneg.h         xr12,   xr11
+    xvbitsel.v      xr9,    xr11,  xr12,    xr19
+    xvst            xr9,    a0,    0
+
+    xvpermi.q       xr10,   xr9,   0x01
+    vor.v           vr11,   vr10,  vr9
+    vpickve2gr.d    t0,     vr11,  0
+    vpickve2gr.d    t1,     vr11,  1
+    or              t2,     t0,    t1
+    addi.w          t3,     zero,  1
+    maskeqz         a0,     t3,    t2
+endfunc
+
+/*
+ * int quant_4x4_dc( dctcoef dct[16], int mf, int bias )
+ */
+function quant_4x4_dc_lasx
+    xvld            xr0,    a0,    0
+
+    xvreplgr2vr.w   xr1,    a1
+    xvreplgr2vr.w   xr2,    a2
+
+    xvslei.h        xr3,    xr0,   0
+
+    vext2xv.w.h     xr4,    xr0
+    xvpermi.q       xr5,    xr0,   0x01
+    vext2xv.w.h     xr5,    xr5
+
+    xvadda.w        xr4,    xr2,   xr4
+    xvadda.w        xr5,    xr2,   xr5
+    xvmul.w         xr4,    xr4,   xr1
+    xvmul.w         xr5,    xr5,   xr1
+    xvsrani.h.w     xr5,    xr4,   16
+    xvpermi.d       xr5,    xr5,   0xd8
+    xvneg.h         xr8,    xr5
+    xvbitsel.v      xr9,    xr5,   xr8,   xr3
+    xvst            xr9,    a0,    0
+
+    xvpermi.q       xr10,   xr9,   0x01
+    vor.v           vr11,   vr10,  vr9
+    vpickve2gr.d    t0,     vr11,  0
+    vpickve2gr.d    t1,     vr11,  1
+    or              t2,     t0,    t1
+    addi.w          t3,     zero,  1
+    maskeqz         a0,     t3,    t2
+endfunc
+
+/*
+ * int quant_2x2_dc( dctcoef dct[4], int mf, int bias )
+ */
+function quant_2x2_dc_lsx
+    fld.d           f0,     a0,    0
+    vreplgr2vr.w    vr1,    a1
+    vreplgr2vr.w    vr2,    a2
+
+    vslei.h         vr3,    vr0,   0
+
+    vsllwil.w.h     vr4,    vr0,   0
+    vadda.w         vr4,    vr4,   vr2
+    vmul.w          vr4,    vr4,   vr1
+    vsrani.h.w      vr4,    vr4,   16
+    vneg.h          vr8,    vr4
+    vbitsel.v       vr9,    vr4,   vr8,    vr3
+    vstelm.d        vr9,    a0,    0,      0
+
+    vpickve2gr.w   t0,      vr9,   0
+    vpickve2gr.w   t1,      vr9,   1
+    or             t2,      t0,    t1
+    addi.w         t3,      zero,  1
+    maskeqz        a0,      t3,    t2
+endfunc
+
+/*
+ * int coeff_last64_c(dctcoef *l)
+ */
+function coeff_last64_lasx
+    addi.w          t0,     zero,  63
+    xvxor.v         xr20,   xr0,   xr0
+
+    xvld            xr0,    a0,    0
+    xvld            xr1,    a0,    32
+    xvld            xr2,    a0,    64
+    xvld            xr3,    a0,    96
+
+    xvldi           xr4,    1
+    la.local        t1,     last64_shuf
+    xvld            xr7,    t1,    0
+    xvldi           xr9,    0x408
+    xvldi           xr10,   0x401
+
+    xvssrlni.bu.h   xr1,    xr0,   0
+    xvssrlni.bu.h   xr3,    xr2,   0
+    xvsle.bu        xr5,    xr4,   xr1
+    xvsle.bu        xr6,    xr4,   xr3
+    xvssrlni.bu.h   xr6,    xr5,   4
+    xvperm.w        xr6,    xr6,   xr7
+    xvclz.w         xr7,    xr6
+    xvssrlni.hu.w   xr7,    xr7,   2
+    xvpermi.d       xr8,    xr7,   0xd8
+
+    xvsub.h         xr9,    xr9,   xr8
+    xvsll.h         xr10,   xr10,  xr9
+    xvssrlni.bu.h   xr10,   xr10,  1
+    xvclz.d         xr11,   xr10
+    xvpickve2gr.w   t3,     xr11,  0
+    sub.w           a0,     t0,    t3
+endfunc
+
+/*
+ * int coeff_last16_c(dctcoef *l)
+ */
+function coeff_last16_lasx
+    addi.w          t0,     zero,  15
+
+    xvld            xr0,    a0,    0
+    xvldi           xr2,    1
+
+    xvssrlni.bu.h   xr0,    xr0,   0
+    xvpermi.d       xr1,    xr0,   0xd8
+    xvsle.bu        xr3,    xr2,   xr1
+    xvssrlni.bu.h   xr3,    xr3,   4
+    xvclz.d         xr4,    xr3
+    xvpickve2gr.w   t1,     xr4,   0
+
+    srai.w          t1,     t1,    2
+    sub.w           a0,     t0,    t1
+endfunc
+
+/*
+ * int coeff_last15_c(dctcoef *l)
+ */
+function coeff_last15_lasx
+    addi.w          t0,     zero,  15
+
+    vld             vr0,    a0,    0
+    vld             vr1,    a0,    16
+    xvldi           xr3,    1
+
+    vinsgr2vr.h     vr1,    zero,  7
+    xvpermi.q       xr1,    xr0,   0x20
+
+    xvssrlni.bu.h   xr1,    xr1,   0
+    xvpermi.d       xr2,    xr1,   0xd8
+    xvsle.bu        xr4,    xr3,   xr2
+    xvssrlni.bu.h   xr4,    xr4,   4
+    xvclz.d         xr5,    xr4
+    xvpickve2gr.w   t1,     xr5,   0
+
+    srai.w          t1,     t1,    2
+    sub.w           a0,     t0,    t1
+endfunc
+
+// (dct[i] * dequant_mf[i]) << (i_qbits)
+.macro DCT_MF a0, a1, in0, out0, out1
+    vld             vr1,    \a0,   0
+    xvld            xr2,    \a1,   0
+
+    vext2xv.w.h     xr5,    xr1
+    xvmul.w         xr5,    xr5,   xr2
+    xvsll.w         \out0,  xr5,   \in0
+
+    vld             vr1,    \a0,   16
+    xvld            xr2,    \a1,   32
+    vext2xv.w.h     xr5,    xr1
+    xvmul.w         xr5,    xr5,   xr2
+    xvsll.w         \out1,  xr5,   \in0
+.endm
+
+// (dct[i] * dequant_mf[i] + f) >> (-i_qbits)
+.macro DCT_MF_F a0, a1, in0, out0, out1
+    vld             vr1,    \a0,   0
+    xvld            xr2,    \a1,   0
+
+    vext2xv.w.h     xr5,    xr1
+    xvmul.w         xr5,    xr5,   xr2
+    xvsrar.w        \out0,  xr5,   \in0
+
+    vld             vr1,    \a0,   16
+    xvld            xr2,    \a1,   32
+    vext2xv.w.h     xr5,    xr1
+    xvmul.w         xr5,    xr5,   xr2
+    xvsrar.w        \out1,  xr5,   \in0
+.endm
+
+/*
+ * void dequant_4x4( dctcoef dct[16], int dequant_mf[6][16], int i_qp )
+ */
+function dequant_4x4_lasx
+    addi.w          t1,     zero,  6
+    addi.w          t2,     zero,  4
+    div.w           t0,     a2,    t1
+    sub.w           t0,     t0,    t2  // i_qp/6 - 4
+    mod.w           t1,     a2,    t1  // i_qp%6
+    slli.w          t1,     t1,    6
+    add.d           a1,     a1,    t1
+
+    blt             t0,     zero,  .DQ4x4_DEQUANT_SHR
+
+    // i_qbits >= 0
+    xvreplgr2vr.w   xr0,    t0
+    DCT_MF a0, a1, xr0, xr6, xr7
+    b               .DQ4x4_END
+
+.DQ4x4_DEQUANT_SHR:
+    sub.w           t4,     zero,  t0
+    xvreplgr2vr.w   xr4,    t4
+    DCT_MF_F a0, a1, xr4, xr6, xr7
+
+.DQ4x4_END:
+    xvpickev.h      xr8,    xr7,   xr6
+    xvpermi.d       xr8,    xr8,   0xd8
+    xvst            xr8,    a0,    0
+endfunc
+
+/*
+ * void dequant_8x8( dctcoef dct[64], int dequant_mf[6][64], int i_qp )
+ */
+function dequant_8x8_lasx
+    addi.w          t1,     zero,  6
+    div.w           t0,     a2,    t1
+    sub.w           t0,     t0,    t1
+    mod.w           t1,     a2,    t1  // i_qp%6
+    slli.w          t1,     t1,    8
+    add.d           a1,     a1,    t1
+
+    blt             t0,     zero,  .DQ8x8_DEQUANT_SHR
+    // i_qbits >= 0
+    xvreplgr2vr.w   xr0,    t0
+    DCT_MF a0, a1, xr0, xr6, xr7
+    xvpickev.h      xr8,    xr7,   xr6
+    xvpermi.d       xr8,    xr8,   0xd8
+    xvst            xr8,    a0,    0
+
+.rept 3
+    addi.d          a0,     a0,    32
+    addi.d          a1,     a1,    64
+    DCT_MF a0, a1, xr0, xr6, xr7
+    xvpickev.h      xr8,    xr7,   xr6
+    xvpermi.d       xr8,    xr8,   0xd8
+    xvst            xr8,    a0,    0
+.endr
+    b               .DQ8x8_END
+
+// i_qbits < 0
+.DQ8x8_DEQUANT_SHR:
+    sub.w           t4,     zero,  t0
+    xvreplgr2vr.w   xr4,    t4
+
+    DCT_MF_F a0, a1, xr4, xr6, xr7
+    xvpickev.h      xr8,    xr7,   xr6
+    xvpermi.d       xr8,    xr8,   0xd8
+    xvst            xr8,    a0,    0
+
+.rept 3
+    addi.d          a0,     a0,    32
+    addi.d          a1,     a1,    64
+    DCT_MF_F a0, a1, xr4, xr6, xr7
+    xvpickev.h      xr8,    xr7,   xr6
+    xvpermi.d       xr8,    xr8,   0xd8
+    xvst            xr8,    a0,    0
+.endr
+
+.DQ8x8_END:
+
+endfunc
+
+/*
+ * void dequant_4x4_dc( dctcoef dct[16], int dequant_mf[6][16], int i_qp )
+ */
+function dequant_4x4_dc_lasx
+    addi.w          t0,     zero,  6
+    div.w           t1,     a2,    t0
+    sub.w           t1,     t1,    t0
+
+    blt             t1,     zero,  .DQ4x4DC_LT_ZERO
+    // i_qbits >= 0
+    mod.w           t2,     a2,    t0
+    slli.w          t2,     t2,    6
+    ldx.w           t0,     a1,    t2
+    sll.w           t0,     t0,    t1
+
+    vld             vr1,    a0,    0
+    vld             vr10,   a0,    16
+    xvreplgr2vr.w   xr2,    t0
+
+    vext2xv.w.h     xr3,    xr1
+    xvmul.w         xr6,    xr3,   xr2
+
+    vext2xv.w.h     xr3,    xr10
+    xvmul.w         xr7,    xr3,   xr2
+    b               .DQ4x4DC_END
+
+// i_qbits < 0
+.DQ4x4DC_LT_ZERO:
+    mod.w           t2,     a2,    t0
+    slli.w          t2,     t2,    6
+    ldx.w           t0,     a1,    t2
+    sub.w           t3,     zero,  t1
+
+    vld             vr1,    a0,    0
+    vld             vr10,   a0,    16
+    xvreplgr2vr.w   xr2,    t0
+    xvreplgr2vr.w   xr4,    t3
+
+    vext2xv.w.h     xr5,    xr1
+    xvmul.w         xr5,    xr5,   xr2
+    xvsrar.w        xr6,    xr5,   xr4
+
+    vext2xv.w.h     xr5,    xr10
+    xvmul.w         xr5,    xr5,   xr2
+    xvsrar.w        xr7,    xr5,   xr4
+
+.DQ4x4DC_END:
+    xvpickev.h      xr8,    xr7,   xr6
+    xvpermi.d       xr8,    xr8,   0xd8
+    xvst            xr8,    a0,    0
+endfunc
+
+/*
+ * int decimate_score15( dctcoef *dct )
+ */
+function decimate_score15_lasx
+    addi.w          t0,     zero,  15
+    la.local        t3,     decimate_table4
+
+    addi.d          t4,     a0,    2
+    vld             vr0,    t4,    0
+    vld             vr1,    t4,    16
+    xvldi           xr3,    1
+
+    vinsgr2vr.h     vr1,    zero,  7
+    xvpermi.q       xr1,    xr0,   0x20
+
+    xvssrlni.bu.h   xr1,    xr1,   0
+    xvpermi.d       xr2,    xr1,   0xd8
+    xvsle.bu        xr4,    xr3,   xr2
+    xvssrlni.bu.h   xr4,    xr4,   4
+    xvclz.d         xr5,    xr4
+    xvpickve2gr.w   t1,     xr5,   0
+
+    srai.w          t1,     t1,    2
+    sub.w           t2,     t0,    t1
+    addi.d          t0,     zero,  2
+    move            a0,     zero
+    slli.d          t2,     t2,    1
+.LOOP_SCORE_15:
+    blt             t2,     zero,  .END_SCORE_15
+    ldx.h           t5,     t4,    t2
+    addi.d          t6,     t5,    1
+    bltu            t0,     t6,    .RET_SCORE_15_1
+    addi.d          t2,     t2,    -2
+    move            t5,     zero
+.WHILE_SCORE_15:
+    blt             t2,     zero,  .END_WHILE_15
+    ldx.h           t1,     t4,    t2
+    bnez            t1,     .END_WHILE_15
+    addi.d          t2,     t2,    -2
+    addi.d          t5,     t5,    1
+    b               .WHILE_SCORE_15
+.END_WHILE_15:
+    ldx.b           t1,     t3,    t5
+    add.d           a0,     a0,    t1
+    b               .LOOP_SCORE_15
+.RET_SCORE_15_1:
+    addi.d          a0,     zero,  9
+    jirl            $r0,    $r1,   0x0
+.END_SCORE_15:
+endfunc
+
+/*
+ * int decimate_score16( dctcoef *dct )
+ */
+function decimate_score16_lasx
+    addi.w          t0,     zero,  15
+    la.local        t3,     decimate_table4
+
+    addi.w          t0,     zero,  15
+
+    xvld            xr0,    a0,    0
+    xvldi           xr2,    1
+
+    xvssrlni.bu.h   xr0,    xr0,   0
+    xvpermi.d       xr1,    xr0,   0xd8
+    xvsle.bu        xr3,    xr2,   xr1
+    xvssrlni.bu.h   xr3,    xr3,   4
+    xvclz.d         xr4,    xr3
+    xvpickve2gr.w   t1,     xr4,   0
+
+    srai.w          t1,     t1,    2
+    sub.w           t2,     t0,    t1
+    move            t4,     a0
+    addi.d          t0,     zero,  2
+    move            a0,     zero
+    slli.d          t2,     t2,    1
+.LOOP_SCORE_16:
+    blt             t2,     zero,  .END_SCORE_16
+    ldx.h           t5,     t4,    t2
+    addi.d          t6,     t5,    1
+    bltu            t0,     t6,    .RET_SCORE_16_1
+    addi.d          t2,     t2,    -2
+    move            t5,     zero
+.WHILE_SCORE_16:
+    blt             t2,     zero,  .END_WHILE_16
+    ldx.h           t1,     t4,    t2
+    bnez            t1,     .END_WHILE_16
+    addi.d          t2,     t2,    -2
+    addi.d          t5,     t5,    1
+    b               .WHILE_SCORE_16
+.END_WHILE_16:
+    ldx.b           t1,     t3,    t5
+    add.d           a0,     a0,    t1
+    b               .LOOP_SCORE_16
+.RET_SCORE_16_1:
+    addi.d          a0,     zero,  9
+    jirl            $r0,    $r1,   0x0
+.END_SCORE_16:
+endfunc
+
+/*
+ * int decimate_score64( dctcoef *dct )
+ */
+function decimate_score64_lasx
+    addi.w          t0,     zero,  63
+    xvxor.v         xr20,   xr0,   xr0
+    la.local        t3,     decimate_table8
+
+    xvld            xr0,    a0,    0
+    xvld            xr1,    a0,    32
+    xvld            xr2,    a0,    64
+    xvld            xr3,    a0,    96
+
+    xvldi           xr4,    1
+    la.local        t1,     last64_shuf
+    xvld            xr7,    t1,    0
+    xvldi           xr9,    0x408
+    xvldi           xr10,   0x401
+
+    xvssrlni.bu.h   xr1,    xr0,   0
+    xvssrlni.bu.h   xr3,    xr2,   0
+    xvsle.bu        xr5,    xr4,   xr1
+    xvsle.bu        xr6,    xr4,   xr3
+    xvssrlni.bu.h   xr6,    xr5,   4
+    xvperm.w        xr6,    xr6,   xr7
+    xvclz.w         xr7,    xr6
+    xvssrlni.hu.w   xr7,    xr7,   2
+    xvpermi.d       xr8,    xr7,   0xd8
+
+    xvsub.h         xr9,    xr9,   xr8
+    xvsll.h         xr10,   xr10,  xr9
+    xvssrlni.bu.h   xr10,   xr10,  1
+    xvclz.d         xr11,   xr10
+    xvpickve2gr.w   t1,     xr11,  0
+    sub.w           t2,     t0,    t1
+    move            t4,     a0
+    addi.d          t0,     zero,  2
+    slli.d          t2,     t2,    1
+    move            a0,     zero
+.LOOP_SCORE_64:
+    blt             t2,     zero,  .END_SCORE_64
+    ldx.h           t5,     t4,    t2
+    addi.d          t6,     t5,    1
+    bltu            t0,     t6,    .RET_SCORE_64_1
+    addi.d          t2,     t2,    -2
+    move            t5,     zero
+.WHILE_SCORE_64:
+    blt             t2,     zero,  .END_WHILE_64
+    ldx.h           t1,     t4,    t2
+    bnez            t1,     .END_WHILE_64
+    addi.d          t2,     t2,    -2
+    addi.d          t5,     t5,    1
+    b               .WHILE_SCORE_64
+.END_WHILE_64:
+    ldx.b           t1,     t3,    t5
+    add.d           a0,     a0,    t1
+    b               .LOOP_SCORE_64
+.RET_SCORE_64_1:
+    addi.d          a0,     zero,  9
+    jirl            $r0,    $r1,   0x0
+.END_SCORE_64:
 endfunc
diff --git a/common/loongarch/quant-c.c b/common/loongarch/quant-c.c
deleted file mode 100644
index 0533ee5d..00000000
--- a/common/loongarch/quant-c.c
+++ /dev/null
@@ -1,265 +0,0 @@
-/*****************************************************************************
- * quant-c.c: loongarch quantization and level-run
- *****************************************************************************
- * Copyright (C) 2020 x264 project
- * Copyright (C) 2020 Loongson Technology Corporation Limited
- *
- * Authors: zhou peng <zhoupeng@loongson.cn>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
- *
- * This program is also available under a commercial proprietary license.
- * For more information, contact us at licensing@x264.com.
- *****************************************************************************/
-
-#include "common/common.h"
-#include "loongson_intrinsics.h"
-#include "quant.h"
-
-#if !HIGH_BIT_DEPTH
-
-static inline int32_t avc_quant_4x4_lasx( int16_t *p_dct,
-                                          uint16_t *p_mf,
-                                          uint16_t *p_bias )
-{
-    int32_t non_zero = 0;
-    __m256i zero = __lasx_xvldi( 0 );
-    __m256i dct_mask;
-    __m256i dct, dct0, dct1;
-    __m256i mf, mf0, mf1;
-    __m256i bias, bias0, bias1;
-    __m256i tmp;
-
-    dct = __lasx_xvld( p_dct, 0 );
-    bias = __lasx_xvld( p_bias, 0 );
-    mf = __lasx_xvld( p_mf, 0 );
-
-    dct_mask = __lasx_xvslei_h( dct, 0 );
-
-    LASX_UNPCK_SH( dct, dct0, dct1 );
-    bias0 = __lasx_xvilvl_h( zero, bias );
-    bias1 = __lasx_xvilvh_h( zero, bias );
-    mf0 = __lasx_xvilvl_h( zero, mf );
-    mf1 = __lasx_xvilvh_h( zero, mf );
-
-    dct0 = __lasx_xvadda_w( dct0, bias0 );
-    dct1 = __lasx_xvadda_w( dct1, bias1 );
-    dct0 = __lasx_xvmul_w(dct0, mf0);
-    dct1 = __lasx_xvmul_w(dct1, mf1);
-
-    dct = __lasx_xvsrani_h_w(dct1, dct0, 16);
-
-    tmp = __lasx_xvhaddw_w_h( dct, dct );
-    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
-    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
-    non_zero = __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
-
-    dct0 = __lasx_xvsub_h( zero, dct );
-    dct = __lasx_xvbitsel_v( dct, dct0, dct_mask );
-    __lasx_xvst( dct, p_dct, 0 );
-
-    return !!non_zero;
-}
-
-static inline int32_t avc_quant_8x8_lasx( int16_t *p_dct,
-                                          uint16_t *p_mf,
-                                          uint16_t *p_bias )
-{
-    int32_t non_zero = 0;
-    __m256i zero = __lasx_xvldi( 0 );
-    __m256i dct_mask0, dct_mask1;
-    __m256i dct0, dct1, dct0_0, dct0_1, dct1_0, dct1_1;
-    __m256i mf0, mf1, mf0_0, mf0_1, mf1_0, mf1_1;
-    __m256i bias0, bias1, bias0_0, bias0_1, bias1_0, bias1_1;
-    __m256i tmp;
-
-    dct0 = __lasx_xvld( p_dct, 0 );
-    dct1 = __lasx_xvld( p_dct, 32 );
-    bias0 = __lasx_xvld( p_bias, 0 );
-    bias1 = __lasx_xvld( p_bias, 32 );
-    mf0 = __lasx_xvld( p_mf, 0 );
-    mf1 = __lasx_xvld( p_mf, 32 );
-
-    dct_mask0 = __lasx_xvslei_h( dct0, 0 );
-    dct_mask1 = __lasx_xvslei_h( dct1, 0 );
-
-    LASX_UNPCK_SH( dct0, dct0_0, dct0_1 );
-    LASX_UNPCK_SH( dct1, dct1_0, dct1_1 );
-    bias0_0 = __lasx_xvilvl_h( zero, bias0 );
-    bias0_1 = __lasx_xvilvh_h( zero, bias0 );
-    bias1_0 = __lasx_xvilvl_h( zero, bias1 );
-    bias1_1 = __lasx_xvilvh_h( zero, bias1 );
-    mf0_0 = __lasx_xvilvl_h( zero, mf0 );
-    mf0_1 = __lasx_xvilvh_h( zero, mf0 );
-    mf1_0 = __lasx_xvilvl_h( zero, mf1 );
-    mf1_1 = __lasx_xvilvh_h( zero, mf1 );
-
-    dct0_0 = __lasx_xvadda_w( dct0_0, bias0_0 );
-    dct0_1 = __lasx_xvadda_w( dct0_1, bias0_1 );
-    dct1_0 = __lasx_xvadda_w( dct1_0, bias1_0 );
-    dct1_1 = __lasx_xvadda_w( dct1_1, bias1_1 );
-
-    dct0_0 = __lasx_xvmul_w( dct0_0, mf0_0 );
-    dct0_1 = __lasx_xvmul_w( dct0_1, mf0_1 );
-    dct1_0 = __lasx_xvmul_w( dct1_0, mf1_0 );
-    dct1_1 = __lasx_xvmul_w( dct1_1, mf1_1 );
-
-    dct0 = __lasx_xvsrani_h_w( dct0_1, dct0_0, 16 );
-    dct1 = __lasx_xvsrani_h_w( dct1_1, dct1_0, 16 );
-
-    tmp = __lasx_xvadd_h( dct0, dct1 );
-    tmp = __lasx_xvhaddw_w_h( tmp, tmp );
-    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
-    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
-    non_zero = __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
-
-    dct0_0 = __lasx_xvsub_h( zero, dct0 );
-    dct1_0 = __lasx_xvsub_h( zero, dct1 );
-    dct0 = __lasx_xvbitsel_v( dct0, dct0_0, dct_mask0 );
-    dct1 = __lasx_xvbitsel_v( dct1, dct1_0, dct_mask1 );
-
-    __lasx_xvst( dct0, p_dct, 0 );
-    __lasx_xvst( dct1, p_dct, 32 );
-
-    /* next part */
-    dct0 = __lasx_xvld( p_dct, 64 );
-    dct1 = __lasx_xvld( p_dct, 96 );
-    bias0 = __lasx_xvld( p_bias, 64 );
-    bias1 = __lasx_xvld( p_bias, 96 );
-    mf0 = __lasx_xvld( p_mf, 64 );
-    mf1 = __lasx_xvld( p_mf, 96 );
-
-    dct_mask0 = __lasx_xvslei_h( dct0, 0 );
-    dct_mask1 = __lasx_xvslei_h( dct1, 0 );
-
-    LASX_UNPCK_SH( dct0, dct0_0, dct0_1 );
-    LASX_UNPCK_SH( dct1, dct1_0, dct1_1 );
-    bias0_0 = __lasx_xvilvl_h( zero, bias0 );
-    bias0_1 = __lasx_xvilvh_h( zero, bias0 );
-    bias1_0 = __lasx_xvilvl_h( zero, bias1 );
-    bias1_1 = __lasx_xvilvh_h( zero, bias1 );
-    mf0_0 = __lasx_xvilvl_h( zero, mf0 );
-    mf0_1 = __lasx_xvilvh_h( zero, mf0 );
-    mf1_0 = __lasx_xvilvl_h( zero, mf1 );
-    mf1_1 = __lasx_xvilvh_h( zero, mf1 );
-
-    dct0_0 = __lasx_xvadda_w( dct0_0, bias0_0 );
-    dct0_1 = __lasx_xvadda_w( dct0_1, bias0_1 );
-    dct1_0 = __lasx_xvadda_w( dct1_0, bias1_0 );
-    dct1_1 = __lasx_xvadda_w( dct1_1, bias1_1 );
-
-    dct0_0 = __lasx_xvmul_w( dct0_0, mf0_0 );
-    dct0_1 = __lasx_xvmul_w( dct0_1, mf0_1 );
-    dct1_0 = __lasx_xvmul_w( dct1_0, mf1_0 );
-    dct1_1 = __lasx_xvmul_w( dct1_1, mf1_1 );
-
-    dct0 = __lasx_xvsrani_h_w( dct0_1, dct0_0, 16 );
-    dct1 = __lasx_xvsrani_h_w( dct1_1, dct1_0, 16 );
-
-    tmp = __lasx_xvadd_h( dct0, dct1 );
-    tmp = __lasx_xvhaddw_w_h( tmp, tmp );
-    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
-    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
-    non_zero += __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
-
-    dct0_0 = __lasx_xvsub_h( zero, dct0 );
-    dct1_0 = __lasx_xvsub_h( zero, dct1 );
-    dct0 = __lasx_xvbitsel_v( dct0, dct0_0, dct_mask0 );
-    dct1 = __lasx_xvbitsel_v( dct1, dct1_0, dct_mask1 );
-
-    __lasx_xvst( dct0, p_dct, 64 );
-    __lasx_xvst( dct1, p_dct, 96 );
-
-    return !!non_zero;
-}
-
-int32_t x264_coeff_last16_lasx( int16_t *p_src )
-{
-    __m256i src0, tmp0, tmp1;
-    __m256i one = __lasx_xvldi(1);
-    int32_t result;
-
-    src0 = __lasx_xvld( p_src, 0 );
-    tmp0 = __lasx_xvssrlni_bu_h(src0, src0, 0);
-    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
-    tmp1 = __lasx_xvsle_bu(one, tmp0);
-    tmp0 = __lasx_xvssrlni_bu_h(tmp1, tmp1, 4);
-    tmp1 = __lasx_xvclz_d(tmp0);
-    result = __lasx_xvpickve2gr_w(tmp1, 0);
-
-    return 15 - (result >> 2);
-}
-
-int32_t x264_coeff_last15_lasx( int16_t *psrc )
-{
-    __m256i src0, tmp0, tmp1;
-    __m256i one = __lasx_xvldi(1);
-    int32_t result;
-
-    src0 = __lasx_xvld( psrc, -2 );
-    tmp0 = __lasx_xvssrlni_bu_h(src0, src0, 0);
-    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
-    tmp1 = __lasx_xvsle_bu(one, tmp0);
-    tmp0 = __lasx_xvssrlni_bu_h(tmp1, tmp1, 4);
-    tmp1 = __lasx_xvclz_d(tmp0);
-    result = __lasx_xvpickve2gr_w(tmp1, 0);
-
-    return 14 - (result >> 2);
-}
-
-int32_t x264_coeff_last64_lasx( int16_t *p_src )
-{
-    int32_t result;
-    __m256i src0, src1, src2, src3;
-    __m256i tmp0, tmp1, tmp2, tmp3;
-    __m256i one = __lasx_xvldi(1);
-    __m256i const_8 = __lasx_xvldi(0x408);
-    __m256i const_1 = __lasx_xvldi(0x401);
-    __m256i shift = {0x0000000400000000, 0x0000000500000001,
-                     0x0000000600000002, 0x0000000700000003};
-    src0 = __lasx_xvld( p_src, 0 );
-    src1 = __lasx_xvld( p_src, 32);
-    src2 = __lasx_xvld( p_src, 64);
-    src3 = __lasx_xvld( p_src, 96);
-
-    tmp0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
-    tmp1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
-    tmp2 = __lasx_xvsle_bu(one, tmp0);
-    tmp3 = __lasx_xvsle_bu(one, tmp1);
-    tmp0 = __lasx_xvssrlni_bu_h(tmp3, tmp2, 4);
-    tmp0 = __lasx_xvperm_w(tmp0, shift);
-    tmp1 = __lasx_xvclz_w(tmp0);
-    tmp0 = __lasx_xvssrlni_hu_w(tmp1, tmp1, 2);
-    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
-
-    tmp1 = __lasx_xvsub_h(const_8, tmp0);
-    tmp0 = __lasx_xvsll_h(const_1, tmp1);
-    tmp0 = __lasx_xvssrlni_bu_h(tmp0, tmp0, 1);
-    tmp1 = __lasx_xvclz_d(tmp0);
-    result = __lasx_xvpickve2gr_w(tmp1, 0);
-    return 63 - result;
-}
-
-int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
-{
-    return avc_quant_4x4_lasx( p_dct, p_mf, p_bias );
-}
-
-int32_t x264_quant_8x8_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
-{
-    return avc_quant_8x8_lasx( p_dct, p_mf, p_bias );
-}
-
-#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/quant.h b/common/loongarch/quant.h
index 50a2fee9..a924726c 100644
--- a/common/loongarch/quant.h
+++ b/common/loongarch/quant.h
@@ -33,6 +33,7 @@ int32_t x264_coeff_last64_lasx( int16_t *p_src );
 int32_t x264_coeff_last16_lasx( int16_t *p_src );
 #define x264_coeff_last15_lasx x264_template(coeff_last15_lasx)
 int32_t x264_coeff_last15_lasx( int16_t *p_src );
+
 #define x264_quant_4x4_lasx x264_template(quant_4x4_lasx)
 int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
 #define x264_quant_4x4x4_lasx x264_template(quant_4x4x4_lasx)
@@ -40,5 +41,23 @@ int32_t x264_quant_4x4x4_lasx( int16_t p_dct[4][16],
                                uint16_t pu_mf[16], uint16_t pu_bias[16] );
 #define x264_quant_8x8_lasx x264_template(quant_8x8_lasx)
 int32_t x264_quant_8x8_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
+#define x264_quant_4x4_dc_lasx x264_template(quant_4x4_dc_lasx)
+int32_t x264_quant_4x4_dc_lasx( dctcoef dct[16], int32_t mf, int32_t bias );
+#define x264_quant_2x2_dc_lsx x264_template(quant_2x2_dc_lsx)
+int32_t x264_quant_2x2_dc_lsx( dctcoef dct[4], int32_t mf, int32_t bias );
+
+#define x264_dequant_4x4_lasx x264_template(dequant_4x4_lasx)
+void x264_dequant_4x4_lasx( dctcoef dct[16], int dequant_mf[6][16], int i_qp );
+#define x264_dequant_8x8_lasx x264_template(dequant_8x8_lasx)
+void x264_dequant_8x8_lasx( dctcoef dct[64], int dequant_mf[6][64], int i_qp );
+#define x264_dequant_4x4_dc_lasx x264_template(dequant_4x4_dc_lasx)
+void x264_dequant_4x4_dc_lasx( dctcoef dct[16], int dequant_mf[6][16], int i_qp );
+
+#define x264_decimate_score15_lasx x264_template(decimate_score15_lasx)
+int x264_decimate_score15_lasx( dctcoef *dct );
+#define x264_decimate_score16_lasx x264_template(decimate_score16_lasx)
+int x264_decimate_score16_lasx( dctcoef *dct );
+#define x264_decimate_score64_lasx x264_template(decimate_score64_lasx)
+int x264_decimate_score64_lasx( dctcoef *dct );
 
 #endif/* X264_LOONGARCH_QUANT_H */
diff --git a/common/quant.c b/common/quant.c
index 4949144a..971f47cd 100644
--- a/common/quant.c
+++ b/common/quant.c
@@ -811,12 +811,20 @@ void x264_quant_init( x264_t *h, int cpu, x264_quant_function_t *pf )
 #if HAVE_LASX
     if( cpu&X264_CPU_LASX )
     {
+        pf->quant_2x2_dc   = x264_quant_2x2_dc_lsx;
         pf->quant_4x4      = x264_quant_4x4_lasx;
+        pf->quant_4x4_dc   = x264_quant_4x4_dc_lasx;
         pf->quant_4x4x4    = x264_quant_4x4x4_lasx;
         pf->quant_8x8      = x264_quant_8x8_lasx;
+        pf->dequant_4x4    = x264_dequant_4x4_lasx;
+        pf->dequant_8x8    = x264_dequant_8x8_lasx;
+        pf->dequant_4x4_dc = x264_dequant_4x4_dc_lasx;
         pf->coeff_last[ DCT_LUMA_AC] = x264_coeff_last15_lasx;
         pf->coeff_last[DCT_LUMA_4x4] = x264_coeff_last16_lasx;
         pf->coeff_last[DCT_LUMA_8x8] = x264_coeff_last64_lasx;
+        pf->decimate_score15 = x264_decimate_score15_lasx;
+        pf->decimate_score16 = x264_decimate_score16_lasx;
+        pf->decimate_score64 = x264_decimate_score64_lasx;
     }
 #endif
 #endif // HIGH_BIT_DEPTH
