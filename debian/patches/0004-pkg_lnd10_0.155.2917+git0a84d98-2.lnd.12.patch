diff --git a/Makefile b/Makefile
index ff40f97e..3c9ee50a 100644
--- a/Makefile
+++ b/Makefile
@@ -201,19 +201,20 @@ endif
 # LOONGARCH optimization
 ifeq ($(SYS_ARCH),LOONGARCH)
 SRCASM_X += common/loongarch/cabac-a.S
-ifneq ($(findstring HAVE_LASX 1, $(CONFIG)),)
+
+ifneq ($(findstring HAVE_LSX 1, $(CONFIG)),)
 SRCASM_X += common/loongarch/mc-a.S \
-           common/loongarch/sad-a.S \
-           common/loongarch/pixel-a.S \
-           common/loongarch/dct-a.S \
-           common/loongarch/quant-a.S \
-           common/loongarch/predict-a.S \
-           common/loongarch/deblock-a.S
+            common/loongarch/sad-a.S \
+            common/loongarch/pixel-a.S \
+            common/loongarch/dct-a.S \
+            common/loongarch/quant-a.S \
+            common/loongarch/predict-a.S \
+            common/loongarch/deblock-a.S
 
 SRCS_X += common/loongarch/pixel-c.c \
           common/loongarch/predict-c.c \
           common/loongarch/mc-c.c
-
+endif
 OBJASM +=
 ifneq ($(findstring HAVE_BITDEPTH8 1, $(CONFIG)),)
 OBJASM += $(SRCASM_X:%.S=%-8.o)
@@ -222,7 +223,6 @@ ifneq ($(findstring HAVE_BITDEPTH10 1, $(CONFIG)),)
 OBJASM += $(SRCASM_X:%.S=%-10.o)
 endif
 
-endif
 endif
 
 endif
diff --git a/common/cabac.c b/common/cabac.c
index 8c3e72af..93eb6e61 100644
--- a/common/cabac.c
+++ b/common/cabac.c
@@ -54,7 +54,7 @@ void x264_cabac_encode_init_core( x264_cabac_t *cb )
 {
     cb->i_low   = 0;
     cb->i_range = 0x01FE;
-    cb->i_queue = -9; // the first bit will be shifted away and not written
+    cb->i_queue = -41; // the first bit will be shifted away and not written
     cb->i_bytes_outstanding = 0;
 }
 
@@ -67,18 +67,99 @@ void x264_cabac_encode_init( x264_cabac_t *cb, uint8_t *p_data, uint8_t *p_end )
 }
 
 static inline void cabac_putbyte( x264_cabac_t *cb )
+{
+    if(cb->i_queue >= 0)
+    {
+        int64_t shift = 0x40000000000;
+        for(int i=0;i<5;i++){
+            int64_t out=cb->i_low >> (cb->i_queue+42-i*8);
+            cb->i_low &= ((shift >> (i*8)) << cb->i_queue)-1;
+            if((out & 0xff) == 0xff){
+                cb->i_bytes_outstanding++;
+            }
+            else
+            {
+                int carry = out>>8;
+                int bytes_outstanding = cb->i_bytes_outstanding;
+                cb->p[-1] += carry;
+                while( bytes_outstanding > 0 )
+                {
+                    *(cb->p++) = carry-1;
+                    bytes_outstanding--;
+                }
+                *(cb->p++) = out;
+                cb->i_bytes_outstanding = 0;
+            }
+        }
+        cb->i_queue -= 40;
+    }
+}
+
+static inline void cabac_putbyte_first( x264_cabac_t *cb )
+{
+    if(cb->i_queue < -32)
+    {
+        cb->i_queue += 32;
+    }
+    else
+    {
+        int eps;
+        int queue = cb->i_queue;
+        if(cb->i_queue >= -32 && cb->i_queue < -24){
+            eps = 1;
+            cb->i_queue += 24;
+        }
+        else if(cb->i_queue >= -24 && cb->i_queue < -16){
+            eps = 2;
+            cb->i_queue += 16;
+        }
+        else if(cb->i_queue >= -16 && cb->i_queue < -8 ){
+            eps = 3;
+            cb->i_queue += 8;
+        }
+        else if(cb->i_queue >= -8 && cb->i_queue < 0){
+            eps = 4;
+        }
+
+        int64_t shift = 0x400;
+        for(int i=0; i<eps; i++){
+            int64_t out = cb->i_low >> (queue + 42 - i*8);
+            cb->i_low &= (shift<<(queue + 32 - i*8))-1;
+
+            if( ((out) & 0xff) == 0xff ){
+                cb->i_bytes_outstanding++;
+            }
+            else
+            {
+                int64_t carry = out >> 8;
+                int bytes_outstanding = cb->i_bytes_outstanding;
+                cb->p[-1] += carry;
+                while( bytes_outstanding > 0 )
+                {
+                    *(cb->p++) = carry-1;
+                    bytes_outstanding--;
+                }
+                *(cb->p++) = out;
+                cb->i_bytes_outstanding = 0;
+            }
+        }
+    }
+}
+
+static inline void cabac_putbyte_flush( x264_cabac_t *cb )
 {
     if( cb->i_queue >= 0 )
     {
-        int out = cb->i_low >> (cb->i_queue+10);
+        int64_t out = cb->i_low >> (cb->i_queue+10);
         cb->i_low &= (0x400<<cb->i_queue)-1;
         cb->i_queue -= 8;
 
-        if( (out & 0xff) == 0xff )
+        if( (out & 0xff) == 0xff ){
             cb->i_bytes_outstanding++;
+        }
         else
         {
-            int carry = out >> 8;
+            int64_t carry = out >> 8;
             int bytes_outstanding = cb->i_bytes_outstanding;
             // this can't modify before the beginning of the stream because
             // that would correspond to a probability > 1.
@@ -164,16 +245,18 @@ void x264_cabac_encode_terminal_c( x264_cabac_t *cb )
 
 void x264_cabac_encode_flush( x264_t *h, x264_cabac_t *cb )
 {
+    cabac_putbyte_first( cb );
     cb->i_low += cb->i_range - 2;
     cb->i_low |= 1;
     cb->i_low <<= 9;
     cb->i_queue += 9;
-    cabac_putbyte( cb );
-    cabac_putbyte( cb );
-    cb->i_low <<= -cb->i_queue;
+    cabac_putbyte_flush( cb );
+    cabac_putbyte_flush( cb );
+    cb->i_low <<= -(cb->i_queue);
     cb->i_low |= (0x35a4e4f5 >> (h->i_frame & 31) & 1) << 10;
     cb->i_queue = 0;
-    cabac_putbyte( cb );
+    cabac_putbyte_flush( cb );
+    cb->i_queue = -40;
 
     while( cb->i_bytes_outstanding > 0 )
     {
diff --git a/common/cabac.h b/common/cabac.h
index 8a6eb619..23dfe37a 100644
--- a/common/cabac.h
+++ b/common/cabac.h
@@ -30,7 +30,7 @@
 typedef struct
 {
     /* state */
-    int i_low;
+    uint64_t i_low;
     int i_range;
 
     /* bit stream */
@@ -84,7 +84,7 @@ void x264_cabac_encode_flush( x264_t *h, x264_cabac_t *cb );
 #define x264_cabac_encode_decision x264_cabac_encode_decision_asm
 #define x264_cabac_encode_bypass x264_cabac_encode_bypass_asm
 #define x264_cabac_encode_terminal x264_cabac_encode_terminal_asm
-#elif defined(ARCH_LOONGARCH) && !HIGH_BIT_DEPTH
+#elif HAVE_LOONGARCH_ASM && !HIGH_BIT_DEPTH
 #define x264_cabac_encode_decision x264_cabac_encode_decision_asm
 #define x264_cabac_encode_bypass x264_cabac_encode_bypass_asm
 #define x264_cabac_encode_terminal x264_cabac_encode_terminal_asm
diff --git a/common/cpu.c b/common/cpu.c
index af3511b3..02db7ad2 100644
--- a/common/cpu.c
+++ b/common/cpu.c
@@ -93,8 +93,9 @@ const x264_cpu_name_t x264_cpu_names[] =
     {"NEON",            X264_CPU_NEON},
 #elif ARCH_MIPS
     {"MSA",             X264_CPU_MSA},
-#elif ARCH_LOONGARCH
+#elif HAVE_LSX
     {"LSX",             X264_CPU_LSX},
+#elif HAVE_LASX
     {"LASX",            X264_CPU_LASX},
 #endif
     {"", 0},
diff --git a/common/dct.c b/common/dct.c
index 149b0c47..4e6ff6a6 100644
--- a/common/dct.c
+++ b/common/dct.c
@@ -41,7 +41,7 @@
 #if ARCH_MIPS
 #   include "mips/dct.h"
 #endif
-#if ARCH_LOONGARCH
+#if HAVE_LSX || HAVE_LASX
 #   include "loongarch/dct.h"
 #endif
 
@@ -731,17 +731,30 @@ void x264_dct_init( int cpu, x264_dct_function_t *dctf )
     }
 #endif
 
-#if HAVE_LASX
-    if( cpu&X264_CPU_LASX )
+#if HAVE_LSX || HAVE_LASX
+    if( cpu&X264_CPU_LSX )
     {
         dctf->sub4x4_dct       = x264_sub4x4_dct_lsx;
+        dctf->add4x4_idct      = x264_add4x4_idct_lsx;
+        dctf->dct4x4dc         = x264_dct4x4dc_lsx;
+        dctf->idct4x4dc        = x264_idct4x4dc_lsx;
+        dctf->sub8x8_dct8      = x264_sub8x8_dct8_lsx;
+        dctf->sub8x8_dct       = x264_sub8x8_dct_lsx;
+        dctf->add8x8_idct      = x264_add8x8_idct_lsx;
+        dctf->add8x8_idct8     = x264_add8x8_idct8_lsx;
+        dctf->add8x8_idct_dc   = x264_add8x8_idct_dc_lsx;
+        dctf->add16x16_idct    = x264_add16x16_idct_lsx;
+        dctf->sub16x16_dct     = x264_sub16x16_dct_lsx;
+        dctf->add16x16_idct_dc = x264_add16x16_idct_dc_lsx;
+        dctf->sub16x16_dct8    = x264_sub16x16_dct8_lsx;
+    }
+    if( cpu&X264_CPU_LASX )
+    {
         dctf->sub8x8_dct       = x264_sub8x8_dct_lasx;
         dctf->sub16x16_dct     = x264_sub16x16_dct_lasx;
-        dctf->add4x4_idct      = x264_add4x4_idct_lsx;
         dctf->add8x8_idct      = x264_add8x8_idct_lasx;
         dctf->add8x8_idct8     = x264_add8x8_idct8_lasx;
         dctf->add16x16_idct    = x264_add16x16_idct_lasx;
-        dctf->sub8x8_dct8      = x264_sub8x8_dct8_lsx;
         dctf->sub16x16_dct8    = x264_sub16x16_dct8_lasx;
         dctf->add8x8_idct_dc   = x264_add8x8_idct_dc_lasx;
         dctf->add16x16_idct_dc = x264_add16x16_idct_dc_lasx;
@@ -1111,11 +1124,15 @@ void x264_zigzag_init( int cpu, x264_zigzag_function_t *pf_progressive, x264_zig
     }
 #endif
 
-#if HAVE_LASX
+#if HAVE_LSX || HAVE_LASX
+    if( cpu&X264_CPU_LSX)
+    {
+        pf_progressive->scan_4x4  = x264_zigzag_scan_4x4_frame_lsx;
+    }
     if( cpu&X264_CPU_LASX )
     {
         pf_progressive->scan_4x4  = x264_zigzag_scan_4x4_frame_lasx;
-    }
+    }    
 #endif
 #endif // !HIGH_BIT_DEPTH
 }
diff --git a/common/deblock.c b/common/deblock.c
index 152c55ad..f5d8c907 100644
--- a/common/deblock.c
+++ b/common/deblock.c
@@ -666,7 +666,7 @@ void x264_macroblock_deblock( x264_t *h )
 #if HAVE_MSA
 #include "mips/deblock.h"
 #endif
-#if HAVE_LASX
+#if HAVE_LSX || HAVE_LASX
 #include "loongarch/deblock.h"
 #endif
 
@@ -806,7 +806,13 @@ void x264_deblock_init( int cpu, x264_deblock_function_t *pf, int b_mbaff )
     }
 #endif
 
-#if HAVE_LASX
+#if HAVE_LSX || HAVE_LASX
+    if( cpu&X264_CPU_LSX )
+    {
+        pf->deblock_luma_intra[1] = x264_deblock_v_luma_intra_lsx;
+        pf->deblock_luma_intra[0] = x264_deblock_h_luma_intra_lsx;
+        pf->deblock_strength = x264_deblock_strength_lsx;
+    }
     if( cpu&X264_CPU_LASX )
     {
         pf->deblock_luma[1] = x264_deblock_v_luma_lasx;
diff --git a/common/loongarch/cabac-a.S b/common/loongarch/cabac-a.S
index 622ac399..6b98496b 100644
--- a/common/loongarch/cabac-a.S
+++ b/common/loongarch/cabac-a.S
@@ -25,120 +25,154 @@
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
 
-#include "asm.S"
+#include "loongson_asm.S"
 
 #if !HIGH_BIT_DEPTH
 
 #define LOW_OFFSET 0
-#define RANGE_OFFSET 4
-#define QUEUE_OFFSET 8
-#define OUTSTANDING_OFFSET 12
-#define P_OFFSET 24
+#define RANGE_OFFSET 8
+#define QUEUE_OFFSET 12
+#define OUTSTANDING_OFFSET 16
+#define P_OFFSET 32
 #define STATE_OFFSET 68
 
 /* void cabac_encode_terminal( x264_cabac_t *cb )
  */
 function cabac_encode_terminal_asm
     ld.w        t1, a0, RANGE_OFFSET         /* t1: cb->i_range */
-    ld.w        t2, a0, LOW_OFFSET
+    ld.d        t2, a0, LOW_OFFSET
     ld.w        t5, a0, QUEUE_OFFSET
     addi.w      t1, t1, -2
-    srai.w      t4, t1, 3                    /* t4: cb->i_range>>3 */
+    srai.w      t4, t1, 3
     la.global   t3, x264_cabac_renorm_shift
     ldx.b       t4, t3, t4
-    sll.w       t1, t1, t4                   /* t1: cb->i_range <<= shift */
-    sll.w       t2, t2, t4                   /* t2: cb->i_low <<= shift */
-    add.w       t5, t5, t4                   /* t5: cb->i_queue += shift */
+    sll.w       t1, t1, t4                   /* cb->i_range <<= shift */
+    sll.d       t2, t2, t4                   /* cb->i_low <<= shift */
+    add.w       t5, t5, t4                   /* cb->i_queue += shift */
+
+.CABAC_PUTBYTE_TERMINAL:
     blt         t5, zero, .STORE_TERMINAL
-    addi.w      t3, t5, 10
-    sra.w       t4, t2, t3
-    li.w        t6, 0x400
-    sll.w       t3, t6, t5
-    addi.w      t3, t3, -1
+    li.d        t6, 4
+    li.d        a3, 0
+    li.d        a4, 0x40000000000            /* shift */
+    li.d        a5, 0
+    ld.w        a7, a0, OUTSTANDING_OFFSET   /* t3: cb->outstanding */
+
+    ld.d        t8, a0, P_OFFSET
+    ld.bu       t0, t8, -1
+
+.START_FOR_TERMINAL:
+    blt         t6, zero, .END_FOR_TERMINAL
+    addi.d      t3, t5, 42
+    sub.d       t3, t3, a3
+    srl.d       t4, t2, t3                   /* out */
+    srl.d       a6, a4, a3
+    sll.d       t3, a6, t5
+    addi.d      t3, t3, -1
+    addi.d      a3, a3, 8
     and         t2, t2, t3
-    addi.w      t5, t5, -8
-
-    andi        t7, t4, 0xff
-    ld.w        t3, a0, OUTSTANDING_OFFSET   /* t3: cb->outstanding */
-    li.w        t6, 0xff
-    beq         t7, t6, .EQUAL_TERMINAL
-    srli.w      t6, t4, 8
-    ld.d        t7, a0, P_OFFSET
-    ld.bu       t8, t7, -1                   /* t8: cb->p[-1] */
-    add.w       t8, t8, t6
-    st.b        t8, t7, -1
-
-    addi.w      t6, t6, -1
+
+    andi        a6, t4, 0xff
+    li.d        t3, 0xff
+    beq         a6, t3, .EQUAL_TERMINAL
+    srli.d      t3, t4, 8
+    sll.d       a6, t3, a5
+    add.d       t0, t0, a6
+    addi.d      a5, a5, 8
+
+    addi.d      t3, t3, -1
+    andi        t3, t3, 0xff
 .STARTWHILE_TERMINAL:
-    bge         zero, t3, .ENDWHILE_TERMINAL  /* begin while */
-    addi.w      t3, t3, -1                    /* t3: bytes_outstanding-- */
-    st.b        t6, t7, 0
-    addi.d      t7, t7, 1
+    bge         zero, a7, .ENDWHILE_TERMINAL         /* begin while */
+    addi.d      a7, a7, -1                  /* t3: bytes_outstanding-- */
+    sll.d       a6, t3, a5
+    add.d       t0, t0, a6
+    addi.d      a5, a5, 8
     b          .STARTWHILE_TERMINAL
 .ENDWHILE_TERMINAL:
-    st.b        t4, t7, 0                     /* *(cb->p++)=out */
-    addi.d      t7, t7, 1
-    st.d        t7, a0, P_OFFSET
-    st.w        zero, a0, OUTSTANDING_OFFSET
-    b          .STORE_TERMINAL
+    andi        t4, t4, 0xff
+    sll.d       a6, t4, a5
+    add.d       t0, t0, a6
+    addi.d      t6, t6, -1
+    b           .START_FOR_TERMINAL
 .EQUAL_TERMINAL:
-    addi.w      t3, t3, 1
-    st.w        t3, a0, OUTSTANDING_OFFSET
+    addi.d      a7, a7, 1
+    addi.d      t6, t6, -1
+    b           .START_FOR_TERMINAL
+.END_FOR_TERMINAL:
+    st.w        a7, a0, OUTSTANDING_OFFSET
+    st.d        t0, t8, -1
+    srli.d      a5, a5, 3
+    add.d       t8, t8, a5
+    st.d        t8, a0, P_OFFSET
+    addi.d      t5, t5, -40
 .STORE_TERMINAL:
     st.w        t1, a0, RANGE_OFFSET
-    st.w        t2, a0, LOW_OFFSET
+    st.d        t2, a0, LOW_OFFSET
     st.w        t5, a0, QUEUE_OFFSET
-.ENDFUNC_TERMINAL:
 endfunc
 
 function cabac_encode_bypass_asm
     ld.w        t1, a0, RANGE_OFFSET
-    ld.w        t2, a0, LOW_OFFSET
+    ld.d        t2, a0, LOW_OFFSET
     ld.w        t5, a0, QUEUE_OFFSET
-    slli.w      t2, t2, 1
+    slli.d      t2, t2, 1
     and         t6, a1, t1
-    add.w       t2, t2, t6
+    add.d       t2, t2, t6
     addi.w      t5, t5, 1
+
     blt         t5, zero, .STORE_BYPASS
-    addi.w      t3, t5, 10
-    srl.w       t4, t2, t3
-    li.w        t6, 0x400
-    sll.w       t3, t6, t5
-    addi.w      t5, t5, -8
-    addi.w      t3, t3, -1
+    li.d        t0, 0
+    li.d        a1, 4
+    li.d        a3, 0
+    li.d        a4, 0x40000000000            /* shift */
+
+.START_FOR_BYPASS:
+    blt         a1, zero, .END_FOR_BYPASS
+    addi.d      t3, t5, 42
+    sub.d       t3, t3, a3
+    srl.d       t4, t2, t3                   /* out */
+    srl.d       a6, a4, a3
+    sll.d       t3, a6, t5
+    addi.d      t3, t3, -1
+    addi.d      a3, a3, 8
     and         t2, t2, t3
 
-    andi        t7, t4, 0xff
-    ld.w        t3, a0, OUTSTANDING_OFFSET      /* t3: bytes_outstanding */
-    li.w        t6, 0xff
-    beq         t7, t6, .EQUAL_BYPASS
-    srli.w      t6, t4, 8
-    ld.d        t7, a0, P_OFFSET
-    ld.bu       t8, t7, -1                      /* t8: cb->p[-1] */
-    add.w       t8, t8, t6                      /* t8: cb->p[-1]+=carry */
-    st.b        t8, t7, -1
-
-    addi.w      t6, t6, -1
+    ld.w        a7, a0, OUTSTANDING_OFFSET   /* t3: cb->outstanding */
+    andi        a6, t4, 0xff
+    li.d        t6, 0xff
+    beq         a6, t6, .EQUAL_BYPASS
+    srli.d      t6, t4, 8
+    ld.d        t8, a0, P_OFFSET
+    ld.bu       t0, t8, -1
+    add.d       t0, t0, t6
+    st.b        t0, t8, -1
+
+    addi.d      t6, t6, -1
 .STARTWHILE_BYPASS:
-    bge         zero, t3, .ENDWHILE_BYPASS      /* begin while */
-    addi.w      t3, t3, -1                      /* t3: bytes_outstanding-- */
-    st.b        t6, t7, 0
-    addi.d      t7, t7, 1
+    bge         zero, a7, .ENDWHILE_BYPASS         /* begin while */
+    addi.d      a7, a7, -1                  /* t3: bytes_outstanding-- */
+    st.b        t6, t8, 0
+    addi.d      t8, t8, 1
     b          .STARTWHILE_BYPASS
 .ENDWHILE_BYPASS:
-    st.b        t4, t7, 0                       /* *(cb->p++)=out */
-    addi.d      t7, t7, 1
-    st.d        t7, a0, P_OFFSET
-    st.w        zero, a0, OUTSTANDING_OFFSET
-    b          .STORE_BYPASS
+    st.b        t4, t8, 0
+    addi.d      t8, t8, 1
+    st.d        t8, a0, P_OFFSET
+    st.w        zero,   a0, OUTSTANDING_OFFSET
+    addi.d      a1, a1, -1
+    b           .START_FOR_BYPASS
 .EQUAL_BYPASS:
-    addi.w      t3, t3, 1
-    st.w        t3, a0, OUTSTANDING_OFFSET
+    addi.d      a7, a7, 1
+    st.w        a7, a0, OUTSTANDING_OFFSET
+    addi.d      a1, a1, -1
+    b           .START_FOR_BYPASS
+.END_FOR_BYPASS:
+    addi.d      t5, t5, -40
 .STORE_BYPASS:
     st.w        t1, a0, RANGE_OFFSET
-    st.w        t2, a0, LOW_OFFSET
+    st.d        t2, a0, LOW_OFFSET
     st.w        t5, a0, QUEUE_OFFSET
-.ENDFUNC_BYPASS:
 endfunc
 
 function cabac_encode_decision_asm
@@ -146,7 +180,7 @@ function cabac_encode_decision_asm
     ldx.bu      a4, a3, a1            /* i_state */
     srli.w      a5, a4, 1             /* a5: i_state>>1 */
     ld.w        t1, a0, RANGE_OFFSET  /* t1: cb->i_range */
-    ld.w        t2, a0, LOW_OFFSET
+    ld.d        t2, a0, LOW_OFFSET
     ld.w        t5, a0, QUEUE_OFFSET
     srli.w      a6, t1, 6             /* a6: cb->i_range>>6 */
     addi.w      a6, a6, -4            /* a6: (cb->i_range>>6)-4 */
@@ -158,65 +192,75 @@ function cabac_encode_decision_asm
     sub.w       t1, t1, t4
     li.w        t6, 1
     and         t6, t6, a4
-    beq         a2, t6, .ENDIF_DECISION
-    add.w       t2, t2, t1
+    beq         a2, t6, .ENDIF
+    add.d       t2, t2, t1
     addi.w      t1, t4, 0
-.ENDIF_DECISION:
+.ENDIF:
     la.global   t3, x264_cabac_transition
     li.w        t6, 2
     mul.w       t6, t6, a4
     add.w       t6, t6, a2
     ldx.bu      t4, t3, t6
     stx.b       t4, a3, a1
-
     srai.w      t6, t1, 3
     la.global   t3, x264_cabac_renorm_shift
     ldx.b       t4, t3, t6
     sll.w       t1, t1, t4
-    sll.w       t2, t2, t4
+    sll.d       t2, t2, t4
     add.w       t5, t5, t4
 
     blt         t5, zero, .STORE_DECISION
-    addi.w      t3, t5, 10
-    sra.w       t4, t2, t3
-    li.w        t6, 0x400
-    sll.w       t3, t6, t5
-    addi.w      t5, t5, -8
-    addi.w      t3, t3, -1
+    li.d        t0, 0
+    li.d        a1, 4
+    li.d        a3, 0
+    li.d        a4, 0x40000000000            /* shift */
+
+.START_FOR_DECISION:
+    blt         a1, zero, .END_FOR_DECISION
+    addi.d      t3, t5, 42
+    sub.d       t3, t3, a3
+    srl.d       t4, t2, t3                   /* out */
+    srl.d       a6, a4, a3
+    sll.d       t3, a6, t5
+    addi.d      t3, t3, -1
+    addi.d      a3, a3, 8
     and         t2, t2, t3
 
-    andi        t7, t4, 0xff
-    ld.w        t3, a0, OUTSTANDING_OFFSET
-    li.w        t6, 0xff
-    beq         t7, t6, .EQUAL_DECISION
-    srli.w      t6, t4, 8
-    ld.d        t7, a0, P_OFFSET
-    ld.bu       t8, t7, -1                  /* t8: cb->p[-1] */
-    add.w       t8, t8, t6                  /* t8: cb->p[-1]+=carry */
-    st.b        t8, t7, -1
-
-    addi.w      t6, t6, -1                  /* t6: carry -1 */
+    ld.w        a7, a0, OUTSTANDING_OFFSET   /* t3: cb->outstanding */
+    andi        a6, t4, 0xff
+    li.d        t6, 0xff
+    beq         a6, t6, .EQUAL_DECISION
+    srli.d      t6, t4, 8
+    ld.d        t8, a0, P_OFFSET
+    ld.bu       t0, t8, -1
+    add.d       t0, t0, t6
+    st.b        t0, t8, -1
+
+    addi.d      t6, t6, -1
 .STARTWHILE_DECISION:
-    bge         zero, t3, .ENDWHILE_DECISION  /* begin while */
-    addi.w      t3, t3, -1                  /* t3: bytes_outstanding-- */
-    st.b        t6, t7, 0
-    addi.d      t7, t7, 1
+    bge         zero, a7, .ENDWHILE_DECISION         /* begin while */
+    addi.d      a7, a7, -1                  /* t3: bytes_outstanding-- */
+    st.b        t6, t8, 0
+    addi.d      t8, t8, 1
     b          .STARTWHILE_DECISION
 .ENDWHILE_DECISION:
-    st.b        t4, t7, 0                   /* *(cb->p++)=out */
-    addi.d      t7, t7, 1
-    st.d        t7, a0, P_OFFSET
-    st.w        zero, a0, OUTSTANDING_OFFSET
-    b          .STORE_DECISION
+    st.b        t4, t8, 0
+    addi.d      t8, t8, 1
+    st.d        t8, a0, P_OFFSET
+    st.w        zero,   a0, OUTSTANDING_OFFSET
+    addi.d      a1, a1, -1
+    b           .START_FOR_DECISION
 .EQUAL_DECISION:
-    ld.w        t3, a0, OUTSTANDING_OFFSET  /* t3: cb->outstanding */
-    addi.w      t3, t3, 1
-    st.w        t3, a0, OUTSTANDING_OFFSET
+    addi.d      a7, a7, 1
+    st.w        a7, a0, OUTSTANDING_OFFSET
+    addi.d      a1, a1, -1
+    b           .START_FOR_DECISION
+.END_FOR_DECISION:
+    addi.d      t5, t5, -40
 .STORE_DECISION:
     st.w        t1, a0, RANGE_OFFSET
-    st.w        t2, a0, LOW_OFFSET
+    st.d        t2, a0, LOW_OFFSET
     st.w        t5, a0, QUEUE_OFFSET
-.ENDFUNC_DECISION:
 endfunc
 #endif
 
diff --git a/common/loongarch/dct-a.S b/common/loongarch/dct-a.S
index 41d14682..d740d851 100644
--- a/common/loongarch/dct-a.S
+++ b/common/loongarch/dct-a.S
@@ -23,7 +23,7 @@
  * This program is also available under a commercial proprietary license.
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
-#include "asm.S"
+#include "loongson_asm.S"
 
 const hsub_mul
 .rept 16
@@ -94,7 +94,7 @@ endconst
     LOAD_DIFF8x4_LASX 2, 3, 4, 5, 6, 7, 2, 3, 6, 7
     DCT4_1D_LASX xr0, xr1, xr2, xr3, xr4
     LASX_TRANSPOSE2x4x4_H xr0, xr2, xr3, xr4, xr0, xr1, \
-                          xr2, xr3, xr0, xr2, xr3, xr4, xr1
+                          xr2, xr3, xr10, xr12, xr13
 
     DCT4_1D_LASX xr2, xr0, xr3, xr1, xr4
     xvilvh.d        xr0,    xr2,     xr3 /* 6, 2 */
@@ -113,24 +113,167 @@ endconst
     xvst            xr5,    a0,      16 * 6
 .endm
 
+.macro SUB8x8_DCT_CORE_LSX
+    fld.d           f0,  a1,     FENC_STRIDE * 0
+    fld.d           f1,  a1,     FENC_STRIDE * 1
+    fld.d           f4,  a1,     FENC_STRIDE * 4
+    fld.d           f5,  a1,     FENC_STRIDE * 5
+    fld.d           f2,  a2,     FDEC_STRIDE * 0
+    fld.d           f3,  a2,     FDEC_STRIDE * 1
+    fld.d           f6,  a2,     FDEC_STRIDE * 4
+    fld.d           f7,  a2,     FDEC_STRIDE * 5
+
+    vilvl.b         vr0, vr8,    vr0
+    vilvl.b         vr1, vr8,    vr1
+    vilvl.b         vr4, vr8,    vr4
+    vilvl.b         vr5, vr8,    vr5
+    vilvl.b         vr2, vr8,    vr2
+    vilvl.b         vr3, vr8,    vr3
+    vilvl.b         vr6, vr8,    vr6
+    vilvl.b         vr7, vr8,    vr7
+    vsub.h          vr0, vr0,    vr2
+    vsub.h          vr4, vr4,    vr6
+    vsub.h          vr1, vr1,    vr3
+    vsub.h          vr5, vr5,    vr7
+
+    fld.d           f2,   a1,     FENC_STRIDE * 2
+    fld.d           f3,   a1,     FENC_STRIDE * 3
+    fld.d           f6,   a1,     FENC_STRIDE * 6
+    fld.d           f7,   a1,     FENC_STRIDE * 7
+    fld.d           f9,   a2,     FDEC_STRIDE * 2
+    fld.d           f11,  a2,     FDEC_STRIDE * 3
+    fld.d           f10,  a2,     FDEC_STRIDE * 6
+    fld.d           f12,  a2,     FDEC_STRIDE * 7
+
+    vilvl.b         vr2,  vr8,    vr2
+    vilvl.b         vr3,  vr8,    vr3
+    vilvl.b         vr6,  vr8,    vr6
+    vilvl.b         vr7,  vr8,    vr7
+    vilvl.b         vr9,  vr8,    vr9
+    vilvl.b         vr11, vr8,    vr11
+    vilvl.b         vr10, vr8,    vr10
+    vilvl.b         vr12, vr8,    vr12
+    vsub.h          vr2,  vr2,    vr9
+    vsub.h          vr6,  vr6,    vr10
+    vsub.h          vr3,  vr3,    vr11
+    vsub.h          vr7,  vr7,    vr12
+
+    vadd.h          vr9,    vr3,    vr0
+    vadd.h          vr10,   vr7,    vr4
+    vsub.h          vr0,    vr0,    vr3
+    vsub.h          vr4,    vr4,    vr7
+    vadd.h          vr3,    vr2,    vr1
+    vadd.h          vr7,    vr6,    vr5
+    vsub.h          vr1,    vr1,    vr2
+    vsub.h          vr5,    vr5,    vr6
+
+    vadd.h          vr2,    vr3,    vr9
+    vadd.h          vr6,    vr7,   vr10
+    vsub.h          vr9,    vr9,    vr3
+    vsub.h          vr10,   vr10,   vr7
+
+    vsub.h          vr3,   vr0,    vr1
+    vsub.h          vr7,   vr4,    vr5
+    vsub.h          vr3,   vr3,    vr1
+    vsub.h          vr7,   vr7,    vr5
+    vadd.h          vr0,   vr0,    vr0
+    vadd.h          vr4,   vr4,    vr4
+    vadd.h          vr0,   vr0,    vr1
+    vadd.h          vr4,   vr4,    vr5
+
+    vilvh.h        vr11,  vr0,   vr2
+    vilvh.h        vr12,  vr4,   vr6
+    vilvl.h        vr13,  vr0,   vr2
+    vilvl.h        vr14,  vr4,   vr6
+    vilvh.h        vr15,  vr3,   vr9
+    vilvh.h        vr16,  vr7,   vr10
+    vilvl.h        vr17,  vr3,   vr9
+    vilvl.h        vr18,  vr7,   vr10
+
+    vilvh.w        vr19,  vr17,  vr13
+    vilvh.w        vr20,  vr18,  vr14
+    vilvl.w        vr13,  vr17,  vr13
+    vilvl.w        vr14,  vr18,  vr14
+    vilvh.w        vr17,  vr15,  vr11
+    vilvh.w        vr18,  vr16,  vr12
+    vilvl.w        vr11,  vr15,  vr11
+    vilvl.w        vr12,  vr16,  vr12
+
+    vilvh.d        vr0,  vr11, vr13
+    vilvh.d        vr4,  vr12, vr14
+    vilvl.d        vr2,  vr11, vr13
+    vilvl.d        vr6,  vr12, vr14
+    vilvh.d        vr1,  vr17, vr19
+    vilvh.d        vr5,  vr18, vr20
+    vilvl.d        vr3,  vr17, vr19
+    vilvl.d        vr7,  vr18, vr20
+
+    vadd.h          vr9,    vr1,    vr2
+    vadd.h          vr10,   vr5,    vr6
+    vsub.h          vr2,    vr2,    vr1
+    vsub.h          vr6,    vr6,    vr5
+    vadd.h          vr1,    vr3,    vr0
+    vadd.h          vr5,    vr7,    vr4
+    vsub.h          vr0,    vr0,    vr3
+    vsub.h          vr4,    vr4,    vr7
+
+    vadd.h          vr3,    vr1,    vr9
+    vadd.h          vr7,    vr5,    vr10
+    vsub.h          vr9,    vr9,    vr1
+    vsub.h          vr10,   vr10,   vr5
+
+    vsub.h          vr1,    vr2,    vr0
+    vsub.h          vr5,    vr6,    vr4
+    vsub.h          vr1,    vr1,    vr0
+    vsub.h          vr5,    vr5,    vr4
+    vadd.h          vr2,    vr2,    vr2
+    vadd.h          vr6,    vr6,    vr6
+    vadd.h          vr2,    vr2,    vr0
+    vadd.h          vr6,    vr6,    vr4
+
+    vilvh.d         vr0,    vr2,    vr3
+    vilvh.d         vr4,    vr6,    vr7
+    vilvl.d         vr3,    vr2,    vr3
+    vilvl.d         vr7,    vr6,    vr7
+    vilvh.d         vr2,    vr1,    vr9
+    vilvh.d         vr6,    vr5,    vr10
+    vilvl.d         vr9,    vr1,    vr9
+    vilvl.d         vr10,   vr5,    vr10
+
+    vor.v           vr1,    vr3,    vr3
+    vor.v           vr5,    vr7,    vr7
+
+    vor.v           vr12,   vr4,    vr4
+
+    vst             vr3,    a0,     0
+    vst             vr9,    a0,     16
+    vst             vr0,    a0,     32
+    vst             vr2,    a0,     48
+    vst             vr5,    a0,     64
+    vst             vr10,   a0,     80
+    vst             vr12,   a0,     96
+    vst             vr6,    a0,     112
+.endm
+
 /* void subwxh_dct( dctcoef*, pixel*, pixel* ) */
 function sub4x4_dct_lsx
     fld.s           f0,     a1,      0
     fld.s           f4,     a2,      0
     fld.s           f1,     a1,      FENC_STRIDE
     fld.s           f5,     a2,      FDEC_STRIDE
-    vext2xv.hu.bu   xr0,    xr0
-    vext2xv.hu.bu   xr1,    xr1
-    vext2xv.hu.bu   xr4,    xr4
-    vext2xv.hu.bu   xr5,    xr5
+
+    vsllwil.hu.bu   vr0,    vr0,     0
+    vsllwil.hu.bu   vr1,    vr1,     0
+    vsllwil.hu.bu   vr4,    vr4,     0
+    vsllwil.hu.bu   vr5,    vr5,     0
     fld.s           f2,     a1,      FENC_STRIDE * 2
     fld.s           f6,     a2,      FDEC_STRIDE * 2
     fld.s           f3,     a1,      FENC_STRIDE * 3
     fld.s           f7,     a2,      FDEC_STRIDE * 3
-    vext2xv.hu.bu   xr2,    xr2
-    vext2xv.hu.bu   xr3,    xr3
-    vext2xv.hu.bu   xr6,    xr6
-    vext2xv.hu.bu   xr7,    xr7
+    vsllwil.hu.bu   vr2,    vr2,     0
+    vsllwil.hu.bu   vr3,    vr3,     0
+    vsllwil.hu.bu   vr6,    vr6,     0
+    vsllwil.hu.bu   vr7,    vr7,     0
     vsub.h          vr0,    vr0,     vr4
     vsub.h          vr1,    vr1,     vr5
     vsub.h          vr2,    vr2,     vr6
@@ -150,6 +293,11 @@ function sub8x8_dct_lasx
     SUB8x8_DCT_CORE_LASX
 endfunc
 
+function sub8x8_dct_lsx
+    vxor.v         vr8,    vr8,    vr8
+    SUB8x8_DCT_CORE_LSX
+endfunc
+
 function sub16x16_dct_lasx
     xvxor.v  xr8,  xr8,  xr8
     SUB8x8_DCT_CORE_LASX
@@ -167,6 +315,23 @@ function sub16x16_dct_lasx
     SUB8x8_DCT_CORE_LASX
 endfunc
 
+function sub16x16_dct_lsx
+    vxor.v   vr8,   vr8,    vr8
+    SUB8x8_DCT_CORE_LSX
+    addi.d         a0,      a0,    32 * 4
+    addi.d         a1,      a1,    8
+    addi.d         a2,      a2,    8
+    SUB8x8_DCT_CORE_LSX
+    addi.d         a0,      a0,    32 * 4
+    addi.d         a1,      a1,    8*FENC_STRIDE - 8
+    addi.d         a2,      a2,    8*FDEC_STRIDE - 8
+    SUB8x8_DCT_CORE_LSX
+    addi.d         a0,      a0,    32 * 4
+    addi.d         a1,      a1,    8
+    addi.d         a2,      a2,    8
+    SUB8x8_DCT_CORE_LSX
+endfunc
+
 /*
  * void add4x4_idct( pixel *p_dst, dctcoef dct[16] )
  */
@@ -185,23 +350,22 @@ function add4x4_idct_lsx
     vilvl.h         vr15,   vr2,   vr6
     vilvl.h         vr16,   vr5,   vr4
 
-    vhaddw.w.h      vr7,    vr1,  vr1
-    vhsubw.w.h      vr8,    vr1,  vr1
-    vhaddw.w.h      vr9,    vr15, vr15
-    vhsubw.w.h      vr10,   vr16, vr16
+    vhaddw.w.h      vr7,    vr1,   vr1
+    vhsubw.w.h      vr8,    vr1,   vr1
+    vhaddw.w.h      vr9,    vr15,  vr15
+    vhsubw.w.h      vr10,   vr16,  vr16
 
-    vadd.w          vr1,    vr7,  vr9
-    vadd.w          vr2,    vr8,  vr10
-    vsub.w          vr3,    vr8,  vr10
-    vsub.w          vr4,    vr7,  vr9
+    vadd.w          vr1,    vr7,   vr9
+    vadd.w          vr2,    vr8,   vr10
+    vsub.w          vr3,    vr8,   vr10
+    vsub.w          vr4,    vr7,   vr9
 
-    vpickev.h       vr1,    vr1,  vr1
-    vpickev.h       vr2,    vr2,  vr2
-    vpickev.h       vr3,    vr3,  vr3
-    vpickev.h       vr4,    vr4,  vr4
+    vpickev.h       vr1,    vr1,   vr1
+    vpickev.h       vr2,    vr2,   vr2
+    vpickev.h       vr3,    vr3,   vr3
+    vpickev.h       vr4,    vr4,   vr4
 
     LSX_TRANSPOSE4x4_H vr1, vr2, vr3, vr4, vr1, vr2, vr3, vr4, vr5, vr6
-
     vsrai.h         vr5,    vr2,   1
     vsrai.h         vr6,    vr4,   1
 
@@ -289,7 +453,7 @@ endfunc
     xvpickev.h      xr13,   xr9,   xr9
 
     LASX_TRANSPOSE4x8_H xr10, xr11, xr12, xr13, xr10, xr11, xr12, xr13, \
-                        xr4, xr5, xr6, xr7
+                        xr4, xr5
 
     xvsllwil.w.h    xr10,   xr10,  0
     xvsllwil.w.h    xr11,   xr11,  0
@@ -345,6 +509,135 @@ endfunc
     xvstelm.w       xr3,    a0,    FDEC_STRIDE * 3 + 4,   7
 .endm
 
+.macro LSX_SUMSUB_W sum0, sum1, diff0, diff1, in0, in1, in2, in3
+    vadd.w          \sum0,  \in0,   \in2
+    vadd.w          \sum1,  \in1,   \in3
+    vsub.w          \diff0, \in0,   \in2
+    vsub.w          \diff1, \in1,   \in3
+.endm
+
+.macro add8x4_idct_core_lsx
+    fld.d           f1,     a1,     0
+    fld.d           f2,     a1,     8
+    fld.d           f3,     a1,     16
+    fld.d           f4,     a1,     24
+    fld.d           f5,     a1,     32
+    fld.d           f6,     a1,     40
+    fld.d           f7,     a1,     48
+    fld.d           f8,     a1,     56
+
+    vpermi.w        vr9,    vr6,    0x04
+    vpermi.w        vr9,    vr2,    0x44
+    vpermi.w        vr10,   vr8,    0x04
+    vpermi.w        vr10,   vr4,    0x44
+
+    vsrai.h         vr9,    vr9,    1
+    vsrai.h         vr10,   vr10,   1
+
+    vsllwil.w.h     vr1,    vr1,    0
+    vsllwil.w.h     vr5,    vr5,    0
+    vsllwil.w.h     vr2,    vr2,    0
+    vsllwil.w.h     vr6,    vr6,    0
+    vsllwil.w.h     vr3,    vr3,    0
+    vsllwil.w.h     vr7,    vr7,    0
+    vsllwil.w.h     vr4,    vr4,    0
+    vsllwil.w.h     vr8,    vr8,    0
+    vexth.w.h       vr11,   vr9
+    vsllwil.w.h     vr9,    vr9,    0
+    vexth.w.h       vr12,   vr10
+    vsllwil.w.h     vr10,   vr10,   0
+
+    LSX_SUMSUB_W    vr13, vr14, vr15, vr16, vr1, vr5, vr3, vr7
+    vadd.w          vr17,   vr2,    vr10
+    vadd.w          vr18,   vr6,    vr12
+    vsub.w          vr19,   vr9,    vr4
+    vsub.w          vr20,   vr11,   vr8
+
+    LSX_SUMSUB_W    vr3, vr7, vr10, vr12, vr13, vr14, vr17, vr18
+    LSX_SUMSUB_W    vr4, vr8, vr9,  vr11, vr15, vr16, vr19, vr20
+
+    vpickev.h       vr13,   vr3,    vr3
+    vpickev.h       vr14,   vr7,    vr7
+    vpickev.h       vr15,   vr4,    vr4
+    vpickev.h       vr16,   vr8,    vr8
+    vpickev.h       vr17,   vr9,    vr9
+    vpickev.h       vr18,   vr11,   vr11
+    vpickev.h       vr19,   vr10,   vr10
+    vpickev.h       vr20,   vr12,   vr12
+
+    LSX_TRANSPOSE4x4_H vr13, vr15, vr17, vr19, vr13, vr15, vr17, vr19, vr1, vr3
+    LSX_TRANSPOSE4x4_H vr14, vr16, vr18, vr20, vr14, vr16, vr18, vr20, vr2, vr4
+
+    vsllwil.w.h      vr13,   vr13,   0
+    vsllwil.w.h      vr14,   vr14,   0
+    vsllwil.w.h      vr15,   vr15,   0
+    vsllwil.w.h      vr16,   vr16,   0
+    vsllwil.w.h      vr17,   vr17,   0
+    vsllwil.w.h      vr18,   vr18,   0
+    vsllwil.w.h      vr19,   vr19,   0
+    vsllwil.w.h      vr20,   vr20,   0
+
+    vsrai.w          vr1,    vr15,   1
+    vsrai.w          vr2,    vr16,   1
+    vsrai.w          vr3,    vr19,   1
+    vsrai.w          vr4,    vr20,   1
+
+    LSX_SUMSUB_W     vr5, vr6, vr21, vr22, vr13, vr14, vr17, vr18
+    vadd.w           vr8, vr15, vr3
+    vadd.w           vr9, vr16, vr4
+    vsub.w           vr10, vr1, vr19
+    vsub.w           vr11, vr2, vr20
+
+    LSX_SUMSUB_W     vr13, vr14, vr19, vr20, vr5, vr6, vr8, vr9
+    LSX_SUMSUB_W     vr15, vr16, vr17, vr18, vr21, vr22, vr10, vr11
+
+    vssrarni.h.w     vr15,   vr13,   6
+    vssrarni.h.w     vr16,   vr14,   6
+    vssrarni.h.w     vr19,   vr17,   6
+    vssrarni.h.w     vr20,   vr18,   6
+
+    fld.s           f1,     a0,    0
+    fld.s           f2,     a0,    FDEC_STRIDE
+    fld.s           f3,     a0,    FDEC_STRIDE * 2
+    fld.s           f4,     a0,    FDEC_STRIDE * 3
+    fld.s           f5,     a0,    4
+    fld.s           f6,     a0,    FDEC_STRIDE + 4
+    fld.s           f7,     a0,    FDEC_STRIDE * 2 + 4
+    fld.s           f8,     a0,    FDEC_STRIDE * 3 + 4
+
+    vpickve2gr.w    t0,     vr2,   0
+    vinsgr2vr.w     vr1,    t0,    1
+    vpickve2gr.w    t0,     vr4,   0
+    vinsgr2vr.w     vr3,    t0,    1
+    vpickve2gr.w    t0,     vr6,   0
+    vinsgr2vr.w     vr5,    t0,    1
+    vpickve2gr.w    t0,     vr8,   0
+    vinsgr2vr.w     vr7,    t0,    1
+
+    vilvl.b         vr1,    vr0,    vr1
+    vilvl.b         vr5,    vr0,    vr5
+    vilvl.b         vr3,    vr0,    vr3
+    vilvl.b         vr7,    vr0,    vr7
+
+    vadd.h          vr1,    vr1,    vr15
+    vadd.h          vr5,    vr5,    vr16
+    vadd.h          vr3,    vr3,    vr19
+    vadd.h          vr7,    vr7,    vr20
+
+    vssrarni.bu.h   vr3,    vr1,    0
+    vssrarni.bu.h   vr7,    vr5,    0
+
+    vstelm.w       vr3,    a0,    0,                     0
+    vstelm.w       vr3,    a0,    FDEC_STRIDE,           1
+    vstelm.w       vr3,    a0,    FDEC_STRIDE * 2,       2
+    vstelm.w       vr3,    a0,    FDEC_STRIDE * 3,       3
+
+    vstelm.w       vr7,    a0,    4,                     0
+    vstelm.w       vr7,    a0,    FDEC_STRIDE + 4,       1
+    vstelm.w       vr7,    a0,    FDEC_STRIDE * 2 + 4,   2
+    vstelm.w       vr7,    a0,    FDEC_STRIDE * 3 + 4,   3
+.endm
+
 /*
  * void add8x8_idct( pixel *p_dst, dctcoef dct[4][16] )
  *
@@ -357,6 +650,19 @@ function add8x8_idct_lasx
     addi.d          a1,     a1,    64
     add8x4_idct_core_lasx
 endfunc
+
+.macro add8x8_idct_core_lsx
+    add8x4_idct_core_lsx
+
+    addi.d          a0,     a0,     FDEC_STRIDE * 4
+    addi.d          a1,     a1,     64
+    add8x4_idct_core_lsx
+.endm
+
+function add8x8_idct_lsx
+    vxor.v          vr0,    vr1,    vr1
+    add8x8_idct_core_lsx
+endfunc
 /*
  * void add16x16_idct( pixel *p_dst, dctcoef dct[16][16] )
  */
@@ -385,7 +691,6 @@ function add16x16_idct_lasx
     addi.d          a1,     a1,    64
     add8x4_idct_core_lasx
 
-
     addi.d          a0,     t6,    8
     addi.d          a1,     t5,    384
     add8x4_idct_core_lasx
@@ -394,6 +699,27 @@ function add16x16_idct_lasx
     add8x4_idct_core_lasx
 endfunc
 
+function add16x16_idct_lsx
+    move            t4,     a0
+    move            t5,     a1
+
+    vxor.v          vr0,    vr1,   vr1
+    add8x8_idct_core_lsx
+
+    addi.d          a0,     t4,    8
+    addi.d          a1,     t5,    128
+    add8x8_idct_core_lsx
+
+    addi.d          t6,     t4,    FDEC_STRIDE * 8
+    move            a0,     t6
+    addi.d          a1,     t5,    256
+    add8x8_idct_core_lsx
+
+    addi.d          a0,     t6,    8
+    addi.d          a1,     t5,    384
+    add8x8_idct_core_lsx
+endfunc
+
 /*
  * void add8x8_idct8( pixel *dst, dctcoef dct[64] )
  */
@@ -583,6 +909,327 @@ function add8x8_idct8_lasx
     xvstelm.d       xr18,   a0,    FDEC_STRIDE * 7,    3
 endfunc
 
+function add8x8_idct8_lsx
+    ld.h            t0,     a1,     0
+    addi.w          t0,     t0,     32
+    st.h            t0,     a1,     0
+
+    vld             vr0,    a1,     0
+    vld             vr2,    a1,     32
+    vld             vr4,    a1,     64
+    vld             vr6,    a1,     96
+
+    vsrai.h         vr8,    vr2,    1
+    vsrai.h         vr10,   vr6,    1
+
+    vexth.w.h       vr1,    vr0
+    vsllwil.w.h     vr0,    vr0,    0
+    vexth.w.h       vr3,    vr2
+    vsllwil.w.h     vr2,    vr2,    0
+    vexth.w.h       vr5,    vr4
+    vsllwil.w.h     vr4,    vr4,    0
+    vexth.w.h       vr7,    vr6
+    vsllwil.w.h     vr6,    vr6,    0
+    vexth.w.h       vr9,    vr8
+    vsllwil.w.h     vr8,    vr8,    0
+    vexth.w.h       vr11,   vr10
+    vsllwil.w.h     vr10,   vr10,   0
+
+    LSX_SUMSUB_W    vr12, vr13, vr14, vr15, vr0, vr1, vr4, vr5
+    vsub.w          vr16,   vr8,    vr6
+    vsub.w          vr17,   vr9,    vr7
+    vadd.w          vr18,   vr10,   vr2
+    vadd.w          vr19,   vr11,   vr3
+
+    LSX_SUMSUB_W    vr20, vr21, vr18, vr19, vr12, vr13, vr18, vr19
+    LSX_SUMSUB_W    vr22, vr23, vr16, vr17, vr14, vr15, vr16, vr17
+
+    vld             vr0,    a1,     16
+    vld             vr2,    a1,     48
+    vld             vr4,    a1,     80
+    vld             vr6,    a1,     112
+
+    vsrai.h         vr1,    vr0,    1
+    vsrai.h         vr3,    vr2,    1
+    vsrai.h         vr5,    vr4,    1
+    vsrai.h         vr7,    vr6,    1
+
+    vexth.w.h       vr8,    vr0
+    vsllwil.w.h     vr0,    vr0,    0
+    vexth.w.h       vr10,   vr2
+    vsllwil.w.h     vr2,    vr2,    0
+    vexth.w.h       vr12,   vr4
+    vsllwil.w.h     vr4,    vr4,    0
+    vexth.w.h       vr14,   vr6
+    vsllwil.w.h     vr6,    vr6,    0
+    vexth.w.h       vr9,    vr1
+    vsllwil.w.h     vr1,    vr1,    0
+    vexth.w.h       vr11,   vr3
+    vsllwil.w.h     vr3,    vr3,    0
+    vexth.w.h       vr13,   vr5
+    vsllwil.w.h     vr5,    vr5,    0
+    vexth.w.h       vr15,   vr7
+    vsllwil.w.h     vr7,    vr7,    0
+
+    addi.d          sp,     sp,     -64
+    fst.d           f24,    sp,     0
+    fst.d           f25,    sp,     8
+    fst.d           f26,    sp,     16
+    fst.d           f27,    sp,     24
+    fst.d           f28,    sp,     32
+    fst.d           f29,    sp,     40
+    fst.d           f30,    sp,     48
+    fst.d           f31,    sp,     56
+    LSX_SUMSUB_W    vr24, vr25, vr26, vr27, vr4, vr12, vr2, vr10
+    LSX_SUMSUB_W    vr28, vr29, vr30, vr31, vr6, vr14, vr0, vr8
+
+    vsub.w          vr26,   vr26,   vr6
+    vsub.w          vr27,   vr27,   vr14
+    vsub.w          vr26,   vr26,   vr7
+    vsub.w          vr27,   vr27,   vr15
+    vsub.w          vr28,   vr28,   vr2
+    vsub.w          vr29,   vr29,   vr10
+    vsub.w          vr28,   vr28,   vr3
+    vsub.w          vr29,   vr29,   vr11
+    vadd.w          vr30,   vr30,   vr4
+    vadd.w          vr31,   vr31,   vr12
+    vadd.w          vr30,   vr30,   vr5
+    vadd.w          vr31,   vr31,   vr13
+    vadd.w          vr24,   vr24,   vr0
+    vadd.w          vr25,   vr25,   vr8
+    vadd.w          vr24,   vr24,   vr1
+    vadd.w          vr25,   vr25,   vr9
+
+    vsrai.w         vr1,    vr26,   2
+    vsrai.w         vr9,    vr27,   2
+    vsrai.w         vr2,    vr28,   2
+    vsrai.w         vr10,   vr29,   2
+    vsrai.w         vr3,    vr30,   2
+    vsrai.w         vr11,   vr31,   2
+    vsrai.w         vr4,    vr24,   2
+    vsrai.w         vr12,   vr25,   2
+
+    vadd.w          vr5,    vr4,    vr26
+    vadd.w          vr13,   vr12,   vr27
+    vadd.w          vr6,    vr3,    vr28
+    vadd.w          vr14,   vr11,   vr29
+    vsub.w          vr7,    vr2,    vr30
+    vsub.w          vr15,   vr10,   vr31
+    vsub.w          vr0,    vr24,   vr1
+    vsub.w          vr8,    vr25,   vr9
+
+    LSX_SUMSUB_W    vr1, vr9, vr30, vr31, vr20, vr21, vr0, vr8
+    LSX_SUMSUB_W    vr2, vr10, vr28, vr29, vr22, vr23, vr7, vr15
+    LSX_SUMSUB_W    vr3, vr11, vr26, vr27, vr16, vr17, vr6, vr14
+    LSX_SUMSUB_W    vr4, vr12, vr24, vr25, vr18, vr19, vr5, vr13
+
+    LSX_TRANSPOSE4x4_W vr1, vr2, vr3, vr4, vr5, vr6, vr7, vr0, vr20, vr22
+    LSX_TRANSPOSE4x4_W vr9, vr10, vr11, vr12, vr20, vr22, vr16, vr18, vr1, vr2
+    LSX_TRANSPOSE4x4_W vr24, vr26, vr28, vr30, vr13, vr14, vr15, vr8, vr21, vr23
+    LSX_TRANSPOSE4x4_W vr25, vr27, vr29, vr31, vr21, vr23, vr17, vr19, vr24, vr26
+
+    vsrai.h         vr3,    vr7,    1
+    vsrai.h         vr11,   vr15,   1
+    vsrai.h         vr4,    vr16,   1
+    vsrai.h         vr12,   vr17,   1
+
+    vaddwev.w.h     vr1,    vr5,    vr20
+    vaddwev.w.h     vr9,    vr13,   vr21
+    vsubwev.w.h     vr2,    vr5,    vr20
+    vsubwev.w.h     vr10,   vr13,   vr21
+    vsubwev.w.h     vr3,    vr3,    vr16
+    vsubwev.w.h     vr11,   vr11,   vr17
+    vaddwev.w.h     vr4,    vr4,    vr7
+    vaddwev.w.h     vr12,   vr12,   vr15
+
+    LSX_SUMSUB_W    vr24, vr25, vr30, vr31, vr1, vr9, vr4, vr12
+    LSX_SUMSUB_W    vr26, vr27, vr28, vr29, vr2, vr10, vr3, vr11
+
+    vsrai.h         vr1,    vr6,    1
+    vsrai.h         vr9,    vr14,   1
+    vsrai.h         vr2,    vr0,    1
+    vsrai.h         vr10,   vr8,    1
+    vsrai.h         vr3,    vr22,   1
+    vsrai.h         vr11,   vr23,   1
+    vsrai.h         vr4,    vr18,   1
+    vsrai.h         vr12,   vr19,   1
+
+    vaddwev.w.h     vr5,    vr22,   vr0
+    vaddwev.w.h     vr13,   vr23,   vr8
+    vsubwev.w.h     vr20,   vr22,   vr0
+    vsubwev.w.h     vr21,   vr23,   vr8
+    vaddwev.w.h     vr7,    vr18,   vr6
+    vaddwev.w.h     vr15,   vr19,   vr14
+    vsubwev.w.h     vr16,   vr18,   vr6
+    vsubwev.w.h     vr17,   vr19,   vr14
+
+    vaddwev.w.h     vr4,    vr18,   vr4
+    vaddwev.w.h     vr12,   vr19,   vr12
+    vsub.w          vr20,   vr20,   vr4
+    vsub.w          vr21,   vr21,   vr12
+    vaddwev.w.h     vr2,    vr0,    vr2
+    vaddwev.w.h     vr10,   vr8,    vr10
+    vsub.w          vr7,    vr7,    vr2
+    vsub.w          vr15,   vr15,   vr10
+    vaddwev.w.h     vr3,    vr22,   vr3
+    vaddwev.w.h     vr11,   vr23,   vr11
+    vadd.w          vr16,   vr16,   vr3
+    vadd.w          vr17,   vr17,   vr11
+    vaddwev.w.h     vr1,    vr6,    vr1
+    vaddwev.w.h     vr9,    vr14,   vr9
+    vadd.w          vr5,    vr5,    vr1
+    vadd.w          vr13,   vr13,   vr9
+
+    vsrai.w         vr1,    vr20,   2
+    vsrai.w         vr9,    vr21,   2
+    vsrai.w         vr2,    vr7,    2
+    vsrai.w         vr10,   vr15,   2
+    vsrai.w         vr3,    vr16,   2
+    vsrai.w         vr11,   vr17,   2
+    vsrai.w         vr4,    vr5,    2
+    vsrai.w         vr12,   vr13,   2
+
+    vadd.w          vr20,   vr4,    vr20
+    vadd.w          vr21,   vr12,   vr21
+    vadd.w          vr22,   vr7,   vr3
+    vadd.w          vr23,   vr15,   vr11
+    vsub.w          vr16,   vr2,    vr16
+    vsub.w          vr17,   vr10,   vr17
+    vsub.w          vr18,   vr5,    vr1
+    vsub.w          vr19,   vr13,   vr9
+
+    LSX_SUMSUB_W    vr1, vr9, vr0, vr8, vr24, vr25, vr18, vr19
+    LSX_SUMSUB_W    vr2, vr10, vr7, vr15, vr26, vr27, vr16, vr17
+    LSX_SUMSUB_W    vr3, vr11, vr6, vr14, vr28, vr29, vr22, vr23
+    LSX_SUMSUB_W    vr4, vr12, vr5, vr13, vr30, vr31, vr20, vr21
+
+    vsrai.w         vr24,   vr1,    6
+    vsrai.w         vr25,   vr9,    6
+    vsrai.w         vr26,   vr2,    6
+    vsrai.w         vr27,   vr10,   6
+    vsrai.w         vr28,   vr3,    6
+    vsrai.w         vr29,   vr11,   6
+    vsrai.w         vr30,   vr4,    6
+    vsrai.w         vr31,   vr12,   6
+    vsrai.w         vr20,   vr5,    6
+    vsrai.w         vr21,   vr13,   6
+    vsrai.w         vr22,   vr6,    6
+    vsrai.w         vr23,   vr14,   6
+    vsrai.w         vr16,   vr7,    6
+    vsrai.w         vr17,   vr15,   6
+    vsrai.w         vr18,   vr0,    6
+    vsrai.w         vr19,   vr8,    6
+
+    fld.d           f1,     a0,    0
+    fld.d           f2,     a0,    FDEC_STRIDE
+    fld.d           f3,     a0,    FDEC_STRIDE * 2
+    fld.d           f4,     a0,    FDEC_STRIDE * 3
+
+    fld.d           f5,     a0,    FDEC_STRIDE * 4
+    fld.d           f6,     a0,    FDEC_STRIDE * 5
+    fld.d           f7,     a0,    FDEC_STRIDE * 6
+    fld.d           f8,     a0,    FDEC_STRIDE * 7
+
+    vsllwil.hu.bu   vr1,    vr1,    0
+    vexth.wu.hu     vr9,    vr1
+    vsllwil.wu.hu   vr1,    vr1,    0
+
+    vsllwil.hu.bu   vr2,    vr2,    0
+    vexth.wu.hu     vr10,   vr2
+    vsllwil.wu.hu   vr2,    vr2,    0
+
+    vsllwil.hu.bu   vr3,    vr3,    0
+    vexth.wu.hu     vr11,   vr3
+    vsllwil.wu.hu   vr3,    vr3,    0
+
+    vsllwil.hu.bu   vr4,    vr4,    0
+    vexth.wu.hu     vr12,   vr4
+    vsllwil.wu.hu   vr4,    vr4,    0
+
+    vsllwil.hu.bu   vr5,    vr5,    0
+    vexth.wu.hu     vr13,   vr5
+    vsllwil.wu.hu   vr5,    vr5,    0
+
+    vsllwil.hu.bu   vr6,    vr6,    0
+    vexth.wu.hu     vr14,   vr6
+    vsllwil.wu.hu   vr6,    vr6,    0
+
+    vsllwil.hu.bu   vr7,    vr7,    0
+    vexth.wu.hu     vr15,   vr7
+    vsllwil.wu.hu   vr7,    vr7,    0
+
+    vsllwil.hu.bu   vr8,    vr8,    0
+    vexth.wu.hu     vr0,   vr8
+    vsllwil.wu.hu   vr8,    vr8,    0
+
+    vadd.w          vr1,    vr1,    vr24
+    vadd.w          vr9,    vr9,    vr25
+    vadd.w          vr2,    vr2,    vr26
+    vadd.w          vr10,   vr10,   vr27
+    vadd.w          vr3,    vr3,    vr28
+    vadd.w          vr11,   vr11,   vr29
+    vadd.w          vr4,    vr4,    vr30
+    vadd.w          vr12,   vr12,   vr31
+    vadd.w          vr5,    vr5,    vr20
+    vadd.w          vr13,   vr13,   vr21
+    vadd.w          vr6,    vr6,    vr22
+    vadd.w          vr14,   vr14,   vr23
+    vadd.w          vr7,    vr7,    vr16
+    vadd.w          vr15,   vr15,   vr17
+    vadd.w          vr8,    vr8,    vr18
+    vadd.w          vr0,    vr0,    vr19
+
+    vssrarni.hu.w   vr2,    vr1,    0
+    vssrarni.hu.w   vr10,   vr9,    0
+    vssrarni.hu.w   vr4,    vr3,    0
+    vssrarni.hu.w   vr12,   vr11,   0
+    vssrarni.hu.w   vr6,    vr5,    0
+    vssrarni.hu.w   vr14,   vr13,   0
+    vssrarni.hu.w   vr8,    vr7,    0
+    vssrarni.hu.w   vr0,    vr15,   0
+
+    vpermi.w        vr20,   vr10,   0x0E
+    vpermi.w        vr10,   vr2,    0x44
+    vpermi.w        vr20,   vr2,    0x4E
+
+    vpermi.w        vr21,   vr12,   0x0E
+    vpermi.w        vr12,   vr4,    0x44
+    vpermi.w        vr21,   vr4,    0x4E
+
+    vpermi.w        vr22,   vr14,   0x0E
+    vpermi.w        vr14,   vr6,    0x44
+    vpermi.w        vr22,   vr6,    0x4E
+
+    vpermi.w        vr23,   vr0,    0x0E
+    vpermi.w        vr0,    vr8,    0x44
+    vpermi.w        vr23,   vr8,    0x4E
+
+    vssrlni.bu.h    vr12,   vr10,   0
+    vssrlni.bu.h    vr21,   vr20,   0
+    vssrlni.bu.h    vr0,    vr14,   0
+    vssrlni.bu.h    vr23,   vr22,   0
+
+    vstelm.d        vr12,   a0,     0,                  0
+    vstelm.d        vr21,   a0,     FDEC_STRIDE,        0
+    vstelm.d        vr12,   a0,     FDEC_STRIDE * 2,    1
+    vstelm.d        vr21,   a0,     FDEC_STRIDE * 3,    1
+
+    vstelm.d        vr0,    a0,     FDEC_STRIDE * 4,    0
+    vstelm.d        vr23,   a0,     FDEC_STRIDE * 5,    0
+    vstelm.d        vr0,    a0,     FDEC_STRIDE * 6,    1
+    vstelm.d        vr23,   a0,     FDEC_STRIDE * 7,    1
+
+    fld.d           f24,    sp,     0
+    fld.d           f25,    sp,     8
+    fld.d           f26,    sp,     16
+    fld.d           f27,    sp,     24
+    fld.d           f28,    sp,     32
+    fld.d           f29,    sp,     40
+    fld.d           f30,    sp,     48
+    fld.d           f31,    sp,     56
+    addi.d          sp,     sp,     64
+endfunc
+
 .macro add8x4_idct_dc_lasx
     xvldrepl.h      xr11,   a1,    0
     xvldrepl.h      xr12,   a1,    2
@@ -610,6 +1257,34 @@ endfunc
     xvstelm.d       xr2,    a0,    FDEC_STRIDE * 3,    3
 .endm
 
+.macro add8x4_idct_dc_lsx
+    vldrepl.h       vr11,   a1,     0
+    vldrepl.h       vr12,   a1,     2
+    vilvl.d         vr12,   vr12,   vr11
+    vsrari.h        vr12,   vr12,   6
+
+    fld.d           f0,     a0,    0
+    fld.d           f1,     a0,    FDEC_STRIDE
+    fld.d           f2,     a0,    FDEC_STRIDE * 2
+    fld.d           f3,     a0,    FDEC_STRIDE * 3
+
+    vsllwil.hu.bu   vr0,    vr0,    0
+    vsllwil.hu.bu   vr1,    vr1,    0
+    vsllwil.hu.bu   vr2,    vr2,    0
+    vsllwil.hu.bu   vr3,    vr3,    0
+
+    vadd.h          vr0,    vr0,    vr12
+    vadd.h          vr1,    vr1,    vr12
+    vadd.h          vr2,    vr2,    vr12
+    vadd.h          vr3,    vr3,    vr12
+    vssrarni.bu.h   vr2,    vr0,    0
+    vssrarni.bu.h   vr3,    vr1,    0
+
+    vstelm.d        vr2,    a0,     0,                  0
+    vstelm.d        vr3,    a0,     FDEC_STRIDE,        0
+    vstelm.d        vr2,    a0,     FDEC_STRIDE * 2,    1
+    vstelm.d        vr3,    a0,     FDEC_STRIDE * 3,    1
+.endm
 /*
  * void add8x8_idct_dc( pixel *p_dst, dctcoef dct[4] )
  */
@@ -621,6 +1296,14 @@ function add8x8_idct_dc_lasx
     add8x4_idct_dc_lasx
 endfunc
 
+function add8x8_idct_dc_lsx
+    add8x4_idct_dc_lsx
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    4
+    add8x4_idct_dc_lsx
+endfunc
+
 .macro add_16x16_idct_dc_core_lasx a0, a1
     vldrepl.h       vr11,   \a1,   0
     vldrepl.h       vr12,   \a1,   2
@@ -676,6 +1359,74 @@ function add16x16_idct_dc_lasx
     add_16x16_idct_dc_core_lasx a0, a1
 endfunc
 
+.macro add_16x16_idct_dc_core_lsx a0, a1
+    vldrepl.h       vr11,   \a1,   0
+    vldrepl.h       vr12,   \a1,   2
+    vldrepl.h       vr13,   \a1,   4
+    vldrepl.h       vr14,   \a1,   6
+
+    vpermi.w        vr12,   vr11,   0x44
+    vpermi.w        vr14,   vr13,   0x44
+    vsrari.h        vr12,   vr12,   6
+    vsrari.h        vr14,   vr14,   6
+
+    vld             vr0,    \a0,   0
+    vld             vr1,    \a0,   FDEC_STRIDE
+    vld             vr2,    \a0,   FDEC_STRIDE * 2
+    vld             vr3,    \a0,   FDEC_STRIDE * 3
+
+    vexth.hu.bu     vr5,    vr0
+    vsllwil.hu.bu   vr0,    vr0,    0
+    vexth.hu.bu     vr6,    vr1
+    vsllwil.hu.bu   vr1,    vr1,    0
+    vexth.hu.bu     vr7,    vr2
+    vsllwil.hu.bu   vr2,    vr2,    0
+    vexth.hu.bu     vr8,    vr3
+    vsllwil.hu.bu   vr3,    vr3,    0
+
+    vadd.h          vr0,    vr0,    vr12
+    vadd.h          vr5,    vr5,    vr14
+    vadd.h          vr1,    vr1,    vr12
+    vadd.h          vr6,    vr6,    vr14
+    vadd.h          vr2,    vr2,    vr12
+    vadd.h          vr7,    vr7,    vr14
+    vadd.h          vr3,    vr3,    vr12
+    vadd.h          vr8,    vr8,    vr14
+
+    vssrarni.bu.h   vr1,    vr0,    0
+    vssrarni.bu.h   vr6,    vr5,    0
+    vssrarni.bu.h   vr3,    vr2,    0
+    vssrarni.bu.h   vr8,    vr7,    0
+
+    vpermi.w        vr9,    vr6,    0x0E
+    vpermi.w        vr6,    vr1,    0x44
+    vpermi.w        vr9,    vr1,    0x4E
+    vpermi.w        vr10,   vr8,    0x0E
+    vpermi.w        vr8,    vr3,    0x44
+    vpermi.w        vr10,   vr3,    0x4E
+
+    vst             vr6,    \a0,    0
+    vst             vr9,    \a0,    FDEC_STRIDE
+    vst             vr8,    \a0,    FDEC_STRIDE * 2
+    vst             vr10,   \a0,    FDEC_STRIDE * 3
+.endm
+
+function add16x16_idct_dc_lsx
+    add_16x16_idct_dc_core_lsx a0, a1
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    8
+    add_16x16_idct_dc_core_lsx a0, a1
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    8
+    add_16x16_idct_dc_core_lsx a0, a1
+
+    addi.d          a0,     a0,    FDEC_STRIDE * 4
+    addi.d          a1,     a1,    8
+    add_16x16_idct_dc_core_lsx a0, a1
+endfunc
+
 /*
  * void idct4x4dc( dctcoef d[16] )
  */
@@ -713,6 +1464,78 @@ function idct4x4dc_lasx
     xvst            xr2,    a0,    0
 endfunc
 
+function idct4x4dc_lsx
+    vld             vr0,    a0,     0
+    vld             vr20,   a0,     16
+
+    vshuf4i.b       vr1,    vr0,    0x4E
+    vshuf4i.b       vr11,   vr20,   0x4E
+    vhaddw.w.h      vr2,    vr0,    vr0
+    vhaddw.w.h      vr12,   vr20,   vr20
+    vhsubw.w.h      vr3,    vr1,    vr1
+    vhsubw.w.h      vr13,   vr11,   vr11
+    vshuf4i.h       vr2,    vr2,    0x4E
+    vshuf4i.h       vr12,   vr12,   0x4E
+    vshuf4i.h       vr3,    vr3,    0x4E
+    vshuf4i.h       vr13,   vr13,   0x4E
+
+    vhaddw.d.w      vr4,    vr2,    vr2
+    vhaddw.d.w      vr14,   vr12,   vr12
+    vhsubw.d.w      vr5,    vr2,    vr2
+    vhsubw.d.w      vr15,   vr12,   vr12
+    vhsubw.d.w      vr6,    vr3,    vr3
+    vhsubw.d.w      vr16,   vr13,   vr13
+    vhaddw.d.w      vr7,    vr3,    vr3
+    vhaddw.d.w      vr17,   vr13,   vr13
+
+    vpickev.w       vr8,    vr5,    vr4
+    vpickev.w       vr18,   vr15,   vr14
+    vpickev.w       vr9,    vr7,    vr6
+    vpickev.w       vr19,   vr17,   vr16
+    vpickev.h       vr10,   vr9,    vr8
+    vpickev.h       vr21,   vr19,   vr18
+
+    vpermi.w        vr22,   vr21,   0x0E
+    vpermi.w        vr21,   vr10,   0x44
+    vpermi.w        vr22,   vr10,   0x4E
+    vpermi.w        vr21,   vr21,   0xD8
+    vpermi.w        vr22,   vr22,   0xD8
+
+    vshuf4i.b       vr11,   vr21,   0x4E
+    vshuf4i.b       vr12,   vr22,   0x4E
+    vhaddw.w.h      vr21,   vr21,   vr21
+    vhaddw.w.h      vr22,   vr22,   vr22
+    vhsubw.w.h      vr11,   vr11,   vr11
+    vhsubw.w.h      vr12,   vr12,   vr12
+    vshuf4i.h       vr21,   vr21,   0x4E
+    vshuf4i.h       vr22,   vr22,   0x4E
+    vshuf4i.h       vr11,   vr11,   0x4E
+    vshuf4i.h       vr12,   vr12,   0x4E
+
+    vhaddw.d.w      vr13,   vr21,   vr21
+    vhaddw.d.w      vr14,   vr22,   vr22
+    vhsubw.d.w      vr15,   vr21,   vr21
+    vhsubw.d.w      vr16,   vr22,   vr22
+    vhsubw.d.w      vr17,   vr11,   vr11
+    vhsubw.d.w      vr18,   vr12,   vr12
+    vhaddw.d.w      vr19,   vr11,   vr11
+    vhaddw.d.w      vr20,   vr12,   vr12
+
+    vpackev.w       vr7,    vr15,   vr13
+    vpackev.w       vr8,    vr16,   vr14
+    vpackev.w       vr9,    vr19,   vr17
+    vpackev.w       vr10,   vr20,   vr18
+    vilvl.d         vr0,    vr9,    vr7
+    vilvl.d         vr4,    vr10,   vr8
+    vilvh.d         vr1,    vr9,    vr7
+    vilvh.d         vr5,    vr10,   vr8
+
+    vpickev.h       vr2,    vr1,    vr0
+    vpickev.h       vr3,    vr5,    vr4
+    vst             vr2,    a0,     0
+    vst             vr3,    a0,     16
+endfunc
+
 /*
  * void dct4x4dc( dctcoef d[16] )
  */
@@ -752,6 +1575,83 @@ function dct4x4dc_lasx
     xvst            xr2,    a0,    0
 endfunc
 
+function dct4x4dc_lsx
+    vld             vr0,    a0,     0
+    vld             vr20,   a0,     16
+
+    vshuf4i.b       vr1,    vr0,    0x4E
+    vshuf4i.b       vr11,   vr20,   0x4E
+    vhaddw.w.h      vr2,    vr0,    vr0
+    vhaddw.w.h      vr12,   vr20,   vr20
+    vhsubw.w.h      vr3,    vr1,    vr1
+    vhsubw.w.h      vr13,   vr11,   vr11
+    vshuf4i.h       vr2,    vr2,    0x4E
+    vshuf4i.h       vr12,   vr12,   0x4E
+    vshuf4i.h       vr3,    vr3,    0x4E
+    vshuf4i.h       vr13,   vr13,   0x4E
+
+    vhaddw.d.w      vr4,    vr2,    vr2
+    vhaddw.d.w      vr14,   vr12,   vr12
+    vhsubw.d.w      vr5,    vr2,    vr2
+    vhsubw.d.w      vr15,   vr12,   vr12
+    vhsubw.d.w      vr6,    vr3,    vr3
+    vhsubw.d.w      vr16,   vr13,   vr13
+    vhaddw.d.w      vr7,    vr3,    vr3
+    vhaddw.d.w      vr17,   vr13,   vr13
+
+    vpickev.w       vr8,    vr5,    vr4
+    vpickev.w       vr18,   vr15,   vr14
+    vpickev.w       vr9,    vr7,    vr6
+    vpickev.w       vr19,   vr17,   vr16
+    vpickev.h       vr10,   vr9,    vr8
+    vpickev.h       vr21,   vr19,   vr18
+
+    vpermi.w        vr22,   vr21,   0x0E
+    vpermi.w        vr21,   vr10,   0x44
+    vpermi.w        vr22,   vr10,   0x4E
+    vpermi.w        vr21,   vr21,   0xD8
+    vpermi.w        vr22,   vr22,   0xD8
+
+    vshuf4i.b       vr11,   vr21,   0x4E
+    vshuf4i.b       vr12,   vr22,   0x4E
+    vhaddw.w.h      vr21,   vr21,   vr21
+    vhaddw.w.h      vr22,   vr22,   vr22
+    vhsubw.w.h      vr11,   vr11,   vr11
+    vhsubw.w.h      vr12,   vr12,   vr12
+    vshuf4i.h       vr21,   vr21,   0x4E
+    vshuf4i.h       vr22,   vr22,   0x4E
+    vshuf4i.h       vr11,   vr11,   0x4E
+    vshuf4i.h       vr12,   vr12,   0x4E
+
+    vhaddw.d.w      vr13,   vr21,   vr21
+    vhaddw.d.w      vr14,   vr22,   vr22
+    vhsubw.d.w      vr15,   vr21,   vr21
+    vhsubw.d.w      vr16,   vr22,   vr22
+    vhsubw.d.w      vr17,   vr11,   vr11
+    vhsubw.d.w      vr18,   vr12,   vr12
+    vhaddw.d.w      vr19,   vr11,   vr11
+    vhaddw.d.w      vr20,   vr12,   vr12
+
+    vpackev.w       vr7,    vr15,   vr13
+    vpackev.w       vr8,    vr16,   vr14
+    vpackev.w       vr9,    vr19,   vr17
+    vpackev.w       vr10,   vr20,   vr18
+
+    vsrari.w        vr7,    vr7,    1
+    vsrari.w        vr8,    vr8,    1
+    vsrari.w        vr9,    vr9,    1
+    vsrari.w        vr10,   vr10,   1
+
+    vilvl.d         vr0,    vr9,    vr7
+    vilvl.d         vr4,    vr10,   vr8
+    vilvh.d         vr1,    vr9,    vr7
+    vilvh.d         vr10,   vr10,   vr8
+    vpickev.h       vr2,    vr1,    vr0
+    vpickev.h       vr3,    vr10,   vr4
+    vst             vr2,    a0,     0
+    vst             vr3,    a0,     16
+endfunc
+
 .macro LSX_LOAD_PIX_2 data1, data2
     vld             vr0,    a1,    0
     vld             vr1,    a1,    FENC_STRIDE
@@ -819,7 +1719,7 @@ endfunc
  * void sub8x8_dct8( dctcoef dct[64], pixel *pix1, pixel *pix2 )
  */
 function sub8x8_dct8_lsx
-    vor.v           vr8,    vr0,   vr0
+    vxor.v           vr8,    vr0,   vr0
 
     // vr12 ... vr19
     LSX_LOAD_PIX_2  vr12, vr13
@@ -967,6 +1867,127 @@ function sub16x16_dct8_lasx
     SUB16x8_DCT8_LASX
 endfunc
 
+
+.macro LSX_LOAD_PIX_22 data1, data2, data3, data4
+    vld             vr0,    a1,     0
+    vld             vr4,    a1,     16
+    vld             vr1,    a1,     FENC_STRIDE
+    vld             vr5,    a1,     FENC_STRIDE + 16
+    vld             vr2,    a2,     0
+    vld             vr6,    a2,     16
+    vld             vr3,    a2,     FDEC_STRIDE
+    vld             vr7,    a2,     FDEC_STRIDE + 16
+
+    vpermi.w        vr8,    vr0,    0x0E
+    vpermi.w        vr0,    vr0,    0x44
+    vpermi.w        vr8,    vr8,    0x44
+    vpermi.w        vr9,    vr1,    0x0E
+    vpermi.w        vr1,    vr1,    0x44
+    vpermi.w        vr9,    vr9,    0x44
+    vpermi.w        vr10,   vr2,    0x0E
+    vpermi.w        vr2,    vr2,    0x44
+    vpermi.w        vr10,   vr10,   0x44
+    vpermi.w        vr11,   vr3,    0x0E
+    vpermi.w        vr3,    vr3,    0x44
+    vpermi.w        vr11,   vr11,   0x44
+
+    vxor.v          vr30,   vr0,    vr0
+    vxor.v          vr31,   vr8,    vr8
+
+    vilvl.b         vr0,    vr30,   vr0
+    vilvl.b         vr8,    vr31,   vr8
+    vilvl.b         vr1,    vr30,   vr1
+    vilvl.b         vr9,    vr31,   vr9
+    vilvl.b         vr2,    vr30,   vr2
+    vilvl.b         vr10,   vr31,   vr10
+    vilvl.b         vr3,    vr30,   vr3
+    vilvl.b         vr11,   vr31,   vr11
+
+    vsub.h          \data1, vr0,    vr2
+    vsub.h          \data3, vr8,    vr10
+    vsub.h          \data2, vr1,    vr3
+    vsub.h          \data4, vr9,    vr11
+    addi.d          a1,     a1,     FENC_STRIDE * 2
+    addi.d          a2,     a2,     FDEC_STRIDE * 2
+.endm
+
+.macro SUB16x8_DCT8_LSX
+    LSX_LOAD_PIX_22 vr12,    vr13,   vr22,   vr23
+    LSX_LOAD_PIX_22 vr14,    vr15,   vr24,   vr25
+    LSX_LOAD_PIX_22 vr16,    vr17,   vr26,   vr27
+    LSX_LOAD_PIX_22 vr18,    vr19,   vr28,   vr29
+
+    LSX_DCT8_1D
+    LSX_TRANSPOSE8x8_H vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr19, \
+                        vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr19, \
+                        vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    LSX_DCT8_1D
+
+    vst             vr12,   a0,     0
+    vst             vr13,   a0,     16
+    vst             vr14,   a0,     32
+    vst             vr15,   a0,     48
+    vst             vr16,   a0,     64
+    vst             vr17,   a0,     80
+    vst             vr18,   a0,     96
+    vst             vr19,   a0,     112
+
+    vmov            vr12,   vr22
+    vmov            vr13,   vr23
+    vmov            vr14,   vr24
+    vmov            vr15,   vr25
+    vmov            vr16,   vr26
+    vmov            vr17,   vr27
+    vmov            vr18,   vr28
+    vmov            vr19,   vr29
+
+    LSX_DCT8_1D
+    LSX_TRANSPOSE8x8_H vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr19, \
+                        vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr19, \
+                        vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    LSX_DCT8_1D
+
+    vst             vr12,   a0,     128
+    vst             vr13,   a0,     144
+    vst             vr14,   a0,     160
+    vst             vr15,   a0,     176
+    vst             vr16,   a0,     192
+    vst             vr17,   a0,     208
+    vst             vr18,   a0,     224
+    vst             vr19,   a0,     240
+.endm
+
+function sub16x16_dct8_lsx
+    addi.d          sp,     sp,     -64
+    fst.d           f24,    sp,     0
+    fst.d           f25,    sp,     8
+    fst.d           f26,    sp,     16
+    fst.d           f27,    sp,     24
+    fst.d           f28,    sp,     32
+    fst.d           f29,    sp,     40
+    fst.d           f30,    sp,     48
+    fst.d           f31,    sp,     56
+
+    move            t1,     a1
+    move            t3,     a2
+    SUB16x8_DCT8_LSX
+
+    addi.d          a0,     a0,    256
+    addi.d          a1,     t1,    FENC_STRIDE * 8
+    addi.d          a2,     t3,    FDEC_STRIDE * 8
+    SUB16x8_DCT8_LSX
+
+    fld.d           f24,    sp,     0
+    fld.d           f25,    sp,     8
+    fld.d           f26,    sp,     16
+    fld.d           f27,    sp,     24
+    fld.d           f28,    sp,     32
+    fld.d           f29,    sp,     40
+    fld.d           f30,    sp,     48
+    fld.d           f31,    sp,     56
+    addi.d          sp,     sp,     64
+endfunc
+
 /*
  * void zigzag_scan_4x4_frame( dctcoef level[16], dctcoef dct[16] )
  */
@@ -980,3 +2001,17 @@ function zigzag_scan_4x4_frame_lasx
     xvshuf.h        xr3,    xr2,   xr1
     xvst            xr3,    a0,    0
 endfunc
+
+function zigzag_scan_4x4_frame_lsx
+    vld             vr1,    a1,     0
+    vld             vr2,    a1,     16
+    vor.v           vr3,    vr1,    vr1
+    vor.v           vr4,    vr2,    vr2
+    la.local        t0,     zigzag_scan4
+    vld             vr5,    t0,     0
+    vld             vr6,    t0,     16
+    vshuf.h         vr5,    vr4,    vr1
+    vshuf.h         vr6,    vr4,    vr1
+    vst             vr5,    a0,     0
+    vst             vr6,    a0,     16
+endfunc
diff --git a/common/loongarch/dct.h b/common/loongarch/dct.h
index b91cfeaa..b974f774 100644
--- a/common/loongarch/dct.h
+++ b/common/loongarch/dct.h
@@ -27,18 +27,13 @@
 #ifndef X264_LOONGARCH_DCT_H
 #define X264_LOONGARCH_DCT_H
 
-#define x264_sub4x4_dct_lsx x264_template(sub4x4_dct_lsx)
-void x264_sub4x4_dct_lsx( int16_t p_dst[16], uint8_t *p_src, uint8_t *p_ref );
 #define x264_sub8x8_dct_lasx x264_template(sub8x8_dct_lasx)
-void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src,
-                           uint8_t *p_ref );
+void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src, uint8_t *p_ref );
 #define x264_sub16x16_dct_lasx x264_template(sub16x16_dct_lasx)
-void x264_sub16x16_dct_lasx( int16_t p_dst[16][16], uint8_t *p_src,
-                             uint8_t *p_ref );
+void x264_sub16x16_dct_lasx( int16_t p_dst[16][16], uint8_t *p_src, uint8_t *p_ref );
 
 #define x264_sub8x8_dct8_lsx x264_template(sub8x8_dct8_lsx)
-void x264_sub8x8_dct8_lsx( int16_t pi_dct[64], uint8_t *p_pix1,
-                            uint8_t *p_pix2 );
+void x264_sub8x8_dct8_lsx( int16_t pi_dct[64], uint8_t *p_pix1, uint8_t *p_pix2 );
 #define x264_sub16x16_dct8_lasx x264_template(sub16x16_dct8_lasx)
 void x264_sub16x16_dct8_lasx( int16_t pi_dct[4][64], uint8_t *p_pix1,
                               uint8_t *p_pix2 );
@@ -64,4 +59,38 @@ void x264_dct4x4dc_lasx( int16_t d[16] );
 #define x264_zigzag_scan_4x4_frame_lasx x264_template(zigzag_scan_4x4_frame_lasx)
 void x264_zigzag_scan_4x4_frame_lasx( int16_t level[16], int16_t dct[16] );
 
+#define x264_sub4x4_dct_lsx x264_template(sub4x4_dct_lsx)
+void x264_sub4x4_dct_lsx( int16_t p_dst[16], uint8_t *p_src, uint8_t *p_ref );
+#define x264_sub8x8_dct_lsx x264_template(sub8x8_dct_lsx)
+void x264_sub8x8_dct_lsx( int16_t p_dst[4][16], uint8_t *p_src, uint8_t *p_ref );
+#define x264_sub16x16_dct_lsx x264_template(sub16x16_dct_lsx)
+void x264_sub16x16_dct_lsx( int16_t p_dst[16][16], uint8_t *p_src, uint8_t *p_ref );
+
+#define x264_sub8x8_dct8_lsx x264_template(sub8x8_dct8_lsx)
+void x264_sub8x8_dct8_lsx( int16_t pi_dct[64], uint8_t *p_pix1, uint8_t *p_pix2 );
+#define x264_sub16x16_dct8_lsx x264_template(sub16x16_dct8_lsx)
+void x264_sub16x16_dct8_lsx( int16_t pi_dct[4][64], uint8_t *p_pix1,
+                              uint8_t *p_pix2 );
+
+#define x264_add4x4_idct_lsx x264_template(add4x4_idct_lsx)
+void x264_add4x4_idct_lsx( uint8_t *p_dst, int16_t pi_dct[16] );
+#define x264_add8x8_idct_lsx x264_template(add8x8_idct_lsx)
+void x264_add8x8_idct_lsx( uint8_t *p_dst, int16_t pi_dct[4][16] );
+#define x264_add16x16_idct_lsx x264_template(add16x16_idct_lsx)
+void x264_add16x16_idct_lsx( uint8_t *p_dst, int16_t pi_dct[16][16] );
+#define x264_add8x8_idct8_lsx x264_template(add8x8_idct8_lsx)
+void x264_add8x8_idct8_lsx( uint8_t *p_dst, int16_t pi_dct[64] );
+#define x264_add8x8_idct_dc_lsx x264_template(add8x8_idct_dc_lsx)
+void x264_add8x8_idct_dc_lsx( uint8_t *p_dst, int16_t dct[4] );
+#define x264_add16x16_idct_dc_lsx x264_template(add16x16_idct_dc_lsx)
+void x264_add16x16_idct_dc_lsx( uint8_t *p_dst, int16_t dct[16] );
+
+#define x264_idct4x4dc_lsx x264_template(idct4x4dc_lsx)
+void x264_idct4x4dc_lsx( int16_t d[16] );
+#define x264_dct4x4dc_lsx x264_template(dct4x4dc_lsx)
+void x264_dct4x4dc_lsx( int16_t d[16] );
+
+#define x264_zigzag_scan_4x4_frame_lsx x264_template(zigzag_scan_4x4_frame_lsx)
+void x264_zigzag_scan_4x4_frame_lsx( int16_t level[16], int16_t dct[16] );
+
 #endif
diff --git a/common/loongarch/deblock-a.S b/common/loongarch/deblock-a.S
index c7f9e85a..dd78cbed 100644
--- a/common/loongarch/deblock-a.S
+++ b/common/loongarch/deblock-a.S
@@ -24,7 +24,7 @@
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
 
-#include "asm.S"
+#include "loongson_asm.S"
 
 #if !HIGH_BIT_DEPTH
 
@@ -670,12 +670,9 @@ endfunc
  *                          int16_t mv[2][X264_SCAN8_LUMA_SIZE][2], uint8_t bs[2][8][4],
  *                          int mvy_limit, int bframe )
  */
-const shuf_loc
+const shuf_loc_locn
 .byte 1, 9, 17, 25, 2, 10, 18, 26, 3, 11, 19, 27, 4, 12, 20, 28
-endconst
-
-const shuf_locn
-.byte 0, 8, 16, 24, 1, 9, 17, 25, 2, 10, 18, 26, 3, 11, 19, 27
+.byte 16, 24, 0, 8, 17, 25, 1, 9, 18, 26, 2, 10, 19, 27, 3, 11
 endconst
 
 function deblock_strength_lasx
@@ -686,24 +683,22 @@ function deblock_strength_lasx
     xvreplgr2vr.h   xr20,  t0
     xvreplgr2vr.h   xr21,  a4
 
-    vld             vr0,   a0,    11
-    vld             vr1,   a0,    27
-    la.local        t0,    shuf_loc
-    la.local        t1,    shuf_locn
-    vld             vr2,   t0,    0
-    vld             vr3,   t1,    0
-    vshuf.b         vr4,   vr1,   vr0,   vr2
-    vshuf.b         vr5,   vr1,   vr0,   vr3
+    xvld            xr0,   a0,    11
+    xvpermi.q       xr1,   xr0,   0x01
+    la.local         t0,    shuf_loc_locn
+    xvld            xr23,   t0,    0
+    xvshuf.b        xr4,   xr1,   xr0,    xr23
+    xvpermi.q       xr5,   xr4,   0x01
     vor.v           vr6,   vr4,   vr5
     vseqi.b         vr6,   vr6,   0
     vmov            vr15,  vr6
     vxor.v          vr8,   vr8,   vr8
     vbitsel.v       vr8,   vr18,  vr8,   vr6
 
-    vld             vr0,   a1,    11
-    vld             vr1,   a1,    27
-    vshuf.b         vr4,   vr1,   vr0,   vr2
-    vshuf.b         vr5,   vr1,   vr0,   vr3
+    xvld            xr0,   a1,    11
+    xvpermi.q       xr1,   xr0,   0x01
+    xvshuf.b        xr4,   xr1,   xr0,   xr23
+    xvpermi.q       xr5,   xr4,   0x01
     vseq.b          vr4,   vr4,   vr5
     vseqi.b         vr4,   vr4,   0
 
@@ -711,12 +706,10 @@ function deblock_strength_lasx
     vld             vr1,   a2,    76
     vld             vr5,   a2,    108
     vld             vr6,   a2,    140
-
     vilvl.h         vr9,   vr1,   vr0
     vilvl.h         vr10,  vr6,   vr5
     vilvl.w         vr11,  vr10,  vr9
     vilvh.w         vr12,  vr10,  vr9
-
     vilvh.h         vr9,   vr1,   vr0
     vilvh.h         vr10,  vr6,   vr5
     vilvl.w         vr13,  vr10,  vr9
@@ -755,7 +748,6 @@ function deblock_strength_lasx
     xvpermi.q       xr6,   xr1,   0x20  // mv[0][locn][1]
     xvabsd.h        xr6,   xr0,   xr6
     xvsle.h         xr6,   xr21,  xr6
-
     xvor.v          xr5,   xr5,   xr6
     xvpickev.b      xr5,   xr5,   xr5
     xvpermi.d       xr5,   xr5,   0xd8
@@ -763,10 +755,10 @@ function deblock_strength_lasx
 
     beqz            a5,    .bframe_iszero_0
     // bframe != 0
-    vld             vr0,   a1,    51
-    vld             vr1,   a1,    67
-    vshuf.b         vr4,   vr1,   vr0,   vr2   // ref[1][loc]
-    vshuf.b         vr5,   vr1,   vr0,   vr3   // ref[1][locn]
+    xvld            xr0,   a1,    51
+    xvpermi.q       xr1,   xr0,   0x01
+    xvshuf.b        xr4,   xr1,   xr0,   xr23
+    xvpermi.q       xr5,   xr4,   0x01
     vseq.b          vr4,   vr4,   vr5
     vseqi.b         vr4,   vr4,   0
 
@@ -774,7 +766,6 @@ function deblock_strength_lasx
     vld             vr1,   a2,    236
     vld             vr5,   a2,    268
     vld             vr6,   a2,    300
-
     vilvl.h         vr9,   vr1,   vr0
     vilvl.h         vr10,  vr6,   vr5
     vilvl.w         vr11,  vr10,  vr9
@@ -817,7 +808,6 @@ function deblock_strength_lasx
     xvpermi.q       xr6,   xr1,   0x20  // mv[1][locn][1]
     xvabsd.h        xr6,   xr0,   xr6
     xvsle.h         xr6,   xr21,  xr6
-
     xvor.v          xr5,   xr5,   xr6
     xvpickev.b      xr5,   xr5,   xr5
     xvpermi.d       xr5,   xr5,   0xd8
@@ -828,10 +818,7 @@ function deblock_strength_lasx
     vxor.v          vr22,  vr22,  vr22
     vbitsel.v       vr22,  vr22,  vr19,  vr17
     vbitsel.v       vr22,  vr8,   vr22,  vr15
-    vstelm.w        vr22,  a3,    0,     0
-    vstelm.w        vr22,  a3,    4,     1
-    vstelm.w        vr22,  a3,    8,     2
-    vstelm.w        vr22,  a3,    12,    3
+    vst             vr22,  a3,    0
 
     // dir = 1 s1 = 1 s2 = 8
     vld             vr0,   a0,    4
@@ -927,11 +914,709 @@ function deblock_strength_lasx
     vxor.v          vr22,  vr22,  vr22
     vbitsel.v       vr22,  vr22,  vr19,  vr2
     vbitsel.v       vr22,  vr3,   vr22,  vr15
+    vst             vr22,  a3,    32
+endfunc
+
+/*
+ * void deblock_strength_c( uint8_t nnz[X264_SCAN8_SIZE], int8_t ref[2][X264_SCAN8_LUMA_SIZE],
+ *                          int16_t mv[2][X264_SCAN8_LUMA_SIZE][2], uint8_t bs[2][8][4],
+ *                          int mvy_limit, int bframe )
+ */
+const shuf_loc
+.byte 1, 9, 17, 25, 2, 10, 18, 26, 3, 11, 19, 27, 4, 12, 20, 28
+endconst
 
-    vstelm.w        vr22,  a3,    32,    0
-    vstelm.w        vr22,  a3,    36,    1
-    vstelm.w        vr22,  a3,    40,    2
-    vstelm.w        vr22,  a3,    44,    3
+const shuf_locn
+.byte 0, 8, 16, 24, 1, 9, 17, 25, 2, 10, 18, 26, 3, 11, 19, 27
+endconst
+
+function deblock_strength_lsx
+    // dir = 0 s1 = 8 s2 = 1
+    vldi            vr18,  2
+    vldi            vr19,  1
+    addi.d          t0,    zero,  4
+    vreplgr2vr.h    vr20,  t0
+    vreplgr2vr.h    vr21,  a4
+
+    vld             vr0,   a0,    11
+    vld             vr1,   a0,    27
+    la.local        t0,    shuf_loc
+    la.local        t1,    shuf_locn
+    vld             vr2,   t0,    0
+    vld             vr3,   t1,    0
+    vshuf.b         vr4,   vr1,   vr0,   vr2
+    vshuf.b         vr5,   vr1,   vr0,   vr3
+    vor.v           vr6,   vr4,   vr5
+    vseqi.b         vr6,   vr6,   0
+    vmov            vr15,  vr6
+    vxor.v          vr8,   vr8,   vr8
+    vbitsel.v       vr8,   vr18,  vr8,   vr6
+
+    vld             vr0,   a1,    11
+    vld             vr1,   a1,    27
+    vshuf.b         vr4,   vr1,   vr0,   vr2
+    vshuf.b         vr5,   vr1,   vr0,   vr3
+    vseq.b          vr4,   vr4,   vr5
+    vseqi.b         vr4,   vr4,   0
+
+    vld             vr0,   a2,    44
+    vld             vr1,   a2,    76
+    vld             vr5,   a2,    108
+    vld             vr6,   a2,    140
+    vilvl.h         vr9,   vr1,   vr0
+    vilvl.h         vr10,  vr6,   vr5
+    vilvl.w         vr11,  vr10,  vr9
+    vilvh.w         vr12,  vr10,  vr9
+    vilvh.h         vr9,   vr1,   vr0
+    vilvh.h         vr10,  vr6,   vr5
+    vilvl.w         vr13,  vr10,  vr9
+    vilvh.w         vr14,  vr10,  vr9
+
+    vilvl.d         vr0,   vr13,  vr12
+    ld.h            t0,    a2,    60
+    ld.h            t1,    a2,    92
+    ld.h            t2,    a2,    124
+    ld.h            t3,    a2,    156
+    vmov            vr6,   vr14
+    vinsgr2vr.h     vr6,   t0,    4
+    vinsgr2vr.h     vr6,   t1,    5
+    vinsgr2vr.h     vr6,   t2,    6
+    vinsgr2vr.h     vr6,   t3,    7
+    vilvl.d         vr1,   vr12,  vr11
+    vilvl.d         vr5,   vr14,  vr13
+    vabsd.h         vr9,   vr0,   vr1
+    vabsd.h         vr5,   vr6,   vr5
+    vsle.h          vr9,   vr20,  vr9
+    vsle.h          vr5,   vr20,  vr5
+
+    vilvh.d         vr0,   vr13,  vr12
+    ld.h            t0,    a2,    62
+    ld.h            t1,    a2,    94
+    ld.h            t2,    a2,    126
+    ld.h            t3,    a2,    158
+    vbsrl.v         vr7,   vr14,  8
+    vinsgr2vr.h     vr7,   t0,    4
+    vinsgr2vr.h     vr7,   t1,    5
+    vinsgr2vr.h     vr7,   t2,    6
+    vinsgr2vr.h     vr7,   t3,    7
+    vilvh.d         vr1,   vr12,  vr11
+    vilvh.d         vr6,   vr14,  vr13
+    vabsd.h         vr0,   vr0,   vr1
+    vabsd.h         vr6,   vr7,   vr6
+    vsle.h          vr0,   vr21,  vr0
+    vsle.h          vr6,   vr21,  vr6
+
+    vor.v           vr9,   vr9,   vr0
+    vor.v           vr5,   vr5,   vr6
+    vpickev.b       vr5,   vr5,   vr9
+    vor.v           vr17,  vr4,   vr5
+
+    beqz            a5,    .bframeiszero_0_lsx
+    // bframe != 0
+    vld             vr0,   a1,    51
+    vld             vr1,   a1,    67
+    vshuf.b         vr4,   vr1,   vr0,   vr2
+    vshuf.b         vr5,   vr1,   vr0,   vr3
+    vseq.b          vr4,   vr4,   vr5
+    vseqi.b         vr4,   vr4,   0
+
+    vld             vr0,   a2,    204
+    vld             vr1,   a2,    236
+    vld             vr5,   a2,    268
+    vld             vr6,   a2,    300
+    vilvl.h         vr9,   vr1,   vr0
+    vilvl.h         vr10,  vr6,   vr5
+    vilvl.w         vr11,  vr10,  vr9
+    vilvh.w         vr12,  vr10,  vr9
+    vilvh.h         vr9,   vr1,   vr0
+    vilvh.h         vr10,  vr6,   vr5
+    vilvl.w         vr13,  vr10,  vr9
+    vilvh.w         vr14,  vr10,  vr9
+
+    vilvl.d         vr0,   vr13,  vr12
+    ld.h            t0,    a2,    220
+    ld.h            t1,    a2,    252
+    ld.h            t2,    a2,    284
+    ld.h            t3,    a2,    316
+    vmov            vr6,   vr14
+    vinsgr2vr.h     vr6,   t0,    4
+    vinsgr2vr.h     vr6,   t1,    5
+    vinsgr2vr.h     vr6,   t2,    6
+    vinsgr2vr.h     vr6,   t3,    7
+    vilvl.d         vr1,   vr12,  vr11
+    vilvl.d         vr5,   vr14,  vr13
+    vabsd.h         vr9,   vr0,   vr1
+    vabsd.h         vr5,   vr6,   vr5
+    vsle.h          vr9,   vr20,  vr9
+    vsle.h          vr5,   vr20,  vr5
+
+    vilvh.d         vr0,   vr13,  vr12
+    ld.h            t0,    a2,    222
+    ld.h            t1,    a2,    254
+    ld.h            t2,    a2,    286
+    ld.h            t3,    a2,    318
+    vbsrl.v         vr7,   vr14,  8
+    vinsgr2vr.h     vr7,   t0,    4
+    vinsgr2vr.h     vr7,   t1,    5
+    vinsgr2vr.h     vr7,   t2,    6
+    vinsgr2vr.h     vr7,   t3,    7
+    vilvh.d         vr1,   vr12,  vr11
+    vilvh.d         vr6,   vr14,  vr13
+    vabsd.h         vr0,   vr0,   vr1
+    vabsd.h         vr6,   vr7,   vr6
+    vsle.h          vr0,   vr21,  vr0
+    vsle.h          vr6,   vr21,  vr6
+
+    vor.v           vr9,   vr9,   vr0
+    vor.v           vr5,   vr5,   vr6
+    vpickev.b       vr5,   vr5,   vr9
+    vor.v           vr5,   vr5,   vr4
+    vor.v           vr17,  vr5,   vr17
+
+.bframeiszero_0_lsx:
+    vxor.v          vr22,  vr22,  vr22
+    vbitsel.v       vr22,  vr22,  vr19,  vr17
+    vbitsel.v       vr22,  vr8,   vr22,  vr15
+    vst             vr22,  a3,    0
+
+    // dir = 1 s1 = 1 s2 = 8
+    vld             vr0,   a0,    4
+    vld             vr1,   a0,    20
+    ld.wu           t0,    a0,    36
+    vpickev.w       vr2,   vr1,   vr0
+    vbsrl.v         vr3,   vr2,   4
+    vinsgr2vr.w     vr3,   t0,    3
+    vor.v           vr2,   vr3,   vr2
+    vseqi.b         vr2,   vr2,   0
+    vmov            vr15,  vr2
+    vxor.v          vr3,   vr3,   vr3
+    vbitsel.v       vr3,   vr18,  vr3,   vr2
+
+    vld             vr0,   a1,    4
+    vld             vr1,   a1,    20
+    ld.w            t0,    a1,    36
+    vpickev.w       vr2,   vr1,   vr0
+    vbsrl.v         vr4,   vr2,   4
+    vinsgr2vr.w     vr4,   t0,    3
+    vseq.b          vr2,   vr4,   vr2
+    vseqi.b         vr2,   vr2,   0
+
+    vld             vr0,   a2,    16
+    vld             vr1,   a2,    48
+    vld             vr12,  a2,    80
+    vld             vr13,  a2,    112
+    vld             vr4,   a2,    144
+    vpickev.h       vr5,   vr1,   vr0
+    vpickev.h       vr14,  vr13,  vr12
+    vpickev.h       vr7,   vr4,   vr4
+    vbsrl.v         vr6,   vr5,   8
+    vilvl.d         vr6,   vr14,  vr6
+    vilvh.d         vr9,   vr7,   vr14
+    vabsd.h         vr5,   vr6,   vr5
+    vabsd.h         vr9,   vr9,   vr14
+    vsle.h          vr5,   vr20,  vr5
+    vsle.h          vr9,   vr20,  vr9
+
+    vpickod.h       vr6,   vr1,   vr0
+    vpickod.h       vr14,  vr13,  vr12
+    vpickod.h       vr7,   vr4,   vr4
+    vbsrl.v         vr8,   vr6,   8
+    vilvl.d         vr8,   vr14,  vr8
+    vilvh.d         vr7,   vr7,   vr14
+    vabsd.h         vr8,   vr8,   vr6
+    vabsd.h         vr7,   vr7,   vr14
+    vsle.h          vr8,   vr21,  vr8
+    vsle.h          vr6,   vr21,  vr7
+
+    vor.v           vr5,   vr5,   vr8
+    vor.v           vr6,   vr9,   vr6
+    vpickev.b       vr6,   vr6,   vr5
+    vor.v           vr2,   vr6,   vr2
+
+    beqz            a5,    .bframeiszero_1_lsx
+    // bframe != 0 ref[1]
+    vld             vr0,   a1,    44
+    vld             vr1,   a1,    60
+    ld.w            t0,    a1,    76
+    vpickev.w       vr0,   vr1,   vr0
+    vbsrl.v         vr1,   vr0,   4
+    vinsgr2vr.w     vr1,   t0,    3
+    vseq.b          vr11,  vr1,   vr0
+    vseqi.b         vr11,  vr11,  0
+
+    vld             vr0,   a2,    176
+    vld             vr1,   a2,    208
+    vld             vr12,  a2,    240
+    vld             vr13,  a2,    272
+    vld             vr4,   a2,    304
+    vpickev.h       vr5,   vr1,   vr0
+    vpickev.h       vr14,  vr13,  vr12
+    vpickev.h       vr7,   vr4,   vr4
+    vbsrl.v         vr6,   vr5,   8
+    vilvl.d         vr6,   vr14,  vr6
+    vilvh.d         vr9,   vr7,   vr14
+    vabsd.h         vr5,   vr6,   vr5
+    vabsd.h         vr9,   vr9,   vr14
+    vsle.h          vr5,   vr20,  vr5
+    vsle.h          vr9,   vr20,  vr9
+
+    vpickod.h       vr6,   vr1,   vr0
+    vpickod.h       vr14,  vr13,  vr12
+    vpickod.h       vr7,   vr4,   vr4
+    vbsrl.v         vr8,   vr6,   8
+    vilvl.d         vr8,   vr14,  vr8
+    vilvh.d         vr7,   vr7,   vr14
+    vabsd.h         vr8,   vr8,   vr6
+    vabsd.h         vr6,   vr7,   vr14
+    vsle.h          vr8,   vr21,  vr8
+    vsle.h          vr6,   vr21,  vr6
+
+    vor.v           vr5,   vr5,   vr8
+    vor.v           vr7,   vr9,   vr6
+    vpickev.b       vr6,   vr7,   vr5
+    vor.v           vr6,   vr6,   vr11
+    vor.v           vr2,   vr6,   vr2
+
+.bframeiszero_1_lsx:
+    vxor.v          vr22,  vr22,  vr22
+    vbitsel.v       vr22,  vr22,  vr19,  vr2
+    vbitsel.v       vr22,  vr3,   vr22,  vr15
+    vst             vr22,  a3,    32
+endfunc
+
+/*
+ * void deblock_v_luma_intra_lsx( pixel *pix, intptr_t stride, int alpha, int beta )
+ */
+function deblock_v_luma_intra_lsx
+    slli.d          t0,    a1,    1
+    add.d           t1,    t0,    a1
+    slli.d          t2,    a1,    2
+
+    // Store registers to the stack
+    addi.d          sp,    sp,    -64
+    fst.d           f24,   sp,    0
+    fst.d           f25,   sp,    8
+    fst.d           f26,   sp,    16
+    fst.d           f27,   sp,    24
+    fst.d           f28,   sp,    32
+    fst.d           f29,   sp,    40
+    fst.d           f30,   sp,    48
+    fst.d           f31,   sp,    56
+
+    // Load data from pix
+    sub.d           t3,    a0,    t2 // t3 = a0 - 4 * stride
+    vld             vr3,   t3,    0  // p3
+    vldx            vr2,   t3,    a1 // p2
+    vldx            vr1,   t3,    t0 // p1
+    vldx            vr0,   t3,    t1 // p0
+    vld             vr10,  a0,    0  // q0
+    vldx            vr11,  a0,    a1 // q1
+    vldx            vr12,  a0,    t0 // q2
+    vldx            vr13,  a0,    t1 // q3
+
+    vsllwil.hu.bu   vr7,   vr3,   0
+    vsllwil.hu.bu   vr6,   vr2,   0
+    vsllwil.hu.bu   vr5,   vr1,   0
+    vsllwil.hu.bu   vr4,   vr0,   0
+    vsllwil.hu.bu   vr14,  vr10,  0
+    vsllwil.hu.bu   vr15,  vr11,  0
+    vsllwil.hu.bu   vr16,  vr12,  0
+    vsllwil.hu.bu   vr17,  vr13,  0
+
+    /* p0', p1', p2' */
+    vadd.h          vr8,   vr5,   vr4
+    vadd.h          vr9,   vr8,   vr14
+    vadd.h          vr19,  vr7,   vr6
+    vadd.h          vr18,  vr6,   vr9    // pix[-2*xstride]
+    vslli.h         vr19,  vr19,  1
+    vadd.h          vr20,  vr9,   vr18
+    vadd.h          vr19,  vr19,  vr18   // pix[-3*xstride]
+    vadd.h          vr20,  vr20,  vr15   // pix[-1*xstride]
+
+    /* p0' */
+    vadd.h          vr8,   vr8,   vr15
+    vadd.h          vr21,  vr8,   vr5    // pix[-1*xstride]
+
+    // /* q0', q1', q2' */
+    vadd.h          vr8,   vr15,  vr14
+    vadd.h          vr9,   vr8,   vr4
+    vadd.h          vr23,  vr17,  vr16
+    vadd.h          vr22,  vr9,   vr16  // pix[1*xstride]
+    vslli.h         vr23,  vr23,  1
+    vadd.h          vr24,  vr9,   vr22
+    vadd.h          vr23,  vr23,  vr22  // pix[2*xstride]
+    vadd.h          vr24,  vr24,  vr5   // pix[0*xstride]
+
+    /* q0' */
+    vadd.h          vr8,   vr8,   vr5
+    vadd.h          vr25,  vr8,   vr15  // pix[0*xstride]
+
+    vexth.hu.bu     vr7,   vr3
+    vexth.hu.bu     vr6,   vr2
+    vexth.hu.bu     vr5,   vr1
+    vexth.hu.bu     vr4,   vr0
+    vexth.hu.bu     vr14,  vr10
+    vexth.hu.bu     vr15,  vr11
+    vexth.hu.bu     vr16,  vr12
+    vexth.hu.bu     vr17,  vr13
+
+    /* p0', p1', p2' */
+    vadd.h          vr8,   vr5,   vr4
+    vadd.h          vr9,   vr8,   vr14
+    vadd.h          vr27,  vr6,   vr9   // pix[-2*xstride]
+    vadd.h          vr28,  vr7,   vr6
+    vslli.h         vr28,  vr28,  1
+    vadd.h          vr29,  vr9,   vr27
+    vadd.h          vr28,  vr28,  vr27  // pix[-3*xstride]
+    vadd.h          vr29,  vr29,  vr15  // pix[-1*xstride]
+
+    /* p0' */
+    vadd.h          vr8,   vr8,   vr15
+    vadd.h          vr30,  vr8,   vr5  // pix[-1*xstride]
+
+    /* q0', q1', q2' */
+    vadd.h          vr8,   vr15,  vr14
+    vadd.h          vr9,   vr8,   vr4
+    vadd.h          vr3,   vr17,  vr16
+    vadd.h          vr31,  vr9,   vr16  // pix[1*xstride]
+    vslli.h         vr3,   vr3,   1
+    vadd.h          vr13,  vr9,   vr31
+    vadd.h          vr3,   vr3,   vr31  // pix[2*xstride]
+    vadd.h          vr13,  vr13,  vr5   // pix[0*xstride]
+
+    /* q0' */
+    vadd.h          vr8,   vr8,   vr5
+    vadd.h          vr9,   vr8,   vr15  // pix[0*xstride]
+
+    vsrarni.b.h     vr28,  vr19,  3     // pix[-3*xstride]
+    vsrarni.b.h     vr27,  vr18,  2     // pix[-2*xstride]
+    vsrarni.b.h     vr29,  vr20,  3     // pix[-1*xstride]
+    vsrarni.b.h     vr30,  vr21,  2     // pix[-1*xstride] p0'
+    vsrarni.b.h     vr13,  vr24,  3     // pix[ 0*xstride]
+    vsrarni.b.h     vr31,  vr22,  2     // pix[ 1*xstride]
+    vsrarni.b.h     vr3,   vr23,  3     // pix[ 2*xstride]
+    vsrarni.b.h     vr9,   vr25,  2     // pix[ 0*xstride] q0'
+
+    vreplgr2vr.b    vr18,  a2           // alpha
+    vreplgr2vr.b    vr19,  a3           // beta
+
+    vabsd.bu        vr26,  vr0,   vr10
+    vabsd.bu        vr8,   vr1,   vr0
+    vabsd.bu        vr16,  vr11,  vr10
+    vslt.bu         vr20,  vr26,  vr18
+    vslt.bu         vr21,  vr8,   vr19
+    vslt.bu         vr22,  vr16,  vr19
+    vand.v          vr20,  vr20,  vr21
+    vand.v          vr20,  vr20,  vr22  // if_1
+
+    vsrli.b         vr18,  vr18,  2
+    vaddi.bu        vr18,  vr18,  2
+
+    vslt.bu         vr26,  vr26,  vr18  // if_2
+
+    vabsd.bu        vr23,   vr2,   vr0
+    vslt.bu         vr23,   vr23,  vr19 // if_3
+
+    vand.v          vr16,  vr23,  vr26  // if_2 && if_3
+    vnor.v          vr24,  vr16,  vr16  // !(if_2 && if_3)
+    vand.v          vr24,  vr24,  vr20  // if_1 && !(if_2 && if_3)
+    vand.v          vr16,  vr16,  vr20  // if_1 && if_2 && if_3
+
+    vbitsel.v       vr4,   vr2,   vr28, vr16  // pix[-3*xstride]
+    vbitsel.v       vr5,   vr1,   vr27, vr16  // pix[-2*xstride]
+    vbitsel.v       vr6,   vr0,   vr30, vr24
+    vbitsel.v       vr6,   vr6,   vr29, vr16  // pix[-1*xstride]
+
+    vabsd.bu        vr7,   vr12,  vr10
+    vslt.bu         vr7,   vr7,   vr19  // if_4
+
+    vand.v          vr17,  vr7,   vr26  // if_2 && if_4
+    vnor.v          vr14,  vr17,  vr17  // !(if_2 && if_4)
+    vand.v          vr14,  vr14,  vr20  // if_1 && !(if_2 && if_4)
+    vand.v          vr17,  vr17,  vr20  // if_1 && if_2 && if_4
+
+    vbitsel.v       vr15,  vr10,  vr9,  vr14
+    vbitsel.v       vr15,  vr15,  vr13, vr17 // pix[ 0*xstride]
+    vbitsel.v       vr9,   vr11,  vr31, vr17 // pix[ 1*xstride]
+    vbitsel.v       vr13,  vr12,  vr3,  vr17 // pix[ 2*xstride]
+
+    vstx            vr4,   t3,    a1
+    vstx            vr5,   t3,    t0
+    vstx            vr6,   t3,    t1
+    vst             vr15,  a0,    0
+    vstx            vr9,   a0,    a1
+    vstx            vr13,  a0,    t0
+
+    fld.d           f24,   sp,    0
+    fld.d           f25,   sp,    8
+    fld.d           f26,   sp,    16
+    fld.d           f27,   sp,    24
+    fld.d           f28,   sp,    32
+    fld.d           f29,   sp,    40
+    fld.d           f30,   sp,    48
+    fld.d           f31,   sp,    56
+    addi.d          sp,    sp,    64
 endfunc
 
+/*
+ * void deblock_h_luma_intra_c( pixel *pix, intptr_t stride, int alpha, int beta )
+ */
+function deblock_h_luma_intra_lsx
+    slli.d          t0,    a1,    1
+    slli.d          t2,    a1,    2
+    addi.d          t5,    a0,    -4
+    add.d           t1,    t0,    a1
+
+    // Store registers to the stack
+    addi.d          sp,    sp,    -64
+    fst.d           f24,   sp,    0
+    fst.d           f25,   sp,    8
+    fst.d           f26,   sp,    16
+    fst.d           f27,   sp,    24
+    fst.d           f28,   sp,    32
+    fst.d           f29,   sp,    40
+    fst.d           f30,   sp,    48
+    fst.d           f31,   sp,    56
+
+    // Load data from pix
+    FLDD_LOADX_4    t5,    a1,    t0,    t1,    f10, f11, f12, f13
+    add.d           t5,    t5,    t2
+    FLDD_LOADX_4    t5,    a1,    t0,    t1,    f14, f15, f16, f17
+    add.d           t5,    t5,    t2
+    FLDD_LOADX_4    t5,    a1,    t0,    t1,    f20, f21, f22, f23
+    add.d           t5,    t5,    t2
+    FLDD_LOADX_4    t5,    a1,    t0,    t1,    f24, f25, f26, f27
+
+    vilvl.b         vr11,  vr11,  vr10
+    vilvl.b         vr13,  vr13,  vr12
+    vilvl.b         vr15,  vr15,  vr14
+    vilvl.b         vr17,  vr17,  vr16
+    vilvl.h         vr0,   vr13,  vr11
+    vilvl.h         vr1,   vr17,  vr15
+    vilvh.h         vr2,   vr13,  vr11
+    vilvh.h         vr3,   vr17,  vr15
+    vilvl.w         vr4,   vr1,   vr0
+    vilvl.w         vr6,   vr3,   vr2
+    vilvh.w         vr5,   vr1,   vr0
+    vilvh.w         vr7,   vr3,   vr2
+
+    vilvl.b         vr11,  vr21,  vr20
+    vilvl.b         vr13,  vr23,  vr22
+    vilvl.b         vr15,  vr25,  vr24
+    vilvl.b         vr17,  vr27,  vr26
+    vilvl.h         vr0,   vr13,  vr11
+    vilvl.h         vr1,   vr17,  vr15
+    vilvh.h         vr2,   vr13,  vr11
+    vilvh.h         vr3,   vr17,  vr15
+    vilvl.w         vr24,  vr1,   vr0
+    vilvl.w         vr26,  vr3,   vr2
+    vilvh.w         vr25,  vr1,   vr0
+    vilvh.w         vr27,  vr3,   vr2
+
+    vilvl.d         vr3,  vr24, vr4  // p3
+    vilvh.d         vr2,  vr24, vr4  // p2
+    vilvl.d         vr1,  vr25, vr5  // p1
+    vilvh.d         vr0,  vr25, vr5  // p0
+    vilvl.d         vr10, vr26, vr6  // q0
+    vilvh.d         vr11, vr26, vr6  // q1
+    vilvl.d         vr12, vr27, vr7  // q2
+    vilvh.d         vr13, vr27, vr7  // q3
+
+    vsllwil.hu.bu   vr7,   vr3,   0
+    vsllwil.hu.bu   vr6,   vr2,   0
+    vsllwil.hu.bu   vr5,   vr1,   0
+    vsllwil.hu.bu   vr4,   vr0,   0
+    vsllwil.hu.bu   vr14,  vr10,  0
+    vsllwil.hu.bu   vr15,  vr11,  0
+    vsllwil.hu.bu   vr16,  vr12,  0
+    vsllwil.hu.bu   vr17,  vr13,  0
+
+    /* p0', p1', p2' */
+    vadd.h          vr8,   vr5,   vr4
+    vadd.h          vr9,   vr8,   vr14
+    vadd.h          vr19,  vr7,   vr6
+    vadd.h          vr18,  vr6,   vr9    // pix[-2*xstride]
+    vslli.h         vr19,  vr19,  1
+    vadd.h          vr20,  vr9,   vr18
+    vadd.h          vr19,  vr19,  vr18   // pix[-3*xstride]
+    vadd.h          vr20,  vr20,  vr15   // pix[-1*xstride]
+
+    /* p0' */
+    vadd.h          vr8,   vr8,   vr15
+    vadd.h          vr21,  vr8,   vr5    // pix[-1*xstride]
+
+    /* q0', q1', q2' */
+    vadd.h          vr8,   vr15,  vr14
+    vadd.h          vr9,   vr8,   vr4
+    vadd.h          vr23,  vr17,  vr16
+    vadd.h          vr22,  vr9,   vr16  // pix[1*xstride]
+    vslli.h         vr23,  vr23,  1
+    vadd.h          vr24,  vr9,   vr22
+    vadd.h          vr23,  vr23,  vr22  // pix[2*xstride]
+    vadd.h          vr24,  vr24,  vr5   // pix[0*xstride]
+
+    /* q0' */
+    vadd.h          vr8,   vr8,   vr5
+    vadd.h          vr25,  vr8,   vr15  // pix[0*xstride]
+
+    vexth.hu.bu     vr7,   vr3
+    vexth.hu.bu     vr6,   vr2
+    vexth.hu.bu     vr5,   vr1
+    vexth.hu.bu     vr4,   vr0
+    vexth.hu.bu     vr14,  vr10
+    vexth.hu.bu     vr15,  vr11
+    vexth.hu.bu     vr16,  vr12
+    vexth.hu.bu     vr17,  vr13
+
+    /* p0', p1', p2' */
+    vadd.h          vr8,   vr5,   vr4
+    vadd.h          vr9,   vr8,   vr14
+    vadd.h          vr27,  vr6,   vr9   // pix[-2*xstride]
+    vadd.h          vr28,  vr7,   vr6
+    vslli.h         vr28,  vr28,  1
+    vadd.h          vr29,  vr9,   vr27
+    vadd.h          vr28,  vr28,  vr27  // pix[-3*xstride]
+    vadd.h          vr29,  vr29,  vr15  // pix[-1*xstride]
+
+    /* p0' */
+    vadd.h          vr8,   vr8,   vr15
+    vadd.h          vr30,  vr8,   vr5  // pix[-1*xstride]
+
+    /* q0', q1', q2' */
+    vadd.h          vr8,   vr15,  vr14
+    vadd.h          vr9,   vr8,   vr4
+    vadd.h          vr3,   vr17,  vr16
+    vadd.h          vr31,  vr9,   vr16  // pix[1*xstride]
+    vslli.h         vr3,   vr3,   1
+    vadd.h          vr13,  vr9,   vr31
+    vadd.h          vr3,   vr3,   vr31  // pix[2*xstride]
+    vadd.h          vr13,  vr13,  vr5   // pix[0*xstride]
+
+    /* q0' */
+    vadd.h          vr8,   vr8,   vr5
+    vadd.h          vr9,   vr8,   vr15  // pix[0*xstride]
+
+    vsrarni.b.h     vr28,  vr19,  3     // pix[-3*xstride]
+    vsrarni.b.h     vr27,  vr18,  2     // pix[-2*xstride]
+    vsrarni.b.h     vr29,  vr20,  3     // pix[-1*xstride]
+    vsrarni.b.h     vr30,  vr21,  2     // pix[-1*xstride] p0'
+    vsrarni.b.h     vr13,  vr24,  3     // pix[ 0*xstride]
+    vsrarni.b.h     vr31,  vr22,  2     // pix[ 1*xstride]
+    vsrarni.b.h     vr3,   vr23,  3     // pix[ 2*xstride]
+    vsrarni.b.h     vr9,   vr25,  2     // pix[ 0*xstride] q0'
+
+    vreplgr2vr.b    vr18,  a2           // alpha
+    vreplgr2vr.b    vr19,  a3           // beta
+
+    vabsd.bu        vr26,  vr0,   vr10
+    vabsd.bu        vr8,   vr1,   vr0
+    vabsd.bu        vr16,  vr11,  vr10
+    vslt.bu         vr20,  vr26,  vr18
+    vslt.bu         vr21,  vr8,   vr19
+    vslt.bu         vr22,  vr16,  vr19
+    vand.v          vr20,  vr20,  vr21
+    vand.v          vr20,  vr20,  vr22  // if_1
+
+    vsrli.b         vr18,  vr18,  2
+    vaddi.bu        vr18,  vr18,  2
+
+    vslt.bu         vr26,  vr26,  vr18  // if_2
+
+    vabsd.bu        vr23,   vr2,   vr0
+    vslt.bu         vr23,   vr23,  vr19 // if_3
+
+    vand.v          vr16,  vr23,  vr26        // if_2 && if_3
+    vnor.v          vr24,  vr16,  vr16        // !(if_2 && if_3)
+    vand.v          vr24,  vr24,  vr20        // if_1 && !(if_2 && if_3)
+    vand.v          vr16,  vr16,  vr20        // if_1 && if_2 && if_3
+    vbitsel.v       vr4,   vr2,   vr28, vr16  // pix[-3*xstride]
+    vbitsel.v       vr5,   vr1,   vr27, vr16  // pix[-2*xstride]
+    vbitsel.v       vr6,   vr0,   vr30, vr24
+    vbitsel.v       vr6,   vr6,   vr29, vr16  // pix[-1*xstride]
+
+    vabsd.bu        vr7,   vr12,  vr10
+    vslt.bu         vr7,   vr7,   vr19       // if_4
+
+    vand.v          vr17,  vr7,   vr26       // if_2 && if_4
+    vnor.v          vr14,  vr17,  vr17       // !(if_2 && if_4)
+    vand.v          vr14,  vr14,  vr20       // if_1 && !(if_2 && if_4)
+    vand.v          vr17,  vr17,  vr20       // if_1 && if_2 && if_4
+    vbitsel.v       vr15,  vr10,  vr9,  vr14
+    vbitsel.v       vr15,  vr15,  vr13, vr17 // pix[ 0*xstride]
+    vbitsel.v       vr9,   vr11,  vr31, vr17 // pix[ 1*xstride]
+    vbitsel.v       vr13,  vr12,  vr3,  vr17 // pix[ 2*xstride]
+
+    vilvl.b         vr16,  vr5,   vr4
+    vilvl.b         vr17,  vr15,  vr6
+    vilvl.b         vr18,  vr13,  vr9
+    vilvh.b         vr19,  vr5,   vr4
+    vilvh.b         vr20,  vr15,  vr6
+    vilvh.b         vr21,  vr13,  vr9
+    vilvl.h         vr0,   vr17,  vr16
+    vilvh.h         vr1,   vr17,  vr16
+    vilvl.h         vr2,   vr20,  vr19
+    vilvh.h         vr3,   vr20,  vr19
+
+    addi.d          t6,    a0,    -3     // t6 = a0 -3
+    vstelm.w        vr0,   t6,    0,   0
+    vstelm.h        vr18,  t6,    4,   0
+    add.d           t6,    t6,    a1
+    vstelm.w        vr0,   t6,    0,   1
+    vstelm.h        vr18,  t6,    4,   1
+    add.d           t6,    t6,    a1
+    vstelm.w        vr0,   t6,    0,   2
+    vstelm.h        vr18,  t6,    4,   2
+    add.d           t6,    t6,    a1
+    vstelm.w        vr0,   t6,    0,   3
+    vstelm.h        vr18,  t6,    4,   3
+    add.d           t6,    t6,    a1
+
+    vstelm.w        vr1,   t6,    0,   0
+    vstelm.h        vr18,  t6,    4,   4
+    add.d           t6,    t6,    a1
+    vstelm.w        vr1,   t6,    0,   1
+    vstelm.h        vr18,  t6,    4,   5
+    add.d           t6,    t6,    a1
+    vstelm.w        vr1,   t6,    0,   2
+    vstelm.h        vr18,  t6,    4,   6
+    add.d           t6,    t6,    a1
+    vstelm.w        vr1,   t6,    0,   3
+    vstelm.h        vr18,  t6,    4,   7
+    add.d           t6,    t6,    a1
+
+    vstelm.w        vr2,   t6,    0,   0
+    vstelm.h        vr21,  t6,    4,   0
+    add.d           t6,    t6,    a1
+    vstelm.w        vr2,   t6,    0,   1
+    vstelm.h        vr21,  t6,    4,   1
+    add.d           t6,    t6,    a1
+    vstelm.w        vr2,   t6,    0,   2
+    vstelm.h        vr21,  t6,    4,   2
+    add.d           t6,    t6,    a1
+    vstelm.w        vr2,   t6,    0,   3
+    vstelm.h        vr21,  t6,    4,   3
+    add.d           t6,    t6,    a1
+
+    vstelm.w        vr3,   t6,    0,   0
+    vstelm.h        vr21,  t6,    4,   4
+    add.d           t6,    t6,    a1
+    vstelm.w        vr3,   t6,    0,   1
+    vstelm.h        vr21,  t6,    4,   5
+    add.d           t6,    t6,    a1
+    vstelm.w        vr3,   t6,    0,   2
+    vstelm.h        vr21,  t6,    4,   6
+    add.d           t6,    t6,    a1
+    vstelm.w        vr3,   t6,    0,   3
+    vstelm.h        vr21,  t6,    4,   7
+
+    fld.d           f24,   sp,    0
+    fld.d           f25,   sp,    8
+    fld.d           f26,   sp,    16
+    fld.d           f27,   sp,    24
+    fld.d           f28,   sp,    32
+    fld.d           f29,   sp,    40
+    fld.d           f30,   sp,    48
+    fld.d           f31,   sp,    56
+    addi.d          sp,    sp,    64
+endfunc
 #endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/deblock.h b/common/loongarch/deblock.h
index a0bbc464..5cacaef1 100644
--- a/common/loongarch/deblock.h
+++ b/common/loongarch/deblock.h
@@ -32,10 +32,20 @@
 void x264_deblock_v_luma_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta, int8_t *tc0 );
 #define x264_deblock_h_luma_lasx x264_template(deblock_h_luma_lasx)
 void x264_deblock_h_luma_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta, int8_t *tc0 );
+
+#define x264_deblock_v_luma_intra_lsx x264_template(deblock_v_luma_intra_lsx)
+void x264_deblock_v_luma_intra_lsx( uint8_t *pix, intptr_t stride, int alpha, int beta );
+#define x264_deblock_h_luma_intra_lsx x264_template(deblock_h_luma_intra_lsx)
+void x264_deblock_h_luma_intra_lsx( uint8_t *pix, intptr_t stride, int alpha, int beta );
+
 #define x264_deblock_v_luma_intra_lasx x264_template(deblock_v_luma_intra_lasx)
 void x264_deblock_v_luma_intra_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta );
 #define x264_deblock_h_luma_intra_lasx x264_template(deblock_h_luma_intra_lasx)
 void x264_deblock_h_luma_intra_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta );
+#define x264_deblock_strength_lsx x264_template(deblock_strength_lsx)
+void x264_deblock_strength_lsx( uint8_t nnz[X264_SCAN8_SIZE], int8_t ref[2][X264_SCAN8_LUMA_SIZE],
+                                int16_t mv[2][X264_SCAN8_LUMA_SIZE][2], uint8_t bs[2][8][4], int mvy_limit,
+                                int bframe );
 #define x264_deblock_strength_lasx x264_template(deblock_strength_lasx)
 void x264_deblock_strength_lasx( uint8_t nnz[X264_SCAN8_SIZE], int8_t ref[2][X264_SCAN8_LUMA_SIZE],
                                  int16_t mv[2][X264_SCAN8_LUMA_SIZE][2], uint8_t bs[2][8][4], int mvy_limit,
diff --git a/common/loongarch/asm.S b/common/loongarch/loongson_asm.S
similarity index 67%
rename from common/loongarch/asm.S
rename to common/loongarch/loongson_asm.S
index 44937ca1..1e9b2490 100644
--- a/common/loongarch/asm.S
+++ b/common/loongarch/loongson_asm.S
@@ -1,10 +1,11 @@
-/*****************************************************************************
- * asm.S: LoongArch utility macros
+/*********************************************************************
+ * loongson_asm.S: LoongArch utility macros
  *****************************************************************************
  * Copyright (C) 2015-2022 x264 project
  * Copyright (C) 2022 Loongson Technology Corporation Limited
  *
  * Authors: gxw <guxiwei-hf@loongson.cn>
+ *          Shiyou Yin(yinshiyou-hf@loongson.cn)
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -22,11 +23,27 @@
  *
  * This program is also available under a commercial proprietary license.
  * For more information, contact us at licensing@x264.com.
- *****************************************************************************/
+ *********************************************************************/
 
+/**
+ * MAJOR version: Macro usage changes.
+ * MINOR version: Add new functions, or bug fixes.
+ * MICRO version: Comment changes or implementation changes.
+ */
+#define LML_VERSION_MAJOR 0
+#define LML_VERSION_MINOR 2
+#define LML_VERSION_MICRO 0
+
+/*
+ *============================================================================
+ * macros for specific projetc, set them as needed.
+ * Following LoongML macros for your reference.
+ *============================================================================
+ */
 #define GLUE(a, b) a ## b
 #define JOIN(a, b) GLUE(a, b)
 
+/* Set prefix as needed. */
 #ifdef BIT_DEPTH
 #define  ASM_PREF  JOIN(JOIN(x264_, BIT_DEPTH), _)
 #else
@@ -60,7 +77,28 @@ ASM_PREF\name:
 .endif
 .endm
 
-.macro    const name, align=DEFAULT_ALIGN
+/**
+ *  Attention: If align is not zero, the macro will use
+ *  t7 until the end of function
+ */
+.macro alloc_stack size, align=0
+.if \align
+    .macro clean_stack
+        add.d   sp, sp, t7
+    .endm
+    addi.d  sp, sp, - \size
+    andi.d  t7, sp, \align - 1
+    sub.d   sp, sp, t7
+    addi.d  t7, t7, \size
+.else
+    .macro clean_stack
+        addi.d  sp, sp, \size
+    .endm
+    addi.d  sp, sp, - \size
+.endif
+.endm
+
+.macro  const name, align=DEFAULT_ALIGN
     .macro endconst
     .size  \name, . - \name
     .purgem endconst
@@ -70,9 +108,12 @@ ASM_PREF\name:
 \name:
 .endm
 
-/**********************************************
-* LoongArch register alias
-***********************************************/
+/*
+ *============================================================================
+ * LoongArch register alias
+ *============================================================================
+ */
+
 #define a0 $a0
 #define a1 $a1
 #define a2 $a2
@@ -205,18 +246,18 @@ ASM_PREF\name:
 #define xr30 $xr30
 #define xr31 $xr31
 
-/**********************************************
-* LSX/LASX synthesize instructions
-***********************************************/
 /*
- * =============================================================================
+ *============================================================================
+ * LSX/LASX synthesize instructions
+ *============================================================================
+ */
+
+/*
  * Description : Dot product of byte vector elements
  * Arguments   : Inputs  - vj, vk
  *               Outputs - vd
  *               Return Type - halfword
- * =============================================================================
  */
-
 .macro vdp2.h.bu vd, vj, vk
     vmulwev.h.bu      \vd,    \vj,    \vk
     vmaddwod.h.bu     \vd,    \vj,    \vk
@@ -248,14 +289,11 @@ ASM_PREF\name:
 .endm
 
 /*
- * =============================================================================
  * Description : Dot product & addition of halfword vector elements
  * Arguments   : Inputs  - vj, vk
  *               Outputs - vd
  *               Return Type - twice size of input
- * =============================================================================
  */
-
 .macro vdp2add.h.bu vd, vj, vk
     vmaddwev.h.bu     \vd,    \vj,    \vk
     vmaddwod.h.bu     \vd,    \vj,    \vk
@@ -282,13 +320,10 @@ ASM_PREF\name:
 .endm
 
 /*
- * =============================================================================
  * Description : Range each element of vector
  * clip: vj > vk ? vj : vk && vj < va ? vj : va
  * clip255: vj < 255 ? vj : 255 && vj > 0 ? vj : 0
- * =============================================================================
  */
-
 .macro vclip.h  vd,  vj, vk, va
     vmax.h    \vd,  \vj,   \vk
     vmin.h    \vd,  \vd,   \va
@@ -315,15 +350,12 @@ ASM_PREF\name:
 .endm
 
 /*
- * =============================================================================
  * Description : Store elements of vector
  * vd : Data vector to be stroed
  * rk : Address of data storage
  * ra : Offset of address
  * si : Index of data in vd
- * =============================================================================
  */
-
 .macro vstelmx.b vd, rk, ra, si
     add.d      \rk,  \rk,  \ra
     vstelm.b   \vd,  \rk,  0, \si
@@ -357,24 +389,15 @@ ASM_PREF\name:
     xvstelm.d  \xd, \rk,  0, \si
 .endm
 
-/**********************************************
-* LSX/LASX custom macros
-***********************************************/
-
-.macro LASX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
-    xvld    \out0,    \src,    0
-    xvldx   \out1,    \src,    \stride
-    xvldx   \out2,    \src,    \stride2
-    xvldx   \out3,    \src,    \stride3
-.endm
-
-.macro LSX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
-    vld     \out0,    \src,    0
-    vldx    \out1,    \src,    \stride
-    vldx    \out2,    \src,    \stride2
-    vldx    \out3,    \src,    \stride3
-.endm
+/*
+ *============================================================================
+ * LSX/LASX custom macros
+ *============================================================================
+ */
 
+/*
+ * Load 4 float, double, V128, v256 elements with stride.
+ */
 .macro FLDS_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
     fld.s     \out0,    \src,    0
     fldx.s    \out1,    \src,    \stride
@@ -389,14 +412,25 @@ ASM_PREF\name:
     fldx.d    \out3,    \src,    \stride3
 .endm
 
+.macro LSX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    vld     \out0,    \src,    0
+    vldx    \out1,    \src,    \stride
+    vldx    \out2,    \src,    \stride2
+    vldx    \out3,    \src,    \stride3
+.endm
+
+.macro LASX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    xvld    \out0,    \src,    0
+    xvldx   \out1,    \src,    \stride
+    xvldx   \out2,    \src,    \stride2
+    xvldx   \out3,    \src,    \stride3
+.endm
+
 /*
- * =============================================================================
  * Description : Transpose 4x4 block with half-word elements in vectors
  * Arguments   : Inputs  - in0, in1, in2, in3
  *               Outputs - out0, out1, out2, out3
- * =============================================================================
  */
-
 .macro LSX_TRANSPOSE4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
                           tmp0, tmp1
     vilvl.h   \tmp0,  \in1,   \in0
@@ -408,34 +442,35 @@ ASM_PREF\name:
 .endm
 
 /*
- * =============================================================================
  * Description : Transpose 4x4 block with word elements in vectors
  * Arguments   : Inputs  - in0, in1, in2, in3
  *               Outputs - out0, out1, out2, out3
- * =============================================================================
+ * Details     :
+ * Example     :
+ *               1, 2, 3, 4            1, 5, 9,13
+ *               5, 6, 7, 8    to      2, 6,10,14
+ *               9,10,11,12  =====>    3, 7,11,15
+ *              13,14,15,16            4, 8,12,16
  */
+.macro LSX_TRANSPOSE4x4_W _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3, \
+                          _tmp0, _tmp1
 
-.macro LSX_TRANSPOSE4x4_W in0, in1, in2, in3, out0, out1, out2, out3, \
-                          tmp0, tmp1, tmp2, tmp3
-    vilvl.w    \tmp0, \in1, \in0
-    vilvh.w    \tmp1, \in1, \in0
-    vilvl.w    \tmp2, \in3, \in2
-    vilvh.w    \tmp3, \in3, \in2
+    vilvl.w    \_tmp0,   \_in1,    \_in0
+    vilvh.w    \_out1,   \_in1,    \_in0
+    vilvl.w    \_tmp1,   \_in3,    \_in2
+    vilvh.w    \_out3,   \_in3,    \_in2
 
-    vilvl.d    \out0, \tmp2, \tmp0
-    vilvh.d    \out1, \tmp2, \tmp0
-    vilvl.d    \out2, \tmp3, \tmp1
-    vilvh.d    \out3, \tmp3, \tmp1
+    vilvl.d    \_out0,   \_tmp1,   \_tmp0
+    vilvl.d    \_out2,   \_out3,   \_out1
+    vilvh.d    \_out3,   \_out3,   \_out1
+    vilvh.d    \_out1,   \_tmp1,   \_tmp0
 .endm
 
 /*
- * =============================================================================
  * Description : Transpose 8x8 block with half-word elements in vectors
  * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
  *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * =============================================================================
  */
-
 .macro LSX_TRANSPOSE8x8_H in0, in1, in2, in3, in4, in5, in6, in7, out0, out1,   \
                           out2, out3, out4, out5, out6, out7, tmp0, tmp1, tmp2, \
                           tmp3, tmp4, tmp5, tmp6, tmp7
@@ -471,13 +506,53 @@ ASM_PREF\name:
 .endm
 
 /*
- * =============================================================================
+ * Description : Transpose 16x8 block with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ */
+.macro LASX_TRANSPOSE16X8_B in0, in1, in2, in3, in4, in5, in6, in7,        \
+                            in8, in9, in10, in11, in12, in13, in14, in15,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7,\
+                            tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
+    xvilvl.b   \tmp0,    \in2,     \in0
+    xvilvl.b   \tmp1,    \in3,     \in1
+    xvilvl.b   \tmp2,    \in6,     \in4
+    xvilvl.b   \tmp3,    \in7,     \in5
+    xvilvl.b   \tmp4,    \in10,    \in8
+    xvilvl.b   \tmp5,    \in11,    \in9
+    xvilvl.b   \tmp6,    \in14,    \in12
+    xvilvl.b   \tmp7,    \in15,    \in13
+    xvilvl.b   \out0,    \tmp1,    \tmp0
+    xvilvh.b   \out1,    \tmp1,    \tmp0
+    xvilvl.b   \out2,    \tmp3,    \tmp2
+    xvilvh.b   \out3,    \tmp3,    \tmp2
+    xvilvl.b   \out4,    \tmp5,    \tmp4
+    xvilvh.b   \out5,    \tmp5,    \tmp4
+    xvilvl.b   \out6,    \tmp7,    \tmp6
+    xvilvh.b   \out7,    \tmp7,    \tmp6
+    xvilvl.w   \tmp0,    \out2,    \out0
+    xvilvh.w   \tmp2,    \out2,    \out0
+    xvilvl.w   \tmp4,    \out3,    \out1
+    xvilvh.w   \tmp6,    \out3,    \out1
+    xvilvl.w   \tmp1,    \out6,    \out4
+    xvilvh.w   \tmp3,    \out6,    \out4
+    xvilvl.w   \tmp5,    \out7,    \out5
+    xvilvh.w   \tmp7,    \out7,    \out5
+    xvilvl.d   \out0,    \tmp1,    \tmp0
+    xvilvh.d   \out1,    \tmp1,    \tmp0
+    xvilvl.d   \out2,    \tmp3,    \tmp2
+    xvilvh.d   \out3,    \tmp3,    \tmp2
+    xvilvl.d   \out4,    \tmp5,    \tmp4
+    xvilvh.d   \out5,    \tmp5,    \tmp4
+    xvilvl.d   \out6,    \tmp7,    \tmp6
+    xvilvh.d   \out7,    \tmp7,    \tmp6
+.endm
+
+/*
  * Description : Transpose 4x4 block with half-word elements in vectors
  * Arguments   : Inputs  - in0, in1, in2, in3
  *               Outputs - out0, out1, out2, out3
- * =============================================================================
  */
-
 .macro LASX_TRANSPOSE4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
                            tmp0, tmp1
     xvilvl.h   \tmp0,  \in1,   \in0
@@ -489,34 +564,28 @@ ASM_PREF\name:
 .endm
 
 /*
- * =============================================================================
  * Description : Transpose 4x8 block with half-word elements in vectors
  * Arguments   : Inputs  - in0, in1, in2, in3
  *               Outputs - out0, out1, out2, out3
- * =============================================================================
  */
-
 .macro LASX_TRANSPOSE4x8_H in0, in1, in2, in3, out0, out1, out2, out3, \
-                           tmp0, tmp1, tmp2, tmp3
+                           tmp0, tmp1
     xvilvl.h      \tmp0,    \in2,   \in0
     xvilvl.h      \tmp1,    \in3,   \in1
-    xvilvl.h      \tmp2,    \tmp1,  \tmp0
-    xvilvh.h      \tmp3,    \tmp1,  \tmp0
+    xvilvl.h      \out2,    \tmp1,  \tmp0
+    xvilvh.h      \out3,    \tmp1,  \tmp0
 
-    xvilvl.d      \out0,    \tmp2,  \tmp2
-    xvilvh.d      \out1,    \tmp2,  \tmp2
-    xvilvl.d      \out2,    \tmp3,  \tmp3
-    xvilvh.d      \out3,    \tmp3,  \tmp3
+    xvilvl.d      \out0,    \out2,  \out2
+    xvilvh.d      \out1,    \out2,  \out2
+    xvilvl.d      \out2,    \out3,  \out3
+    xvilvh.d      \out3,    \out3,  \out3
 .endm
 
 /*
- * =============================================================================
  * Description : Transpose 8x8 block with half-word elements in vectors
  * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
  *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * =============================================================================
  */
-
 .macro LASX_TRANSPOSE8x8_H in0, in1, in2, in3, in4, in5, in6, in7,         \
                            out0, out1, out2, out3, out4, out5, out6, out7, \
                            tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
@@ -552,114 +621,141 @@ ASM_PREF\name:
 .endm
 
 /*
- * =============================================================================
- * Description : Transpose 8x8 block with word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * =============================================================================
+ * Description : Transpose 2x4x4 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
  */
+.macro LASX_TRANSPOSE2x4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                             tmp0, tmp1, tmp2
+    xvilvh.h   \tmp1,    \in0,     \in1
+    xvilvl.h   \out1,    \in0,     \in1
+    xvilvh.h   \tmp0,    \in2,     \in3
+    xvilvl.h   \out3,    \in2,     \in3
 
-.macro LASX_TRANSPOSE8x8_W in0, in1, in2, in3, in4, in5, in6, in7,         \
-                           out0, out1, out2, out3, out4, out5, out6, out7, \
-                           tmp0, tmp1, tmp2, tmp3
-    xvilvl.w    \tmp0,    \in2,     \in0
-    xvilvl.w    \tmp1,    \in3,     \in1
-    xvilvh.w    \tmp2,    \in2,     \in0
-    xvilvh.w    \tmp3,    \in3,     \in1
-    xvilvl.w    \out0,    \tmp1,    \tmp0
-    xvilvh.w    \out1,    \tmp1,    \tmp0
-    xvilvl.w    \out2,    \tmp3,    \tmp2
-    xvilvh.w    \out3,    \tmp3,    \tmp2
-
-    xvilvl.w    \tmp0,    \in6,     \in4
-    xvilvl.w    \tmp1,    \in7,     \in5
-    xvilvh.w    \tmp2,    \in6,     \in4
-    xvilvh.w    \tmp3,    \in7,     \in5
-    xvilvl.w    \out4,    \tmp1,    \tmp0
-    xvilvh.w    \out5,    \tmp1,    \tmp0
-    xvilvl.w    \out6,    \tmp3,    \tmp2
-    xvilvh.w    \out7,    \tmp3,    \tmp2
-
-    xmov        \tmp0,    \out0
-    xmov        \tmp1,    \out1
-    xmov        \tmp2,    \out2
-    xmov        \tmp3,    \out3
-    xvpermi.q   \out0,    \out4,    0x02
-    xvpermi.q   \out1,    \out5,    0x02
-    xvpermi.q   \out2,    \out6,    0x02
-    xvpermi.q   \out3,    \out7,    0x02
-    xvpermi.q   \out4,    \tmp0,    0x31
-    xvpermi.q   \out5,    \tmp1,    0x31
-    xvpermi.q   \out6,    \tmp2,    0x31
-    xvpermi.q   \out7,    \tmp3,    0x31
+    xvilvh.w   \tmp2,    \out3,    \out1
+    xvilvl.w   \out3,    \out3,    \out1
+
+    xvilvl.w   \out2,    \tmp0,    \tmp1
+    xvilvh.w   \tmp1,    \tmp0,    \tmp1
+
+    xvilvh.d   \out0,    \out2,    \out3
+    xvilvl.d   \out2,    \out2,    \out3
+    xvilvh.d   \out1,    \tmp1,    \tmp2
+    xvilvl.d   \out3,    \tmp1,    \tmp2
 .endm
 
 /*
- * =============================================================================
- * Description : Transpose 16x8 block with byte elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
- *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
- * =============================================================================
+ * Description : Transpose 4x4 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     :
+ * Example     :
+ *               1, 2, 3, 4,  1, 2, 3, 4        1,5, 9,13, 1,5, 9,13
+ *               5, 6, 7, 8,  5, 6, 7, 8   to   2,6,10,14, 2,6,10,14
+ *               9,10,11,12,  9,10,11,12 =====> 3,7,11,15, 3,7,11,15
+ *              13,14,15,16, 13,14,15,16        4,8,12,16, 4,8,12,16
  */
+.macro LASX_TRANSPOSE4x4_W _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3, \
+                           _tmp0, _tmp1
 
-.macro LASX_TRANSPOSE16X8_B in0, in1, in2, in3, in4, in5, in6, in7,        \
-                            in8, in9, in10, in11, in12, in13, in14, in15,  \
-                            out0, out1, out2, out3, out4, out5, out6, out7,\
-                            tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
-    xvilvl.b   \tmp0,    \in2,     \in0
-    xvilvl.b   \tmp1,    \in3,     \in1
-    xvilvl.b   \tmp2,    \in6,     \in4
-    xvilvl.b   \tmp3,    \in7,     \in5
-    xvilvl.b   \tmp4,    \in10,    \in8
-    xvilvl.b   \tmp5,    \in11,    \in9
-    xvilvl.b   \tmp6,    \in14,    \in12
-    xvilvl.b   \tmp7,    \in15,    \in13
-    xvilvl.b   \out0,    \tmp1,    \tmp0
-    xvilvh.b   \out1,    \tmp1,    \tmp0
-    xvilvl.b   \out2,    \tmp3,    \tmp2
-    xvilvh.b   \out3,    \tmp3,    \tmp2
-    xvilvl.b   \out4,    \tmp5,    \tmp4
-    xvilvh.b   \out5,    \tmp5,    \tmp4
-    xvilvl.b   \out6,    \tmp7,    \tmp6
-    xvilvh.b   \out7,    \tmp7,    \tmp6
-    xvilvl.w   \tmp0,    \out2,    \out0
-    xvilvh.w   \tmp2,    \out2,    \out0
-    xvilvl.w   \tmp4,    \out3,    \out1
-    xvilvh.w   \tmp6,    \out3,    \out1
-    xvilvl.w   \tmp1,    \out6,    \out4
-    xvilvh.w   \tmp3,    \out6,    \out4
-    xvilvl.w   \tmp5,    \out7,    \out5
-    xvilvh.w   \tmp7,    \out7,    \out5
-    xvilvl.d   \out0,    \tmp1,    \tmp0
-    xvilvh.d   \out1,    \tmp1,    \tmp0
-    xvilvl.d   \out2,    \tmp3,    \tmp2
-    xvilvh.d   \out3,    \tmp3,    \tmp2
-    xvilvl.d   \out4,    \tmp5,    \tmp4
-    xvilvh.d   \out5,    \tmp5,    \tmp4
-    xvilvl.d   \out6,    \tmp7,    \tmp6
-    xvilvh.d   \out7,    \tmp7,    \tmp6
+    xvilvl.w    \_tmp0,   \_in1,    \_in0
+    xvilvh.w    \_out1,   \_in1,    \_in0
+    xvilvl.w    \_tmp1,   \_in3,    \_in2
+    xvilvh.w    \_out3,   \_in3,    \_in2
+
+    xvilvl.d    \_out0,   \_tmp1,   \_tmp0
+    xvilvl.d    \_out2,   \_out3,   \_out1
+    xvilvh.d    \_out3,   \_out3,   \_out1
+    xvilvh.d    \_out1,   \_tmp1,   \_tmp0
 .endm
 
 /*
- * =============================================================================
- * Description : Transpose 2x4x4 block with half-word elements in vectors
- * Arguments   : Inputs  - in0, in1, in2, in3
- *               Outputs - out0, out1, out2, out3
- * =============================================================================
+ * Description : Transpose 8x8 block with word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *               _out7
+ * Example     : LASX_TRANSPOSE8x8_W
+ *        _in0 : 1,2,3,4,5,6,7,8
+ *        _in1 : 2,2,3,4,5,6,7,8
+ *        _in2 : 3,2,3,4,5,6,7,8
+ *        _in3 : 4,2,3,4,5,6,7,8
+ *        _in4 : 5,2,3,4,5,6,7,8
+ *        _in5 : 6,2,3,4,5,6,7,8
+ *        _in6 : 7,2,3,4,5,6,7,8
+ *        _in7 : 8,2,3,4,5,6,7,8
+ *
+ *       _out0 : 1,2,3,4,5,6,7,8
+ *       _out1 : 2,2,2,2,2,2,2,2
+ *       _out2 : 3,3,3,3,3,3,3,3
+ *       _out3 : 4,4,4,4,4,4,4,4
+ *       _out4 : 5,5,5,5,5,5,5,5
+ *       _out5 : 6,6,6,6,6,6,6,6
+ *       _out6 : 7,7,7,7,7,7,7,7
+ *       _out7 : 8,8,8,8,8,8,8,8
  */
+.macro LASX_TRANSPOSE8x8_W _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,\
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7,\
+                           _tmp0, _tmp1, _tmp2, _tmp3
+    xvilvl.w    \_tmp0,   \_in2,    \_in0
+    xvilvl.w    \_tmp1,   \_in3,    \_in1
+    xvilvh.w    \_tmp2,   \_in2,    \_in0
+    xvilvh.w    \_tmp3,   \_in3,    \_in1
+    xvilvl.w    \_out0,   \_tmp1,   \_tmp0
+    xvilvh.w    \_out1,   \_tmp1,   \_tmp0
+    xvilvl.w    \_out2,   \_tmp3,   \_tmp2
+    xvilvh.w    \_out3,   \_tmp3,   \_tmp2
+
+    xvilvl.w    \_tmp0,   \_in6,    \_in4
+    xvilvl.w    \_tmp1,   \_in7,    \_in5
+    xvilvh.w    \_tmp2,   \_in6,    \_in4
+    xvilvh.w    \_tmp3,   \_in7,    \_in5
+    xvilvl.w    \_out4,   \_tmp1,   \_tmp0
+    xvilvh.w    \_out5,   \_tmp1,   \_tmp0
+    xvilvl.w    \_out6,   \_tmp3,   \_tmp2
+    xvilvh.w    \_out7,   \_tmp3,   \_tmp2
+
+    xmov        \_tmp0,   \_out0
+    xmov        \_tmp1,   \_out1
+    xmov        \_tmp2,   \_out2
+    xmov        \_tmp3,   \_out3
+    xvpermi.q   \_out0,   \_out4,   0x02
+    xvpermi.q   \_out1,   \_out5,   0x02
+    xvpermi.q   \_out2,   \_out6,   0x02
+    xvpermi.q   \_out3,   \_out7,   0x02
+    xvpermi.q   \_out4,   \_tmp0,   0x31
+    xvpermi.q   \_out5,   \_tmp1,   0x31
+    xvpermi.q   \_out6,   \_tmp2,   0x31
+    xvpermi.q   \_out7,   \_tmp3,   0x31
+.endm
 
-.macro LASX_TRANSPOSE2x4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
-                             tmp0, tmp1, tmp2, tmp3, tmp4
-    xvilvh.h   \tmp4,    \in0,     \in1
-    xvilvl.h   \tmp1,    \in0,     \in1
-    xvilvh.h   \tmp0,    \in2,     \in3
-    xvilvl.h   \tmp3,    \in2,     \in3
-    xvilvh.w   \tmp2,    \tmp3,    \tmp1
-    xvilvl.w   \tmp1,    \tmp3,    \tmp1
-    xvilvh.w   \tmp3,    \tmp0,    \tmp4
-    xvilvl.w   \tmp4,    \tmp0,    \tmp4
-    xvilvh.d   \out0,    \tmp4,    \tmp1
-    xvilvl.d   \out2,    \tmp4,    \tmp1
-    xvilvh.d   \out1,    \tmp3,    \tmp2
-    xvilvl.d   \out3,    \tmp3,    \tmp2
+/*
+ * Description : Transpose 4x4 block with double-word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Example     : LASX_TRANSPOSE4x4_D
+ *        _in0 : 1,2,3,4
+ *        _in1 : 1,2,3,4
+ *        _in2 : 1,2,3,4
+ *        _in3 : 1,2,3,4
+ *
+ *       _out0 : 1,1,1,1
+ *       _out1 : 2,2,2,2
+ *       _out2 : 3,3,3,3
+ *       _out3 : 4,4,4,4
+ */
+.macro LASX_TRANSPOSE4x4_D _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3, \
+                           _tmp0, _tmp1
+    xvilvl.d    \_tmp0,   \_in1,    \_in0
+    xvilvh.d    \_out1,   \_in1,    \_in0
+    xvilvh.d    \_tmp1,   \_in3,    \_in2
+    xvilvl.d    \_out2,   \_in3,    \_in2
+
+    xvor.v      \_out0,   \_tmp0,   \_tmp0
+    xvor.v      \_out3,   \_tmp1,   \_tmp1
+
+    xvpermi.q   \_out0,   \_out2,   0x02
+    xvpermi.q   \_out2,   \_tmp0,   0x31
+    xvpermi.q   \_out3,   \_out1,   0x31
+    xvpermi.q   \_out1,   \_tmp1,   0x02
 .endm
+
diff --git a/common/loongarch/mc-a.S b/common/loongarch/mc-a.S
index 25246077..e51efe8f 100644
--- a/common/loongarch/mc-a.S
+++ b/common/loongarch/mc-a.S
@@ -24,7 +24,7 @@
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
 
-#include "asm.S"
+#include "loongson_asm.S"
 
 const ch_shuf
 .byte 0, 2, 2, 4, 4, 6, 6, 8, 1, 3, 3, 5, 5, 7, 7, 9
@@ -634,8 +634,8 @@ function hpel_filter_lasx
 endfunc
 
 /*
- * void pixel_avg_wxh( pixel *dst, intptr_t dst_stride, pixel *src1, intptr_t src1_stride,
- *                     pixel *src2, intptr_t src2_stride, int weight );
+ * void pixel_avg_wxh(pixel *dst, intptr_t dst_stride, pixel *src1, intptr_t src1_stride,
+ *                    pixel *src2, intptr_t src2_stride, int weight);
  */
 .macro PIXEL_AVG w, h
 function pixel_avg_\w\()x\h\()_lasx
@@ -987,7 +987,7 @@ function mc_copy_w16_lasx
     alsl.d            a0,     a1,    a0,   2
     alsl.d            a2,     a3,    a2,   2
     addi.w            a4,     a4,    -4
-    bnez              a4,    .LOOP_COPYW16
+    blt               zero,   a4,    .LOOP_COPYW16
 endfunc
 
 /*
@@ -1013,7 +1013,7 @@ function mc_copy_w8_lasx
     alsl.d            a0,     a1,     a0,   2
     alsl.d            a2,     a3,     a2,   2
     addi.w            a4,     a4,     -4
-    bnez              a4,     .LOOP_COPYW8
+    blt               zero,   a4,     .LOOP_COPYW8
 endfunc
 
 /*
@@ -1039,16 +1039,912 @@ function mc_copy_w4_lasx
     alsl.d            a0,     a1,     a0,   2
     alsl.d            a2,     a3,     a2,   2
     addi.w            a4,     a4,     -4
-    bnez              a4,     .LOOP_COPYW4
+    blt               zero,   a4,     .LOOP_COPYW4
 endfunc
 
 /*
- * void store_interleave_chroma( uint8_t *p_dst, intptr_t i_dst_stride,
- *                               uint8_t *p_src0, uint8_t *p_src1,
- *                               int32_t i_height )
+ * void *memcpy_aligned(void *dst, const void *src, size_t n)
+ */
+function memcpy_aligned_lasx
+    andi             t0,      a2,     16
+    beqz             t0,      2f
+    addi.d           a2,      a2,     -16
+    vld              vr0,     a1,     0
+    vst              vr0,     a0,     0
+    addi.d           a1,      a1,     16
+    addi.d           a0,      a0,     16
+2:
+    andi             t0,      a2,     32
+    beqz             t0,      3f
+    addi.d           a2,      a2,     -32
+    xvld             xr0,     a1,     0
+    xvst             xr0,     a0,     0
+    addi.d           a1,      a1,     32
+    addi.d           a0,      a0,     32
+3:
+    beqz             a2,      5f
+4:
+    addi.d           a2,      a2,     -64
+    xvld             xr0,     a1,     32
+    xvld             xr1,     a1,     0
+    xvst             xr0,     a0,     32
+    xvst             xr1,     a0,     0
+    addi.d           a1,      a1,     64
+    addi.d           a0,      a0,     64
+    blt              zero,    a2,     4b
+5:
+endfunc
+
+/*
+ * void memzero_aligned( void *p_dst, size_t n )
+ */
+function memzero_aligned_lasx
+    xvxor.v          xr1,     xr1,    xr1
+.memzero_loop:
+    addi.d           a1,      a1,     -128
+.rept 4
+    xvst             xr1,     a0,     0
+    addi.d           a0,      a0,     32
+.endr
+    blt              zero,    a1,     .memzero_loop
+endfunc
+
+/*
+ * void frame_init_lowres_core( pixel *src0, pixel *dst0, pixel *dsth,
+ *                              pixel *dstv, pixel *dstc, intptr_t src_stride,
+ *                              intptr_t dst_stride, int width, int height )
+ */
+function frame_init_lowres_core_lasx
+    andi             t1,      a7,     15
+    sub.w            t0,      a7,     t1
+    slli.d           t2,      a5,     1
+    ldptr.w          a7,      sp,     0  // use a7 as height variable
+
+.height_loop:
+    add.d            t4,      zero,   t0
+    addi.d           t3,      a0,     0
+    addi.d           t5,      a1,     0
+    addi.d           t6,      a2,     0
+    addi.d           t7,      a3,     0
+    addi.d           t8,      a4,     0
+.width16_loop:
+    xvld             xr0,     t3,     0
+    xvldx            xr1,     t3,     a5
+    xvldx            xr2,     t3,     t2
+    xvavgr.bu        xr3,     xr0,    xr1
+    xvavgr.bu        xr4,     xr1,    xr2
+    xvhaddw.hu.bu    xr5,     xr3,    xr3
+    xvhaddw.hu.bu    xr6,     xr4,    xr4
+    xvssrarni.bu.h   xr6,     xr5,    1
+    xvpermi.d        xr7,     xr6,    0xd8
+    vst              vr7,     t5,     0
+    xvpermi.q        xr7,     xr7,    0x11
+    vst              vr7,     t7,     0
+
+    addi.d           t3,      t3,     1
+    xvld             xr0,     t3,     0
+    xvldx            xr1,     t3,     a5
+    xvldx            xr2,     t3,     t2
+    xvavgr.bu        xr3,     xr0,    xr1
+    xvavgr.bu        xr4,     xr1,    xr2
+    xvhaddw.hu.bu    xr5,     xr3,    xr3
+    xvhaddw.hu.bu    xr6,     xr4,    xr4
+    xvssrarni.bu.h   xr6,     xr5,    1
+    xvpermi.d        xr7,     xr6,    0xd8
+    vst              vr7,     t6,     0
+    xvpermi.q        xr7,     xr7,    0x11
+    vst              vr7,     t8,     0
+    addi.d           t3,      t3,     31
+    addi.d           t5,      t5,     16
+    addi.d           t6,      t6,     16
+    addi.d           t7,      t7,     16
+    addi.d           t8,      t8,     16
+    addi.w           t4,      t4,     -16
+    blt              zero,    t4,     .width16_loop
+
+    beqz             t1,      .width16_end
+    vld              vr0,     t3,     0
+    vldx             vr1,     t3,     a5
+    vldx             vr2,     t3,     t2
+    vavgr.bu         vr3,     vr0,    vr1
+    vavgr.bu         vr4,     vr1,    vr2
+    vhaddw.hu.bu     vr5,     vr3,    vr3
+    vhaddw.hu.bu     vr6,     vr4,    vr4
+    vssrarni.bu.h    vr6,     vr5,    1
+    fst.d            f6,      t5,     0
+    vstelm.d         vr6,     t7,     0,    1
+
+    addi.d           t3,      t3,     1
+    vld              vr0,     t3,     0
+    vldx             vr1,     t3,     a5
+    vldx             vr2,     t3,     t2
+    vavgr.bu         vr3,     vr0,    vr1
+    vavgr.bu         vr4,     vr1,    vr2
+    vhaddw.hu.bu     vr5,     vr3,    vr3
+    vhaddw.hu.bu     vr6,     vr4,    vr4
+    vssrarni.bu.h    vr6,     vr5,    1
+    fst.d            f6,      t6,     0
+    vstelm.d         vr6,     t8,     0,    1
+
+.width16_end:
+    add.d            a0,      a0,     t2
+    add.d            a1,      a1,     a6
+    add.d            a2,      a2,     a6
+    add.d            a3,      a3,     a6
+    add.d            a4,      a4,     a6
+    addi.w           a7,      a7,     -1
+    blt              zero,    a7,     .height_loop
+endfunc
+
+/*
+ * void mc_chroma(uint8_t *p_dst_u, uint8_t *p_dst_v,
+ *                intptr_t i_dst_stride,
+ *                uint8_t *p_src, intptr_t i_src_stride,
+ *                int32_t m_vx, int32_t m_vy,
+ *                int32_t i_width, int32_t i_height)
+ */
+
+function mc_chroma_lsx
+    MC_CHROMA_START
+    andi             a5,      a5,     0x07    /* m_vx & 0x07 */
+    andi             a6,      a6,     0x07    /* m_vy & 0x07 */
+    li.d             t8,      8
+    sub.d            t1,      t8,     a5     // 8-d8x
+    sub.d            t2,      t8,     a6     // 8-d8y
+    mul.d            t3,      t1,     t2     // CA
+    mul.d            t4,      a5,     t2     // CB
+    mul.d            t5,      t1,     a6     // CC
+    mul.d            t6,      a5,     a6     // CD
+    vreplgr2vr.b     vr0,     t3
+    vreplgr2vr.b     vr1,     t4
+    vreplgr2vr.b     vr2,     t5
+    vreplgr2vr.b     vr3,     t6
+
+    add.d            t0,      a3,     a4
+    ldptr.w          t1,      sp,     0       /* i_height */
+    move             t3,      t0
+    addi.d           t4,      zero,   1
+    addi.d           t5,      zero,   3
+    addi.d           t6,      zero,   7
+    bge              t6,      a7,     .ENDLOOP_W8
+.LOOP_W8:
+    vld              vr4,     a3,     0
+    vld              vr5,     t0,     0
+    vld              vr6,     a3,     2
+    vld              vr7,     t0,     2
+    vmulwev.h.bu     vr8,     vr4,    vr0
+    vmulwod.h.bu     vr9,     vr4,    vr0
+    vmulwev.h.bu     vr10,    vr5,    vr2
+    vmulwod.h.bu     vr11,    vr5,    vr2
+    vmaddwev.h.bu    vr8,     vr6,    vr1
+    vmaddwod.h.bu    vr9,     vr6,    vr1
+    vmaddwev.h.bu    vr10,    vr7,    vr3
+    vmaddwod.h.bu    vr11,    vr7,    vr3
+    vadd.h           vr12,    vr8,    vr10
+    vadd.h           vr13,    vr9,    vr11
+    vssrarni.bu.h    vr13,    vr12,   6
+    vstelm.d         vr13,    a0,     0,   0
+    vstelm.d         vr13,    a1,     0,   1
+
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    addi.d           t1,      t1,     -1
+    move             a3,      t3
+    add.d            t3,      t3,     a4
+    move             t0,      t3
+    blt              zero,    t1,     .LOOP_W8
+    b                .ENDLOOP_W2
+.ENDLOOP_W8:
+    bge              t5,      a7,     .ENDLOOP_W4
+.LOOP_W4:
+    vld              vr4,     a3,     0
+    vld              vr5,     t0,     0
+    vld              vr6,     a3,     2
+    vld              vr7,     t0,     2
+    vmulwev.h.bu     vr8,     vr4,    vr0
+    vmulwod.h.bu     vr9,     vr4,    vr0
+    vmulwev.h.bu     vr10,    vr5,    vr2
+    vmulwod.h.bu     vr11,    vr5,    vr2
+    vmaddwev.h.bu    vr8,     vr6,    vr1
+    vmaddwod.h.bu    vr9,     vr6,    vr1
+    vmaddwev.h.bu    vr10,    vr7,    vr3
+    vmaddwod.h.bu    vr11,    vr7,    vr3
+    vadd.h           vr12,    vr8,    vr10
+    vadd.h           vr13,    vr9,    vr11
+    vssrarni.bu.h    vr13,    vr12,   6
+    vstelm.w         vr13,    a0,     0,   0
+    vstelm.w         vr13,    a1,     0,   2
+
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    move             a3,      t3
+    add.d            t3,      t3,     a4
+    move             t0,      t3
+    addi.d           t1,      t1,     -1
+    blt              zero,    t1,     .LOOP_W4
+    b                .ENDLOOP_W2
+.ENDLOOP_W4:
+    bge              t4,      a7,     .ENDLOOP_W2
+.LOOP_W2:
+    vld              vr4,     a3,     0
+    vld              vr5,     t0,     0
+    vld              vr6,     a3,     2
+    vld              vr7,     t0,     2
+    vmulwev.h.bu     vr8,     vr4,    vr0
+    vmulwod.h.bu     vr9,     vr4,    vr0
+    vmulwev.h.bu     vr10,    vr5,    vr2
+    vmulwod.h.bu     vr11,    vr5,    vr2
+    vmaddwev.h.bu    vr8,     vr6,    vr1
+    vmaddwod.h.bu    vr9,     vr6,    vr1
+    vmaddwev.h.bu    vr10,    vr7,    vr3
+    vmaddwod.h.bu    vr11,    vr7,    vr3
+    vadd.h           vr12,    vr8,    vr10
+    vadd.h           vr13,    vr9,    vr11
+    vssrarni.bu.h    vr13,    vr12,   6
+    vstelm.h         vr13,    a0,     0,   0
+    vstelm.h         vr13,    a1,     0,   4
+
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    move             a3,      t3
+    add.d            t3,      t3,     a4
+    move             t0,      t3
+    addi.d           t1,      t1,     -1
+    blt              zero,    t1,     .LOOP_W2
+.ENDLOOP_W2:
+endfunc
+
+function pixel_avg_weight_w4_lsx export=0
+    addi.d           t0,      zero,  64
+    sub.d            t0,      t0,    a6
+    vreplgr2vr.b     vr0,     a6
+    vreplgr2vr.b     vr1,     t0
+    vpackev.b        vr8,     vr1,   vr0
+.LOOP_AVG_WEIGHT_W4:
+    fld.s            f0,      a2,    0
+    fldx.s           f1,      a2,    a3
+    fld.s            f2,      a4,    0
+    fldx.s           f3,      a4,    a5
+    vilvl.w          vr0,     vr1,   vr0
+    vilvl.w          vr2,     vr3,   vr2
+    vilvl.b          vr0,     vr2,   vr0
+    vmulwev.h.bu.b   vr1,     vr0,   vr8
+    vmaddwod.h.bu.b  vr1,     vr0,   vr8
+    vssrarni.bu.h    vr1,     vr1,   6
+    fst.s            f1,      a0,    0
+    add.d            a0,      a0,    a1
+    vstelm.w         vr1,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a5,    a4,   1
+    addi.w           a7,      a7,    -2
+    bnez             a7,      .LOOP_AVG_WEIGHT_W4
+endfunc
+
+function pixel_avg_w4_lsx export=0
+.LOOP_AVG_W4:
+    fld.s            f0,      a2,    0
+    fldx.s           f1,      a2,    a3
+    fld.s            f4,      a4,    0
+    fldx.s           f5,      a4,    a5
+    vilvl.w          vr0,     vr1,   vr0
+    vilvl.w          vr4,     vr5,   vr4
+    vavgr.bu         vr0,     vr0,   vr4
+    fst.s            f0,      a0,    0
+    add.d            a0,      a0,    a1
+    vstelm.w         vr0,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a5,    a4,   1
+    addi.w           a7,      a7,    -2
+    bnez             a7,      .LOOP_AVG_W4
+endfunc
+
+function pixel_avg_weight_w8_lsx export=0
+    addi.d           t0,      zero,  64
+    sub.d            t0,      t0,    a6
+    slli.d           t5,      a1,    1
+    add.d            t6,      a1,    t5
+    add.d            t7,      a1,    t6
+    vreplgr2vr.b     vr0,     a6
+    vreplgr2vr.b     vr1,     t0
+    vpackev.b        vr8,     vr1,   vr0
+    PIXEL_AVG_START_W8
+.LOOP_AVG_HEIGHT_W8:
+    fld.d            f0,      a2,    0
+    fldx.d           f1,      a2,    a3
+    fldx.d           f2,      a2,    t0
+    fldx.d           f3,      a2,    t1
+    fld.d            f4,      a4,    0
+    fldx.d           f5,      a4,    a5
+    fldx.d           f6,      a4,    t3
+    fldx.d           f7,      a4,    t4
+    vilvl.b          vr0,     vr4,   vr0
+    vilvl.b          vr1,     vr5,   vr1
+    vilvl.b          vr2,     vr6,   vr2
+    vilvl.b          vr3,     vr7,   vr3
+    vmulwev.h.bu.b   vr4,     vr0,   vr8
+    vmulwev.h.bu.b   vr5,     vr1,   vr8
+    vmulwev.h.bu.b   vr6,     vr2,   vr8
+    vmulwev.h.bu.b   vr7,     vr3,   vr8
+    vmaddwod.h.bu.b  vr4,     vr0,   vr8
+    vmaddwod.h.bu.b  vr5,     vr1,   vr8
+    vmaddwod.h.bu.b  vr6,     vr2,   vr8
+    vmaddwod.h.bu.b  vr7,     vr3,   vr8
+    vssrarni.bu.h    vr4,     vr4,   6
+    vssrarni.bu.h    vr5,     vr5,   6
+    vssrarni.bu.h    vr6,     vr6,   6
+    vssrarni.bu.h    vr7,     vr7,   6
+    fst.d            f4,      a0,    0
+    fstx.d           f5,      a0,    a1
+    fstx.d           f6,      a0,    t5
+    fstx.d           f7,      a0,    t6
+    add.d            a0,      a0,    t7
+    alsl.d           a2,      a3,    a2,   2
+    alsl.d           a4,      a5,    a4,   2
+    addi.w           a7,      a7,    -4
+    bnez             a7,      .LOOP_AVG_HEIGHT_W8
+endfunc
+
+function pixel_avg_w8_lsx export=0
+    PIXEL_AVG_START_W8
+.LOOP_AVG_W8:
+    fld.d            f0,      a2,    0
+    fldx.d           f1,      a2,    a3
+    fldx.d           f2,      a2,    t0
+    fldx.d           f3,      a2,    t1
+    fld.d            f4,      a4,    0
+    fldx.d           f5,      a4,    a5
+    fldx.d           f6,      a4,    t3
+    fldx.d           f7,      a4,    t4
+    vilvl.d          vr0,     vr1,   vr0
+    vilvl.d          vr2,     vr3,   vr2
+    vilvl.d          vr4,     vr5,   vr4
+    vilvl.d          vr6,     vr7,   vr6
+    vavgr.bu         vr0,     vr0,   vr4
+    vavgr.bu         vr2,     vr2,   vr6
+    fst.d            f0,      a0,    0
+    add.d            a0,      a0,    a1
+    vstelm.d         vr0,     a0,    0,    1
+    fstx.d           f2,      a0,    a1
+    alsl.d           a0,      a1,    a0,   1
+    vstelm.d         vr2,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   2
+    alsl.d           a4,      a5,    a4,   2
+    addi.w           a7,      a7,    -4
+    bnez             a7,      .LOOP_AVG_W8
+endfunc
+
+function pixel_avg_weight_w16_lsx export=0
+    addi.d           t0,      zero,  64
+    sub.d            t0,      t0,    a6
+    vreplgr2vr.b     vr8,     a6
+    vreplgr2vr.b     vr9,     t0
+    PIXEL_AVG_START
+.LOOP_AVG_HEIGHT_W16:
+    LSX_LOADX_4 a2, a3, t0, t1, vr0, vr1, vr2, vr3
+    LSX_LOADX_4 a4, a5, t3, t4, vr4, vr5, vr6, vr7
+
+    vmulwev.h.bu.b   vr10,    vr0,   vr8
+    vmulwev.h.bu.b   vr11,    vr1,   vr8
+    vmulwev.h.bu.b   vr12,    vr2,   vr8
+    vmulwev.h.bu.b   vr13,    vr3,   vr8
+    vmulwod.h.bu.b   vr14,    vr0,   vr8
+    vmulwod.h.bu.b   vr15,    vr1,   vr8
+    vmulwod.h.bu.b   vr16,    vr2,   vr8
+    vmulwod.h.bu.b   vr17,    vr3,   vr8
+    vmaddwev.h.bu.b  vr10,    vr4,   vr9
+    vmaddwev.h.bu.b  vr11,    vr5,   vr9
+    vmaddwev.h.bu.b  vr12,    vr6,   vr9
+    vmaddwev.h.bu.b  vr13,    vr7,   vr9
+    vmaddwod.h.bu.b  vr14,    vr4,   vr9
+    vmaddwod.h.bu.b  vr15,    vr5,   vr9
+    vmaddwod.h.bu.b  vr16,    vr6,   vr9
+    vmaddwod.h.bu.b  vr17,    vr7,   vr9
+    vssrarni.bu.h    vr11,    vr10,  6
+    vssrarni.bu.h    vr13,    vr12,  6
+    vssrarni.bu.h    vr15,    vr14,  6
+    vssrarni.bu.h    vr17,    vr16,  6
+    vilvl.b          vr10,    vr15,  vr11
+    vilvh.b          vr11,    vr15,  vr11
+    vilvl.b          vr12,    vr17,  vr13
+    vilvh.b          vr13,    vr17,  vr13
+
+    vst              vr10,    a0,    0
+    vstx             vr11,    a0,    a1
+    vstx             vr12,    a0,    t6
+    vstx             vr13,    a0,    t7
+    add.d            a2,      a2,    t2
+    add.d            a4,      a4,    t5
+    add.d            a0,      a0,    t8
+    addi.d           a7,      a7,    -4
+    bnez             a7,      .LOOP_AVG_HEIGHT_W16
+endfunc
+
+function pixel_avg_w16_lsx export=0
+    PIXEL_AVG_START
+.LOOP_AVG_W16:
+    vld              vr0,     a2,    0
+    vldx             vr1,     a2,    a3
+    vldx             vr2,     a2,    t0
+    vldx             vr3,     a2,    t1
+    vld              vr4,     a4,    0
+    vldx             vr5,     a4,    a5
+    vldx             vr6,     a4,    t3
+    vldx             vr7,     a4,    t4
+    vavgr.bu         vr0,     vr0,   vr4
+    vavgr.bu         vr1,     vr1,   vr5
+    vavgr.bu         vr2,     vr2,   vr6
+    vavgr.bu         vr3,     vr3,   vr7
+    vst              vr0,     a0,    0
+    vstx             vr1,     a0,    a1
+    vstx             vr2,     a0,    t6
+    vstx             vr3,     a0,    t7
+    add.d            a0,      a0,    t8
+    add.d            a2,      a2,    t2
+    add.d            a4,      a4,    t5
+
+    vld              vr0,     a2,    0
+    vldx             vr1,     a2,    a3
+    vldx             vr2,     a2,    t0
+    vldx             vr3,     a2,    t1
+    vld              vr4,     a4,    0
+    vldx             vr5,     a4,    a5
+    vldx             vr6,     a4,    t3
+    vldx             vr7,     a4,    t4
+    vavgr.bu         vr0,     vr0,   vr4
+    vavgr.bu         vr1,     vr1,   vr5
+    vavgr.bu         vr2,     vr2,   vr6
+    vavgr.bu         vr3,     vr3,   vr7
+    vst              vr0,     a0,    0
+    vstx             vr1,     a0,    a1
+    vstx             vr2,     a0,    t6
+    vstx             vr3,     a0,    t7
+    add.d            a2,      a2,    t2
+    add.d            a4,      a4,    t5
+    add.d            a0,      a0,    t8
+    addi.d           a7,      a7,    -8
+    bnez             a7,      .LOOP_AVG_W16
+endfunc
+
+/*
+ * void pixel_avg_wxh(pixel *dst, intptr_t dst_stride, pixel *src1, intptr_t src1_stride,
+ *                    pixel *src2, intptr_t src2_stride, int weight);
+ */
+.macro PIXEL_AVG_LSX w, h
+function pixel_avg_\w\()x\h\()_lsx
+    addi.d           t0,      a6,    -32
+    addi.d           a7,      zero,  \h
+    bne              t0,      zero,  pixel_avg_weight_w\w\()_lsx
+    b                pixel_avg_w\w\()_lsx
+endfunc
+.endm
+
+PIXEL_AVG_LSX 16, 16
+PIXEL_AVG_LSX 16,  8
+PIXEL_AVG_LSX  8, 16
+PIXEL_AVG_LSX  8,  8
+PIXEL_AVG_LSX  8,  4
+PIXEL_AVG_LSX  4, 16
+PIXEL_AVG_LSX  4,  8
+PIXEL_AVG_LSX  4,  4
+PIXEL_AVG_LSX  4,  2
+
+function mc_weight_w20_noden_lsx
+    vldrepl.b        vr0,     a4,    36   // scale
+    vldrepl.h        vr1,     a4,    40   // offset
+.LOOP_WEIGHT_W20_NODEN:
+    vld              vr3,     a2,    0
+    vld              vr4,     a2,    16
+    add.d            a2,      a2,    a3
+    vld              vr5,     a2,    0
+    vld              vr6,     a2,    16
+    vilvl.w          vr4,     vr6,   vr4
+    vmulwev.h.bu.b   vr7,     vr3,   vr0
+    vmulwod.h.bu.b   vr8,     vr3,   vr0
+    vmulwev.h.bu.b   vr9,     vr4,   vr0
+    vmulwod.h.bu.b   vr10,    vr4,   vr0
+    vmulwev.h.bu.b   vr11,    vr5,   vr0
+    vmulwod.h.bu.b   vr12,    vr5,   vr0
+    vadd.h           vr7,     vr7,   vr1
+    vadd.h           vr8,     vr8,   vr1
+    vadd.h           vr9,     vr9,   vr1
+    vadd.h           vr10,    vr10,  vr1
+    vadd.h           vr11,    vr11,  vr1
+    vadd.h           vr12,    vr12,  vr1
+    vssrani.bu.h     vr11,    vr7,   0
+    vssrani.bu.h     vr12,    vr8,   0
+    vssrani.bu.h     vr9,     vr9,   0
+    vssrani.bu.h     vr10,    vr10,  0
+    vilvl.b          vr7,     vr12,  vr11
+    vilvl.b          vr9,     vr10,  vr9
+    vilvh.b          vr11,    vr12,  vr11
+
+    vst              vr7,     a0,     0
+    vstelm.w         vr9,     a0,     16,    0
+    add.d            a0,      a0,     a1
+    vst              vr11,    a0,     0
+    vstelm.w         vr9,     a0,     16,    1
+    add.d            a0,      a0,     a1
+    add.d            a2,      a2,     a3
+    addi.w           a5,      a5,     -2
+    blt              zero,    a5,     .LOOP_WEIGHT_W20_NODEN
+endfunc
+
+function mc_weight_w16_noden_lsx
+    vldrepl.b        vr0,     a4,    36   // scale
+    vldrepl.h        vr1,     a4,    40   // offset
+.LOOP_WEIGHT_W16_NODEN:
+    vld              vr3,     a2,    0
+    vldx             vr4,     a2,    a3
+    vmulwev.h.bu.b   vr5,     vr3,   vr0
+    vmulwod.h.bu.b   vr6,     vr3,   vr0
+    vmulwev.h.bu.b   vr7,     vr4,   vr0
+    vmulwod.h.bu.b   vr8,     vr4,   vr0
+    vadd.h           vr5,     vr5,   vr1
+    vadd.h           vr6,     vr6,   vr1
+    vadd.h           vr7,     vr7,   vr1
+    vadd.h           vr8,     vr8,   vr1
+    vssrani.bu.h     vr7,     vr5,   0
+    vssrani.bu.h     vr8,     vr6,   0
+    vilvl.b          vr5,     vr8,   vr7
+    vilvh.b          vr7,     vr8,   vr7
+    vst              vr5,     a0,    0
+    vstx             vr7,     a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a0,      a1,    a0,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHT_W16_NODEN
+endfunc
+
+function mc_weight_w8_noden_lsx
+    vldrepl.b        vr0,     a4,    36   // scale
+    vldrepl.h        vr1,     a4,    40   // offset
+.LOOP_WEIGHT_W8_NODEN:
+    fld.d            f3,      a2,    0
+    fldx.d           f4,      a2,    a3
+    vilvl.d          vr3,     vr4,   vr3
+    vmulwev.h.bu.b   vr5,     vr3,   vr0
+    vmulwod.h.bu.b   vr6,     vr3,   vr0
+    vadd.h           vr5,     vr5,   vr1
+    vadd.h           vr6,     vr6,   vr1
+    vssrani.bu.h     vr5,     vr5,   0
+    vssrani.bu.h     vr6,     vr6,   0
+    vilvl.b          vr7,     vr6,   vr5
+    vstelm.d         vr7,     a0,    0,    0
+    add.d            a0,      a0,    a1
+    vstelm.d         vr7,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHT_W8_NODEN
+endfunc
+
+function mc_weight_w4_noden_lsx
+    vldrepl.h        vr0,     a4,    36   // scale
+    vldrepl.h        vr1,     a4,    40   // offset
+.LOOP_WEIGHT_W4_NODEN:
+    fld.s            f3,      a2,    0
+    fldx.s           f4,      a2,    a3
+    vilvl.w          vr3,     vr4,   vr3
+    vsllwil.hu.bu    vr3,     vr3,   0
+    vmul.h           vr3,     vr3,   vr0
+    vadd.h           vr3,     vr3,   vr1
+    vssrani.bu.h     vr3,     vr3,   0
+    vstelm.w         vr3,     a0,    0,    0
+    add.d            a0,      a0,    a1
+    vstelm.w         vr3,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHT_W4_NODEN
+endfunc
+
+function mc_weight_w20_lsx
+    vldrepl.h        vr1,     a4,    40   // offset
+    vldrepl.b        vr0,     a4,    36   // scale
+    vldrepl.h        vr2,     a4,    32   // denom
+    vsll.h           vr1,     vr1,   vr2
+.LOOP_WEIGHT_W20:
+    vld              vr3,     a2,    0
+    vld              vr4,     a2,    16
+    add.d            a2,      a2,    a3
+    vld              vr5,     a2,    0
+    vld              vr6,     a2,    16
+    vilvl.w          vr4,     vr6,   vr4
+
+    vmulwev.h.bu.b   vr7,     vr3,   vr0
+    vmulwod.h.bu.b   vr8,     vr3,   vr0
+    vmulwev.h.bu.b   vr9,     vr4,   vr0
+    vmulwod.h.bu.b   vr10,    vr4,   vr0
+    vmulwev.h.bu.b   vr11,    vr5,   vr0
+    vmulwod.h.bu.b   vr12,    vr5,   vr0
+    vsadd.h          vr7,     vr7,   vr1
+    vsadd.h          vr8,     vr8,   vr1
+    vsadd.h          vr9,     vr9,   vr1
+    vsadd.h          vr10,    vr10,  vr1
+    vsadd.h          vr11,    vr11,  vr1
+    vsadd.h          vr12,    vr12,  vr1
+    vssrarn.bu.h     vr7,     vr7,   vr2
+    vssrarn.bu.h     vr8,     vr8,   vr2
+    vssrarn.bu.h     vr9,     vr9,   vr2
+    vssrarn.bu.h     vr10,    vr10,  vr2
+    vssrarn.bu.h     vr11,    vr11,  vr2
+    vssrarn.bu.h     vr12,    vr12,  vr2
+    vilvl.b          vr7,     vr8,   vr7
+    vilvl.b          vr9,     vr10,  vr9
+    vilvl.b          vr11,    vr12,  vr11
+
+    vst              vr7,     a0,     0
+    vstelm.w         vr9,     a0,     16,    0
+    add.d            a0,      a0,     a1
+    vst              vr11,    a0,     0
+    vstelm.w         vr9,     a0,     16,    1
+    add.d            a0,      a0,     a1
+    add.d            a2,      a2,     a3
+    addi.w           a5,      a5,     -2
+    blt              zero,    a5,     .LOOP_WEIGHT_W20
+endfunc
+
+function mc_weight_w16_lsx
+    vldrepl.h        vr1,     a4,    40   // offset
+    vldrepl.b        vr0,     a4,    36   // scale
+    vldrepl.h        vr2,     a4,    32   // denom
+    vsll.h           vr1,     vr1,   vr2
+.LOOP_WEIGHT_W16:
+    vld              vr3,     a2,    0
+    vldx             vr4,     a2,    a3
+    vmulwev.h.bu.b   vr5,     vr3,   vr0
+    vmulwod.h.bu.b   vr6,     vr3,   vr0
+    vmulwev.h.bu.b   vr7,     vr4,   vr0
+    vmulwod.h.bu.b   vr8,     vr4,   vr0
+    vsadd.h          vr5,     vr5,   vr1
+    vsadd.h          vr6,     vr6,   vr1
+    vsadd.h          vr7,     vr7,   vr1
+    vsadd.h          vr8,     vr8,   vr1
+    vssrarn.bu.h     vr5,     vr5,   vr2
+    vssrarn.bu.h     vr6,     vr6,   vr2
+    vssrarn.bu.h     vr7,     vr7,   vr2
+    vssrarn.bu.h     vr8,     vr8,   vr2
+    vilvl.b          vr5,     vr6,   vr5
+    vilvl.b          vr7,     vr8,   vr7
+    vst              vr5,     a0,    0
+    vstx             vr7,     a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a0,      a1,    a0,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHT_W16
+endfunc
+
+function mc_weight_w8_lsx
+    vldrepl.h        vr1,     a4,    40   // offset
+    vldrepl.b        vr0,     a4,    36   // scale
+    vldrepl.h        vr2,     a4,    32   // denom
+    vsll.h           vr1,     vr1,   vr2
+.LOOP_WEIGHT_W8:
+    fld.d            f3,      a2,    0
+    fldx.d           f4,      a2,    a3
+    vilvl.d          vr3,     vr4,   vr3
+    vmulwev.h.bu.b   vr5,     vr3,   vr0
+    vmulwod.h.bu.b   vr6,     vr3,   vr0
+    vsadd.h          vr5,     vr5,   vr1
+    vsadd.h          vr6,     vr6,   vr1
+    vssrarn.bu.h     vr5,     vr5,   vr2
+    vssrarn.bu.h     vr6,     vr6,   vr2
+    vilvl.b          vr7,     vr6,   vr5
+    vstelm.d         vr7,     a0,    0,    0
+    add.d            a0,      a0,    a1
+    vstelm.d         vr7,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHT_W8
+endfunc
+
+function mc_weight_w4_lsx
+    vldrepl.h        vr1,     a4,    40   // offset
+    vldrepl.h        vr0,     a4,    36   // scale
+    vldrepl.h        vr2,     a4,    32   // denom
+    vsll.h           vr1,     vr1,   vr2
+.LOOP_WEIGHT_W4:
+    fld.s            f3,      a2,    0
+    fldx.s           f4,      a2,    a3
+    vilvl.w          vr3,     vr4,   vr3
+    vsllwil.hu.bu    vr3,     vr3,   0
+    vmul.h           vr3,     vr3,   vr0
+    vsadd.h          vr3,     vr3,   vr1
+    vssrarn.bu.h     vr3,     vr3,   vr2
+    vstelm.w         vr3,     a0,    0,    0
+    add.d            a0,      a0,    a1
+    vstelm.w         vr3,     a0,    0,    1
+    add.d            a0,      a0,    a1
+    alsl.d           a2,      a3,    a2,   1
+    addi.w           a5,      a5,    -2
+    blt              zero,    a5,    .LOOP_WEIGHT_W4
+endfunc
+
+/*
+ * void x264_pixel_avg2_w4(uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+ *                         intptr_t i_src_stride, uint8_t *src2, int i_height)
+ */
+function pixel_avg2_w4_lsx
+.LOOP_AVG2_W4:
+    addi.d           a5,      a5,    -2
+    fld.s            f0,      a2,    0
+    fld.s            f1,      a4,    0
+    fldx.s           f2,      a2,    a3
+    fldx.s           f3,      a4,    a3
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a3,    a4,   1
+    vavgr.bu         vr0,     vr0,   vr1
+    vavgr.bu         vr1,     vr2,   vr3
+    fst.s            f0,      a0,    0
+    fstx.s           f1,      a0,    a1
+    alsl.d           a0,      a1,    a0,   1
+    blt              zero,    a5,    .LOOP_AVG2_W4
+endfunc
+
+/*
+ * void x264_pixel_avg2_w8(uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+ *                         intptr_t i_src_stride, uint8_t *src2, int i_height)
+ */
+function pixel_avg2_w8_lsx
+.LOOP_AVG2_W8:
+    addi.d           a5,      a5,    -2
+    fld.d            f0,      a2,    0
+    fld.d            f1,      a4,    0
+    fldx.d           f2,      a2,    a3
+    fldx.d           f3,      a4,    a3
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a3,    a4,   1
+    vavgr.bu         vr0,     vr0,   vr1
+    vavgr.bu         vr1,     vr2,   vr3
+    fst.d            f0,      a0,    0
+    fstx.d           f1,      a0,    a1
+    alsl.d           a0,      a1,    a0,   1
+    blt              zero,    a5,    .LOOP_AVG2_W8
+endfunc
+
+/*
+ * void x264_pixel_avg2_w16(uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+ *                          intptr_t i_src_stride, uint8_t *src2, int i_height)
+ */
+function pixel_avg2_w16_lsx
+.LOOP_AVG2_W16:
+    addi.d           a5,      a5,    -2
+    vld              vr0,     a2,    0
+    vldx             vr1,     a2,    a3
+    vld              vr2,     a4,    0
+    vldx             vr3,     a4,    a3
+    alsl.d           a2,      a3,    a2,   1
+    alsl.d           a4,      a3,    a4,   1
+    vavgr.bu         vr0,     vr0,   vr2
+    vavgr.bu         vr1,     vr1,   vr3
+    vst              vr0,     a0,    0
+    vstx             vr1,     a0,    a1
+    alsl.d           a0,      a1,    a0,   1
+    blt              zero,    a5,    .LOOP_AVG2_W16
+endfunc
+
+/*
+ * void x264_pixel_avg2_w20(uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+ *                          intptr_t i_src_stride, uint8_t *src2, int i_height)
+ */
+function pixel_avg2_w20_lsx
+.LOOP_AVG2_W20:
+    addi.d           a5,      a5,    -2
+    vld              vr0,     a2,    0
+    vld              vr1,     a2,    16
+    vld              vr2,     a4,    0
+    vld              vr3,     a4,    16
+    add.d            a2,      a2,    a3
+    add.d            a4,      a4,    a3
+    vld              vr4,     a2,    0
+    vld              vr5,     a2,    16
+    vld              vr6,     a4,    0
+    vld              vr7,     a4,    16
+    vavgr.bu         vr0,     vr0,   vr2
+    vavgr.bu         vr1,     vr1,   vr3
+    vavgr.bu         vr4,     vr4,   vr6
+    vavgr.bu         vr5,     vr5,   vr7
+
+    vst              vr0,     a0,    0
+    vstelm.w         vr1,     a0,    16,   0
+    add.d            a0,      a0,    a1
+    vst              vr4,     a0,    0
+    vstelm.w         vr5,     a0,    16,   0
+    add.d            a2,      a2,    a3
+    add.d            a4,      a4,    a3
+    add.d            a0,      a0,    a1
+    blt              zero,    a5,    .LOOP_AVG2_W20
+endfunc
+
+/*
+ * void mc_copy_width16( uint8_t *p_dst, int32_t i_dst_stride,
+ *                       uint8_t *p_src, int32_t i_src_stride,
+ *                       int32_t i_height )
+ */
+function mc_copy_w16_lsx
+    slli.d            t0,     a3,     1
+    add.d             t1,     t0,     a3
+    slli.d            t2,     a1,     1
+    add.d             t3,     t2,     a1
+.LOOP_COPY_W16:
+    vld               vr1,    a2,     0
+    vldx              vr2,    a2,     a3
+    vldx              vr3,    a2,     t0
+    vldx              vr4,    a2,     t1
+
+    vst               vr1,    a0,     0
+    vstx              vr2,    a0,     a1
+    vstx              vr3,    a0,     t2
+    vstx              vr4,    a0,     t3
+    alsl.d            a0,     a1,     a0,   2
+    alsl.d            a2,     a3,     a2,   2
+    addi.w            a4,     a4,     -4
+    blt               zero,   a4,     .LOOP_COPY_W16
+endfunc
+
+/*
+ * void mc_copy_w8(uint8_t *p_dst, intptr_t i_dst_stride,
+ *                 uint8_t *p_src, intptr_t i_src_stride,
+ *                 int32_t i_height)
+ */
+function mc_copy_w8_lsx
+    slli.d            t0,     a3,     1
+    add.d             t1,     t0,     a3
+    slli.d            t2,     a1,     1
+    add.d             t3,     t2,     a1
+.LOOP_COPY_W8:
+    fld.d             f0,     a2,     0
+    fldx.d            f1,     a2,     a3
+    fldx.d            f2,     a2,     t0
+    fldx.d            f3,     a2,     t1
+
+    fst.d             f0,     a0,     0
+    fstx.d            f1,     a0,     a1
+    fstx.d            f2,     a0,     t2
+    fstx.d            f3,     a0,     t3
+    alsl.d            a0,     a1,     a0,   2
+    alsl.d            a2,     a3,     a2,   2
+    addi.w            a4,     a4,     -4
+    blt               zero,   a4,     .LOOP_COPY_W8
+endfunc
+
+/*
+ * void mc_copy_w4(uint8_t *p_dst, intptr_t i_dst_stride,
+ *                 uint8_t *p_src, intptr_t i_src_stride,
+ *                 int32_t i_height)
+ */
+function mc_copy_w4_lsx
+    slli.d            t0,     a3,     1
+    add.d             t1,     t0,     a3
+    slli.d            t2,     a1,     1
+    add.d             t3,     t2,     a1
+.LOOP_COPY_W4:
+    fld.s             f0,     a2,     0
+    fldx.s            f1,     a2,     a3
+    fldx.s            f2,     a2,     t0
+    fldx.s            f3,     a2,     t1
+
+    fst.s             f0,     a0,     0
+    fstx.s            f1,     a0,     a1
+    fstx.s            f2,     a0,     t2
+    fstx.s            f3,     a0,     t3
+    alsl.d            a0,     a1,     a0,   2
+    alsl.d            a2,     a3,     a2,   2
+    addi.w            a4,     a4,     -4
+    blt               zero,   a4,     .LOOP_COPY_W4
+endfunc
+
+/*
+ * void store_interleave_chroma(uint8_t *p_dst, intptr_t i_dst_stride,
+ *                              uint8_t *p_src0, uint8_t *p_src1,
+ *                              int32_t i_height)
  */
-function store_interleave_chroma_lasx
-.LOOP_interleave_chroma:
+function store_interleave_chroma_lsx
+.loop_interleave_chroma:
     fld.d             f0,     a2,    0
     fld.d             f1,     a3,    0
     addi.d            a2,     a2,    FDEC_STRIDE
@@ -1057,18 +1953,18 @@ function store_interleave_chroma_lasx
     vst               vr0,    a0,    0
     add.d             a0,     a0,    a1
     addi.w            a4,     a4,    -1
-    blt               zero,   a4,    .LOOP_interleave_chroma
+    blt               zero,   a4,    .loop_interleave_chroma
 endfunc
 
 /*
- * void load_deinterleave_chroma_fenc( pixel *dst, pixel *src,
- *                                     intptr_t i_src, int height )
+ * void load_deinterleave_chroma_fenc(pixel *dst, pixel *src,
+ *                                    intptr_t i_src, int height)
  */
-function load_deinterleave_chroma_fenc_lasx
+function load_deinterleave_chroma_fenc_lsx
     addi.d            t0,     a0,     FENC_STRIDE/2
     andi              t1,     a3,     1
     sub.w             t2,     a3,     t1
-.LOOP_deinterleave_fenc:
+.loop_deinterleave_fenc:
     vld               vr0,    a1,     0
     vldx              vr1,    a1,     a2
     vpickev.b         vr2,    vr1,    vr0
@@ -1081,26 +1977,26 @@ function load_deinterleave_chroma_fenc_lasx
     addi.d            t0,     t0,     FENC_STRIDE * 2
     alsl.d            a1,     a2,     a1,    1
     addi.w            t2,     t2,     -2
-    blt               zero,   t2,     .LOOP_deinterleave_fenc
+    blt               zero,   t2,     .loop_deinterleave_fenc
 
-    beqz              t1,     .LOOP_deinterleave_fenc_end
+    beqz              t1,     .loop_deinterleave_fenc_end
     vld               vr0,    a1,     0
     vpickev.b         vr1,    vr0,    vr0
     vpickod.b         vr2,    vr0,    vr0
     fst.d             f1,     a0,     0
     fst.d             f2,     t0,     0
-.LOOP_deinterleave_fenc_end:
+.loop_deinterleave_fenc_end:
 endfunc
 
 /*
- * void load_deinterleave_chroma_fdec( pixel *dst, pixel *src,
- *                                     intptr_t i_src, int height )
+ * void load_deinterleave_chroma_fdec(pixel *dst, pixel *src,
+ *                                    intptr_t i_src, int height)
  */
-function load_deinterleave_chroma_fdec_lasx
+function load_deinterleave_chroma_fdec_lsx
     addi.d            t0,     a0,     FDEC_STRIDE/2
     andi              t1,     a3,     1
     sub.w             t2,     a3,     t1
-.LOOP_deinterleave_fdec:
+.loop_deinterleave_fdec:
     vld               vr0,    a1,     0
     vldx              vr1,    a1,     a2
     vpickev.b         vr2,    vr1,    vr0
@@ -1113,29 +2009,29 @@ function load_deinterleave_chroma_fdec_lasx
     addi.d            t0,     t0,     FDEC_STRIDE * 2
     alsl.d            a1,     a2,     a1,    1
     addi.w            t2,     t2,     -2
-    blt               zero,   t2,     .LOOP_deinterleave_fdec
+    blt               zero,   t2,     .loop_deinterleave_fdec
 
-    beqz              t1,     .LOOP_deinterleave_fdec_end
+    beqz              t1,     .loop_deinterleave_fdec_end
     vld               vr0,    a1,     0
     vpickev.b         vr1,    vr0,    vr0
     vpickod.b         vr2,    vr0,    vr0
     fst.d             f1,     a0,     0
     fst.d             f2,     t0,     0
-.LOOP_deinterleave_fdec_end:
+.loop_deinterleave_fdec_end:
 endfunc
 
 /*
- * x264_plane_copy_interleave_c( pixel *dst,  intptr_t i_dst,
- *                               pixel *srcu, intptr_t i_srcu,
- *                               pixel *srcv, intptr_t i_srcv, int w, int h )
+ * x264_plane_copy_interleave(pixel *dst,  intptr_t i_dst,
+ *                            pixel *srcu, intptr_t i_srcu,
+ *                            pixel *srcv, intptr_t i_srcv, int w, int h)
  */
-function plane_copy_interleave_core_lasx
-.LOOP_h:
+function plane_copy_interleave_core_lsx
+.loop_h:
     add.d             t0,     a0,     zero
     add.d             t2,     a2,     zero
     add.d             t4,     a4,     zero
     add.d             t6,     a6,     zero
-.LOOP_copy_interleavew16:
+.loop_copy_interleavew16:
     vld               vr0,    t2,     0
     vld               vr1,    t4,     0
     vilvl.b           vr2,    vr1,    vr0
@@ -1146,27 +2042,27 @@ function plane_copy_interleave_core_lasx
     addi.d            t4,     t4,     16
     addi.d            t0,     t0,     32
     addi.w            t6,     t6,     -16
-    blt               zero,   t6,     .LOOP_copy_interleavew16
+    blt               zero,   t6,     .loop_copy_interleavew16
 
     add.d             a2,     a2,     a3
     add.d             a4,     a4,     a5
     add.d             a0,     a0,     a1
     addi.w            a7,     a7,     -1
-    blt               zero,   a7,     .LOOP_h
+    blt               zero,   a7,     .loop_h
 endfunc
 
 /*
- * void x264_plane_copy_deinterleave_c( pixel *dsta, intptr_t i_dsta,
- *                                      pixel *dstb, intptr_t i_dstb,
- *                                      pixel *src,  intptr_t i_src, int w, int h )
+ * void x264_plane_copy_deinterleave(pixel *dsta, intptr_t i_dsta,
+ *                                   pixel *dstb, intptr_t i_dstb,
+ *                                   pixel *src,  intptr_t i_src, int w, int h)
  */
-function plane_copy_deinterleave_lasx
-.LOOP_deinterleave_h:
+function plane_copy_deinterleave_lsx
+.LOOP_PLANE_COPY_H:
     add.d             t0,     a0,     zero
     add.d             t2,     a2,     zero
     add.d             t4,     a4,     zero
     add.d             t6,     a6,     zero
-.LOOP_copy_deinterleavew16:
+.LOOP_PLANE_COPY_W16:
     vld               vr0,    t4,     0
     vld               vr1,    t4,     16
     vpickev.b         vr2,    vr1,    vr0
@@ -1177,19 +2073,19 @@ function plane_copy_deinterleave_lasx
     addi.d            t0,     t0,     16
     addi.d            t2,     t2,     16
     addi.w            t6,     t6,     -16
-    blt               zero,   t6,     .LOOP_copy_deinterleavew16
+    blt               zero,   t6,     .LOOP_PLANE_COPY_W16
 
     add.d             a2,     a2,     a3
     add.d             a4,     a4,     a5
     add.d             a0,     a0,     a1
     addi.w            a7,     a7,     -1
-    blt               zero,   a7,     .LOOP_deinterleave_h
+    blt               zero,   a7,     .LOOP_PLANE_COPY_H
 endfunc
 
 /*
- * void prefetch_ref( uint8_t *pix, intptr_t stride, int32_t parity )
+ * void prefetch_ref(uint8_t *pix, intptr_t stride, int32_t parity)
  */
-function prefetch_ref_lasx
+function prefetch_ref_lsx
     addi.d            a2,     a2,     -1
     addi.d            a0,     a0,     64
     and               a2,     a2,     a1
@@ -1213,11 +2109,11 @@ function prefetch_ref_lasx
 endfunc
 
 /*
- * void prefetch_fenc_422( uint8_t *pix_y, intptr_t stride_y,
- *                         uint8_t *pix_uv, intptr_t stride_uv,
- *                         int32_t mb_x )
+ * void prefetch_fenc_422(uint8_t *pix_y, intptr_t stride_y,
+ *                        uint8_t *pix_uv, intptr_t stride_uv,
+ *                        int32_t mb_x)
  */
-function prefetch_fenc_422_lasx
+function prefetch_fenc_422_lsx
     andi              t0,     a4,     3
     mul.d             t0,     t0,     a1
     andi              a4,     a4,     6
@@ -1243,11 +2139,11 @@ function prefetch_fenc_422_lasx
 endfunc
 
 /*
- * void prefetch_fenc_420( uint8_t *pix_y, intptr_t stride_y,
- *                         uint8_t *pix_uv, intptr_t stride_uv,
- *                         int32_t mb_x )
+ * void prefetch_fenc_420(uint8_t *pix_y, intptr_t stride_y,
+ *                        uint8_t *pix_uv, intptr_t stride_uv,
+ *                        int32_t mb_x)
  */
-function prefetch_fenc_420_lasx
+function prefetch_fenc_420_lsx
     andi             t0,      a4,     3
     mul.d            t0,      t0,     a1
     andi             a4,      a4,     6
@@ -1271,7 +2167,7 @@ endfunc
 /*
  * void *memcpy_aligned(void *dst, const void *src, size_t n)
  */
-function memcpy_aligned_lasx
+function memcpy_aligned_lsx
     andi             t0,      a2,     16
     beqz             t0,      2f
     addi.d           a2,      a2,     -16
@@ -1283,18 +2179,24 @@ function memcpy_aligned_lasx
     andi             t0,      a2,     32
     beqz             t0,      3f
     addi.d           a2,      a2,     -32
-    xvld             xr0,     a1,     0
-    xvst             xr0,     a0,     0
+    vld              vr0,     a1,     0
+    vld              vr1,     a1,     16
+    vst              vr0,     a0,     0
+    vst              vr1,     a0,     16
     addi.d           a1,      a1,     32
     addi.d           a0,      a0,     32
 3:
     beqz             a2,      5f
 4:
     addi.d           a2,      a2,     -64
-    xvld             xr0,     a1,     32
-    xvld             xr1,     a1,     0
-    xvst             xr0,     a0,     32
-    xvst             xr1,     a0,     0
+    vld              vr0,     a1,     48
+    vld              vr1,     a1,     32
+    vld              vr2,     a1,     16
+    vld              vr3,     a1,     0
+    vst              vr0,     a0,     48
+    vst              vr1,     a0,     32
+    vst              vr2,     a0,     16
+    vst              vr3,     a0,     0
     addi.d           a1,      a1,     64
     addi.d           a0,      a0,     64
     blt              zero,    a2,     4b
@@ -1302,103 +2204,733 @@ function memcpy_aligned_lasx
 endfunc
 
 /*
- * void memzero_aligned( void *p_dst, size_t n )
+ * void memzero_aligned(void *p_dst, size_t n)
  */
-function memzero_aligned_lasx
-    xvxor.v          xr1,     xr1,    xr1
-.memzero_loop:
+function memzero_aligned_lsx
+    vxor.v           vr1,     vr1,    vr1
+.loop_memzero:
     addi.d           a1,      a1,     -128
-.rept 4
-    xvst             xr1,     a0,     0
-    addi.d           a0,      a0,     32
-.endr
-    blt              zero,    a1,     .memzero_loop
+    vst              vr1,     a0,     0
+    vst              vr1,     a0,     16
+    vst              vr1,     a0,     32
+    vst              vr1,     a0,     48
+    vst              vr1,     a0,     64
+    vst              vr1,     a0,     80
+    vst              vr1,     a0,     96
+    vst              vr1,     a0,     112
+    addi.d           a0,      a0,     128
+    blt              zero,    a1,     .loop_memzero
 endfunc
 
 /*
- * void frame_init_lowres_core( pixel *src0, pixel *dst0, pixel *dsth,
- *                              pixel *dstv, pixel *dstc, intptr_t src_stride,
- *                              intptr_t dst_stride, int width, int height )
+ * void hpel_filter( uint8_t *dsth, uint8_t *dstv, uint8_t *dstc,
+ *                   uint8_t *src, intptr_t stride, int width, int height )
  */
-function frame_init_lowres_core_lasx
-    andi             t1,      a7,     15
-    sub.w            t0,      a7,     t1
-    slli.d           t2,      a5,     1
-    ldptr.w          a7,      sp,     0  // use a7 as height variable
+function hpel_filter_lsx
+    addi.d           sp,      sp,    -64
+    fst.d            f24,     sp,    0
+    fst.d            f25,     sp,    8
+    fst.d            f26,     sp,    16
+    fst.d            f27,     sp,    24
+    fst.d            f28,     sp,    32
+    fst.d            f29,     sp,    40
+    fst.d            f30,     sp,    48
+    fst.d            f31,     sp,    56
 
-.height_loop:
-    add.d            t4,      zero,   t0
-    addi.d           t3,      a0,     0
-    addi.d           t5,      a1,     0
-    addi.d           t6,      a2,     0
-    addi.d           t7,      a3,     0
-    addi.d           t8,      a4,     0
-.width16_loop:
-    xvld             xr0,     t3,     0
-    xvldx            xr1,     t3,     a5
-    xvldx            xr2,     t3,     t2
-    xvavgr.bu        xr3,     xr0,    xr1
-    xvavgr.bu        xr4,     xr1,    xr2
-    xvhaddw.hu.bu    xr5,     xr3,    xr3
-    xvhaddw.hu.bu    xr6,     xr4,    xr4
-    xvssrarni.bu.h   xr6,     xr5,    1
-    xvpermi.d        xr7,     xr6,    0xd8
-    vst              vr7,     t5,     0
-    xvpermi.q        xr7,     xr7,    0x11
-    vst              vr7,     t7,     0
+    move             a7,      a3
+    addi.d           a5,      a5,    -32
+    move             t0,      a1
+    andi             a7,      a7,    31
+    sub.d            a3,      a3,    a7
+    add.d            a0,      a0,    a5
+    add.d            t0,      t0,    a5
+    add.d            a7,      a7,    a5
+    add.d            a5,      a5,    a2
+    move             a2,      a4
+    sub.d            a7,      zero,  a7
+    add.d            a1,      a3,    a2
+    sub.d            a3,      a3,    a2
+    sub.d            a3,      a3,    a2
+    move             a4,      a7
+    la.local         t1,      filt_mul51
+    vld              vr0,     t1,    0
+    la.local         t2,      filt_mul15
+    vld              vr12,    t2,    0
+    la.local         t3,      filt_mul20
+    vld              vr14,    t3,    0
+    la.local         t4,      pw_1024
+    vld              vr15,    t4,    0
+    la.local         t1,      hpel_shuf
+    vld              vr22,    t1,    0
+    la.local         t2,      shuf_12
+    vld              vr23,    t2,    0
+    la.local         t3,      shuf_1
+    vld              vr26,    t3,    0
+    vaddi.bu         vr24,    vr23,  2 /* shuf_14  */
+    vaddi.bu         vr25,    vr23,  3 /* shuf_15  */
+    vaddi.bu         vr27,    vr26,  1 /* shuf_2   */
+    vaddi.bu         vr28,    vr26,  2 /* shuf_3   */
+    vaddi.bu         vr29,    vr26,  3 /* shuf_4   */
+    vaddi.bu         vr30,    vr26,  5 /* shuf_6   */
+.FILTER_LOOP_Y:
+    alsl.d           t1,      a2,    a1,  1  /* t1 = a1 + 2 * a2 */
+    alsl.d           t2,      a2,    a3,  1  /* t2 = a3 + 2 * a2 */
+    vld              vr1,     a3,    0
+    vldx             vr2,     a3,    a2
+    vld              vr4,     t2,    0
+    vld              vr3,     a1,    0
+    vldx             vr8,     a1,    a2
+    vld              vr7,     t1,    0
+    vilvh.b          vr16,    vr2,   vr1
+    vilvl.b          vr17,    vr2,   vr1
+    vilvh.b          vr18,    vr7,   vr8
+    vilvl.b          vr19,    vr7,   vr8
+    vilvh.b          vr20,    vr4,   vr3
+    vilvl.b          vr21,    vr4,   vr3
+    vdp2.h.bu.b      vr1,     vr17,  vr12
+    vdp2.h.bu.b      vr4,     vr16,  vr12
+    vdp2.h.bu.b      vr8,     vr19,  vr0
+    vdp2.h.bu.b      vr2,     vr18,  vr0
+    vdp2.h.bu.b      vr3,     vr21,  vr14
+    vdp2.h.bu.b      vr7,     vr20,  vr14
+    vadd.h           vr1,     vr1,   vr8
+    vadd.h           vr4,     vr4,   vr2
+    vadd.h           vr1,     vr1,   vr3
+    vadd.h           vr4,     vr4,   vr7
+    addi.d           a3,      a3,    16
+    addi.d           a1,      a1,    16
+    vmov             vr11,    vr1
+    vmov             vr10,    vr4
+
+    vmulwev.w.h      vr16,    vr1,   vr15
+    vmulwev.w.h      vr17,    vr4,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr1,   vr15
+    vmulwod.w.h      vr19,    vr4,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr1,     vr19,  vr17
+    addi.d           t3,      a4,    0
+    vstx             vr1,     t0,    t3
+
+    vld              vr1,     a3,    0
+    vldx             vr2,     a3,    a2
+    vld              vr4,     t2,    16
+    vld              vr3,     a1,    0
+    vldx             vr8,     a1,    a2
+    vld              vr7,     t1,    16
+    vilvh.b          vr16,    vr2,   vr1
+    vilvl.b          vr17,    vr2,   vr1
+    vilvh.b          vr18,    vr7,   vr8
+    vilvl.b          vr19,    vr7,   vr8
+    vilvh.b          vr20,    vr4,   vr3
+    vilvl.b          vr21,    vr4,   vr3
+    vdp2.h.bu.b      vr1,     vr17,  vr12
+    vdp2.h.bu.b      vr4,     vr16,  vr12
+    vdp2.h.bu.b      vr8,     vr19,  vr0
+    vdp2.h.bu.b      vr2,     vr18,  vr0
+    vdp2.h.bu.b      vr3,     vr21,  vr14
+    vdp2.h.bu.b      vr7,     vr20,  vr14
+    vadd.h           vr1,     vr1,   vr8
+    vadd.h           vr4,     vr4,   vr2
+    vadd.h           vr1,     vr1,   vr3
+    vadd.h           vr4,     vr4,   vr7
+    addi.d           a3,      a3,    16
+    addi.d           a1,      a1,    16
+    vmov             vr6,     vr1
+    vmov             vr5,     vr4
+
+    vmulwev.w.h      vr16,    vr1,   vr15
+    vmulwev.w.h      vr17,    vr4,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr1,   vr15
+    vmulwod.w.h      vr19,    vr4,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr1,     vr19,  vr17
+    addi.d           t3,      a4,    16
+    vstx             vr1,     t0,    t3
+
+.FILTER_LOOP_X:
+    alsl.d           t1,      a2,    a1,  1  /* t1 = a1 + 2 * a2 */
+    alsl.d           t2,      a2,    a3,  1  /* t2 = a3 + 2 * a2 */
+    vld              vr1,     a3,    0
+    vldx             vr2,     a3,    a2
+    vld              vr4,     t2,    0
+    vld              vr3,     a1,    0
+    vldx             vr8,     a1,    a2
+    vld              vr7,     t1,    0
+    vilvh.b          vr16,    vr2,   vr1
+    vilvl.b          vr17,    vr2,   vr1
+    vilvh.b          vr18,    vr7,   vr8
+    vilvl.b          vr19,    vr7,   vr8
+    vilvh.b          vr20,    vr4,   vr3
+    vilvl.b          vr21,    vr4,   vr3
+    vdp2.h.bu.b      vr1,     vr17,  vr12
+    vdp2.h.bu.b      vr4,     vr16,  vr12
+    vdp2.h.bu.b      vr8,     vr19,  vr0
+    vdp2.h.bu.b      vr2,     vr18,  vr0
+    vdp2.h.bu.b      vr3,     vr21,  vr14
+    vdp2.h.bu.b      vr7,     vr20,  vr14
+    vadd.h           vr1,     vr1,   vr8
+    vadd.h           vr4,     vr4,   vr2
+    vadd.h           vr1,     vr1,   vr3
+    vadd.h           vr4,     vr4,   vr7
+    addi.d           a3,      a3,    16
+    addi.d           a1,      a1,    16
+    vmov             vr31,    vr1
+    vmov             vr13,    vr4
+
+    vmulwev.w.h      vr16,    vr1,   vr15
+    vmulwev.w.h      vr17,    vr4,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr1,   vr15
+    vmulwod.w.h      vr19,    vr4,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr1,     vr19,  vr17
+    addi.d           t3,      a4,    32
+    vstx             vr1,     t0,    t3
+
+    vld              vr1,     a3,    0
+    vldx             vr2,     a3,    a2
+    vld              vr4,     t2,    16
+    vld              vr3,     a1,    0
+    vldx             vr8,     a1,    a2
+    vld              vr7,     t1,    16
+    vilvh.b          vr16,    vr2,   vr1
+    vilvl.b          vr17,    vr2,   vr1
+    vilvh.b          vr18,    vr7,   vr8
+    vilvl.b          vr19,    vr7,   vr8
+    vilvh.b          vr20,    vr4,   vr3
+    vilvl.b          vr21,    vr4,   vr3
+    vdp2.h.bu.b      vr1,     vr17,  vr12
+    vdp2.h.bu.b      vr4,     vr16,  vr12
+    vdp2.h.bu.b      vr8,     vr19,  vr0
+    vdp2.h.bu.b      vr2,     vr18,  vr0
+    vdp2.h.bu.b      vr3,     vr21,  vr14
+    vdp2.h.bu.b      vr7,     vr20,  vr14
+    vadd.h           vr1,     vr1,   vr8
+    vadd.h           vr4,     vr4,   vr2
+    vadd.h           vr1,     vr1,   vr3
+    vadd.h           vr4,     vr4,   vr7
+    addi.d           a3,      a3,    16
+    addi.d           a1,      a1,    16
+    vmov             vr8,     vr1
+    vmov             vr7,     vr4
+
+    vmulwev.w.h      vr16,    vr1,   vr15
+    vmulwev.w.h      vr17,    vr4,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr1,   vr15
+    vmulwod.w.h      vr19,    vr4,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr1,     vr19,  vr17
+    addi.d           t3,      a4,    48
+    vstx             vr1,     t0,    t3
+
+.FILTER_LAST_X:
+    vsrli.h          vr15,    vr15,  1
+    vmov             vr3,     vr9
+    vshuf.b          vr1,     vr11,  vr3,  vr23
+    vshuf.b          vr2,     vr11,  vr3,  vr24
+    vshuf.b          vr3,     vr10,  vr11, vr29
+    vshuf.b          vr4,     vr10,  vr11, vr27
+    vadd.h           vr3,     vr2,   vr3
+    vmov             vr2,     vr10
+    vmov             vr9,     vr6
+    vshuf.b          vr20,    vr2,   vr11, vr30
+    vadd.h           vr4,     vr4,   vr11
+    vadd.h           vr20,    vr20,  vr1
+    vsub.h           vr20,    vr20,  vr3
+    vsrai.h          vr20,    vr20,  2
+    vsub.h           vr20,    vr20,  vr3
+    vadd.h           vr20,    vr20,  vr4
+    vsrai.h          vr20,    vr20,  2
+    vadd.h           vr20,    vr20,  vr4
+
+    vshuf.b          vr1,     vr9,   vr10, vr23
+    vshuf.b          vr2,     vr9,   vr10, vr24
+    vmov             vr16,     vr5
+    vshuf.b          vr3,     vr16,  vr9,  vr29
+    vshuf.b          vr4,     vr16,  vr9,  vr27
+    vadd.h           vr3,     vr2,   vr3
+    vmov             vr2,     vr16
+    vshuf.b          vr21,    vr2,   vr9,  vr30
+    vadd.h           vr4,     vr4,   vr9
+    vadd.h           vr21,    vr21,  vr1
+    vsub.h           vr21,    vr21,  vr3
+    vsrai.h          vr21,    vr21,  2
+    vsub.h           vr21,    vr21,  vr3
+    vadd.h           vr21,    vr21,  vr4
+    vsrai.h          vr21,    vr21,  2
+    vadd.h           vr21,    vr21,  vr4
+
+    vmulwev.w.h      vr16,    vr20,  vr15
+    vmulwev.w.h      vr17,    vr21,  vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr20,  vr15
+    vmulwod.w.h      vr19,    vr21,  vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr21,    vr19,  vr17
+
+    vmov             vr3,     vr11
+    vmov             vr11,    vr31
+    vshuf.b          vr1,     vr10,  vr3,  vr23
+    vshuf.b          vr2,     vr10,  vr3,  vr24
+    vmov             vr9,     vr6
+    vshuf.b          vr3,     vr9,   vr10, vr29
+    vshuf.b          vr4,     vr9,   vr10, vr27
+    vadd.h           vr3,     vr2,   vr3
+    vmov             vr2,     vr9
+    vmov             vr9,     vr5
+    vshuf.b          vr20,    vr2,   vr10, vr30
+    vadd.h           vr4,     vr4,   vr10
+    vadd.h           vr20,    vr20,  vr1
+    vsub.h           vr20,    vr20,  vr3
+    vsrai.h          vr20,    vr20,  2
+    vsub.h           vr20,    vr20,  vr3
+    vadd.h           vr20,    vr20,  vr4
+    vsrai.h          vr20,    vr20,  2
+    vadd.h           vr20,    vr20,  vr4
+
+    vmov             vr3,     vr6
+    vshuf.b          vr1,     vr9,   vr3,  vr23
+    vshuf.b          vr2,     vr9,   vr3,  vr24
+    vshuf.b          vr3,     vr31,  vr9,  vr29
+    vshuf.b          vr4,     vr31,  vr9,  vr27
+    vadd.h           vr3,     vr2,   vr3
+    vmov             vr2,     vr31
+    vmov             vr10,    vr13
+    vshuf.b          vr2,     vr2,   vr9,  vr30
+    vadd.h           vr4,     vr4,   vr9
+    vadd.h           vr2,     vr2,   vr1
+    vsub.h           vr2,     vr2,   vr3
+    vsrai.h          vr2,     vr2,   2
+    vsub.h           vr2,     vr2,   vr3
+    vadd.h           vr2,     vr2,   vr4
+    vsrai.h          vr2,     vr2,   2
+    vadd.h           vr2,     vr2,   vr4
+    vmulwev.w.h      vr16,    vr20,  vr15
+    vmulwev.w.h      vr17,    vr2,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr20,  vr15
+    vmulwod.w.h      vr19,    vr2,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr19,    vr19,  vr17
+    vilvh.d          vr2,     vr19,  vr21
+    vilvl.d          vr21,    vr19,  vr21
+
+    vstx             vr21,    a5,    a4
+    addi.d           a4,      a4,    16
+    vstx             vr2,     a5,    a4
+    addi.d           a4,      a4,    -16
+    vadd.h           vr15,    vr15,  vr15
+    vmov             vr6,     vr8
+    vmov             vr5,     vr7
 
-    addi.d           t3,      t3,     1
-    xvld             xr0,     t3,     0
-    xvldx            xr1,     t3,     a5
-    xvldx            xr2,     t3,     t2
-    xvavgr.bu        xr3,     xr0,    xr1
-    xvavgr.bu        xr4,     xr1,    xr2
-    xvhaddw.hu.bu    xr5,     xr3,    xr3
-    xvhaddw.hu.bu    xr6,     xr4,    xr4
-    xvssrarni.bu.h   xr6,     xr5,    1
-    xvpermi.d        xr7,     xr6,    0xd8
-    vst              vr7,     t6,     0
-    xvpermi.q        xr7,     xr7,    0x11
-    vst              vr7,     t8,     0
-    addi.d           t3,      t3,     31
-    addi.d           t5,      t5,     16
-    addi.d           t6,      t6,     16
-    addi.d           t7,      t7,     16
-    addi.d           t8,      t8,     16
-    addi.w           t4,      t4,     -16
-    blt              zero,    t4,     .width16_loop
+    addi.d           a4,      a4,    32
+    blt              a4,      zero,  .FILTER_LOOP_X
+    addi.d           t1,      a4,    -32
+    blt              t1,      zero,  .FILTER_LAST_X
 
-    beqz             t1,      .width16_end
-    vld              vr0,     t3,     0
-    vldx             vr1,     t3,     a5
-    vldx             vr2,     t3,     t2
-    vavgr.bu         vr3,     vr0,    vr1
-    vavgr.bu         vr4,     vr1,    vr2
-    vhaddw.hu.bu     vr5,     vr3,    vr3
-    vhaddw.hu.bu     vr6,     vr4,    vr4
-    vssrarni.bu.h    vr6,     vr5,    1
-    fst.d            f6,      t5,     0
-    vstelm.d         vr6,     t7,     0,    1
+    addi.d           a3,      a3,    -64
+    alsl.d           t2,      a2,    a3,  1  /* t2 = a3 + 2 * a2 */
+    vld              vr11,    t2,    0
+    vld              vr10,    t2,    16
+    addi.d           a3,      a3,    32
+    alsl.d           t2,      a2,    a3,  1  /* t2 = a3 + 2 * a2 */
+    vld              vr8,     t2,    0
+    vld              vr7,     t2,    16
+    addi.d           a3,      a3,    32
 
-    addi.d           t3,      t3,     1
-    vld              vr0,     t3,     0
-    vldx             vr1,     t3,     a5
-    vldx             vr2,     t3,     t2
-    vavgr.bu         vr3,     vr0,    vr1
-    vavgr.bu         vr4,     vr1,    vr2
-    vhaddw.hu.bu     vr5,     vr3,    vr3
-    vhaddw.hu.bu     vr6,     vr4,    vr4
-    vssrarni.bu.h    vr6,     vr5,    1
-    fst.d            f6,      t6,     0
-    vstelm.d         vr6,     t8,     0,    1
+    vxor.v           vr9,     vr9,   vr9
+    vshuf.b          vr1,     vr11,  vr9,  vr24
+    vshuf.b          vr2,     vr11,  vr9,  vr25
+    vshuf.b          vr4,     vr10,  vr11, vr26
+    vshuf.b          vr5,     vr10,  vr11, vr27
+    vshuf.b          vr6,     vr10,  vr11, vr28
+    vdp2.h.bu.b      vr16,    vr1,   vr12
+    vdp2.h.bu.b      vr17,    vr2,   vr12
+    vdp2.h.bu.b      vr18,    vr11,  vr14
+    vdp2.h.bu.b      vr19,    vr4,   vr14
+    vdp2.h.bu.b      vr20,    vr5,   vr0
+    vdp2.h.bu.b      vr21,    vr6,   vr0
+    vadd.h           vr1,     vr16,  vr18
+    vadd.h           vr2,     vr17,  vr19
+    vadd.h           vr1,     vr1,   vr20
+    vadd.h           vr2,     vr2,   vr21
+    vmulwev.w.h      vr16,    vr1,   vr15
+    vmulwev.w.h      vr17,    vr2,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr1,   vr15
+    vmulwod.w.h      vr19,    vr2,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr1,     vr19,  vr17
+    vshuf.b          vr1,     vr1,   vr1,  vr22
+    addi.d           a4,      a4,    -64
+    vstx             vr1,     a0,    a4
+
+    vshuf.b          vr1,     vr10,  vr11, vr24
+    vshuf.b          vr2,     vr10,  vr11, vr25
+    vshuf.b          vr4,     vr8,   vr10, vr26
+    vshuf.b          vr5,     vr8,   vr10, vr27
+    vshuf.b          vr6,     vr8,   vr10, vr28
+    vdp2.h.bu.b      vr16,    vr1,   vr12
+    vdp2.h.bu.b      vr17,    vr2,   vr12
+    vdp2.h.bu.b      vr18,    vr10,  vr14
+    vdp2.h.bu.b      vr19,    vr4,   vr14
+    vdp2.h.bu.b      vr20,    vr5,   vr0
+    vdp2.h.bu.b      vr21,    vr6,   vr0
+    vadd.h           vr1,     vr16,  vr18
+    vadd.h           vr2,     vr17,  vr19
+    vadd.h           vr1,     vr1,   vr20
+    vadd.h           vr2,     vr2,   vr21
+    vmulwev.w.h      vr16,    vr1,   vr15
+    vmulwev.w.h      vr17,    vr2,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr1,   vr15
+    vmulwod.w.h      vr19,    vr2,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr1,     vr19,  vr17
+    vshuf.b          vr1,     vr1,   vr1,  vr22
+    addi.d           a4,      a4,    16
+    vstx             vr1,     a0,    a4
+
+    vshuf.b          vr1,     vr8,   vr10, vr24
+    vshuf.b          vr2,     vr8,   vr10, vr25
+    vshuf.b          vr4,     vr7,   vr8,  vr26
+    vshuf.b          vr5,     vr7,   vr8,  vr27
+    vshuf.b          vr6,     vr7,   vr8,  vr28
+    vdp2.h.bu.b      vr16,    vr1,   vr12
+    vdp2.h.bu.b      vr17,    vr2,   vr12
+    vdp2.h.bu.b      vr18,    vr8,   vr14
+    vdp2.h.bu.b      vr19,    vr4,   vr14
+    vdp2.h.bu.b      vr20,    vr5,   vr0
+    vdp2.h.bu.b      vr21,    vr6,   vr0
+    vadd.h           vr1,     vr16,  vr18
+    vadd.h           vr2,     vr17,  vr19
+    vadd.h           vr1,     vr1,   vr20
+    vadd.h           vr2,     vr2,   vr21
+    vmulwev.w.h      vr16,    vr1,   vr15
+    vmulwev.w.h      vr17,    vr2,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr1,   vr15
+    vmulwod.w.h      vr19,    vr2,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr1,     vr19,  vr17
+    vshuf.b          vr1,     vr1,   vr1,  vr22
+    addi.d           a4,      a4,    16
+    vstx             vr1,     a0,    a4
+
+    vshuf.b          vr1,     vr7,   vr8,  vr24
+    vshuf.b          vr2,     vr7,   vr8,  vr25
+    vshuf.b          vr4,     vr8,   vr7,  vr26
+    vshuf.b          vr5,     vr8,   vr7,  vr27
+    vshuf.b          vr6,     vr8,   vr7,  vr28
+    vdp2.h.bu.b      vr16,    vr1,   vr12
+    vdp2.h.bu.b      vr17,    vr2,   vr12
+    vdp2.h.bu.b      vr18,    vr7,   vr14
+    vdp2.h.bu.b      vr19,    vr4,   vr14
+    vdp2.h.bu.b      vr20,    vr5,   vr0
+    vdp2.h.bu.b      vr21,    vr6,   vr0
+    vadd.h           vr1,     vr16,  vr18
+    vadd.h           vr2,     vr17,  vr19
+    vadd.h           vr1,     vr1,   vr20
+    vadd.h           vr2,     vr2,   vr21
+    vmulwev.w.h      vr16,    vr1,   vr15
+    vmulwev.w.h      vr17,    vr2,   vr15
+    vsrarni.h.w      vr17,    vr16,  15
+    vmaxi.h          vr17,    vr17,  0
+    vsat.hu          vr17,    vr17,  7
+    vmulwod.w.h      vr18,    vr1,   vr15
+    vmulwod.w.h      vr19,    vr2,   vr15
+    vsrarni.h.w      vr19,    vr18,  15
+    vmaxi.h          vr19,    vr19,  0
+    vsat.hu          vr19,    vr19,  7
+    vpackev.b        vr1,     vr19,  vr17
+    vshuf.b          vr1,     vr1,   vr1,  vr22
+    addi.d           a4,      a4,    16
+    vstx             vr1,     a0,    a4
+    addi.d           a4,      a4,    16
 
-.width16_end:
-    add.d            a0,      a0,     t2
-    add.d            a1,      a1,     a6
-    add.d            a2,      a2,     a6
-    add.d            a3,      a3,     a6
-    add.d            a4,      a4,     a6
-    addi.w           a7,      a7,     -1
-    blt              zero,    a7,     .height_loop
+    //setup regs for next y
+    sub.d            a4,      a4,    a7
+    sub.d            a4,      a4,    a2
+    sub.d            a1,      a1,    a4
+    sub.d            a3,      a3,    a4
+    add.d            a0,      a0,    a2
+    add.d            t0,      t0,    a2
+    add.d            a5,      a5,    a2
+    move             a4,      a7
+    addi.d           a6,      a6,    -1
+    blt              zero,    a6,    .FILTER_LOOP_Y
+    fld.d            f24,     sp,    0
+    fld.d            f25,     sp,    8
+    fld.d            f26,     sp,    16
+    fld.d            f27,     sp,    24
+    fld.d            f28,     sp,    32
+    fld.d            f29,     sp,    40
+    fld.d            f30,     sp,    48
+    fld.d            f31,     sp,    56
+    addi.d           sp,      sp,    64
+endfunc
+
+/*
+ * void frame_init_lowres_core(pixel *src0, pixel *dst0, pixel *dsth,
+ *                             pixel *dstv, pixel *dstc, intptr_t src_stride,
+ *                             intptr_t dst_stride, int width, int height)
+ */
+function frame_init_lowres_core_lsx
+    addi.d           t0,      zero,   15
+    addi.d           t1,      zero,   7
+    addi.d           t2,      zero,   3
+    addi.d           t3,      zero,   1
+    ld.d             t4,      sp,     0
+    addi.d           sp,      sp,     -16
+    st.d             s0,      sp,     0
+    st.d             s1,      sp,     8
+    slli.d           s0,      a5,     1
+.LOOPH:
+    bge              zero,    t4,     .ENDLOOPH
+    addi.d           t4,      t4,     -1
+    add.d            t5,      a0,     a5
+    add.d            t7,      t5,     a5
+    move             t6,      a7
+.LOOPW16:
+    bge              t0,      t6,     .ENDLOOPW16
+    vld              vr0,     a0,     0
+    vld              vr1,     t5,     0
+    vld              vr2,     t7,     0
+    vld              vr3,     a0,     1
+    vld              vr4,     t5,     1
+    vld              vr5,     t7,     1
+    vld              vr6,     a0,     16
+    vld              vr7,     t5,     16
+    vld              vr8,     t7,     16
+    vld              vr9,     a0,     17
+    vld              vr10,    t5,     17
+    vld              vr11,    t7,     17
+
+    // Calculate dst0, dsth, dstv and dstc
+    vavgr.bu         vr12,    vr0,    vr1
+    vavgr.bu         vr13,    vr1,    vr2
+    vavgr.bu         vr14,    vr3,    vr4
+    vavgr.bu         vr15,    vr4,    vr5
+    vavgr.bu         vr16,    vr6,    vr7
+    vavgr.bu         vr17,    vr7,    vr8
+    vavgr.bu         vr18,    vr9,    vr10
+    vavgr.bu         vr19,    vr10,   vr11
+    vhaddw.hu.bu     vr12,    vr12,   vr12
+    vhaddw.hu.bu     vr13,    vr13,   vr13
+    vhaddw.hu.bu     vr14,    vr14,   vr14
+    vhaddw.hu.bu     vr15,    vr15,   vr15
+    vhaddw.hu.bu     vr16,    vr16,   vr16
+    vhaddw.hu.bu     vr17,    vr17,   vr17
+    vhaddw.hu.bu     vr18,    vr18,   vr18
+    vhaddw.hu.bu     vr19,    vr19,   vr19
+    vssrarni.bu.h    vr13,    vr12,   1
+    vssrarni.bu.h    vr15,    vr14,   1
+    vssrarni.bu.h    vr17,    vr16,   1
+    vssrarni.bu.h    vr19,    vr18,   1
+    vilvl.d          vr12,    vr17,   vr13
+    vilvl.d          vr14,    vr19,   vr15
+    vilvh.d          vr13,    vr17,   vr13
+    vilvh.d          vr15,    vr19,   vr15
+    vst              vr12,    a1,     0
+    vst              vr14,    a2,     0
+    vst              vr13,    a3,     0
+    vst              vr15,    a4,     0
+
+    addi.d           a1,      a1,     16
+    addi.d           a2,      a2,     16
+    addi.d           a3,      a3,     16
+    addi.d           a4,      a4,     16
+    addi.d           a0,      a0,     32
+    addi.d           t5,      t5,     32
+    addi.d           t7,      t7,     32
+    addi.d           t6,      t6,     -16
+    b                .LOOPW16
+.ENDLOOPW16:
+.LOOPW8:
+    bge              t1,      t6,     .ENDLOOPW8
+    vld              vr0,     a0,     0
+    vld              vr1,     t5,     0
+    vld              vr2,     t7,     0
+    vld              vr3,     a0,     1
+    vld              vr4,     t5,     1
+    vld              vr5,     t7,     1
+
+    // Calculate dst0, dsth, dstv and dstc
+    vavgr.bu         vr12,    vr0,    vr1
+    vavgr.bu         vr13,    vr1,    vr2
+    vavgr.bu         vr14,    vr3,    vr4
+    vavgr.bu         vr15,    vr4,    vr5
+    vhaddw.hu.bu     vr12,    vr12,   vr12
+    vhaddw.hu.bu     vr13,    vr13,   vr13
+    vhaddw.hu.bu     vr14,    vr14,   vr14
+    vhaddw.hu.bu     vr15,    vr15,   vr15
+    vssrarni.bu.h    vr13,    vr12,   1
+    vssrarni.bu.h    vr15,    vr14,   1
+    vstelm.d         vr13,    a1,     0,   0
+    vstelm.d         vr15,    a2,     0,   0
+    vstelm.d         vr13,    a3,     0,   1
+    vstelm.d         vr15,    a4,     0,   1
+
+    addi.d           a1,      a1,     8
+    addi.d           a2,      a2,     8
+    addi.d           a3,      a3,     8
+    addi.d           a4,      a4,     8
+    addi.d           a0,      a0,     16
+    addi.d           t5,      t5,     16
+    addi.d           t7,      t7,     16
+    addi.d           t6,      t6,     -8
+    b                .LOOPW8
+.ENDLOOPW8:
+.LOOPW4:
+    bge              t2,      t6,    .ENDLOOPW4
+    vld              vr0,     a0,    0
+    vld              vr1,     t5,    0
+    vld              vr2,     t7,    0
+    vld              vr3,     a0,    1
+    vld              vr4,     t5,    1
+    vld              vr5,     t7,    1
+
+    // Calculate dst0, dsth, dstv and dstc
+    vavgr.bu         vr12,    vr0,   vr1
+    vavgr.bu         vr13,    vr1,   vr2
+    vavgr.bu         vr14,    vr3,   vr4
+    vavgr.bu         vr15,    vr4,   vr5
+    vhaddw.hu.bu     vr12,    vr12,  vr12
+    vhaddw.hu.bu     vr13,    vr13,  vr13
+    vhaddw.hu.bu     vr14,    vr14,  vr14
+    vhaddw.hu.bu     vr15,    vr15,  vr15
+    vssrarni.bu.h    vr13,    vr12,  1
+    vssrarni.bu.h    vr15,    vr14,  1
+    vstelm.w         vr13,    a1,    0,   0
+    vstelm.w         vr15,    a2,    0,   0
+    vstelm.w         vr13,    a3,    0,   2
+    vstelm.w         vr15,    a4,    0,   2
+
+    addi.d           a1,      a1,    4
+    addi.d           a2,      a2,    4
+    addi.d           a3,      a3,    4
+    addi.d           a4,      a4,    4
+    addi.d           a0,      a0,    8
+    addi.d           t5,      t5,    8
+    addi.d           t7,      t7,    8
+    addi.d           t6,      t6,    -4
+    b                .LOOPW4
+.ENDLOOPW4:
+.LOOPW2:
+    bge              t3,      t6,    .ENDLOOPW2
+    vld              vr0,     a0,    0
+    vld              vr1,     t5,    0
+    vld              vr2,     t7,    0
+    vld              vr3,     a0,    1
+    vld              vr4,     t5,    1
+    vld              vr5,     t7,    1
+
+    // Calculate dst0, dsth, dstv and dstc
+    vavgr.bu         vr12,    vr0,   vr1
+    vavgr.bu         vr13,    vr1,   vr2
+    vavgr.bu         vr14,    vr3,   vr4
+    vavgr.bu         vr15,    vr4,   vr5
+    vhaddw.hu.bu     vr12,    vr12,  vr12
+    vhaddw.hu.bu     vr13,    vr13,  vr13
+    vhaddw.hu.bu     vr14,    vr14,  vr14
+    vhaddw.hu.bu     vr15,    vr15,  vr15
+    vssrarni.bu.h    vr13,    vr12,  1
+    vssrarni.bu.h    vr15,    vr14,  1
+    vstelm.h         vr13,    a1,    0,   0
+    vstelm.h         vr15,    a2,    0,   0
+    vstelm.h         vr13,    a3,    0,   4
+    vstelm.h         vr15,    a4,    0,   4
+
+    addi.d           a1,      a1,    2
+    addi.d           a2,      a2,    2
+    addi.d           a3,      a3,    2
+    addi.d           a4,      a4,    2
+    addi.d           a0,      a0,    4
+    addi.d           t5,      t5,    4
+    addi.d           t7,      t7,    4
+    addi.d           t6,      t6,    -2
+    b                .LOOPW2
+.ENDLOOPW2:
+.LOOPW1:
+    bge              zero,    t6,    .ENDLOOPW1
+    vld              vr0,     a0,    0
+    vld              vr1,     t5,    0
+    vld              vr2,     t7,    0
+    vld              vr3,     a0,    1
+    vld              vr4,     t5,    1
+    vld              vr5,     t7,    1
+
+    // Calculate dst0, dsth, dstv and dstc
+    vavgr.bu         vr12,    vr0,   vr1
+    vavgr.bu         vr13,    vr1,   vr2
+    vavgr.bu         vr14,    vr3,   vr4
+    vavgr.bu         vr15,    vr4,   vr5
+    vhaddw.hu.bu     vr12,    vr12,  vr12
+    vhaddw.hu.bu     vr13,    vr13,  vr13
+    vhaddw.hu.bu     vr14,    vr14,  vr14
+    vhaddw.hu.bu     vr15,    vr15,  vr15
+    vssrarni.bu.h    vr13,    vr12,  1
+    vssrarni.bu.h    vr15,    vr14,  1
+    vstelm.b         vr13,    a1,    0,   0
+    vstelm.b         vr15,    a2,    0,   0
+    vstelm.b         vr13,    a3,    0,   8
+    vstelm.b         vr15,    a4,    0,   8
+.ENDLOOPW1:
+    sub.d            s1,      a7,    t6
+    sub.d            a0,      a0,    s1
+    sub.d            a0,      a0,    s1
+    add.d            a0,      a0,    s0
+    sub.d            a1,      a1,    s1
+    add.d            a1,      a1,    a6
+    sub.d            a2,      a2,    s1
+    add.d            a2,      a2,    a6
+    sub.d            a3,      a3,    s1
+    add.d            a3,      a3,    a6
+    sub.d            a4,      a4,    s1
+    add.d            a4,      a4,    a6
+    b                .LOOPH
+.ENDLOOPH:
+    ld.d             s0,      sp,    0
+    ld.d             s1,      sp,    8
+    addi.d           sp,      sp,    16
 endfunc
 #endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/mc-c.c b/common/loongarch/mc-c.c
index 6d63ee59..f69867d0 100644
--- a/common/loongarch/mc-c.c
+++ b/common/loongarch/mc-c.c
@@ -29,6 +29,17 @@
 
 #if !HIGH_BIT_DEPTH
 
+#define MC_WEIGHT_LSX(func)                                                                                        \
+static void (* mc##func##_wtab_lsx[6])( uint8_t *, intptr_t, uint8_t *, intptr_t, const x264_weight_t *, int ) =   \
+{                                                                                                                  \
+    x264_mc_weight_w4##func##_lsx,                                                                                 \
+    x264_mc_weight_w4##func##_lsx,                                                                                 \
+    x264_mc_weight_w8##func##_lsx,                                                                                 \
+    x264_mc_weight_w16##func##_lsx,                                                                                \
+    x264_mc_weight_w16##func##_lsx,                                                                                \
+    x264_mc_weight_w20##func##_lsx,                                                                                \
+};
+
 #define MC_WEIGHT(func)                                                                                             \
 static void (* mc##func##_wtab_lasx[6])( uint8_t *, intptr_t, uint8_t *, intptr_t, const x264_weight_t *, int ) =   \
 {                                                                                                                   \
@@ -41,10 +52,51 @@ static void (* mc##func##_wtab_lasx[6])( uint8_t *, intptr_t, uint8_t *, intptr_
 };
 
 #if !HIGH_BIT_DEPTH
+MC_WEIGHT_LSX()
+MC_WEIGHT_LSX(_noden)
 MC_WEIGHT()
 MC_WEIGHT(_noden)
 #endif
 
+static void weight_cache_lsx( x264_t *h, x264_weight_t *w )
+{
+    if ( w->i_denom >= 1)
+    {
+        w->weightfn = mc_wtab_lsx;
+    }
+    else
+        w->weightfn = mc_noden_wtab_lsx;
+}
+
+static weight_fn_t mc_weight_wtab_lsx[6] =
+{
+    x264_mc_weight_w4_lsx,
+    x264_mc_weight_w4_lsx,
+    x264_mc_weight_w8_lsx,
+    x264_mc_weight_w16_lsx,
+    x264_mc_weight_w16_lsx,
+    x264_mc_weight_w20_lsx,
+};
+
+static void (* const pixel_avg_wtab_lsx[6])(uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int ) =
+{
+    NULL,
+    x264_pixel_avg2_w4_lsx,
+    x264_pixel_avg2_w8_lsx,
+    x264_pixel_avg2_w16_lsx,
+    x264_pixel_avg2_w16_lsx,
+    x264_pixel_avg2_w20_lsx,
+};
+
+static void (* const mc_copy_wtab_lsx[5])( uint8_t *, intptr_t, uint8_t *, intptr_t, int ) =
+{
+    NULL,
+    x264_mc_copy_w4_lsx,
+    x264_mc_copy_w8_lsx,
+    NULL,
+    x264_mc_copy_w16_lsx,
+};
+
 static void weight_cache_lasx( x264_t *h, x264_weight_t *w )
 {
     if ( w->i_denom >= 1)
@@ -84,6 +136,99 @@ static void (* const mc_copy_wtab_lasx[5])( uint8_t *, intptr_t, uint8_t *, intp
     x264_mc_copy_w16_lasx,
 };
 
+static uint8_t *get_ref_lsx( uint8_t *p_dst, intptr_t *p_dst_stride,
+                             uint8_t *p_src[4], intptr_t i_src_stride,
+                             int32_t m_vx, int32_t m_vy,
+                             int32_t i_width, int32_t i_height,
+                             const x264_weight_t *pWeight )
+{
+    int32_t i_qpel_idx;
+    int32_t i_offset;
+    uint8_t *p_src1;
+    int32_t r_vy = m_vy & 3;
+    int32_t r_vx = m_vx & 3;
+    int32_t width = i_width >> 2;
+
+    i_qpel_idx = ( r_vy << 2 ) + r_vx;
+    i_offset = ( m_vy >> 2 ) * i_src_stride + ( m_vx >> 2 );
+    p_src1 = p_src[x264_hpel_ref0[i_qpel_idx]] + i_offset +
+           ( 3 == r_vy ) * i_src_stride;
+
+    if( i_qpel_idx & 5 )
+    {
+        uint8_t *p_src2 = p_src[x264_hpel_ref1[i_qpel_idx]] +
+                          i_offset + ( 3 == r_vx );
+        pixel_avg_wtab_lsx[width](
+                p_dst, *p_dst_stride, p_src1, i_src_stride,
+                p_src2, i_height );
+
+        if( pWeight->weightfn )
+        {
+            pWeight->weightfn[width](p_dst, *p_dst_stride, p_dst, *p_dst_stride, pWeight, i_height);
+        }
+        return p_dst;
+    }
+    else if ( pWeight->weightfn )
+    {
+        pWeight->weightfn[width]( p_dst, *p_dst_stride, p_src1, i_src_stride, pWeight, i_height );
+        return p_dst;
+    }
+    else
+    {
+        *p_dst_stride = i_src_stride;
+        return p_src1;
+    }
+}
+
+static void mc_luma_lsx( uint8_t *p_dst, intptr_t i_dst_stride,
+                         uint8_t *p_src[4], intptr_t i_src_stride,
+                         int32_t m_vx, int32_t m_vy,
+                         int32_t i_width, int32_t i_height,
+                         const x264_weight_t *pWeight )
+{
+    int32_t  i_qpel_idx;
+    int32_t  i_offset;
+    uint8_t  *p_src1;
+
+    i_qpel_idx = ( ( m_vy & 3 ) << 2 ) + ( m_vx & 3 );
+    i_offset = ( m_vy >> 2 ) * i_src_stride + ( m_vx >> 2 );
+    p_src1 = p_src[x264_hpel_ref0[i_qpel_idx]] + i_offset +
+             ( 3 == ( m_vy & 3 ) ) * i_src_stride;
+
+    if( i_qpel_idx & 5 )
+    {
+        uint8_t *p_src2 = p_src[x264_hpel_ref1[i_qpel_idx]] +
+                          i_offset + ( 3 == ( m_vx & 3 ) );
+
+        pixel_avg_wtab_lsx[i_width >> 2](
+                p_dst, i_dst_stride, p_src1, i_src_stride,
+                p_src2, i_height );
+
+        if( pWeight->weightfn )
+        {
+            pWeight->weightfn[i_width>>2]( p_dst, i_dst_stride, p_dst, i_dst_stride, pWeight, i_height );
+        }
+    }
+    else if( pWeight->weightfn )
+    {
+        pWeight->weightfn[i_width>>2]( p_dst, i_dst_stride, p_src1, i_src_stride, pWeight, i_height );
+    }
+    else
+    {
+        mc_copy_wtab_lsx[i_width>>2]( p_dst, i_dst_stride, p_src1, i_src_stride, i_height );
+    }
+}
+
+PLANE_INTERLEAVE(lsx)
+PLANE_COPY_YUYV(32, lsx)
+
+#define x264_mc_chroma_lsx x264_template(mc_chroma_lsx)
+void x264_mc_chroma_lsx( uint8_t *p_dst_u, uint8_t *p_dst_v,
+                         intptr_t i_dst_stride,
+                         uint8_t *p_src, intptr_t i_src_stride,
+                         int32_t m_vx, int32_t m_vy,
+                         int32_t i_width, int32_t i_height );
+
 static uint8_t *get_ref_lasx( uint8_t *p_dst, intptr_t *p_dst_stride,
                               uint8_t *p_src[4], intptr_t i_src_stride,
                               int32_t m_vx, int32_t m_vy,
@@ -181,6 +326,50 @@ void x264_mc_chroma_lasx( uint8_t *p_dst_u, uint8_t *p_dst_v,
 void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
 {
 #if !HIGH_BIT_DEPTH
+    if( cpu & X264_CPU_LSX )
+    {
+        pf->mc_luma = mc_luma_lsx;
+        pf->mc_chroma = x264_mc_chroma_lsx;
+        pf->get_ref = get_ref_lsx;
+
+        pf->avg[PIXEL_16x16]= x264_pixel_avg_16x16_lsx;
+        pf->avg[PIXEL_16x8] = x264_pixel_avg_16x8_lsx;
+        pf->avg[PIXEL_8x16] = x264_pixel_avg_8x16_lsx;
+        pf->avg[PIXEL_8x8] = x264_pixel_avg_8x8_lsx;
+        pf->avg[PIXEL_8x4] = x264_pixel_avg_8x4_lsx;
+        pf->avg[PIXEL_4x16] = x264_pixel_avg_4x16_lsx;
+        pf->avg[PIXEL_4x8] = x264_pixel_avg_4x8_lsx;
+        pf->avg[PIXEL_4x4] = x264_pixel_avg_4x4_lsx;
+        pf->avg[PIXEL_4x2] = x264_pixel_avg_4x2_lsx;
+
+        pf->weight = mc_weight_wtab_lsx;
+        pf->offsetadd = mc_weight_wtab_lsx;
+        pf->offsetsub = mc_weight_wtab_lsx;
+        pf->weight_cache = weight_cache_lsx;
+
+        pf->copy_16x16_unaligned = x264_mc_copy_w16_lsx;
+        pf->copy[PIXEL_16x16] = x264_mc_copy_w16_lsx;
+        pf->copy[PIXEL_8x8] = x264_mc_copy_w8_lsx;
+        pf->copy[PIXEL_4x4] = x264_mc_copy_w4_lsx;
+
+        pf->store_interleave_chroma = x264_store_interleave_chroma_lsx;
+        pf->load_deinterleave_chroma_fenc = x264_load_deinterleave_chroma_fenc_lsx;
+        pf->load_deinterleave_chroma_fdec = x264_load_deinterleave_chroma_fdec_lsx;
+
+        pf->plane_copy_interleave = plane_copy_interleave_lsx;
+        pf->plane_copy_deinterleave = x264_plane_copy_deinterleave_lsx;
+        pf->plane_copy_deinterleave_yuyv = plane_copy_deinterleave_yuyv_lsx;
+
+        pf->hpel_filter = x264_hpel_filter_lsx;
+        pf->memcpy_aligned = x264_memcpy_aligned_lsx;
+        pf->memzero_aligned = x264_memzero_aligned_lsx;
+        pf->frame_init_lowres_core = x264_frame_init_lowres_core_lsx;
+
+        pf->prefetch_fenc_420 = x264_prefetch_fenc_420_lsx;
+        pf->prefetch_fenc_422 = x264_prefetch_fenc_422_lsx;
+        pf->prefetch_ref  = x264_prefetch_ref_lsx;
+    }
+
     if( cpu & X264_CPU_LASX )
     {
         pf->mc_luma = mc_luma_lasx;
@@ -207,22 +396,10 @@ void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
         pf->copy[PIXEL_8x8] = x264_mc_copy_w8_lasx;
         pf->copy[PIXEL_4x4] = x264_mc_copy_w4_lasx;
 
-        pf->store_interleave_chroma = x264_store_interleave_chroma_lasx;
-        pf->load_deinterleave_chroma_fenc = x264_load_deinterleave_chroma_fenc_lasx;
-        pf->load_deinterleave_chroma_fdec = x264_load_deinterleave_chroma_fdec_lasx;
-
-        pf->plane_copy_interleave = plane_copy_interleave_lasx;
-        pf->plane_copy_deinterleave = x264_plane_copy_deinterleave_lasx;
-        pf->plane_copy_deinterleave_yuyv = plane_copy_deinterleave_yuyv_lasx;
-
         pf->hpel_filter = x264_hpel_filter_lasx;
         pf->memcpy_aligned = x264_memcpy_aligned_lasx;
         pf->memzero_aligned = x264_memzero_aligned_lasx;
         pf->frame_init_lowres_core = x264_frame_init_lowres_core_lasx;
-
-        pf->prefetch_fenc_420 = x264_prefetch_fenc_420_lasx;
-        pf->prefetch_fenc_422 = x264_prefetch_fenc_422_lasx;
-        pf->prefetch_ref  = x264_prefetch_ref_lasx;
     }
 #endif // !HIGH_BIT_DEPTH
 }
diff --git a/common/loongarch/mc.h b/common/loongarch/mc.h
index fc582944..c826887e 100644
--- a/common/loongarch/mc.h
+++ b/common/loongarch/mc.h
@@ -30,6 +30,96 @@
 #define x264_mc_init_loongarch x264_template(mc_init_loongarch)
 void x264_mc_init_loongarch( int cpu, x264_mc_functions_t *pf );
 
+#define x264_pixel_avg_16x16_lsx x264_template(pixel_avg_16x16_lsx)
+void x264_pixel_avg_16x16_lsx( pixel *, intptr_t, pixel *, intptr_t, pixel *, intptr_t, int );
+#define x264_pixel_avg_16x8_lsx x264_template(pixel_avg_16x8_lsx)
+void x264_pixel_avg_16x8_lsx( pixel *, intptr_t, pixel *, intptr_t, pixel *, intptr_t, int );
+#define x264_pixel_avg_8x16_lsx x264_template(pixel_avg_8x16_lsx)
+void x264_pixel_avg_8x16_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_8x8_lsx x264_template(pixel_avg_8x8_lsx)
+void x264_pixel_avg_8x8_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_8x4_lsx x264_template(pixel_avg_8x4_lsx)
+void x264_pixel_avg_8x4_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_4x16_lsx x264_template(pixel_avg_4x16_lsx)
+void x264_pixel_avg_4x16_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_4x8_lsx x264_template(pixel_avg_4x8_lsx)
+void x264_pixel_avg_4x8_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_4x4_lsx x264_template(pixel_avg_4x4_lsx)
+void x264_pixel_avg_4x4_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_pixel_avg_4x2_lsx x264_template(pixel_avg_4x2_lsx)
+void x264_pixel_avg_4x2_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+
+#define x264_pixel_avg2_w4_lsx x264_template(pixel_avg2_w4_lsx)
+void x264_pixel_avg2_w4_lsx ( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int );
+#define x264_pixel_avg2_w8_lsx x264_template(pixel_avg2_w8_lsx)
+void x264_pixel_avg2_w8_lsx ( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int );
+#define x264_pixel_avg2_w16_lsx x264_template(pixel_avg2_w16_lsx)
+void x264_pixel_avg2_w16_lsx ( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int );
+#define x264_pixel_avg2_w20_lsx x264_template(pixel_avg2_w20_lsx)
+void x264_pixel_avg2_w20_lsx ( uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int );
+
+#define x264_mc_weight_w20_lsx x264_template(mc_weight_w20_lsx)
+void x264_mc_weight_w20_lsx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w20_noden_lsx x264_template(mc_weight_w20_noden_lsx)
+void x264_mc_weight_w20_noden_lsx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w16_lsx x264_template(mc_weight_w16_lsx)
+void x264_mc_weight_w16_lsx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w16_noden_lsx x264_template(mc_weight_w16_noden_lsx)
+void x264_mc_weight_w16_noden_lsx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w8_lsx x264_template(mc_weight_w8_lsx)
+void x264_mc_weight_w8_lsx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w8_noden_lsx x264_template(mc_weight_w8_noden_lsx)
+void x264_mc_weight_w8_noden_lsx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w4_lsx x264_template(mc_weight_w4_lsx)
+void x264_mc_weight_w4_lsx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+#define x264_mc_weight_w4_noden_lsx x264_template(mc_weight_w4_noden_lsx)
+void x264_mc_weight_w4_noden_lsx( pixel *, intptr_t, pixel *, intptr_t, const x264_weight_t *, int );
+
+#define x264_mc_copy_w16_lsx x264_template(mc_copy_w16_lsx)
+void x264_mc_copy_w16_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_mc_copy_w8_lsx x264_template(mc_copy_w8_lsx)
+void x264_mc_copy_w8_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+#define x264_mc_copy_w4_lsx x264_template(mc_copy_w4_lsx)
+void x264_mc_copy_w4_lsx( uint8_t *, intptr_t, uint8_t *, intptr_t, int );
+
+#define x264_store_interleave_chroma_lsx x264_template(store_interleave_chroma_lsx)
+void x264_store_interleave_chroma_lsx( pixel *dst, intptr_t i_dst, pixel *srcu, pixel *srcv, int height );
+#define x264_load_deinterleave_chroma_fenc_lsx x264_template(load_deinterleave_chroma_fenc_lsx)
+void x264_load_deinterleave_chroma_fenc_lsx( pixel *dst, pixel *src, intptr_t i_src, int height );
+#define x264_load_deinterleave_chroma_fdec_lsx x264_template(load_deinterleave_chroma_fdec_lsx)
+void x264_load_deinterleave_chroma_fdec_lsx( pixel *dst, pixel *src, intptr_t i_src, int height );
+
+#define x264_plane_copy_interleave_core_lsx x264_template(plane_copy_interleave_core_lsx)
+void x264_plane_copy_interleave_core_lsx( pixel *dst,  intptr_t i_dst,
+                                          pixel *srcu, intptr_t i_srcu,
+                                          pixel *srcv, intptr_t i_srcv, int w, int h );
+#define x264_plane_copy_deinterleave_lsx x264_template(plane_copy_deinterleave_lsx)
+void x264_plane_copy_deinterleave_lsx( pixel *dstu, intptr_t i_dstu,
+                                       pixel *dstv, intptr_t i_dstv,
+                                       pixel *src,  intptr_t i_src, int w, int h );
+
+#define x264_prefetch_fenc_420_lsx x264_template(prefetch_fenc_420_lsx)
+void x264_prefetch_fenc_420_lsx( uint8_t *pix_y, intptr_t stride_y,
+                                 uint8_t *pix_uv, intptr_t stride_uv,
+                                 int32_t mb_x );
+#define x264_prefetch_fenc_422_lsx x264_template(prefetch_fenc_422_lsx)
+void x264_prefetch_fenc_422_lsx( uint8_t *pix_y, intptr_t stride_y,
+                                 uint8_t *pix_uv, intptr_t stride_uv,
+                                 int32_t mb_x );
+#define x264_prefetch_ref_lsx x264_template(prefetch_ref_lsx)
+void x264_prefetch_ref_lsx( uint8_t *pix, intptr_t stride, int32_t parity );
+
+#define x264_memcpy_aligned_lsx x264_template(memcpy_aligned_lsx)
+void *x264_memcpy_aligned_lsx( void *dst, const void *src, size_t n );
+#define x264_memzero_aligned_lsx x264_template(memzero_aligned_lsx)
+void x264_memzero_aligned_lsx( void *p_dst, size_t n );
+
+#define x264_hpel_filter_lsx x264_template(hpel_filter_lsx)
+void x264_hpel_filter_lsx( pixel *, pixel *, pixel *, pixel *, intptr_t, int, int, int16_t * );
+#define x264_frame_init_lowres_core_lsx x264_template(frame_init_lowres_core_lsx)
+void x264_frame_init_lowres_core_lsx( uint8_t *, uint8_t *, uint8_t *, uint8_t *,
+                                      uint8_t *, intptr_t, intptr_t, int, int );
+
 #define x264_pixel_avg_16x16_lasx x264_template(pixel_avg_16x16_lasx)
 void x264_pixel_avg_16x16_lasx( pixel *, intptr_t, pixel *, intptr_t, pixel *, intptr_t, int );
 #define x264_pixel_avg_16x8_lasx x264_template(pixel_avg_16x8_lasx)
@@ -82,32 +172,10 @@ void x264_mc_copy_w8_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, int );
 #define x264_mc_copy_w4_lasx x264_template(mc_copy_w4_lasx)
 void x264_mc_copy_w4_lasx( uint8_t *, intptr_t, uint8_t *, intptr_t, int );
 
-#define x264_store_interleave_chroma_lasx x264_template(store_interleave_chroma_lasx)
-void x264_store_interleave_chroma_lasx( pixel *dst, intptr_t i_dst, pixel *srcu, pixel *srcv, int height );
-#define x264_load_deinterleave_chroma_fenc_lasx x264_template(load_deinterleave_chroma_fenc_lasx)
-void x264_load_deinterleave_chroma_fenc_lasx( pixel *dst, pixel *src, intptr_t i_src, int height );
-#define x264_load_deinterleave_chroma_fdec_lasx x264_template(load_deinterleave_chroma_fdec_lasx)
-void x264_load_deinterleave_chroma_fdec_lasx( pixel *dst, pixel *src, intptr_t i_src, int height );
-
 #define x264_plane_copy_interleave_core_lasx x264_template(plane_copy_interleave_core_lasx)
 void x264_plane_copy_interleave_core_lasx( pixel *dst,  intptr_t i_dst,
                                            pixel *srcu, intptr_t i_srcu,
                                            pixel *srcv, intptr_t i_srcv, int w, int h );
-#define x264_plane_copy_deinterleave_lasx x264_template(plane_copy_deinterleave_lasx)
-void x264_plane_copy_deinterleave_lasx( pixel *dstu, intptr_t i_dstu,
-                                        pixel *dstv, intptr_t i_dstv,
-                                        pixel *src,  intptr_t i_src, int w, int h );
-
-#define x264_prefetch_fenc_420_lasx x264_template(prefetch_fenc_420_lasx)
-void x264_prefetch_fenc_420_lasx( uint8_t *pix_y, intptr_t stride_y,
-                                  uint8_t *pix_uv, intptr_t stride_uv,
-                                  int32_t mb_x );
-#define x264_prefetch_fenc_422_lasx x264_template(prefetch_fenc_422_lasx)
-void x264_prefetch_fenc_422_lasx( uint8_t *pix_y, intptr_t stride_y,
-                                  uint8_t *pix_uv, intptr_t stride_uv,
-                                  int32_t mb_x );
-#define x264_prefetch_ref_lasx x264_template(prefetch_ref_lasx)
-void x264_prefetch_ref_lasx( uint8_t *pix, intptr_t stride, int32_t parity );
 
 #define x264_memcpy_aligned_lasx x264_template(memcpy_aligned_lasx)
 void *x264_memcpy_aligned_lasx( void *dst, const void *src, size_t n );
diff --git a/common/loongarch/pixel-a.S b/common/loongarch/pixel-a.S
index b22fbf41..658690e6 100644
--- a/common/loongarch/pixel-a.S
+++ b/common/loongarch/pixel-a.S
@@ -24,7 +24,7 @@
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
 
-#include "asm.S"
+#include "loongson_asm.S"
 #if !HIGH_BIT_DEPTH
 
 const hmul_8p
@@ -1001,18 +1001,10 @@ function pixel_satd_4x8_lasx
     srli.d          a0,    t6,   1
 endfunc
 
-/* int x264_pixel_satd_wxh4x4_lasx(pixel *pix1, intptr_t i_pix1,
- *                                 pixel *pix2, intptr_t i_pix2)
+/* int x264_pixel_satd_4x4_lsx(pixel *pix1, intptr_t i_pix1,
+ *                             pixel *pix2, intptr_t i_pix2)
  */
-function pixel_satd_4x4_lasx
-    slli.d          t2,    a1,   1
-    slli.d          t3,    a3,   1
-    add.d           t4,    a1,   t2
-    add.d           t5,    a3,   t3
-
-    // Load data from pix1 and pix2
-    LSX_LOADX_4     a0,    a1,   t2,  t4,  vr1, vr2, vr3, vr4
-    LSX_LOADX_4     a2,    a3,   t3,  t5,  vr5, vr6, vr7, vr8
+.macro pixel_satd_4x4_lsx_core out
     vilvl.w         vr1,   vr2,  vr1
     vilvl.w         vr3,   vr4,  vr3
     vilvl.d         vr1,   vr3,  vr1
@@ -1038,11 +1030,23 @@ function pixel_satd_4x4_lasx
     vsub.h          vr12,  vr9,  vr10
     vpackev.d       vr9,   vr12, vr11
     vpackod.d       vr10,  vr12, vr11
-    vadda.h         vr9,   vr9,  vr10
-    vhaddw.wu.hu    vr9,   vr9,  vr9
-    vhaddw.du.wu    vr9,   vr9,  vr9
-    vhaddw.qu.du    vr9,   vr9,  vr9
-    vpickve2gr.wu   t5,    vr9,  0
+    vadda.h         \out,  vr9,  vr10
+.endm
+
+function pixel_satd_4x4_lsx
+    slli.d          t2,    a1,   1
+    slli.d          t3,    a3,   1
+    add.d           t4,    a1,   t2
+    add.d           t5,    a3,   t3
+
+    // Load data from pix1 and pix2
+    FLDS_LOADX_4    a0,    a1,   t2,  t4,  f1, f2, f3, f4
+    FLDS_LOADX_4    a2,    a3,   t3,  t5,  f5, f6, f7, f8
+    pixel_satd_4x4_lsx_core vr13
+    vhaddw.wu.hu    vr13,  vr13, vr13
+    vhaddw.du.wu    vr13,  vr13, vr13
+    vhaddw.qu.du    vr13,  vr13, vr13
+    vpickve2gr.wu   t5,    vr13,  0
     srli.d          a0,    t5,   1
 endfunc
 
@@ -2394,4 +2398,1538 @@ function pixel_var2_8x8_lasx
     add.w          a0,    t3,    t5
 endfunc
 
+
+/*
+ * uint64_t x264_pixel_hadamard_ac_8x8( pixel *pix, intptr_t stride )
+ */
+function hadamard_ac_8x8_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    FLDD_LOADX_4 a0, a1, t0, t1, f0, f1, f2, f3
+    alsl.d        a0,     a1,     a0,   2
+    FLDD_LOADX_4 a0, a1, t0, t1, f4, f5, f6, f7
+
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+
+    vpickev.b     vr2,    vr1,    vr0
+    vpickod.b     vr3,    vr1,    vr0
+    vaddwev.h.bu  vr6,    vr2,    vr3
+    vaddwod.h.bu  vr7,    vr2,    vr3
+    vsubwev.h.bu  vr8,    vr2,    vr3
+    vsubwod.h.bu  vr9,    vr2,    vr3
+    vadd.h        vr10,   vr6,    vr7
+    vadd.h        vr11,   vr8,    vr9
+    vsub.h        vr12,   vr6,    vr7
+    vsub.h        vr13,   vr8,    vr9
+
+    vilvl.h       vr6,    vr11,   vr10
+    vilvh.h       vr7,    vr11,   vr10
+    vilvl.h       vr8,    vr13,   vr12
+    vilvh.h       vr9,    vr13,   vr12
+    vilvl.w       vr10,   vr8,    vr6
+    vilvh.w       vr11,   vr8,    vr6
+    vilvl.w       vr12,   vr9,    vr7
+    vilvh.w       vr13,   vr9,    vr7
+
+    vadd.h        vr6,    vr10,   vr11
+    vadd.h        vr7,    vr12,   vr13
+    vsub.h        vr8,    vr10,   vr11
+    vsub.h        vr9,    vr12,   vr13
+    vadd.h        vr10,   vr6,    vr7
+    vadd.h        vr11,   vr8,    vr9
+    vsub.h        vr12,   vr6,    vr7
+    vsub.h        vr13,   vr8,    vr9
+
+    vpickev.b     vr2,    vr5,    vr4
+    vpickod.b     vr3,    vr5,    vr4
+    vaddwev.h.bu  vr6,    vr2,    vr3
+    vaddwod.h.bu  vr7,    vr2,    vr3
+    vsubwev.h.bu  vr8,    vr2,    vr3
+    vsubwod.h.bu  vr9,    vr2,    vr3
+    vadd.h        vr14,   vr6,    vr7
+    vadd.h        vr15,   vr8,    vr9
+    vsub.h        vr16,   vr6,    vr7
+    vsub.h        vr17,   vr8,    vr9
+
+    vilvl.h       vr6,    vr15,   vr14
+    vilvh.h       vr7,    vr15,   vr14
+    vilvl.h       vr8,    vr17,   vr16
+    vilvh.h       vr9,    vr17,   vr16
+    vilvl.w       vr14,   vr8,    vr6
+    vilvh.w       vr15,   vr8,    vr6
+    vilvl.w       vr16,   vr9,    vr7
+    vilvh.w       vr17,   vr9,    vr7
+
+    vadd.h        vr6,    vr14,   vr15
+    vadd.h        vr7,    vr16,   vr17
+    vsub.h        vr8,    vr14,   vr15
+    vsub.h        vr9,    vr16,   vr17
+    vadd.h        vr14,   vr6,    vr7
+    vadd.h        vr15,   vr8,    vr9
+    vsub.h        vr16,   vr6,    vr7
+    vsub.h        vr17,   vr8,    vr9
+
+    vadd.h        vr18,   vr10,   vr14
+    vpickve2gr.hu t0,     vr18,   0
+    vpickve2gr.hu t1,     vr18,   4
+    add.d         t1,     t0,     t1   // dc
+
+    vadda.h       vr4,    vr11,   vr10
+    vadda.h       vr5,    vr13,   vr12
+    vadda.h       vr6,    vr15,   vr14
+    vadda.h       vr7,    vr17,   vr16
+    vadd.h        vr4,    vr5,    vr4
+    vadd.h        vr6,    vr7,    vr6
+    vadd.h        vr4,    vr4,    vr6
+    vhaddw.wu.hu  vr4,    vr4,    vr4
+    vhaddw.du.wu  vr4,    vr4,    vr4
+    vhaddw.qu.du  vr4,    vr4,    vr4
+    vpickve2gr.wu t0,     vr4,    0    // sum4
+
+    vpackev.h     vr0,    vr11,   vr10
+    vpackev.h     vr1,    vr13,   vr12
+    vpackev.h     vr2,    vr15,   vr14
+    vpackev.h     vr3,    vr17,   vr16
+    vpackod.h     vr4,    vr11,   vr10
+    vpackod.h     vr5,    vr13,   vr12
+    vpackod.h     vr6,    vr15,   vr14
+    vpackod.h     vr7,    vr17,   vr16
+
+    vilvl.d       vr10,   vr1,    vr0
+    vilvh.d       vr11,   vr1,    vr0
+    vilvl.d       vr12,   vr3,    vr2
+    vilvh.d       vr13,   vr3,    vr2
+    vilvl.d       vr14,   vr5,    vr4
+    vilvh.d       vr15,   vr5,    vr4
+    vilvl.d       vr16,   vr7,    vr6
+    vilvh.d       vr17,   vr7,    vr6
+
+    vadd.h        vr0,    vr10,   vr11
+    vadd.h        vr1,    vr12,   vr13
+    vadd.h        vr2,    vr14,   vr16
+    vadd.h        vr3,    vr15,   vr17
+    vsub.h        vr4,    vr10,   vr11
+    vsub.h        vr5,    vr12,   vr13
+    vsub.h        vr6,    vr14,   vr16
+    vsub.h        vr7,    vr15,   vr17
+
+    vadd.h        vr10,    vr0,   vr1
+    vadd.h        vr11,    vr2,   vr3
+    vadd.h        vr12,    vr4,   vr5
+    vadd.h        vr13,    vr6,   vr7
+    vsub.h        vr14,    vr0,   vr1
+    vsub.h        vr15,    vr2,   vr3
+    vsub.h        vr16,    vr4,   vr5
+    vsub.h        vr17,    vr6,   vr7
+
+    vadda.h       vr10,   vr10,   vr11
+    vadda.h       vr11,   vr12,   vr13
+    vadda.h       vr12,   vr14,   vr15
+    vadda.h       vr13,   vr16,   vr17
+    vadd.h        vr10,   vr10,   vr11
+    vadd.h        vr11,   vr12,   vr13
+    vadd.h        vr10,   vr10,   vr11
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.du.wu  vr10,   vr10,   vr10
+    vhaddw.qu.du  vr10,   vr10,   vr10
+    vpickve2gr.wu t2,     vr10,   0     // sum8
+
+    sub.d         t0,     t0,     t1
+    sub.d         t2,     t2,     t1
+    slli.d        t2,     t2,     32
+    add.d         a0,     t2,     t0
+endfunc
+
+/*
+ * int x264_pixel_satd_4x8( pixel *pix1, intptr_t i_pix1,
+ *                          pixel *pix2, intptr_t i_pix2 )
+ */
+function pixel_satd_4x8_lsx
+    slli.d        t2,     a1,     1
+    slli.d        t3,     a3,     1
+    add.d         t4,     a1,     t2
+    add.d         t5,     a3,     t3
+
+    // Load data from pix1 and pix2
+    FLDS_LOADX_4  a0,     a1,     t2,  t4,  f1, f2, f3, f4
+    FLDS_LOADX_4  a2,     a3,     t3,  t5,  f5, f6, f7, f8
+    pixel_satd_4x4_lsx_core vr13
+    alsl.d        a0,     a1,     a0,  2
+    alsl.d        a2,     a3,     a2,  2
+    FLDS_LOADX_4  a0,     a1,     t2,  t4,  f1, f2, f3, f4
+    FLDS_LOADX_4  a2,     a3,     t3,  t5,  f5, f6, f7, f8
+    pixel_satd_4x4_lsx_core vr14
+    vadd.h        vr13,   vr14,   vr13
+    vhaddw.wu.hu  vr13,   vr13,   vr13
+    vhaddw.du.wu  vr13,   vr13,   vr13
+    vhaddw.qu.du  vr13,   vr13,   vr13
+    vpickve2gr.wu t5,     vr13,   0
+    srli.d        a0,     t5,     1
+endfunc
+
+/*
+ * int x264_pixel_satd_4x16( uint8_t *p_pix1, intptr_t i_stride,
+ *                           uint8_t *p_pix2, intptr_t i_stride2 )
+ */
+function pixel_satd_4x16_lsx
+    slli.d        t2,     a1,     1
+    slli.d        t3,     a3,     1
+    add.d         t4,     a1,     t2
+    add.d         t5,     a3,     t3
+
+    // Load data from pix1 and pix2
+    FLDS_LOADX_4  a0,     a1,     t2,  t4,  f1, f2, f3, f4
+    FLDS_LOADX_4  a2,     a3,     t3,  t5,  f5, f6, f7, f8
+    pixel_satd_4x4_lsx_core vr13
+    alsl.d        a0,     a1,     a0,  2
+    alsl.d        a2,     a3,     a2,  2
+    FLDS_LOADX_4  a0,     a1,     t2,  t4,  f1, f2, f3, f4
+    FLDS_LOADX_4  a2,     a3,     t3,  t5,  f5, f6, f7, f8
+    pixel_satd_4x4_lsx_core vr14
+
+    alsl.d        a0,     a1,     a0,  2
+    alsl.d        a2,     a3,     a2,  2
+    FLDS_LOADX_4  a0,     a1,     t2,  t4,  f1, f2, f3, f4
+    FLDS_LOADX_4  a2,     a3,     t3,  t5,  f5, f6, f7, f8
+    pixel_satd_4x4_lsx_core vr15
+
+    alsl.d        a0,     a1,     a0,  2
+    alsl.d        a2,     a3,     a2,  2
+    FLDS_LOADX_4  a0,     a1,     t2,  t4,  f1, f2, f3, f4
+    FLDS_LOADX_4  a2,     a3,     t3,  t5,  f5, f6, f7, f8
+    pixel_satd_4x4_lsx_core vr16
+
+    vadd.h        vr13,   vr14,   vr13
+    vadd.h        vr15,   vr16,   vr15
+    vadd.h        vr13,   vr15,   vr13
+    vhaddw.wu.hu  vr13,   vr13,   vr13
+    vhaddw.du.wu  vr13,   vr13,   vr13
+    vhaddw.qu.du  vr13,   vr13,   vr13
+    vpickve2gr.wu t5,     vr13,   0
+    srli.d        a0,     t5,     1
+endfunc
+
+.macro pixel_satd_8x4_lsx_core out0, out1, out2, out3
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr2,    vr5,    vr4
+    vilvl.d       vr3,    vr7,    vr6
+
+    vsubwev.h.bu  vr4,    vr0,    vr2
+    vsubwod.h.bu  vr5,    vr0,    vr2
+    vsubwev.h.bu  vr6,    vr1,    vr3
+    vsubwod.h.bu  vr7,    vr1,    vr3
+    vadd.h        vr0,    vr4,    vr5
+    vsub.h        vr1,    vr4,    vr5
+    vadd.h        vr2,    vr6,    vr7
+    vsub.h        vr3,    vr6,    vr7
+    vpackev.h     vr4,    vr1,    vr0
+    vpackod.h     vr5,    vr1,    vr0
+    vpackev.h     vr6,    vr3,    vr2
+    vpackod.h     vr7,    vr3,    vr2
+    vadd.h        vr8,    vr4,    vr5
+    vsub.h        vr9,    vr4,    vr5
+    vadd.h        vr10,   vr6,    vr7
+    vsub.h        vr11,   vr6,    vr7
+    vilvl.d       vr4,    vr9,    vr8
+    vilvh.d       vr5,    vr9,    vr8
+    vilvl.d       vr6,    vr11,   vr10
+    vilvh.d       vr7,    vr11,   vr10
+    vadd.h        vr8,    vr4,    vr5
+    vsub.h        vr9,    vr4,    vr5
+    vadd.h        vr10,   vr6,    vr7
+    vsub.h        vr11,   vr6,    vr7
+    vadd.h        \out0,  vr8,    vr10
+    vsub.h        \out1,  vr8,    vr10
+    vadd.h        \out2,  vr9,    vr11
+    vsub.h        \out3,  vr9,    vr11
+.endm
+
+/*
+ * int x264_pixel_satd_8x4( uint8_t *p_pix1, intptr_t i_stride,
+ *                          uint8_t *p_pix2, intptr_t i_stride2 )
+ */
+function pixel_satd_8x4_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    slli.d        t2,     a3,     1
+    add.d         t3,     t2,     a3
+
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr12, vr13, vr14, vr15
+    vadda.h       vr12,   vr13,   vr12
+    vadda.h       vr13,   vr15,   vr14
+
+    vadd.h        vr12,   vr13,   vr12
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.du.wu  vr12,   vr12,   vr12
+    vhaddw.qu.du  vr12,   vr12,   vr12
+    vpickve2gr.wu t4,     vr12,   0
+    srli.d        a0,     t4,     1
+endfunc
+
+/*
+ * int x264_pixel_satd_8x8( uint8_t *p_pix1, intptr_t i_stride,
+ *                          uint8_t *p_pix2, intptr_t i_stride2 )
+ */
+function pixel_satd_8x8_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    slli.d        t2,     a3,     1
+    add.d         t3,     t2,     a3
+
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr12, vr13, vr14, vr15
+    vadda.h       vr12,   vr13,   vr12
+    vadda.h       vr13,   vr15,   vr14
+    vadd.h        vr12,   vr13,   vr12
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr13, vr14, vr15, vr16
+    vadda.h       vr13,   vr14,   vr13
+    vadda.h       vr14,   vr16,   vr15
+    vadd.h        vr13,   vr14,   vr13
+
+    vadd.h        vr12,   vr13,   vr12
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.du.wu  vr12,   vr12,   vr12
+    vhaddw.qu.du  vr12,   vr12,   vr12
+    vpickve2gr.wu t4,     vr12,   0
+    srli.d        a0,     t4,     1
+endfunc
+
+/*
+ * int x264_pixel_satd_8x8( uint8_t *p_pix1, intptr_t i_stride,
+ *                          uint8_t *p_pix2, intptr_t i_stride2 )
+ */
+function pixel_satd_8x16_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    slli.d        t2,     a3,     1
+    add.d         t3,     t2,     a3
+
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr12, vr13, vr14, vr15
+    vadda.h       vr12,   vr13,   vr12
+    vadda.h       vr13,   vr15,   vr14
+    vadd.h        vr12,   vr13,   vr12
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr13, vr14, vr15, vr16
+    vadda.h       vr13,   vr14,   vr13
+    vadda.h       vr14,   vr16,   vr15
+    vadd.h        vr13,   vr14,   vr13
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr14, vr15, vr16, vr17
+    vadda.h       vr14,   vr15,   vr14
+    vadda.h       vr15,   vr17,   vr16
+    vadd.h        vr14,   vr15,   vr14
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr15, vr16, vr17, vr18
+    vadda.h       vr15,   vr16,   vr15
+    vadda.h       vr16,   vr18,   vr17
+    vadd.h        vr15,   vr16,   vr15
+
+    vadd.h        vr12,   vr12,   vr13
+    vadd.h        vr14,   vr14,   vr15
+    vadd.h        vr12,   vr12,   vr14
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.du.wu  vr12,   vr12,   vr12
+    vhaddw.qu.du  vr12,   vr12,   vr12
+    vpickve2gr.wu t4,     vr12,   0
+    srli.d        a0,     t4,     1
+endfunc
+
+/*
+ * int x264_pixel_satd_16x8( uint8_t *p_pix1, intptr_t i_stride,
+ *                           uint8_t *p_pix2, intptr_t i_stride2 )
+ */
+function pixel_satd_16x8_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    slli.d        t2,     a3,     1
+    add.d         t3,     t2,     a3
+
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr12, vr13, vr14, vr15
+    vadda.h       vr12,   vr13,   vr12
+    vadda.h       vr13,   vr15,   vr14
+    vadd.h        vr12,   vr13,   vr12
+
+    addi.d        t5,     a0,     8
+    addi.d        t6,     a2,     8
+    FLDD_LOADX_4  t5,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  t6,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr13, vr14, vr15, vr16
+    vadda.h       vr13,   vr14,   vr13
+    vadda.h       vr14,   vr16,   vr15
+    vadd.h        vr13,   vr14,   vr13
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr14, vr15, vr16, vr17
+    vadda.h       vr14,   vr15,   vr14
+    vadda.h       vr15,   vr17,   vr16
+    vadd.h        vr14,   vr15,   vr14
+
+    addi.d        t5,     a0,     8
+    addi.d        t6,     a2,     8
+    FLDD_LOADX_4  t5,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  t6,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr15, vr16, vr17, vr18
+    vadda.h       vr15,   vr16,   vr15
+    vadda.h       vr16,   vr18,   vr17
+    vadd.h        vr15,   vr16,   vr15
+
+    vadd.h        vr12,   vr13,   vr12
+    vadd.h        vr14,   vr15,   vr14
+    vadd.h        vr12,   vr14,   vr12
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.du.wu  vr12,   vr12,   vr12
+    vhaddw.qu.du  vr12,   vr12,   vr12
+    vpickve2gr.wu t4,     vr12,   0
+    srli.d        a0,     t4,     1
+endfunc
+
+/*
+ * int x264_pixel_satd_16x16( uint8_t *p_pix1, intptr_t i_stride,
+ *                           uint8_t *p_pix2, intptr_t i_stride2 )
+ */
+function pixel_satd_16x16_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    slli.d        t2,     a3,     1
+    add.d         t3,     t2,     a3
+
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr12, vr13, vr14, vr15
+    vadda.h       vr12,   vr13,   vr12
+    vadda.h       vr13,   vr15,   vr14
+    vadd.h        vr12,   vr13,   vr12
+
+    addi.d        t5,     a0,     8
+    addi.d        t6,     a2,     8
+    FLDD_LOADX_4  t5,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  t6,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr13, vr14, vr15, vr16
+    vadda.h       vr13,   vr14,   vr13
+    vadda.h       vr14,   vr16,   vr15
+    vadd.h        vr13,   vr14,   vr13
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr14, vr15, vr16, vr17
+    vadda.h       vr14,   vr15,   vr14
+    vadda.h       vr15,   vr17,   vr16
+    vadd.h        vr14,   vr15,   vr14
+
+    addi.d        t5,     a0,     8
+    addi.d        t6,     a2,     8
+    FLDD_LOADX_4  t5,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  t6,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr15, vr16, vr17, vr18
+    vadda.h       vr15,   vr16,   vr15
+    vadda.h       vr16,   vr18,   vr17
+    vadd.h        vr15,   vr16,   vr15
+
+    vadd.h        vr12,   vr13,   vr12
+    vadd.h        vr14,   vr15,   vr14
+    vadd.h        vr19,   vr14,   vr12
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr12, vr13, vr14, vr15
+    vadda.h       vr12,   vr13,   vr12
+    vadda.h       vr13,   vr15,   vr14
+    vadd.h        vr12,   vr13,   vr12
+
+    addi.d        t5,     a0,     8
+    addi.d        t6,     a2,     8
+    FLDD_LOADX_4  t5,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  t6,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr13, vr14, vr15, vr16
+    vadda.h       vr13,   vr14,   vr13
+    vadda.h       vr14,   vr16,   vr15
+    vadd.h        vr13,   vr14,   vr13
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr14, vr15, vr16, vr17
+    vadda.h       vr14,   vr15,   vr14
+    vadda.h       vr15,   vr17,   vr16
+    vadd.h        vr14,   vr15,   vr14
+
+    addi.d        t5,     a0,     8
+    addi.d        t6,     a2,     8
+    FLDD_LOADX_4  t5,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  t6,     a3,     t2,   t3, f4, f5, f6, f7
+    pixel_satd_8x4_lsx_core vr15, vr16, vr17, vr18
+    vadda.h       vr15,   vr16,   vr15
+    vadda.h       vr16,   vr18,   vr17
+    vadd.h        vr15,   vr16,   vr15
+
+    vadd.h        vr12,   vr13,   vr12
+    vadd.h        vr14,   vr15,   vr14
+    vadd.h        vr12,   vr14,   vr12
+    vadd.h        vr12,   vr19,   vr12
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.du.wu  vr12,   vr12,   vr12
+    vhaddw.qu.du  vr12,   vr12,   vr12
+    vpickve2gr.wu t4,     vr12,   0
+    srli.d        a0,     t4,     1
+endfunc
+
+/*
+ * int x264_pixel_ssd_4x4( pixel *pix1, intptr_t i_stride_pix1,
+ *                         pixel *pix2, intptr_t i_stride_pix2 )
+ */
+function pixel_ssd_4x4_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    slli.d        t2,     a3,     1
+    add.d         t3,     a3,     t2
+
+    FLDS_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDS_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+
+    vilvl.w       vr0,    vr1,    vr0
+    vilvl.w       vr1,    vr3,    vr2
+    vilvl.w       vr4,    vr5,    vr4
+    vilvl.w       vr5,    vr7,    vr6
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr4,    vr5,    vr4
+    vsubwev.h.bu  vr1,    vr0,    vr4
+    vsubwod.h.bu  vr2,    vr0,    vr4
+    vmul.h        vr5,    vr1,    vr1
+    vmul.h        vr6,    vr2,    vr2
+    vhaddw.wu.hu  vr5,    vr5,    vr5
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vadd.w        vr5,    vr5,    vr6
+    vhaddw.d.w    vr5,    vr5,    vr5
+    vhaddw.q.d    vr5,    vr5,    vr5
+    vpickve2gr.w  a0,     vr5,    0
+endfunc
+
+/*
+ * int x264_pixel_ssd_4x8( pixel *pix1, intptr_t i_stride_pix1,
+ *                         pixel *pix2, intptr_t i_stride_pix2 )
+ */
+function pixel_ssd_4x8_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    slli.d        t2,     a3,     1
+    add.d         t3,     a3,     t2
+
+    FLDS_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDS_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.w       vr0,    vr1,    vr0
+    vilvl.w       vr1,    vr3,    vr2
+    vilvl.w       vr4,    vr5,    vr4
+    vilvl.w       vr5,    vr7,    vr6
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr4,    vr5,    vr4
+    vsubwev.h.bu  vr1,    vr0,    vr4
+    vsubwod.h.bu  vr2,    vr0,    vr4
+    vmul.h        vr5,    vr1,    vr1
+    vmul.h        vr6,    vr2,    vr2
+    vhaddw.wu.hu  vr5,    vr5,    vr5
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vadd.w        vr10,   vr5,    vr6
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDS_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDS_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.w       vr0,    vr1,    vr0
+    vilvl.w       vr1,    vr3,    vr2
+    vilvl.w       vr4,    vr5,    vr4
+    vilvl.w       vr5,    vr7,    vr6
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr4,    vr5,    vr4
+    vsubwev.h.bu  vr1,    vr0,    vr4
+    vsubwod.h.bu  vr2,    vr0,    vr4
+    vmul.h        vr5,    vr1,    vr1
+    vmul.h        vr6,    vr2,    vr2
+    vhaddw.wu.hu  vr5,    vr5,    vr5
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vadd.w        vr5,    vr5,    vr6
+
+    vadd.w        vr5,    vr5,    vr10
+    vhaddw.d.w    vr5,    vr5,    vr5
+    vhaddw.q.d    vr5,    vr5,    vr5
+    vpickve2gr.w  a0,     vr5,    0
+endfunc
+
+/*
+ * int x264_pixel_ssd_4x16( pixel *pix1, intptr_t i_stride_pix1,
+ *                          pixel *pix2, intptr_t i_stride_pix2 )
+ */
+function pixel_ssd_4x16_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    slli.d        t2,     a3,     1
+    add.d         t3,     a3,     t2
+
+    FLDS_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDS_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.w       vr0,    vr1,    vr0
+    vilvl.w       vr1,    vr3,    vr2
+    vilvl.w       vr4,    vr5,    vr4
+    vilvl.w       vr5,    vr7,    vr6
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr4,    vr5,    vr4
+    vsubwev.h.bu  vr1,    vr0,    vr4
+    vsubwod.h.bu  vr2,    vr0,    vr4
+    vmul.h        vr5,    vr1,    vr1
+    vmul.h        vr6,    vr2,    vr2
+    vhaddw.wu.hu  vr5,    vr5,    vr5
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vadd.w        vr10,   vr5,    vr6
+
+.rept 3
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDS_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDS_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.w       vr0,    vr1,    vr0
+    vilvl.w       vr1,    vr3,    vr2
+    vilvl.w       vr4,    vr5,    vr4
+    vilvl.w       vr5,    vr7,    vr6
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr4,    vr5,    vr4
+    vsubwev.h.bu  vr1,    vr0,    vr4
+    vsubwod.h.bu  vr2,    vr0,    vr4
+    vmul.h        vr5,    vr1,    vr1
+    vmul.h        vr6,    vr2,    vr2
+    vhaddw.wu.hu  vr5,    vr5,    vr5
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vadd.w        vr5,    vr5,    vr6
+    vadd.w        vr10,   vr5,    vr10
+.endr
+
+    vhaddw.d.w    vr10,   vr10,   vr10
+    vhaddw.q.d    vr10,   vr10,   vr10
+    vpickve2gr.w  a0,     vr10,   0
+endfunc
+
+/*
+ * int x264_pixel_ssd_8x4( pixel *pix1, intptr_t i_stride_pix1,
+ *                         pixel *pix2, intptr_t i_stride_pix2 )
+ */
+function pixel_ssd_8x4_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    slli.d        t2,     a3,     1
+    add.d         t3,     a3,     t2
+
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vsubwev.h.bu  vr2,    vr0,    vr4
+    vsubwod.h.bu  vr3,    vr0,    vr4
+    vsubwev.h.bu  vr6,    vr1,    vr5
+    vsubwod.h.bu  vr7,    vr1,    vr5
+    vmul.h        vr2,    vr2,    vr2
+    vmul.h        vr3,    vr3,    vr3
+    vmul.h        vr6,    vr6,    vr6
+    vmul.h        vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.wu.hu  vr3,    vr3,    vr3
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vadd.w        vr2,    vr2,    vr3
+    vadd.w        vr6,    vr6,    vr7
+    vadd.w        vr2,    vr2,    vr6
+    vhaddw.d.w    vr2,    vr2,    vr2
+    vhaddw.q.d    vr2,    vr2,    vr2
+    vpickve2gr.w  a0,     vr2,    0
+endfunc
+
+/*
+ * int x264_pixel_ssd_8x8( pixel *pix1, intptr_t i_stride_pix1,
+ *                         pixel *pix2, intptr_t i_stride_pix2 )
+ */
+function pixel_ssd_8x8_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    slli.d        t2,     a3,     1
+    add.d         t3,     a3,     t2
+
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vsubwev.h.bu  vr2,    vr0,    vr4
+    vsubwod.h.bu  vr3,    vr0,    vr4
+    vsubwev.h.bu  vr6,    vr1,    vr5
+    vsubwod.h.bu  vr7,    vr1,    vr5
+    vmul.h        vr2,    vr2,    vr2
+    vmul.h        vr3,    vr3,    vr3
+    vmul.h        vr6,    vr6,    vr6
+    vmul.h        vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.wu.hu  vr3,    vr3,    vr3
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vadd.w        vr2,    vr2,    vr3
+    vadd.w        vr6,    vr6,    vr7
+    vadd.w        vr10,   vr2,    vr6
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vsubwev.h.bu  vr2,    vr0,    vr4
+    vsubwod.h.bu  vr3,    vr0,    vr4
+    vsubwev.h.bu  vr6,    vr1,    vr5
+    vsubwod.h.bu  vr7,    vr1,    vr5
+    vmul.h        vr2,    vr2,    vr2
+    vmul.h        vr3,    vr3,    vr3
+    vmul.h        vr6,    vr6,    vr6
+    vmul.h        vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.wu.hu  vr3,    vr3,    vr3
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vadd.w        vr2,    vr2,    vr3
+    vadd.w        vr6,    vr6,    vr7
+    vadd.w        vr11,   vr2,    vr6
+
+    vadd.w        vr10,   vr10,   vr11
+    vhaddw.d.w    vr10,   vr10,   vr10
+    vhaddw.q.d    vr10,   vr10,   vr10
+    vpickve2gr.w  a0,     vr10,   0
+endfunc
+
+/*
+ * int x264_pixel_ssd_8x16( pixel *pix1, intptr_t i_stride_pix1,
+ *                          pixel *pix2, intptr_t i_stride_pix2 )
+ */
+function pixel_ssd_8x16_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    slli.d        t2,     a3,     1
+    add.d         t3,     a3,     t2
+
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vsubwev.h.bu  vr2,    vr0,    vr4
+    vsubwod.h.bu  vr3,    vr0,    vr4
+    vsubwev.h.bu  vr6,    vr1,    vr5
+    vsubwod.h.bu  vr7,    vr1,    vr5
+    vmul.h        vr2,    vr2,    vr2
+    vmul.h        vr3,    vr3,    vr3
+    vmul.h        vr6,    vr6,    vr6
+    vmul.h        vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.wu.hu  vr3,    vr3,    vr3
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vadd.w        vr2,    vr2,    vr3
+    vadd.w        vr6,    vr6,    vr7
+    vadd.w        vr10,   vr2,    vr6
+
+.rept 3
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    FLDD_LOADX_4  a0,     a1,     t0,   t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2,   t3, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vsubwev.h.bu  vr2,    vr0,    vr4
+    vsubwod.h.bu  vr3,    vr0,    vr4
+    vsubwev.h.bu  vr6,    vr1,    vr5
+    vsubwod.h.bu  vr7,    vr1,    vr5
+    vmul.h        vr2,    vr2,    vr2
+    vmul.h        vr3,    vr3,    vr3
+    vmul.h        vr6,    vr6,    vr6
+    vmul.h        vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.wu.hu  vr3,    vr3,    vr3
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vadd.w        vr2,    vr2,    vr3
+    vadd.w        vr6,    vr6,    vr7
+    vadd.w        vr11,   vr2,    vr6
+    vadd.w        vr10,   vr10,   vr11
+.endr
+
+    vhaddw.d.w    vr10,   vr10,   vr10
+    vhaddw.q.d    vr10,   vr10,   vr10
+    vpickve2gr.w  a0,     vr10,   0
+endfunc
+
+/*
+ * int x264_pixel_ssd_16x8( pixel *pix1, intptr_t i_stride_pix1,
+ *                          pixel *pix2, intptr_t i_stride_pix2 )
+ */
+function pixel_ssd_16x8_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    slli.d        t2,     a3,     1
+    add.d         t3,     a3,     t2
+
+    LSX_LOADX_4   a0,     a1,     t0,   t1, vr0, vr1, vr2, vr3
+    LSX_LOADX_4   a2,     a3,     t2,   t3, vr4, vr5, vr6, vr7
+    vsubwev.h.bu  vr8,    vr0,    vr4
+    vsubwod.h.bu  vr9,    vr0,    vr4
+    vsubwev.h.bu  vr10,   vr1,    vr5
+    vsubwod.h.bu  vr11,   vr1,    vr5
+    vsubwev.h.bu  vr12,   vr2,    vr6
+    vsubwod.h.bu  vr13,   vr2,    vr6
+    vsubwev.h.bu  vr14,   vr3,    vr7
+    vsubwod.h.bu  vr15,   vr3,    vr7
+    vmul.h        vr8,    vr8,    vr8
+    vmul.h        vr9,    vr9,    vr9
+    vmul.h        vr10,   vr10,   vr10
+    vmul.h        vr11,   vr11,   vr11
+    vmul.h        vr12,   vr12,   vr12
+    vmul.h        vr13,   vr13,   vr13
+    vmul.h        vr14,   vr14,   vr14
+    vmul.h        vr15,   vr15,   vr15
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.wu.hu  vr13,   vr13,   vr13
+    vhaddw.wu.hu  vr14,   vr14,   vr14
+    vhaddw.wu.hu  vr15,   vr15,   vr15
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr9,    vr10,   vr11
+    vadd.w        vr10,   vr12,   vr13
+    vadd.w        vr11,   vr14,   vr15
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr9,    vr10,   vr11
+    vadd.w        vr16,   vr8,    vr9
+
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    LSX_LOADX_4   a0,     a1,     t0,   t1, vr0, vr1, vr2, vr3
+    LSX_LOADX_4   a2,     a3,     t2,   t3, vr4, vr5, vr6, vr7
+    vsubwev.h.bu  vr8,    vr0,    vr4
+    vsubwod.h.bu  vr9,    vr0,    vr4
+    vsubwev.h.bu  vr10,   vr1,    vr5
+    vsubwod.h.bu  vr11,   vr1,    vr5
+    vsubwev.h.bu  vr12,   vr2,    vr6
+    vsubwod.h.bu  vr13,   vr2,    vr6
+    vsubwev.h.bu  vr14,   vr3,    vr7
+    vsubwod.h.bu  vr15,   vr3,    vr7
+    vmul.h        vr8,    vr8,    vr8
+    vmul.h        vr9,    vr9,    vr9
+    vmul.h        vr10,   vr10,   vr10
+    vmul.h        vr11,   vr11,   vr11
+    vmul.h        vr12,   vr12,   vr12
+    vmul.h        vr13,   vr13,   vr13
+    vmul.h        vr14,   vr14,   vr14
+    vmul.h        vr15,   vr15,   vr15
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.wu.hu  vr13,   vr13,   vr13
+    vhaddw.wu.hu  vr14,   vr14,   vr14
+    vhaddw.wu.hu  vr15,   vr15,   vr15
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr9,    vr10,   vr11
+    vadd.w        vr10,   vr12,   vr13
+    vadd.w        vr11,   vr14,   vr15
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr9,    vr10,   vr11
+    vadd.w        vr17,   vr8,    vr9
+
+    vadd.w        vr10,   vr16,   vr17
+    vhaddw.d.w    vr10,   vr10,   vr10
+    vhaddw.q.d    vr10,   vr10,   vr10
+    vpickve2gr.w  a0,     vr10,   0
+endfunc
+
+/*
+ * int x264_pixel_ssd_16x16( pixel *pix1, intptr_t i_stride_pix1,
+ *                          pixel *pix2, intptr_t i_stride_pix2 )
+ */
+function pixel_ssd_16x16_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    slli.d        t2,     a3,     1
+    add.d         t3,     a3,     t2
+
+    LSX_LOADX_4   a0,     a1,     t0,   t1, vr0, vr1, vr2, vr3
+    LSX_LOADX_4   a2,     a3,     t2,   t3, vr4, vr5, vr6, vr7
+    vsubwev.h.bu  vr8,    vr0,    vr4
+    vsubwod.h.bu  vr9,    vr0,    vr4
+    vsubwev.h.bu  vr10,   vr1,    vr5
+    vsubwod.h.bu  vr11,   vr1,    vr5
+    vsubwev.h.bu  vr12,   vr2,    vr6
+    vsubwod.h.bu  vr13,   vr2,    vr6
+    vsubwev.h.bu  vr14,   vr3,    vr7
+    vsubwod.h.bu  vr15,   vr3,    vr7
+    vmul.h        vr8,    vr8,    vr8
+    vmul.h        vr9,    vr9,    vr9
+    vmul.h        vr10,   vr10,   vr10
+    vmul.h        vr11,   vr11,   vr11
+    vmul.h        vr12,   vr12,   vr12
+    vmul.h        vr13,   vr13,   vr13
+    vmul.h        vr14,   vr14,   vr14
+    vmul.h        vr15,   vr15,   vr15
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.wu.hu  vr13,   vr13,   vr13
+    vhaddw.wu.hu  vr14,   vr14,   vr14
+    vhaddw.wu.hu  vr15,   vr15,   vr15
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr9,    vr10,   vr11
+    vadd.w        vr10,   vr12,   vr13
+    vadd.w        vr11,   vr14,   vr15
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr9,    vr10,   vr11
+    vadd.w        vr16,   vr8,    vr9
+
+.rept 3
+    alsl.d        a0,     a1,     a0,   2
+    alsl.d        a2,     a3,     a2,   2
+    LSX_LOADX_4   a0,     a1,     t0,   t1, vr0, vr1, vr2, vr3
+    LSX_LOADX_4   a2,     a3,     t2,   t3, vr4, vr5, vr6, vr7
+    vsubwev.h.bu  vr8,    vr0,    vr4
+    vsubwod.h.bu  vr9,    vr0,    vr4
+    vsubwev.h.bu  vr10,   vr1,    vr5
+    vsubwod.h.bu  vr11,   vr1,    vr5
+    vsubwev.h.bu  vr12,   vr2,    vr6
+    vsubwod.h.bu  vr13,   vr2,    vr6
+    vsubwev.h.bu  vr14,   vr3,    vr7
+    vsubwod.h.bu  vr15,   vr3,    vr7
+    vmul.h        vr8,    vr8,    vr8
+    vmul.h        vr9,    vr9,    vr9
+    vmul.h        vr10,   vr10,   vr10
+    vmul.h        vr11,   vr11,   vr11
+    vmul.h        vr12,   vr12,   vr12
+    vmul.h        vr13,   vr13,   vr13
+    vmul.h        vr14,   vr14,   vr14
+    vmul.h        vr15,   vr15,   vr15
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vhaddw.wu.hu  vr13,   vr13,   vr13
+    vhaddw.wu.hu  vr14,   vr14,   vr14
+    vhaddw.wu.hu  vr15,   vr15,   vr15
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr9,    vr10,   vr11
+    vadd.w        vr10,   vr12,   vr13
+    vadd.w        vr11,   vr14,   vr15
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr9,    vr10,   vr11
+    vadd.w        vr17,   vr8,    vr9
+    vadd.w        vr16,   vr16,   vr17
+.endr
+    vhaddw.d.w    vr16,   vr16,   vr16
+    vhaddw.q.d    vr16,   vr16,   vr16
+    vpickve2gr.w  a0,     vr16,   0
+endfunc
+
+/*
+ * int x264_pixel_sa8d_8x8( pixel *pix1, intptr_t i_pix1, pixel *pix2, intptr_t i_pix2 )
+ */
+.macro pixel_sa8d_8x8_lsx_core out0, out1, out2, out3
+    FLDD_LOADX_4  a0,     a1,     t0, t1, f0, f1, f2, f3
+    FLDD_LOADX_4  a2,     a3,     t2, t3, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vsubwev.h.bu  vr2,    vr0,    vr4
+    vsubwod.h.bu  vr3,    vr0,    vr4
+    vsubwev.h.bu  vr6,    vr1,    vr5
+    vsubwod.h.bu  vr7,    vr1,    vr5
+    vadd.h        vr8,    vr2,    vr3
+    vsub.h        vr9,    vr2,    vr3
+    vadd.h        vr10,   vr6,    vr7
+    vsub.h        vr11,   vr6,    vr7
+    vpackev.h     vr0,    vr9,    vr8
+    vpackod.h     vr1,    vr9,    vr8
+    vpackev.h     vr2,    vr11,   vr10
+    vpackod.h     vr3,    vr11,   vr10
+    vadd.h        vr4,    vr0,    vr1
+    vsub.h        vr5,    vr0,    vr1
+    vadd.h        vr6,    vr2,    vr3
+    vsub.h        vr7,    vr2,    vr3
+    vilvl.d       vr0,    vr5,    vr4
+    vilvh.d       vr1,    vr5,    vr4
+    vilvl.d       vr2,    vr7,    vr6
+    vilvh.d       vr3,    vr7,    vr6
+    vadd.h        vr12,   vr0,    vr1
+    vsub.h        vr13,   vr0,    vr1
+    vadd.h        vr14,   vr2,    vr3
+    vsub.h        vr15,   vr2,    vr3
+
+    alsl.d        t4,     a1,     a0,    2
+    alsl.d        t5,     a3,     a2,    2
+    FLDD_LOADX_4  t4,     a1,     t0, t1, f0, f1, f2, f3
+    FLDD_LOADX_4  t5,     a3,     t2, t3, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vsubwev.h.bu  vr2,    vr0,    vr4
+    vsubwod.h.bu  vr3,    vr0,    vr4
+    vsubwev.h.bu  vr6,    vr1,    vr5
+    vsubwod.h.bu  vr7,    vr1,    vr5
+    vadd.h        vr8,    vr2,    vr3
+    vsub.h        vr9,    vr2,    vr3
+    vadd.h        vr10,   vr6,    vr7
+    vsub.h        vr11,   vr6,    vr7
+    vpackev.h     vr0,    vr9,    vr8
+    vpackod.h     vr1,    vr9,    vr8
+    vpackev.h     vr2,    vr11,   vr10
+    vpackod.h     vr3,    vr11,   vr10
+    vadd.h        vr4,    vr0,    vr1
+    vsub.h        vr5,    vr0,    vr1
+    vadd.h        vr6,    vr2,    vr3
+    vsub.h        vr7,    vr2,    vr3
+    vilvl.d       vr0,    vr5,    vr4
+    vilvh.d       vr1,    vr5,    vr4
+    vilvl.d       vr2,    vr7,    vr6
+    vilvh.d       vr3,    vr7,    vr6
+    vadd.h        vr4,    vr0,    vr1
+    vsub.h        vr5,    vr0,    vr1
+    vadd.h        vr6,    vr2,    vr3
+    vsub.h        vr7,    vr2,    vr3
+
+    // vr12 vr13 vr14 vr15
+    vpickev.w     vr0,    vr13,   vr12
+    vpickod.w     vr1,    vr13,   vr12
+    vpickev.w     vr2,    vr15,   vr14
+    vpickod.w     vr3,    vr15,   vr14
+    vadd.h        vr8,    vr0,    vr1
+    vsub.h        vr9,    vr0,    vr1
+    vadd.h        vr10,   vr2,    vr3
+    vsub.h        vr11,   vr2,    vr3
+    vadd.h        vr12,   vr8,    vr10
+    vadd.h        vr13,   vr9,    vr11
+    vsub.h        vr14,   vr8,    vr10
+    vsub.h        vr15,   vr9,    vr11
+
+    // vr4 vr5 vr6 vr7
+    vpickev.w     vr0,    vr5,    vr4
+    vpickod.w     vr1,    vr5,    vr4
+    vpickev.w     vr2,    vr7,    vr6
+    vpickod.w     vr3,    vr7,    vr6
+    vadd.h        vr8,    vr0,    vr1
+    vsub.h        vr9,    vr0,    vr1
+    vadd.h        vr10,   vr2,    vr3
+    vsub.h        vr11,   vr2,    vr3
+    vadd.h        vr4,    vr8,    vr10
+    vadd.h        vr5,    vr9,    vr11
+    vsub.h        vr6,    vr8,    vr10
+    vsub.h        vr7,    vr9,    vr11
+
+    vadd.h        vr0,    vr12,   vr4
+    vadd.h        vr1,    vr13,   vr5
+    vadd.h        vr2,    vr14,   vr6
+    vadd.h        vr3,    vr15,   vr7
+    vsub.h        vr8,    vr12,   vr4
+    vsub.h        vr9,    vr13,   vr5
+    vsub.h        vr10,   vr14,   vr6
+    vsub.h        vr11,   vr15,   vr7
+    vadda.h       \out0,  vr0,    vr8
+    vadda.h       \out1,  vr1,    vr9
+    vadda.h       \out2,  vr2,    vr10
+    vadda.h       \out3,  vr3,    vr11
+.endm
+
+function pixel_sa8d_8x8_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    slli.d        t2,     a3,     1
+    add.d         t3,     t2,     a3
+    pixel_sa8d_8x8_lsx_core vr0, vr1, vr2, vr3
+    vadd.h        vr0,    vr0,    vr1
+    vadd.h        vr1,    vr2,    vr3
+    vadd.h        vr17,   vr0,    vr1
+    vhaddw.wu.hu  vr17,   vr17,   vr17
+    vhaddw.du.wu  vr17,   vr17,   vr17
+    vhaddw.qu.du  vr17,   vr17,   vr17
+    vpickve2gr.wu t5,     vr17,   0
+    addi.d        t5,     t5,     2
+    srli.d        a0,     t5,     2
+endfunc
+
+/*
+ * int x264_pixel_sa8d_16x16( pixel *pix1, intptr_t i_pix1,
+ *                            pixel *pix2, intptr_t i_pix2 )
+ */
+function pixel_sa8d_16x16_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    slli.d        t2,     a3,     1
+    add.d         t3,     t2,     a3
+    add.d         t6,     a0,     zero
+    add.d         t7,     a2,     zero
+    pixel_sa8d_8x8_lsx_core vr0, vr1, vr2, vr3
+    vadd.h        vr0,    vr0,    vr1
+    vadd.h        vr1,    vr2,    vr3
+    vadd.h        vr16,   vr0,    vr1
+
+    addi.d        a0,     t6,     8
+    addi.d        a2,     t7,     8
+    pixel_sa8d_8x8_lsx_core vr0, vr1, vr2, vr3
+    vadd.h        vr0,    vr0,    vr1
+    vadd.h        vr1,    vr2,    vr3
+    vadd.h        vr17,   vr0,    vr1
+
+    alsl.d        a0,     a1,     t6,   3
+    alsl.d        a2,     a3,     t7,   3
+    pixel_sa8d_8x8_lsx_core vr0, vr1, vr2, vr3
+    vadd.h        vr0,    vr0,    vr1
+    vadd.h        vr1,    vr2,    vr3
+    vadd.h        vr18,   vr0,    vr1
+
+    addi.d        a0,     a0,     8
+    addi.d        a2,     a2,     8
+    pixel_sa8d_8x8_lsx_core vr0, vr1, vr2, vr3
+    vadd.h        vr0,    vr0,    vr1
+    vadd.h        vr1,    vr2,    vr3
+    vadd.h        vr19,   vr0,    vr1
+
+    vhaddw.wu.hu  vr16,   vr16,   vr16
+    vhaddw.wu.hu  vr17,   vr17,   vr17
+    vhaddw.wu.hu  vr18,   vr18,   vr18
+    vhaddw.wu.hu  vr19,   vr19,   vr19
+    vadd.w        vr16,   vr17,   vr16
+    vadd.w        vr18,   vr19,   vr18
+    vadd.w        vr17,   vr18,   vr16
+    vhaddw.du.wu  vr17,   vr17,   vr17
+    vhaddw.qu.du  vr17,   vr17,   vr17
+    vpickve2gr.wu t5,     vr17,   0
+    addi.d        t5,     t5,     2
+    srli.d        a0,     t5,     2
+endfunc
+
+/*
+ * uint64_t pixel_var_8x8( pixel *pix, intptr_t i_stride )
+ */
+function pixel_var_8x8_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    FLDD_LOADX_4  a0,     a1,     t0, t1, f0, f1, f2, f3
+    alsl.d        a0,     a1,     a0,   2
+    FLDD_LOADX_4  a0,     a1,     t0, t1, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vhaddw.hu.bu  vr2,    vr0,    vr0
+    vhaddw.hu.bu  vr3,    vr1,    vr1
+    vhaddw.hu.bu  vr6,    vr4,    vr4
+    vhaddw.hu.bu  vr7,    vr5,    vr5
+    vadd.h        vr2,    vr2,    vr3
+    vadd.h        vr6,    vr6,    vr7
+    vadd.h        vr2,    vr2,    vr6
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.du.wu  vr2,    vr2,    vr2
+    vhaddw.qu.du  vr2,    vr2,    vr2
+    vpickve2gr.wu t5,     vr2,    0     // sum
+
+    vmulwev.h.bu  vr2,    vr0,    vr0
+    vmulwod.h.bu  vr3,    vr0,    vr0
+    vmulwev.h.bu  vr6,    vr1,    vr1
+    vmulwod.h.bu  vr7,    vr1,    vr1
+    vmulwev.h.bu  vr8,    vr4,    vr4
+    vmulwod.h.bu  vr9,    vr4,    vr4
+    vmulwev.h.bu  vr10,   vr5,    vr5
+    vmulwod.h.bu  vr11,   vr5,    vr5
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.wu.hu  vr3,    vr3,    vr3
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+
+    vadd.w        vr2,    vr2,    vr3
+    vadd.w        vr6,    vr6,    vr7
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr10,   vr10,   vr11
+    vadd.w        vr2,    vr2,    vr6
+    vadd.w        vr8,    vr8,    vr10
+    vadd.w        vr2,    vr2,    vr8
+    vhaddw.du.wu  vr2,    vr2,    vr2
+    vhaddw.qu.du  vr2,    vr2,    vr2
+    vpickve2gr.du t6,     vr2,    0     // sqr
+
+    slli.d        t4,     t6,     32
+    add.d         a0,     t4,     t5
+endfunc
+
+/*
+ * uint64_t pixel_var_8x16( pixel *pix, intptr_t i_stride )
+ */
+function pixel_var_8x16_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     a1,     t0
+    FLDD_LOADX_4  a0,     a1,     t0, t1, f0, f1, f2, f3
+    alsl.d        a0,     a1,     a0,   2
+    FLDD_LOADX_4  a0,     a1,     t0, t1, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vhaddw.hu.bu  vr2,    vr0,    vr0
+    vhaddw.hu.bu  vr3,    vr1,    vr1
+    vhaddw.hu.bu  vr6,    vr4,    vr4
+    vhaddw.hu.bu  vr7,    vr5,    vr5
+    vadd.h        vr2,    vr2,    vr3
+    vadd.h        vr6,    vr6,    vr7
+    vadd.h        vr16,   vr2,    vr6
+
+    vmulwev.h.bu  vr2,    vr0,    vr0
+    vmulwod.h.bu  vr3,    vr0,    vr0
+    vmulwev.h.bu  vr6,    vr1,    vr1
+    vmulwod.h.bu  vr7,    vr1,    vr1
+    vmulwev.h.bu  vr8,    vr4,    vr4
+    vmulwod.h.bu  vr9,    vr4,    vr4
+    vmulwev.h.bu  vr10,   vr5,    vr5
+    vmulwod.h.bu  vr11,   vr5,    vr5
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.wu.hu  vr3,    vr3,    vr3
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+    vadd.w        vr12,   vr2,    vr3
+    vadd.w        vr13,   vr6,    vr7
+    vadd.w        vr14,   vr8,    vr9
+    vadd.w        vr15,   vr10,   vr11
+    vadd.w        vr12,   vr12,   vr13
+    vadd.w        vr14,   vr14,   vr15
+    vadd.w        vr12,   vr12,   vr14
+
+    alsl.d        a0,     a1,     a0,   2
+    FLDD_LOADX_4  a0,     a1,     t0, t1, f0, f1, f2, f3
+    alsl.d        a0,     a1,     a0,   2
+    FLDD_LOADX_4  a0,     a1,     t0, t1, f4, f5, f6, f7
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr4,    vr5,    vr4
+    vilvl.d       vr5,    vr7,    vr6
+    vhaddw.hu.bu  vr2,    vr0,    vr0
+    vhaddw.hu.bu  vr3,    vr1,    vr1
+    vhaddw.hu.bu  vr6,    vr4,    vr4
+    vhaddw.hu.bu  vr7,    vr5,    vr5
+    vadd.h        vr2,    vr2,    vr3
+    vadd.h        vr6,    vr6,    vr7
+    vadd.h        vr2,    vr2,    vr6
+    vadd.h        vr2,    vr2,    vr16
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.du.wu  vr2,    vr2,    vr2
+    vhaddw.qu.du  vr2,    vr2,    vr2
+    vpickve2gr.wu t5,     vr2,    0     // sum
+
+    vmulwev.h.bu  vr2,    vr0,    vr0
+    vmulwod.h.bu  vr3,    vr0,    vr0
+    vmulwev.h.bu  vr6,    vr1,    vr1
+    vmulwod.h.bu  vr7,    vr1,    vr1
+    vmulwev.h.bu  vr8,    vr4,    vr4
+    vmulwod.h.bu  vr9,    vr4,    vr4
+    vmulwev.h.bu  vr10,   vr5,    vr5
+    vmulwod.h.bu  vr11,   vr5,    vr5
+    vhaddw.wu.hu  vr2,    vr2,    vr2
+    vhaddw.wu.hu  vr3,    vr3,    vr3
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+    vadd.w        vr2,    vr2,    vr3
+    vadd.w        vr6,    vr6,    vr7
+    vadd.w        vr8,    vr8,    vr9
+    vadd.w        vr10,   vr10,   vr11
+    vadd.w        vr2,    vr2,    vr6
+    vadd.w        vr8,    vr8,    vr10
+    vadd.w        vr2,    vr2,    vr8
+    vadd.w        vr2,    vr2,    vr12
+    vhaddw.du.wu  vr2,    vr2,    vr2
+    vhaddw.qu.du  vr2,    vr2,    vr2
+    vpickve2gr.du t6,     vr2,    0     // sqr
+    slli.d        t4,     t6,     32
+    add.d         a0,     t4,     t5
+endfunc
+
+/*
+ * uint64_t pixel_var_16x16( pixel *pix, intptr_t i_stride )
+ */
+function pixel_var_16x16_lsx
+    slli.d        t0,     a1,     1
+    add.d         t1,     t0,     a1
+    LSX_LOADX_4   a0,     a1,     t0, t1, vr0, vr1, vr2, vr3
+    vhaddw.hu.bu  vr4,    vr0,    vr0
+    vhaddw.hu.bu  vr5,    vr1,    vr1
+    vhaddw.hu.bu  vr6,    vr2,    vr2
+    vhaddw.hu.bu  vr7,    vr3,    vr3
+    vadd.h        vr4,    vr5,    vr4
+    vadd.h        vr5,    vr7,    vr6
+    vadd.h        vr13,   vr5,    vr4
+
+    vmulwev.h.bu  vr5,    vr0,    vr0
+    vmulwod.h.bu  vr6,    vr0,    vr0
+    vmulwev.h.bu  vr7,    vr1,    vr1
+    vmulwod.h.bu  vr8,    vr1,    vr1
+    vmulwev.h.bu  vr9,    vr2,    vr2
+    vmulwod.h.bu  vr10,   vr2,    vr2
+    vmulwev.h.bu  vr11,   vr3,    vr3
+    vmulwod.h.bu  vr12,   vr3,    vr3
+    vhaddw.wu.hu  vr5,    vr5,    vr5
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vadd.w        vr5,    vr5,    vr6
+    vadd.w        vr6,    vr8,    vr7
+    vadd.w        vr7,    vr10,   vr9
+    vadd.w        vr8,    vr12,   vr11
+    vadd.w        vr0,    vr5,    vr6
+    vadd.w        vr1,    vr8,    vr7
+    vadd.w        vr14,   vr1,    vr0
+
+.rept 3
+    alsl.d        a0,     a1,     a0,   2
+    LSX_LOADX_4   a0,     a1,     t0, t1, vr0, vr1, vr2, vr3
+    vhaddw.hu.bu  vr4,    vr0,    vr0
+    vhaddw.hu.bu  vr5,    vr1,    vr1
+    vhaddw.hu.bu  vr6,    vr2,    vr2
+    vhaddw.hu.bu  vr7,    vr3,    vr3
+    vadd.h        vr4,    vr5,    vr4
+    vadd.h        vr5,    vr7,    vr6
+    vadd.h        vr4,    vr5,    vr4
+    vadd.h        vr13,   vr4,    vr13
+
+    vmulwev.h.bu  vr5,    vr0,    vr0
+    vmulwod.h.bu  vr6,    vr0,    vr0
+    vmulwev.h.bu  vr7,    vr1,    vr1
+    vmulwod.h.bu  vr8,    vr1,    vr1
+    vmulwev.h.bu  vr9,    vr2,    vr2
+    vmulwod.h.bu  vr10,   vr2,    vr2
+    vmulwev.h.bu  vr11,   vr3,    vr3
+    vmulwod.h.bu  vr12,   vr3,    vr3
+    vhaddw.wu.hu  vr5,    vr5,    vr5
+    vhaddw.wu.hu  vr6,    vr6,    vr6
+    vhaddw.wu.hu  vr7,    vr7,    vr7
+    vhaddw.wu.hu  vr8,    vr8,    vr8
+    vhaddw.wu.hu  vr9,    vr9,    vr9
+    vhaddw.wu.hu  vr10,   vr10,   vr10
+    vhaddw.wu.hu  vr11,   vr11,   vr11
+    vhaddw.wu.hu  vr12,   vr12,   vr12
+    vadd.w        vr5,    vr5,    vr6
+    vadd.w        vr6,    vr8,    vr7
+    vadd.w        vr7,    vr10,   vr9
+    vadd.w        vr8,    vr12,   vr11
+    vadd.w        vr0,    vr5,    vr6
+    vadd.w        vr1,    vr8,    vr7
+    vadd.w        vr0,    vr1,    vr0
+    vadd.w        vr14,   vr0,    vr14
+.endr
+    vhaddw.wu.hu  vr13,   vr13,   vr13
+    vhaddw.du.wu  vr13,   vr13,   vr13
+    vhaddw.qu.du  vr13,   vr13,   vr13
+    vpickve2gr.wu t4,     vr13,   0
+
+    vhaddw.du.wu  vr14,   vr14,   vr14
+    vhaddw.qu.du  vr14,   vr14,   vr14
+    vpickve2gr.du t6,     vr14,   0     // sqr
+
+    slli.d        t5,     t6,     32
+    add.d         a0,     t4,     t5
+endfunc
+
+.macro sse_diff_8width_lsx in0, in1, in2, in3
+    fld.d         f0,     \in0,   0
+    fld.d         f1,     \in0,   FENC_STRIDE
+    fld.d         f2,     \in0,   FENC_STRIDE * 2
+    fld.d         f3,     \in0,   FENC_STRIDE * 3
+    fld.d         f4,     \in1,   0
+    fld.d         f5,     \in1,   FDEC_STRIDE
+    fld.d         f6,     \in1,   FDEC_STRIDE * 2
+    fld.d         f7,     \in1,   FDEC_STRIDE * 3
+
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr1,    vr3,    vr2
+    vilvl.d       vr2,    vr5,    vr4
+    vilvl.d       vr3,    vr7,    vr6
+    vsubwev.h.bu  vr4,    vr0,    vr2
+    vsubwod.h.bu  vr5,    vr0,    vr2
+    vsubwev.h.bu  vr6,    vr1,    vr3
+    vsubwod.h.bu  vr7,    vr1,    vr3
+    // sqr_u
+    vdp2add.w.h   \in2,   vr4,    vr4
+    vdp2add.w.h   \in2,   vr5,    vr5
+    vdp2add.w.h   \in2,   vr6,    vr6
+    vdp2add.w.h   \in2,   vr7,    vr7
+    // sum_u
+    vadd.h        vr4,    vr4,    vr5
+    vadd.h        vr6,    vr6,    vr7
+    vadd.h        \in3,   vr4,    vr6
+.endm
+
+/*
+ * int pixel_var2_8x8( pixel *fenc, pixel *fdec, int ssd[2] )
+ */
+function pixel_var2_8x8_lsx
+    vxor.v        vr8,    vr8,    vr8
+    sse_diff_8width_lsx a0, a1, vr8, vr9
+    addi.d        t0,     a0,     FENC_STRIDE * 4
+    addi.d        t1,     a1,     FDEC_STRIDE * 4
+    sse_diff_8width_lsx t0, t1, vr8, vr10
+    vhaddw.d.w    vr8,    vr8,    vr8
+    vhaddw.q.d    vr8,    vr8,    vr8
+    vpickve2gr.w  t2,     vr8,    0       // sqr_u
+    vadd.h        vr8,    vr10,   vr9
+    vhaddw.w.h    vr8,    vr8,    vr8
+    vhaddw.d.w    vr8,    vr8,    vr8
+    vhaddw.q.d    vr8,    vr8,    vr8
+    vpickve2gr.w  t3,     vr8,    0       // sum_u
+
+    addi.d        a0,     a0,     FENC_STRIDE / 2
+    addi.d        a1,     a1,     FDEC_STRIDE / 2
+    vxor.v        vr8,    vr8,    vr8
+    sse_diff_8width_lsx a0, a1, vr8, vr9
+    addi.d        t0,     a0,     FENC_STRIDE * 4
+    addi.d        t1,     a1,     FDEC_STRIDE * 4
+    sse_diff_8width_lsx t0, t1, vr8, vr10
+    vhaddw.d.w    vr8,    vr8,    vr8
+    vhaddw.q.d    vr8,    vr8,    vr8
+    vpickve2gr.w  t4,     vr8,    0       // sqr_v
+    vadd.h        vr8,    vr10,   vr9
+    vhaddw.w.h    vr8,    vr8,    vr8
+    vhaddw.d.w    vr8,    vr8,    vr8
+    vhaddw.q.d    vr8,    vr8,    vr8
+    vpickve2gr.w  t5,     vr8,    0       // sum_v
+
+    st.w          t2,     a2,     0
+    st.w          t4,     a2,     4
+    mul.w         t3,     t3,     t3
+    mul.w         t5,     t5,     t5
+    srai.w        t3,     t3,     6
+    srai.w        t5,     t5,     6
+    sub.w         t2,     t2,     t3
+    sub.w         t4,     t4,     t5
+    add.w         a0,     t2,     t4
+endfunc
+
+/*
+ * int pixel_var2_8x16( pixel *fenc, pixel *fdec, int ssd[2] )
+ */
+function pixel_var2_8x16_lsx
+    vxor.v        vr8,    vr8,    vr8
+    sse_diff_8width_lsx a0, a1, vr8, vr9
+    addi.d        t0,     a0,     FENC_STRIDE * 4
+    addi.d        t1,     a1,     FDEC_STRIDE * 4
+    sse_diff_8width_lsx t0, t1, vr8, vr10
+    addi.d        t0,     t0,     FENC_STRIDE * 4
+    addi.d        t1,     t1,     FDEC_STRIDE * 4
+    sse_diff_8width_lsx t0, t1, vr8, vr11
+    addi.d        t0,     t0,     FENC_STRIDE * 4
+    addi.d        t1,     t1,     FDEC_STRIDE * 4
+    sse_diff_8width_lsx t0, t1, vr8, vr12
+    vhaddw.d.w    vr8,    vr8,    vr8
+    vhaddw.q.d    vr8,    vr8,    vr8
+    vpickve2gr.w  t2,     vr8,    0       // sqr_u
+    vadd.h        vr8,    vr10,   vr9
+    vadd.h        vr8,    vr11,   vr8
+    vadd.h        vr8,    vr12,   vr8
+    vhaddw.w.h    vr8,    vr8,    vr8
+    vhaddw.d.w    vr8,    vr8,    vr8
+    vhaddw.q.d    vr8,    vr8,    vr8
+    vpickve2gr.w  t3,     vr8,    0       // sum_u
+
+    addi.d        a0,     a0,     FENC_STRIDE / 2
+    addi.d        a1,     a1,     FDEC_STRIDE / 2
+    vxor.v        vr8,    vr8,    vr8
+    sse_diff_8width_lsx a0, a1, vr8, vr9
+    addi.d        t0,     a0,     FENC_STRIDE * 4
+    addi.d        t1,     a1,     FDEC_STRIDE * 4
+    sse_diff_8width_lsx t0, t1, vr8, vr10
+    addi.d        t0,     t0,     FENC_STRIDE * 4
+    addi.d        t1,     t1,     FDEC_STRIDE * 4
+    sse_diff_8width_lsx t0, t1, vr8, vr11
+    addi.d        t0,     t0,     FENC_STRIDE * 4
+    addi.d        t1,     t1,     FDEC_STRIDE * 4
+    sse_diff_8width_lsx t0, t1, vr8, vr12
+    vhaddw.d.w    vr8,    vr8,    vr8
+    vhaddw.q.d    vr8,    vr8,    vr8
+    vpickve2gr.w  t4,     vr8,    0       // sqr_v
+    vadd.h        vr8,    vr10,   vr9
+    vadd.h        vr8,    vr11,   vr8
+    vadd.h        vr8,    vr12,   vr8
+    vhaddw.w.h    vr8,    vr8,    vr8
+    vhaddw.d.w    vr8,    vr8,    vr8
+    vhaddw.q.d    vr8,    vr8,    vr8
+    vpickve2gr.w  t5,     vr8,    0       // sum_v
+
+    st.w          t2,     a2,     0
+    st.w          t4,     a2,     4
+    mul.w         t3,     t3,     t3
+    mul.w         t5,     t5,     t5
+    srai.w        t3,     t3,     7
+    srai.w        t5,     t5,     7
+    sub.w         t2,     t2,     t3
+    sub.w         t4,     t4,     t5
+    add.w         a0,     t2,     t4
+endfunc
 #endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/pixel-c.c b/common/loongarch/pixel-c.c
index 89f29670..c09e7422 100644
--- a/common/loongarch/pixel-c.c
+++ b/common/loongarch/pixel-c.c
@@ -30,6 +30,47 @@
 
 #if !HIGH_BIT_DEPTH
 
+uint64_t x264_pixel_hadamard_ac_8x8_lsx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = x264_hadamard_ac_8x8_lsx( p_pix, i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_8x16_lsx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = x264_hadamard_ac_8x8_lsx( p_pix, i_stride );
+    u_sum += x264_hadamard_ac_8x8_lsx( p_pix + 8 * i_stride, i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_16x8_lsx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = x264_hadamard_ac_8x8_lsx( p_pix, i_stride );
+    u_sum += x264_hadamard_ac_8x8_lsx( p_pix + 8, i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_16x16_lsx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = x264_hadamard_ac_8x8_lsx( p_pix, i_stride );
+    u_sum += x264_hadamard_ac_8x8_lsx( p_pix + 8, i_stride );
+    u_sum += x264_hadamard_ac_8x8_lsx( p_pix + 8 * i_stride, i_stride );
+    u_sum += x264_hadamard_ac_8x8_lsx( p_pix + 8 * i_stride + 8, i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
 uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
 {
     uint64_t u_sum;
@@ -49,6 +90,24 @@ uint64_t x264_pixel_hadamard_ac_8x16_lasx( uint8_t *p_pix, intptr_t i_stride )
     return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
 }
 
+void x264_intra_sa8d_x3_8x8_lsx( uint8_t *p_enc, uint8_t p_edge[36],
+                                 int32_t p_sad_array[3] )
+{
+    ALIGNED_ARRAY_16( uint8_t, pix, [8 * FDEC_STRIDE] );
+
+    x264_predict_8x8_v_lsx( pix, p_edge );
+    p_sad_array[0] = x264_pixel_sa8d_8x8_lsx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_predict_8x8_h_lsx( pix, p_edge );
+    p_sad_array[1] = x264_pixel_sa8d_8x8_lsx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_predict_8x8_dc_lsx( pix, p_edge );
+    p_sad_array[2] = x264_pixel_sa8d_8x8_lsx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
 void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
                                   int32_t p_sad_array[3] )
 {
@@ -67,20 +126,36 @@ void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
                                                p_enc, FENC_STRIDE );
 }
 
-void x264_intra_satd_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
-                                  int32_t p_sad_array[3] )
+void x264_intra_satd_x3_4x4_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                 int32_t p_sad_array[3] )
 {
     x264_predict_4x4_v_lsx( p_dec );
-    p_sad_array[0] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
-                                               p_enc, FENC_STRIDE );
+    p_sad_array[0] = x264_pixel_satd_4x4_lsx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
 
     x264_predict_4x4_h_lsx( p_dec );
-    p_sad_array[1] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
-                                               p_enc, FENC_STRIDE );
+    p_sad_array[1] = x264_pixel_satd_4x4_lsx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
 
     x264_predict_4x4_dc_lsx( p_dec );
-    p_sad_array[2] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
-                                               p_enc, FENC_STRIDE );
+    p_sad_array[2] = x264_pixel_satd_4x4_lsx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
+void x264_intra_satd_x3_16x16_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] )
+{
+    x264_predict_16x16_v_lsx( p_dec );
+    p_sad_array[0] = x264_pixel_satd_16x16_lsx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
+
+    x264_predict_16x16_h_lsx( p_dec );
+    p_sad_array[1] = x264_pixel_satd_16x16_lsx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
+
+    x264_predict_16x16_dc_lsx( p_dec );
+    p_sad_array[2] = x264_pixel_satd_16x16_lsx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
 }
 
 void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
@@ -99,6 +174,22 @@ void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                                  p_enc, FENC_STRIDE );
 }
 
+void x264_intra_satd_x3_8x8c_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] )
+{
+    x264_predict_8x8c_dc_lsx( p_dec );
+    p_sad_array[0] = x264_pixel_satd_8x8_lsx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_predict_8x8c_h_lsx( p_dec );
+    p_sad_array[1] = x264_pixel_satd_8x8_lsx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_predict_8x8c_v_lsx( p_dec );
+    p_sad_array[2] = x264_pixel_satd_8x8_lsx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
 void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                    int32_t p_sad_array[3] )
 {
@@ -115,20 +206,36 @@ void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                                p_enc, FENC_STRIDE );
 }
 
-void x264_intra_sad_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
-                                 int32_t p_sad_array[3] )
+void x264_intra_sad_x3_4x4_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                int32_t p_sad_array[3] )
 {
     x264_predict_4x4_v_lsx( p_dec );
-    p_sad_array[0] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
-                                              p_enc, FENC_STRIDE );
+    p_sad_array[0] = x264_pixel_sad_4x4_lsx( p_dec, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
 
     x264_predict_4x4_h_lsx( p_dec );
-    p_sad_array[1] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
-                                              p_enc, FENC_STRIDE );
+    p_sad_array[1] = x264_pixel_sad_4x4_lsx( p_dec, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
 
     x264_predict_4x4_dc_lsx( p_dec );
-    p_sad_array[2] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
-                                              p_enc, FENC_STRIDE );
+    p_sad_array[2] = x264_pixel_sad_4x4_lsx( p_dec, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_16x16_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] )
+{
+    x264_predict_16x16_v_lsx( p_dec );
+    p_sad_array[0] = x264_pixel_sad_16x16_lsx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_predict_16x16_h_lsx( p_dec );
+    p_sad_array[1] = x264_pixel_sad_16x16_lsx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_predict_16x16_dc_lsx( p_dec );
+    p_sad_array[2] = x264_pixel_sad_16x16_lsx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
 }
 
 void x264_intra_sad_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
@@ -147,6 +254,24 @@ void x264_intra_sad_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                                 p_enc, FENC_STRIDE );
 }
 
+void x264_intra_sad_x3_8x8_lsx( uint8_t *p_enc, uint8_t p_edge[36],
+                                int32_t p_sad_array[3] )
+{
+    ALIGNED_ARRAY_16( uint8_t, pix, [8 * FDEC_STRIDE] );
+
+    x264_predict_8x8_v_lsx( pix, p_edge );
+    p_sad_array[0] = x264_pixel_sad_8x8_lsx( pix, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
+
+    x264_predict_8x8_h_lsx( pix, p_edge );
+    p_sad_array[1] = x264_pixel_sad_8x8_lsx( pix, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
+
+    x264_predict_8x8_dc_lsx( pix, p_edge );
+    p_sad_array[2] = x264_pixel_sad_8x8_lsx( pix, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
+}
+
 void x264_intra_sad_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
                                  int32_t p_sad_array[3] )
 {
@@ -165,6 +290,22 @@ void x264_intra_sad_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
                                               p_enc, FENC_STRIDE );
 }
 
+void x264_intra_sad_x3_8x8c_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                 int32_t p_sad_array[3] )
+{
+    x264_predict_8x8c_dc_lsx( p_dec );
+    p_sad_array[0] = x264_pixel_sad_8x8_lsx( p_dec, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
+
+    x264_predict_8x8c_h_lsx( p_dec );
+    p_sad_array[1] = x264_pixel_sad_8x8_lsx( p_dec, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
+
+    x264_predict_8x8c_v_lsx( p_dec );
+    p_sad_array[2] = x264_pixel_sad_8x8_lsx( p_dec, FDEC_STRIDE,
+                                             p_enc, FENC_STRIDE );
+}
+
 void x264_intra_sad_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                   int32_t p_sad_array[3] )
 {
diff --git a/common/loongarch/pixel.h b/common/loongarch/pixel.h
index 958400d3..405cbf86 100644
--- a/common/loongarch/pixel.h
+++ b/common/loongarch/pixel.h
@@ -27,9 +27,31 @@
 #ifndef X264_LOONGARCH_PIXEL_H
 #define X264_LOONGARCH_PIXEL_H
 
-#define x264_pixel_satd_4x4_lasx x264_template(pixel_satd_4x4_lasx)
-int32_t x264_pixel_satd_4x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+#define x264_pixel_satd_4x4_lsx x264_template(pixel_satd_4x4_lsx)
+int32_t x264_pixel_satd_4x4_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                 uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_4x8_lsx x264_template(pixel_satd_4x8_lsx)
+int32_t x264_pixel_satd_4x8_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                 uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_4x16_lsx x264_template(pixel_satd_4x16_lsx)
+int32_t x264_pixel_satd_4x16_lsx( uint8_t *p_pix1, intptr_t i_stride,
                                   uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x4_lsx x264_template(pixel_satd_8x4_lsx)
+int32_t x264_pixel_satd_8x4_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                 uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x8_lsx x264_template(pixel_satd_8x8_lsx)
+int32_t x264_pixel_satd_8x8_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                 uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x16_lsx x264_template(pixel_satd_8x16_lsx)
+int32_t x264_pixel_satd_8x16_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_16x8_lsx x264_template(pixel_satd_16x8_lsx)
+int32_t x264_pixel_satd_16x8_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_16x16_lsx x264_template(pixel_satd_16x16_lsx)
+int32_t x264_pixel_satd_16x16_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 );
+
 #define x264_pixel_satd_4x8_lasx x264_template(pixel_satd_4x8_lasx)
 int32_t x264_pixel_satd_4x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
                                   uint8_t *p_pix2, intptr_t i_stride2 );
@@ -52,6 +74,37 @@ int32_t x264_pixel_satd_16x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
 int32_t x264_pixel_satd_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
                                     uint8_t *p_pix2, intptr_t i_stride2 );
 
+#define x264_pixel_sad_x4_16x16_lsx x264_template(pixel_sad_x4_16x16_lsx)
+void x264_pixel_sad_x4_16x16_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_16x8_lsx x264_template(pixel_sad_x4_16x8_lsx)
+void x264_pixel_sad_x4_16x8_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x16_lsx x264_template(pixel_sad_x4_8x16_lsx)
+void x264_pixel_sad_x4_8x16_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x8_lsx x264_template(pixel_sad_x4_8x8_lsx)
+void x264_pixel_sad_x4_8x8_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                uint8_t *p_ref1, uint8_t *p_ref2,
+                                uint8_t *p_ref3, intptr_t i_ref_stride,
+                                int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x4_lsx x264_template(pixel_sad_x4_8x4_lsx)
+void x264_pixel_sad_x4_8x4_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                uint8_t *p_ref1, uint8_t *p_ref2,
+                                uint8_t *p_ref3, intptr_t i_ref_stride,
+                                int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_4x8_lsx x264_template(pixel_sad_x4_4x8_lsx)
+void x264_pixel_sad_x4_4x8_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                uint8_t *p_ref1, uint8_t *p_ref2,
+                                uint8_t *p_ref3, intptr_t i_ref_stride,
+                                int32_t p_sad_array[4] );
+
 #define x264_pixel_sad_x4_16x16_lasx x264_template(pixel_sad_x4_16x16_lasx)
 void x264_pixel_sad_x4_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
                                    uint8_t *p_ref1, uint8_t *p_ref2,
@@ -88,6 +141,37 @@ void x264_pixel_sad_x4_4x4_lsx( uint8_t *p_src, uint8_t *p_ref0,
                                 uint8_t *p_ref3, intptr_t i_ref_stride,
                                 int32_t p_sad_array[4] );
 
+#define x264_pixel_sad_x3_16x16_lsx x264_template(pixel_sad_x3_16x16_lsx)
+void x264_pixel_sad_x3_16x16_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_16x8_lsx x264_template(pixel_sad_x3_16x8_lsx)
+void x264_pixel_sad_x3_16x8_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x16_lsx x264_template(pixel_sad_x3_8x16_lsx)
+void x264_pixel_sad_x3_8x16_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x8_lsx x264_template(pixel_sad_x3_8x8_lsx)
+void x264_pixel_sad_x3_8x8_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                uint8_t *p_ref1, uint8_t *p_ref2,
+                                intptr_t i_ref_stride,
+                                int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x4_lsx x264_template(pixel_sad_x3_8x4_lsx)
+void x264_pixel_sad_x3_8x4_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                uint8_t *p_ref1, uint8_t *p_ref2,
+                                intptr_t i_ref_stride,
+                                int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_4x8_lsx x264_template(pixel_sad_x3_4x8_lsx)
+void x264_pixel_sad_x3_4x8_lsx( uint8_t *p_src, uint8_t *p_ref0,
+                                uint8_t *p_ref1, uint8_t *p_ref2,
+                                intptr_t i_ref_stride,
+                                int32_t p_sad_array[3] );
+
 #define x264_pixel_sad_x3_16x16_lasx x264_template(pixel_sad_x3_16x16_lasx)
 void x264_pixel_sad_x3_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
                                   uint8_t *p_ref1, uint8_t *p_ref2,
@@ -124,6 +208,31 @@ void x264_pixel_sad_x3_4x4_lsx( uint8_t *p_src, uint8_t *p_ref0,
                                 intptr_t i_ref_stride,
                                 int32_t p_sad_array[3] );
 
+#define x264_pixel_sad_16x16_lsx x264_template(pixel_sad_16x16_lsx)
+int32_t x264_pixel_sad_16x16_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_16x8_lsx x264_template(pixel_sad_16x8_lsx)
+int32_t x264_pixel_sad_16x8_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x16_lsx x264_template(pixel_sad_8x16_lsx)
+int32_t x264_pixel_sad_8x16_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x8_lsx x264_template(pixel_sad_8x8_lsx)
+int32_t x264_pixel_sad_8x8_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x4_lsx x264_template(pixel_sad_8x4_lsx)
+int32_t x264_pixel_sad_8x4_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x16_lsx x264_template(pixel_sad_4x16_lsx)
+int32_t x264_pixel_sad_4x16_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x8_lsx x264_template(pixel_sad_4x8_lsx)
+int32_t x264_pixel_sad_4x8_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x4_lsx x264_template(pixel_sad_4x4_lsx)
+int32_t x264_pixel_sad_4x4_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                uint8_t *p_ref, intptr_t i_ref_stride );
+
 #define x264_pixel_sad_16x16_lasx x264_template(pixel_sad_16x16_lasx)
 int32_t x264_pixel_sad_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                    uint8_t *p_ref, intptr_t i_ref_stride );
@@ -145,9 +254,17 @@ int32_t x264_pixel_sad_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
 #define x264_pixel_sad_4x8_lasx x264_template(pixel_sad_4x8_lasx)
 int32_t x264_pixel_sad_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride );
-#define x264_pixel_sad_4x4_lasx x264_template(pixel_sad_4x4_lasx)
-int32_t x264_pixel_sad_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
-                                 uint8_t *p_ref, intptr_t i_ref_stride );
+
+#define x264_hadamard_ac_8x8_lsx x264_template(hadamard_ac_8x8_lsx)
+uint64_t x264_hadamard_ac_8x8_lsx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_8x8_lsx x264_template(pixel_hadamard_ac_8x8_lsx)
+uint64_t x264_pixel_hadamard_ac_8x8_lsx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_8x16_lsx x264_template(pixel_hadamard_ac_8x16_lsx)
+uint64_t x264_pixel_hadamard_ac_8x16_lsx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_16x8_lsx x264_template(pixel_hadamard_ac_16x8_lsx)
+uint64_t x264_pixel_hadamard_ac_16x8_lsx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_16x16_lsx x264_template(pixel_hadamard_ac_16x16_lsx)
+uint64_t x264_pixel_hadamard_ac_16x16_lsx( uint8_t *p_pix, intptr_t i_stride );
 
 #define x264_hadamard_ac_8x8_lasx x264_template(hadamard_ac_8x8_lasx)
 uint64_t x264_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride );
@@ -160,9 +277,15 @@ uint64_t x264_pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix, intptr_t i_stride );
 #define x264_pixel_hadamard_ac_16x16_lasx x264_template(pixel_hadamard_ac_16x16_lasx)
 uint64_t x264_pixel_hadamard_ac_16x16_lasx( uint8_t *p_pix, intptr_t i_stride );
 
-#define x264_intra_satd_x3_4x4_lasx x264_template(intra_satd_x3_4x4_lasx)
-void x264_intra_satd_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+#define x264_intra_satd_x3_16x16_lsx x264_template(intra_satd_x3_16x16_lsx)
+void x264_intra_satd_x3_16x16_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] );
+#define x264_intra_satd_x3_8x8c_lsx x264_template(intra_satd_x3_8x8c_lsx)
+void x264_intra_satd_x3_8x8c_lsx( uint8_t *p_enc, uint8_t *p_dec,
                                   int32_t p_sad_array[3] );
+#define x264_intra_satd_x3_4x4_lsx x264_template(intra_satd_x3_4x4_lsx)
+void x264_intra_satd_x3_4x4_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                 int32_t p_sad_array[3] );
 #define x264_intra_satd_x3_16x16_lasx x264_template(intra_satd_x3_16x16_lasx)
 void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                     int32_t p_sad_array[3] );
@@ -170,6 +293,31 @@ void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
 void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                    int32_t p_sad_array[3] );
 
+#define x264_pixel_ssd_16x16_lsx x264_template(pixel_ssd_16x16_lsx)
+int32_t x264_pixel_ssd_16x16_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_16x8_lsx x264_template(pixel_ssd_16x8_lsx)
+int32_t x264_pixel_ssd_16x8_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x16_lsx x264_template(pixel_ssd_8x16_lsx)
+int32_t x264_pixel_ssd_8x16_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x8_lsx x264_template(pixel_ssd_8x8_lsx)
+int32_t x264_pixel_ssd_8x8_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x4_lsx x264_template(pixel_ssd_8x4_lsx)
+int32_t x264_pixel_ssd_8x4_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x16_lsx x264_template(pixel_ssd_4x16_lsx)
+int32_t x264_pixel_ssd_4x16_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x8_lsx x264_template(pixel_ssd_4x8_lsx)
+int32_t x264_pixel_ssd_4x8_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x4_lsx x264_template(pixel_ssd_4x4_lsx)
+int32_t x264_pixel_ssd_4x4_lsx( uint8_t *p_src, intptr_t i_src_stride,
+                                uint8_t *p_ref, intptr_t i_ref_stride );
+
 #define x264_pixel_ssd_16x16_lasx x264_template(pixel_ssd_16x16_lasx)
 int32_t x264_pixel_ssd_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                    uint8_t *p_ref, intptr_t i_ref_stride );
@@ -195,6 +343,19 @@ int32_t x264_pixel_ssd_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
 int32_t x264_pixel_ssd_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
                                  uint8_t *p_ref, intptr_t i_ref_stride );
 
+#define x264_pixel_var2_8x16_lsx x264_template(pixel_var2_8x16_lsx)
+int32_t x264_pixel_var2_8x16_lsx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                   int32_t ssd[2] );
+#define x264_pixel_var2_8x8_lsx x264_template(pixel_var2_8x8_lsx)
+int32_t x264_pixel_var2_8x8_lsx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                 int32_t ssd[2] );
+#define x264_pixel_var_16x16_lsx x264_template(pixel_var_16x16_lsx)
+uint64_t x264_pixel_var_16x16_lsx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_var_8x16_lsx x264_template(pixel_var_8x16_lsx)
+uint64_t x264_pixel_var_8x16_lsx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_var_8x8_lsx x264_template(pixel_var_8x8_lsx)
+uint64_t x264_pixel_var_8x8_lsx( uint8_t *p_pix, intptr_t i_stride );
+
 #define x264_pixel_var_16x16_lasx x264_template(pixel_var_16x16_lasx)
 uint64_t x264_pixel_var_16x16_lasx( uint8_t *p_pix, intptr_t i_stride );
 #define x264_pixel_var_8x16_lasx x264_template(pixel_var_8x16_lasx)
@@ -208,6 +369,16 @@ int32_t x264_pixel_var2_8x16_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
 int32_t x264_pixel_var2_8x8_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
                                   int32_t ssd[2] );
 
+#define x264_pixel_sa8d_8x8_lsx x264_template(pixel_sa8d_8x8_lsx)
+int32_t x264_pixel_sa8d_8x8_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                 uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_sa8d_16x16_lsx x264_template(pixel_sa8d_16x16_lsx)
+int32_t x264_pixel_sa8d_16x16_lsx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 );
+
+#define x264_intra_sa8d_x3_8x8_lsx x264_template(intra_sa8d_x3_8x8_lsx)
+void x264_intra_sa8d_x3_8x8_lsx( uint8_t *p_enc, uint8_t p_edge[36],
+                                 int32_t p_sad_array[3] );
 #define x264_intra_sa8d_x3_8x8_lasx x264_template(intra_sa8d_x3_8x8_lasx)
 void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
                                   int32_t p_sad_array[3] );
@@ -218,9 +389,19 @@ int32_t x264_pixel_sa8d_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
 int32_t x264_pixel_sa8d_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
                                     uint8_t *p_pix2, intptr_t i_stride2 );
 
-#define x264_intra_sad_x3_4x4_lasx x264_template(intra_sad_x3_4x4_lasx)
-void x264_intra_sad_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+#define x264_intra_sad_x3_16x16_lsx x264_template(intra_sad_x3_16x16_lsx)
+void x264_intra_sad_x3_16x16_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_8x8_lsx x264_template(intra_sad_x3_8x8_lsx)
+void x264_intra_sad_x3_8x8_lsx( uint8_t *p_enc, uint8_t p_edge[36],
+                                int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_8x8c_lsx x264_template(intra_sad_x3_8x8c_lsx)
+void x264_intra_sad_x3_8x8c_lsx( uint8_t *p_enc, uint8_t *p_dec,
                                  int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_4x4_lsx x264_template(intra_sad_x3_4x4_lsx)
+void x264_intra_sad_x3_4x4_lsx( uint8_t *p_enc, uint8_t *p_dec,
+                                int32_t p_sad_array[3] );
+
 #define x264_intra_sad_x3_16x16_lasx x264_template(intra_sad_x3_16x16_lasx)
 void x264_intra_sad_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
                                    int32_t p_sad_array[3] );
diff --git a/common/loongarch/predict-a.S b/common/loongarch/predict-a.S
index 938df142..caf1f52f 100644
--- a/common/loongarch/predict-a.S
+++ b/common/loongarch/predict-a.S
@@ -25,7 +25,7 @@
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
 
-#include "asm.S"
+#include "loongson_asm.S"
 
 #if !HIGH_BIT_DEPTH
 
@@ -485,6 +485,30 @@ function predict_8x8_h_lasx
     xvstelm.d       xr1,    a0,    FDEC_STRIDE * 7,     2
 endfunc
 
+function predict_8x8_h_lsx
+    fld.d           f0,     a1,     7
+    vreplvei.w      vr1,    vr0,    0
+
+    vreplvei.b      vr4,    vr0,    7
+    vreplvei.b      vr5,    vr1,    7
+    vreplvei.b      vr6,    vr0,    6
+    vreplvei.b      vr7,    vr1,    6
+    vreplvei.b      vr8,    vr0,    5
+    vreplvei.b      vr9,    vr1,    5
+    vreplvei.b      vr10,   vr0,    4
+    vreplvei.b      vr11,   vr1,    4
+
+    fst.d           f4,     a0,     0
+    fst.d           f6,     a0,     FDEC_STRIDE
+    fst.d           f8,     a0,     FDEC_STRIDE * 2
+    fst.d           f10,    a0,     FDEC_STRIDE * 3
+
+    vstelm.d        vr5,    a0,     FDEC_STRIDE * 4,    0
+    vstelm.d        vr7,    a0,     FDEC_STRIDE * 5,    0
+    vstelm.d        vr9,    a0,     FDEC_STRIDE * 6,    0
+    vstelm.d        vr11,   a0,     FDEC_STRIDE * 7,    0
+endfunc
+
 /* void predict_8x8_dc_c( pixel *src, pixel edge[36] )
  */
 function predict_8x8_dc_lsx
@@ -581,23 +605,60 @@ function predict_8x8_ddl_lasx
     xvadd.h         xr9,    xr8,   xr7
     xvssrarni.bu.h  xr9,    xr9,   2
     xvpermi.d       xr9,    xr9,   0x08
-
-    vstelm.d        vr9,    a0,    0,                 0
     vbsrl.v         vr10,   vr9,   1
-    vstelm.d        vr10,   a0,    FDEC_STRIDE,       0
-    vbsrl.v         vr10,   vr9,   2
-    vstelm.d        vr10,   a0,    FDEC_STRIDE * 2,   0
-    vbsrl.v         vr10,   vr9,   3
-    vstelm.d        vr10,   a0,    FDEC_STRIDE * 3,   0
-
-    vbsrl.v         vr10,   vr9,   4
-    vstelm.d        vr10,   a0,    FDEC_STRIDE * 4,  0
-    vbsrl.v         vr10,   vr9,   5
-    vstelm.d        vr10,   a0,    FDEC_STRIDE * 5,   0
-    vbsrl.v         vr10,   vr9,   6
-    vstelm.d        vr10,   a0,    FDEC_STRIDE * 6,   0
-    vbsrl.v         vr10,   vr9,   7
-    vstelm.d        vr10,   a0,    FDEC_STRIDE * 7,   0
+    vbsrl.v         vr11,   vr9,   2
+    vbsrl.v         vr12,   vr9,   3
+    vbsrl.v         vr13,   vr9,   4
+    vbsrl.v         vr14,   vr9,   5
+    vbsrl.v         vr15,   vr9,   6
+    vbsrl.v         vr16,   vr9,   7
+
+    fst.d           f9,    a0,    0
+    fst.d           f10,   a0,    FDEC_STRIDE
+    fst.d           f11,   a0,    FDEC_STRIDE * 2
+    fst.d           f12,   a0,    FDEC_STRIDE * 3
+    fst.d           f13,   a0,    FDEC_STRIDE * 4
+    fst.d           f14,   a0,    FDEC_STRIDE * 5
+    fst.d           f15,   a0,    FDEC_STRIDE * 6
+    fst.d           f16,   a0,    FDEC_STRIDE * 7
+endfunc
+
+function predict_8x8_ddl_lsx
+    vld             vr1,    a1,     16
+    vbsrl.v         vr2,    vr1,    1
+    vbsrl.v         vr3,    vr1,    2
+
+    vextrins.b      vr3,    vr1,    0xef
+    vsllwil.hu.bu   vr5,    vr1,    0
+    vexth.hu.bu     vr15,   vr1
+    vsllwil.hu.bu   vr6,    vr2,    0
+    vexth.hu.bu     vr16,   vr2
+    vsllwil.hu.bu   vr7,    vr3,    0
+    vexth.hu.bu     vr17,   vr3
+
+    vslli.h         vr6,    vr6,    1
+    vslli.h         vr16,   vr16,   1
+    vadd.h          vr8,    vr5,    vr6
+    vadd.h          vr18,   vr15,   vr16
+    vadd.h          vr19,   vr8,    vr7
+    vadd.h          vr9,    vr18,   vr17
+    vssrarni.bu.h   vr9,    vr19,   2
+    vbsrl.v         vr10,   vr9,    1
+    vbsrl.v         vr11,   vr9,    2
+    vbsrl.v         vr12,   vr9,    3
+    vbsrl.v         vr13,   vr9,    4
+    vbsrl.v         vr14,   vr9,    5
+    vbsrl.v         vr15,   vr9,    6
+    vbsrl.v         vr16,   vr9,    7
+
+    fst.d           f9,    a0,    0
+    fst.d           f10,   a0,    FDEC_STRIDE
+    fst.d           f11,   a0,    FDEC_STRIDE * 2
+    fst.d           f12,   a0,    FDEC_STRIDE * 3
+    fst.d           f13,   a0,    FDEC_STRIDE * 4
+    fst.d           f14,   a0,    FDEC_STRIDE * 5
+    fst.d           f15,   a0,    FDEC_STRIDE * 6
+    fst.d           f16,   a0,    FDEC_STRIDE * 7
 endfunc
 
 /* void predict_8x8_ddr_c( pixel *src, pixel edge[36] )
@@ -620,22 +681,64 @@ function predict_8x8_ddr_lasx
     xvssrarni.bu.h  xr5,    xr5,   2
     xvpermi.d       xr6,    xr5,   0x08
 
-    vbsrl.v         vr7,    vr6,   7
-    vstelm.d        vr7,    a0,    0,                  0
-    vbsrl.v         vr7,    vr6,   6
-    vstelm.d        vr7,    a0,    FDEC_STRIDE,        0
-    vbsrl.v         vr7,    vr6,   5
-    vstelm.d        vr7,    a0,    FDEC_STRIDE * 2,    0
-    vbsrl.v         vr7,    vr6,   4
-    vstelm.d        vr7,    a0,    FDEC_STRIDE * 3,    0
-
-    vbsrl.v         vr7,    vr6,   3
-    vstelm.d        vr7,    a0,    FDEC_STRIDE * 4,    0
-    vbsrl.v         vr7,    vr6,   2
-    vstelm.d        vr7,    a0,    FDEC_STRIDE * 5,    0
-    vbsrl.v         vr7,    vr6,   1
-    vstelm.d        vr7,    a0,    FDEC_STRIDE * 6,    0
-    vstelm.d        vr6,    a0,    FDEC_STRIDE * 7,    0
+    vbsrl.v         vr7,   vr6,   7
+    vbsrl.v         vr8,   vr6,   6
+    vbsrl.v         vr9,   vr6,   5
+    vbsrl.v         vr10,  vr6,   4
+    vbsrl.v         vr11,  vr6,   3
+    vbsrl.v         vr12,  vr6,   2
+    vbsrl.v         vr13,  vr6,   1
+
+    fst.d           f7,    a0,    0
+    fst.d           f8,    a0,    FDEC_STRIDE
+    fst.d           f9,    a0,    FDEC_STRIDE * 2
+    fst.d           f10,   a0,    FDEC_STRIDE * 3
+    fst.d           f11,   a0,    FDEC_STRIDE * 4
+    fst.d           f12,   a0,    FDEC_STRIDE * 5
+    fst.d           f13,   a0,    FDEC_STRIDE * 6
+    fst.d           f6,    a0,    FDEC_STRIDE * 7
+endfunc
+
+function predict_8x8_ddr_lsx
+    vld             vr1,    a1,    7
+    vbsrl.v         vr2,    vr1,   1
+    vbsrl.v         vr3,    vr1,   2
+
+    // edge[23]
+    ld.bu           t0,     a1,    23
+    vinsgr2vr.b     vr3,    t0,    0xe
+
+    vexth.hu.bu     vr11,   vr1
+    vsllwil.hu.bu   vr1,    vr1,    0
+    vexth.hu.bu     vr12,   vr2
+    vsllwil.hu.bu   vr2,    vr2,    0
+    vexth.hu.bu     vr13,   vr3
+    vsllwil.hu.bu   vr3,    vr3,    0
+
+    vslli.h         vr2,    vr2,   1
+    vslli.h         vr12,   vr12,  1
+    vadd.h          vr4,    vr1,   vr2
+    vadd.h          vr14,   vr11,  vr12
+    vadd.h          vr5,    vr4,   vr3
+    vadd.h          vr15,   vr14,  vr13
+    vssrarni.bu.h   vr15,   vr5,   2
+
+    vbsrl.v         vr7,   vr15,    7
+    vbsrl.v         vr8,   vr15,    6
+    vbsrl.v         vr9,   vr15,    5
+    vbsrl.v         vr10,  vr15,    4
+    vbsrl.v         vr11,  vr15,    3
+    vbsrl.v         vr12,  vr15,    2
+    vbsrl.v         vr13,  vr15,    1
+
+    fst.d           f7,    a0,      0
+    fst.d           f8,    a0,      FDEC_STRIDE
+    fst.d           f9,    a0,      FDEC_STRIDE * 2
+    fst.d           f10,   a0,      FDEC_STRIDE * 3
+    fst.d           f11,   a0,      FDEC_STRIDE * 4
+    fst.d           f12,   a0,      FDEC_STRIDE * 5
+    fst.d           f13,   a0,      FDEC_STRIDE * 6
+    fst.d           f15,   a0,      FDEC_STRIDE * 7
 endfunc
 
 /* void predict_8x8_vr_c( pixel *src, pixel edge[36] )
@@ -658,28 +761,78 @@ function predict_8x8_vr_lasx
     xvpermi.d       xr14,   xr10,  0x08
 
     vbsrl.v         vr15,   vr13,  6
-    vstelm.d        vr15,   a0,    FDEC_STRIDE,       0
-    vbsll.v         vr15,   vr15,  1
-    vextrins.b      vr15,   vr13,  0x04
-    vstelm.d        vr15,   a0,    FDEC_STRIDE * 3,   0
-    vbsll.v         vr15,   vr15,  1
-    vextrins.b      vr15,   vr13,  0x02
-    vstelm.d        vr15,   a0,    FDEC_STRIDE * 5,   0
-    vbsll.v         vr15,   vr15,  1
-    vextrins.b      vr15,   vr13,  0x00
-    vstelm.d        vr15,   a0,    FDEC_STRIDE * 7,   0
+    vbsll.v         vr16,   vr15,  1
+    vextrins.b      vr16,   vr13,  0x04
+    vbsll.v         vr17,   vr16,  1
+    vextrins.b      vr17,   vr13,  0x02
+    vbsll.v         vr18,   vr17,  1
+    vextrins.b      vr18,   vr13,  0x00
+
+    fst.d           f15,    a0,    FDEC_STRIDE
+    fst.d           f16,    a0,    FDEC_STRIDE * 3
+    fst.d           f17,    a0,    FDEC_STRIDE * 5
+    fst.d           f18,    a0,    FDEC_STRIDE * 7
 
     vbsrl.v         vr16,   vr14,  7
-    vstelm.d        vr16,   a0,    0,                 0
-    vbsll.v         vr16,   vr16,  1
-    vextrins.b      vr16,   vr13,  0x05
-    vstelm.d        vr16,   a0,    FDEC_STRIDE * 2,   0
-    vbsll.v         vr16,   vr16,  1
-    vextrins.b      vr16,   vr13,  0x03
-    vstelm.d        vr16,   a0,    FDEC_STRIDE * 4,   0
-    vbsll.v         vr16,   vr16,  1
-    vextrins.b      vr16,   vr13,  0x01
-    vstelm.d        vr16,   a0,    FDEC_STRIDE * 6,   0
+    vbsll.v         vr17,   vr16,  1
+    vextrins.b      vr17,   vr13,  0x05
+    vbsll.v         vr18,   vr17,  1
+    vextrins.b      vr18,   vr13,  0x03
+    vbsll.v         vr19,   vr18,  1
+    vextrins.b      vr19,   vr13,  0x01
+
+    fst.d           f16,    a0,    0
+    fst.d           f17,    a0,    FDEC_STRIDE * 2
+    fst.d           f18,    a0,    FDEC_STRIDE * 4
+    fst.d           f19,    a0,    FDEC_STRIDE * 6
+endfunc
+
+function predict_8x8_vr_lsx
+    vld             vr0,    a1,    8
+    vbsrl.v         vr1,    vr0,   1
+    vbsrl.v         vr2,    vr0,   2
+
+    vexth.hu.bu     vr5,    vr0
+    vsllwil.hu.bu   vr0,    vr0,    0
+    vexth.hu.bu     vr6,    vr1
+    vsllwil.hu.bu   vr1,    vr1,    0
+    vexth.hu.bu     vr7,    vr2
+    vsllwil.hu.bu   vr2,    vr2,    0
+
+    vadd.h          vr9,    vr0,    vr1
+    vadd.h          vr10,   vr5,    vr6
+    vadd.h          vr11,   vr9,    vr1
+    vadd.h          vr12,   vr10,   vr6
+    vadd.h          vr13,   vr11,   vr2
+    vadd.h          vr14,   vr12,   vr7
+    vssrarni.bu.h   vr14,   vr13,   2
+    vssrarni.bu.h   vr10,   vr9,    1
+
+    vbsrl.v         vr15,   vr14,  6
+    vbsll.v         vr16,   vr15,  1
+    vextrins.b      vr16,   vr14,  0x04
+    vbsll.v         vr17,   vr16,  1
+    vextrins.b      vr17,   vr14,  0x02
+    vbsll.v         vr18,   vr17,  1
+    vextrins.b      vr18,   vr14,  0x00
+
+    fst.d           f15,    a0,    FDEC_STRIDE
+    fst.d           f16,    a0,    FDEC_STRIDE * 3
+    fst.d           f17,    a0,    FDEC_STRIDE * 5
+    fst.d           f18,    a0,    FDEC_STRIDE * 7
+
+    vbsrl.v         vr16,   vr10,  7
+    vbsll.v         vr17,   vr16,  1
+    vextrins.b      vr17,   vr14,  0x05
+    vbsll.v         vr18,   vr17,  1
+    vextrins.b      vr18,   vr14,  0x03
+    vbsll.v         vr19,   vr18,  1
+    vextrins.b      vr19,   vr14,  0x01
+
+    fst.d           f16,    a0,    0
+    fst.d           f17,    a0,    FDEC_STRIDE * 2
+    fst.d           f18,    a0,    FDEC_STRIDE * 4
+    fst.d           f19,    a0,    FDEC_STRIDE * 6
 endfunc
 
 /* void predict_8x8_vl_c( pixel *src, pixel edge[36] );
@@ -701,21 +854,60 @@ function predict_8x8_vl_lasx
     xvpermi.d       xr6,    xr3,   0x8
     xvpermi.d       xr7,    xr5,   0x8
 
-    vstelm.d        vr6,    a0,    0,                  0
-    vstelm.d        vr7,    a0,    FDEC_STRIDE,        0
     vbsrl.v         vr8,    vr6,   1
-    vstelm.d        vr8,    a0,    FDEC_STRIDE * 2,    0
     vbsrl.v         vr9,    vr7,   1
-    vstelm.d        vr9,    a0,    FDEC_STRIDE * 3,    0
+
+    fst.d           f6,     a0,    0
+    fst.d           f7,     a0,    FDEC_STRIDE
+    fst.d           f8,     a0,    FDEC_STRIDE * 2
+    fst.d           f9,     a0,    FDEC_STRIDE * 3
+
+    vbsrl.v         vr10,    vr8,   1
+    vbsrl.v         vr11,    vr9,   1
+    vbsrl.v         vr12,    vr10,   1
+    vbsrl.v         vr13,    vr11,   1
+    fst.d           f10,     a0,    FDEC_STRIDE * 4
+    fst.d           f11,     a0,    FDEC_STRIDE * 5
+    fst.d           f12,     a0,    FDEC_STRIDE * 6
+    fst.d           f13,     a0,    FDEC_STRIDE * 7
+endfunc
+
+function predict_8x8_vl_lsx
+    vld             vr0,    a1,    16
+    vbsrl.v         vr1,    vr0,   1
+    vbsrl.v         vr2,    vr0,   2
+
+    vexth.hu.bu     vr5,    vr0
+    vsllwil.hu.bu   vr0,    vr0,    0
+    vexth.hu.bu     vr6,    vr1
+    vsllwil.hu.bu   vr1,    vr1,    0
+    vexth.hu.bu     vr7,    vr2
+    vsllwil.hu.bu   vr2,    vr2,    0
+
+    vadd.h          vr3,    vr0,    vr1
+    vadd.h          vr13,   vr5,    vr6
+    vadd.h          vr4,    vr3,    vr1
+    vadd.h          vr14,   vr13,   vr6
+    vadd.h          vr5,    vr4,    vr2
+    vadd.h          vr15,   vr14,   vr7
+    vssrarni.bu.h   vr13,   vr3,    1
+    vssrarni.bu.h   vr15,   vr5,    2
+
+    vbsrl.v         vr8,    vr13,   1
+    vbsrl.v         vr9,    vr15,   1
+    fst.d           f13,    a0,    0
+    fst.d           f15,    a0,    FDEC_STRIDE
+    fst.d           f8,     a0,    FDEC_STRIDE * 2
+    fst.d           f9,     a0,    FDEC_STRIDE * 3
 
     vbsrl.v         vr8,    vr8,   1
-    vstelm.d        vr8,    a0,    FDEC_STRIDE * 4,    0
-    vbsrl.v         vr9,    vr9,   1
-    vstelm.d        vr9,    a0,    FDEC_STRIDE * 5,    0
-    vbsrl.v         vr8,    vr8,   1
-    vstelm.d        vr8,    a0,    FDEC_STRIDE * 6,    0
     vbsrl.v         vr9,    vr9,   1
-    vstelm.d        vr9,    a0,    FDEC_STRIDE * 7,    0
+    vbsrl.v         vr10,   vr8,   1
+    vbsrl.v         vr11,   vr9,   1
+    fst.d           f8,     a0,    FDEC_STRIDE * 4
+    fst.d           f9,     a0,    FDEC_STRIDE * 5
+    fst.d           f10,    a0,    FDEC_STRIDE * 6
+    fst.d           f11,    a0,    FDEC_STRIDE * 7
 endfunc
 
 /****************************************************************************
@@ -1093,4 +1285,103 @@ function predict_16x16_p_lasx
     add.w         t5,    t5,    t1
 .endr
 endfunc
+
+function predict_16x16_p_lsx
+    la.local      t0,    mulc
+    vld           vr3,   t0,    0
+    fld.d         f4,    a0,    8  - FDEC_STRIDE
+    fld.d         f5,    a0,    -1 - FDEC_STRIDE
+    vxor.v        vr0,   vr0,   vr0
+    vilvl.b       vr4,   vr0,   vr4
+    vilvl.b       vr5,   vr0,   vr5
+    vshuf4i.h     vr5,   vr5,   0x1b
+    vbsll.v       vr6,   vr5,   8
+    vpackod.d     vr5,   vr6,   vr5
+    vsub.h        vr4,   vr4,   vr5
+    vmul.h        vr4,   vr4,   vr3
+    vhaddw.w.h    vr4,   vr4,   vr4
+    vhaddw.d.w    vr4,   vr4,   vr4
+    vhaddw.q.d    vr4,   vr4,   vr4
+    vpickve2gr.w  t0,    vr4,   0       /* H */
+
+    fld.d         f6,    a0,    FDEC_STRIDE * 8 - 1
+    fld.d         f7,    a0,    FDEC_STRIDE * 9  - 1
+    fld.d         f8,    a0,    FDEC_STRIDE * 10 - 1
+    fld.d         f9,    a0,    FDEC_STRIDE * 11 - 1
+    fld.d         f10,   a0,    FDEC_STRIDE * 12 - 1
+    fld.d         f11,   a0,    FDEC_STRIDE * 13 - 1
+    fld.d         f12,   a0,    FDEC_STRIDE * 14 - 1
+    fld.d         f13,   a0,    FDEC_STRIDE * 15 - 1
+    vilvl.b       vr6,   vr7,   vr6
+    vilvl.b       vr8,   vr9,   vr8
+    vilvl.b       vr10,  vr11,  vr10
+    vilvl.b       vr12,  vr13,  vr12
+    vilvl.h       vr6,   vr8,   vr6
+    vilvl.h       vr10,  vr12,  vr10
+    vilvl.w       vr6,   vr10,  vr6
+
+    fld.d         f7,    a0,    FDEC_STRIDE * 6 - 1
+    fld.d         f8,    a0,    FDEC_STRIDE * 5 - 1
+    fld.d         f9,    a0,    FDEC_STRIDE * 4 - 1
+    fld.d         f10,   a0,    FDEC_STRIDE * 3 - 1
+    fld.d         f11,   a0,    FDEC_STRIDE * 2 - 1
+    fld.d         f12,   a0,    FDEC_STRIDE - 1
+    fld.d         f13,   a0,    -1
+    fld.d         f14,   a0,    -FDEC_STRIDE - 1
+    vilvl.b       vr7,   vr8,   vr7
+    vilvl.b       vr9,   vr10,  vr9
+    vilvl.b       vr11,  vr12,  vr11
+    vilvl.b       vr13,  vr14,  vr13
+    vilvl.h       vr7,   vr9,   vr7
+    vilvl.h       vr11,  vr13,  vr11
+    vilvl.w       vr7,   vr11,  vr7
+
+    vilvl.b       vr6,   vr0,   vr6
+    vilvl.b       vr7,   vr0,   vr7
+    vsub.h        vr6,   vr6,   vr7
+    vmul.h        vr6,   vr6,   vr3
+    vhaddw.w.h    vr6,   vr6,   vr6
+    vhaddw.d.w    vr6,   vr6,   vr6
+    vhaddw.q.d    vr6,   vr6,   vr6
+    vpickve2gr.w  t1,    vr6,   0       /* V */
+
+    ld.bu         t2,    a0,    FDEC_STRIDE * 15 - 1
+    ld.bu         t3,    a0,    15 - FDEC_STRIDE
+    add.w         t2,    t2,    t3
+    slli.w        t2,    t2,    4       /* a */
+
+    slli.w        t3,    t0,    2
+    add.w         t0,    t0,    t3
+    addi.w        t0,    t0,    32
+    srai.w        t0,    t0,    6       /* b */
+
+    slli.w        t3,    t1,    2
+    add.w         t1,    t1,    t3
+    addi.w        t1,    t1,    32
+    srai.w        t1,    t1,    6       /* c */
+
+    add.w         t3,    t0,    t1
+    slli.w        t4,    t3,    3
+    sub.w         t4,    t4,    t3
+    sub.w         t5,    t2,    t4
+    addi.w        t5,    t5,    16      /* i00 */
+
+    la.local      t3,    muld
+    vld           vr14,  t3,    0
+    vld           vr20,  t3,    16
+    vreplgr2vr.h  vr12,  t0
+    vmul.h        vr22,  vr12,  vr14
+    vmul.h        vr23,  vr12,  vr20
+.rept 16
+    vreplgr2vr.h  vr14,  t5
+    vadd.h        vr13,  vr22,  vr14
+    vadd.h        vr16,  vr23,  vr14
+    vssrani.bu.h  vr15,  vr13,  5
+    vssrani.bu.h  vr17,  vr16,  5
+    vpermi.w      vr17,  vr15,  0x44
+    vst           vr17,  a0,    0
+    addi.d        a0,    a0,    FDEC_STRIDE
+    add.w         t5,    t5,    t1
+.endr
+endfunc
 #endif /* !HIGH_BIT_DEPT H */
diff --git a/common/loongarch/predict-c.c b/common/loongarch/predict-c.c
index fc12f3d8..4cfb68a2 100644
--- a/common/loongarch/predict-c.c
+++ b/common/loongarch/predict-c.c
@@ -27,27 +27,31 @@
 #include "common/common.h"
 #include "predict.h"
 
-void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] )
+void x264_predict_16x16_init_loongarch( int cpu, x264_predict_t pf[7] )
 {
-    if( cpu&X264_CPU_LASX )
-    {
 #if !HIGH_BIT_DEPTH
+    if( cpu&X264_CPU_LSX )
+    {
         pf[I_PRED_16x16_V ]     = x264_predict_16x16_v_lsx;
         pf[I_PRED_16x16_H ]     = x264_predict_16x16_h_lsx;
         pf[I_PRED_16x16_DC]     = x264_predict_16x16_dc_lsx;
-        pf[I_PRED_16x16_P ]     = x264_predict_16x16_p_lasx;
         pf[I_PRED_16x16_DC_LEFT]= x264_predict_16x16_dc_left_lsx;
         pf[I_PRED_16x16_DC_TOP ]= x264_predict_16x16_dc_top_lsx;
         pf[I_PRED_16x16_DC_128 ]= x264_predict_16x16_dc_128_lsx;
-#endif
+        pf[I_PRED_16x16_P ]     = x264_predict_16x16_p_lsx;
+    }
+    if( cpu&X264_CPU_LASX )
+    {
+        pf[I_PRED_16x16_P ]     = x264_predict_16x16_p_lasx;
     }
+#endif
 }
 
-void x264_predict_8x8c_init_lasx( int cpu, x264_predict_t pf[7] )
+void x264_predict_8x8c_init_loongarch( int cpu, x264_predict_t pf[7] )
 {
-    if( cpu&X264_CPU_LASX )
-    {
 #if !HIGH_BIT_DEPTH
+    if( cpu&X264_CPU_LSX )
+    {
         pf[I_PRED_CHROMA_P]      = x264_predict_8x8c_p_lsx;
         pf[I_PRED_CHROMA_V]      = x264_predict_8x8c_v_lsx;
         pf[I_PRED_CHROMA_H]      = x264_predict_8x8c_h_lsx;
@@ -55,34 +59,42 @@ void x264_predict_8x8c_init_lasx( int cpu, x264_predict_t pf[7] )
         pf[I_PRED_CHROMA_DC_128] = x264_predict_8x8c_dc_128_lsx;
         pf[I_PRED_CHROMA_DC_TOP] = x264_predict_8x8c_dc_top_lsx;
         pf[I_PRED_CHROMA_DC_LEFT]= x264_predict_8x8c_dc_left_lsx;
-#endif
     }
+#endif
 }
 
-void x264_predict_8x8_init_lasx( int cpu, x264_predict8x8_t pf[12], x264_predict_8x8_filter_t *predict_filter )
+void x264_predict_8x8_init_loongarch( int cpu, x264_predict8x8_t pf[12], x264_predict_8x8_filter_t *predict_filter )
 {
-    if( cpu&X264_CPU_LASX )
-    {
 #if !HIGH_BIT_DEPTH
+    if( cpu&X264_CPU_LSX )
+    {
         pf[I_PRED_8x8_V]      = x264_predict_8x8_v_lsx;
-        pf[I_PRED_8x8_H]      = x264_predict_8x8_h_lasx;
         pf[I_PRED_8x8_DC]     = x264_predict_8x8_dc_lsx;
+        pf[I_PRED_8x8_DC_LEFT]= x264_predict_8x8_dc_left_lsx;
+        pf[I_PRED_8x8_DC_TOP] = x264_predict_8x8_dc_top_lsx;
+        pf[I_PRED_8x8_DC_128] = x264_predict_8x8_dc_128_lsx;
+        pf[I_PRED_8x8_H]      = x264_predict_8x8_h_lsx;
+        pf[I_PRED_8x8_DDL]    = x264_predict_8x8_ddl_lsx;
+        pf[I_PRED_8x8_DDR]    = x264_predict_8x8_ddr_lsx;
+        pf[I_PRED_8x8_VR]     = x264_predict_8x8_vr_lsx;
+        pf[I_PRED_8x8_VL]     = x264_predict_8x8_vl_lsx;
+    }
+    if( cpu&X264_CPU_LASX )
+    {
+        pf[I_PRED_8x8_H]      = x264_predict_8x8_h_lasx;
         pf[I_PRED_8x8_DDL]    = x264_predict_8x8_ddl_lasx;
         pf[I_PRED_8x8_DDR]    = x264_predict_8x8_ddr_lasx;
         pf[I_PRED_8x8_VR]     = x264_predict_8x8_vr_lasx;
         pf[I_PRED_8x8_VL]     = x264_predict_8x8_vl_lasx;
-        pf[I_PRED_8x8_DC_LEFT]= x264_predict_8x8_dc_left_lsx;
-        pf[I_PRED_8x8_DC_TOP] = x264_predict_8x8_dc_top_lsx;
-        pf[I_PRED_8x8_DC_128] = x264_predict_8x8_dc_128_lsx;
-#endif
     }
+#endif
 }
 
-void x264_predict_4x4_init_lasx( int cpu, x264_predict_t pf[12] )
+void x264_predict_4x4_init_loongarch( int cpu, x264_predict_t pf[12] )
 {
-    if( cpu&X264_CPU_LASX )
-    {
 #if !HIGH_BIT_DEPTH
+    if( cpu&X264_CPU_LSX )
+    {
         pf[I_PRED_4x4_V]      = x264_predict_4x4_v_lsx;
         pf[I_PRED_4x4_H]      = x264_predict_4x4_h_lsx;
         pf[I_PRED_4x4_DC]     = x264_predict_4x4_dc_lsx;
@@ -90,6 +102,6 @@ void x264_predict_4x4_init_lasx( int cpu, x264_predict_t pf[12] )
         pf[I_PRED_4x4_DC_LEFT]= x264_predict_4x4_dc_left_lsx;
         pf[I_PRED_4x4_DC_TOP] = x264_predict_4x4_dc_top_lsx;
         pf[I_PRED_4x4_DC_128] = x264_predict_4x4_dc_128_lsx;
-#endif
     }
+#endif
 }
diff --git a/common/loongarch/predict.h b/common/loongarch/predict.h
index d4657f4c..06651855 100644
--- a/common/loongarch/predict.h
+++ b/common/loongarch/predict.h
@@ -69,12 +69,18 @@ void x264_predict_16x16_v_lsx( pixel *src );
 #define x264_predict_16x16_p_lasx x264_template(predict_16x16_p_lasx)
 void x264_predict_16x16_p_lasx( pixel *src );
 
+#define x264_predict_16x16_p_lsx x264_template(predict_16x16_p_lsx)
+void x264_predict_16x16_p_lsx( pixel *src );
+
 #define x264_predict_8x8_v_lsx x264_template(predict_8x8_v_lsx)
 void x264_predict_8x8_v_lsx( pixel *src, pixel edge[36] );
 
 #define x264_predict_8x8_h_lasx x264_template(predict_8x8_h_lasx)
 void x264_predict_8x8_h_lasx( pixel *src, pixel edge[36] );
 
+#define x264_predict_8x8_h_lsx x264_template(predict_8x8_h_lsx)
+void x264_predict_8x8_h_lsx( pixel *src, pixel edge[36] );
+
 #define x264_predict_8x8_dc_lsx x264_template(predict_8x8_dc_lsx)
 void x264_predict_8x8_dc_lsx( pixel *src, pixel edge[36] );
 
@@ -90,15 +96,27 @@ void x264_predict_8x8_dc_128_lsx( pixel *src, pixel edge[36] );
 #define x264_predict_8x8_ddl_lasx x264_template(predict_8x8_ddl_lasx)
 void x264_predict_8x8_ddl_lasx( pixel *src, pixel edge[36] );
 
+#define x264_predict_8x8_ddl_lsx x264_template(predict_8x8_ddl_lsx)
+void x264_predict_8x8_ddl_lsx( pixel *src, pixel edge[36] );
+
 #define x264_predict_8x8_ddr_lasx x264_template(predict_8x8_ddr_lasx)
 void x264_predict_8x8_ddr_lasx( pixel *src, pixel edge[36] );
 
+#define x264_predict_8x8_ddr_lsx x264_template(predict_8x8_ddr_lsx)
+void x264_predict_8x8_ddr_lsx( pixel *src, pixel edge[36] );
+
 #define x264_predict_8x8_vr_lasx x264_template(predict_8x8_vr_lasx)
 void x264_predict_8x8_vr_lasx( pixel *src, pixel edge[36] );
 
+#define x264_predict_8x8_vr_lsx x264_template(predict_8x8_vr_lsx)
+void x264_predict_8x8_vr_lsx( pixel *src, pixel edge[36] );
+
 #define x264_predict_8x8_vl_lasx x264_template(predict_8x8_vl_lasx)
 void x264_predict_8x8_vl_lasx( pixel *src, pixel edge[36] );
 
+#define x264_predict_8x8_vl_lsx x264_template(predict_8x8_vl_lsx)
+void x264_predict_8x8_vl_lsx( pixel *src, pixel edge[36] );
+
 #define x264_predict_4x4_v_lsx x264_template(predict_4x4_v_lsx)
 void x264_predict_4x4_v_lsx( pixel *p_src );
 
@@ -120,13 +138,14 @@ void x264_predict_4x4_dc_left_lsx( pixel *p_src );
 #define x264_predict_4x4_dc_128_lsx x264_template(predict_4x4_dc_128_lsx)
 void x264_predict_4x4_dc_128_lsx( pixel *p_src );
 
-#define x264_predict_4x4_init_lasx x264_template(predict_4x4_init_lasx)
-void x264_predict_4x4_init_lasx( int cpu, x264_predict_t pf[12] );
-#define x264_predict_8x8_init_lasx x264_template(predict_8x8_init_lasx)
-void x264_predict_8x8_init_lasx( int cpu, x264_predict8x8_t pf[12], x264_predict_8x8_filter_t *predict_filter );
-#define x264_predict_8x8c_init_lasx x264_template(predict_8x8c_init_lasx)
-void x264_predict_8x8c_init_lasx( int cpu, x264_predict_t pf[7] );
-#define x264_predict_16x16_init_lasx x264_template(predict_16x16_init_lasx)
-void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] );
+#define x264_predict_4x4_init_loongarch x264_template(predict_4x4_init_loongarch)
+void x264_predict_4x4_init_loongarch( int cpu, x264_predict_t pf[12] );
+#define x264_predict_8x8_init_loongarch x264_template(predict_8x8_init_loongarch)
+void x264_predict_8x8_init_loongarch( int cpu, x264_predict8x8_t pf[12],
+                                      x264_predict_8x8_filter_t *predict_filter );
+#define x264_predict_8x8c_init_loongarch x264_template(predict_8x8c_init_loongarch)
+void x264_predict_8x8c_init_loongarch( int cpu, x264_predict_t pf[7] );
+#define x264_predict_16x16_init_loongarch x264_template(predict_16x16_init_loongarch)
+void x264_predict_16x16_init_loongarch( int cpu, x264_predict_t pf[7] );
 
 #endif
diff --git a/common/loongarch/quant-a.S b/common/loongarch/quant-a.S
index 196546bf..0b9b257c 100644
--- a/common/loongarch/quant-a.S
+++ b/common/loongarch/quant-a.S
@@ -24,7 +24,7 @@
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
 
-#include "asm.S"
+#include "loongson_asm.S"
 
 const last64_shuf
 .int 0, 4, 1, 5, 2, 6, 3, 7
@@ -74,6 +74,48 @@ function quant_4x4x4_lasx
     xori           a0,      t0,    0xf
 endfunc
 
+.macro QUANT_ONE_LSX tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
+    vld             vr0,      \tmp1,    0
+    vld             vr1,      \tmp1,    16
+    vadda.h         \tmp6,    vr0,      \tmp4
+    vadda.h         \tmp7,    vr1,      \tmp5
+    vmuh.hu         \tmp6,    \tmp6,    \tmp2
+    vmuh.hu         \tmp7,    \tmp7,    \tmp3
+    vsigncov.h      \tmp6,    vr0,      \tmp6
+    vsigncov.h      \tmp7,    vr1,      \tmp7
+    vst             \tmp6,    \tmp1,    0
+    vst             \tmp7,    \tmp1,    16
+.endm
+
+function quant_4x4x4_lsx
+    vld             vr2,    a1,     0
+    vld             vr3,    a1,     16
+    vld             vr4,    a2,     0
+    vld             vr5,    a2,     16
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr6,    vr7
+    addi.d          a0,     a0,     32
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr8,    vr9
+    vssrlni.h.w     vr8,    vr6,    0
+    vssrlni.h.w     vr9,    vr7,    0
+    addi.d          a0,     a0,     32
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr10,   vr11
+    addi.d          a0,     a0,     32
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr12,   vr13
+    vssrlni.h.w     vr12,   vr10,   0
+    vssrlni.h.w     vr13,   vr11,   0
+    vssrlni.h.w     vr12,   vr8,    0
+    vssrlni.h.w     vr13,   vr9,    0
+    vseqi.w         vr12,   vr12,   0
+    vseqi.w         vr13,   vr13,   0
+    vmskltz.w       vr12,   vr12
+    vmskltz.w       vr13,   vr13
+    vpickve2gr.w    t0,     vr12,   0
+    vpickve2gr.w    t1,     vr13,   0
+    alsl.d          t0,     t1,     t0,     4
+    and             t0,     t0,     t1
+    xori            a0,     t0,     0xf
+endfunc
+
 /*
  * int quant_8x8( dctcoef dct[64], udctcoef mf[64], udctcoef bias[64] )
  */
@@ -191,6 +233,48 @@ function quant_8x8_lasx
     maskeqz         a0,     t3,    t2
 endfunc
 
+function quant_8x8_lsx
+    vld             vr2,    a1,     0
+    vld             vr3,    a1,     16
+    vld             vr4,    a2,     0
+    vld             vr5,    a2,     16
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr12,   vr13
+
+    addi.d          a0,     a0,     32
+    vld             vr2,    a1,     32
+    vld             vr3,    a1,     48
+    vld             vr4,    a2,     32
+    vld             vr5,    a2,     48
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr14,   vr15
+
+    addi.d          a0,     a0,     32
+    vld             vr2,    a1,     64
+    vld             vr3,    a1,     80
+    vld             vr4,    a2,     64
+    vld             vr5,    a2,     80
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr22,   vr23
+
+    addi.d          a0,     a0,     32
+    vld             vr2,    a1,     96
+    vld             vr3,    a1,     112
+    vld             vr4,    a2,     96
+    vld             vr5,    a2,     112
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr7,    vr8
+
+    vor.v           vr12,   vr12,   vr14
+    vor.v           vr13,   vr13,   vr15
+    vor.v           vr22,   vr22,   vr7
+    vor.v           vr23,   vr23,   vr8
+    vor.v           vr12,   vr12,   vr22
+    vor.v           vr13,   vr13,   vr23
+    vor.v           vr11,   vr12,   vr13
+    vpickve2gr.d    t0,     vr11,   0
+    vpickve2gr.d    t1,     vr11,   1
+    or              t2,     t0,     t1
+    addi.w          t3,     zero,   1
+    maskeqz         a0,     t3,     t2
+endfunc
+
 /*
  * int quant_4x4( dctcoef dct[16], udctcoef mf[16], udctcoef bias[16] )
  */
@@ -229,6 +313,20 @@ function quant_4x4_lasx
     maskeqz         a0,     t3,    t2
 endfunc
 
+function quant_4x4_lsx
+    vld             vr2,    a1,     0
+    vld             vr3,    a1,     16
+    vld             vr4,    a2,     0
+    vld             vr5,    a2,     16
+    QUANT_ONE_LSX   a0,     vr2,    vr3,    vr4,    vr5,    vr10,   vr11
+    vor.v           vr22,   vr10,   vr11
+    vpickve2gr.d    t0,     vr22,   0
+    vpickve2gr.d    t1,     vr22,   1
+    or              t2,     t0,     t1
+    addi.w          t3,     zero,   1
+    maskeqz         a0,     t3,     t2
+endfunc
+
 /*
  * int quant_4x4_dc( dctcoef dct[16], int mf, int bias )
  */
@@ -263,6 +361,46 @@ function quant_4x4_dc_lasx
     maskeqz         a0,     t3,    t2
 endfunc
 
+function quant_4x4_dc_lsx
+    vld             vr0,    a0,     0
+    vld             vr1,    a0,     16
+    vreplgr2vr.w    vr2,    a1
+    vreplgr2vr.w    vr3,    a2
+    vslei.h         vr4,    vr0,    0
+    vslei.h         vr5,    vr1,    0
+
+    vexth.w.h       vr7,    vr0
+    vsllwil.w.h     vr6,    vr0,    0
+    vexth.w.h       vr9,    vr1
+    vsllwil.w.h     vr8,    vr1,    0
+    vadda.w         vr6,    vr3,    vr6
+    vadda.w         vr7,    vr3,    vr7
+    vadda.w         vr8,    vr3,    vr8
+    vadda.w         vr9,    vr3,    vr9
+    vmul.w          vr6,    vr6,    vr2
+    vmul.w          vr7,    vr7,    vr2
+    vmul.w          vr8,    vr8,    vr2
+    vmul.w          vr9,    vr9,    vr2
+    vsrani.h.w      vr8,    vr6,    16
+    vsrani.h.w      vr9,    vr7,    16
+    vpermi.w        vr10,   vr9,    0x0E
+    vpermi.w        vr9,    vr8,    0x44
+    vpermi.w        vr10,   vr8,    0x4E
+    vneg.h          vr11,   vr9
+    vneg.h          vr12,   vr10
+    vbitsel.v       vr13,   vr9,    vr11,   vr4
+    vbitsel.v       vr14,   vr10,   vr12,   vr5
+    vst             vr13,   a0,     0
+    vst             vr14,   a0,     16
+
+    vor.v           vr15,   vr11,   vr12
+    vpickve2gr.d    t0,     vr15,   0
+    vpickve2gr.d    t1,     vr15,   1
+    or              t2,     t0,     t1
+    addi.w          t3,     zero,   1
+    maskeqz         a0,     t3,     t2
+endfunc
+
 /*
  * int quant_2x2_dc( dctcoef dct[4], int mf, int bias )
  */
@@ -324,6 +462,67 @@ function coeff_last64_lasx
     sub.w           a0,     t0,    t3
 endfunc
 
+function coeff_last64_lsx
+    addi.w          t0,     zero,  63
+    vxor.v          vr20,   vr0,   vr0
+    vld             vr0,    a0,    0
+    vld             vr1,    a0,    16
+    vld             vr2,    a0,    32
+    vld             vr3,    a0,    48
+    vld             vr4,    a0,    64
+    vld             vr5,    a0,    80
+    vld             vr6,    a0,    96
+    vld             vr7,    a0,    112
+    vldi            vr8,    1
+    vldi            vr9,    0x408
+    vldi            vr10,   0x401
+
+    vssrlni.bu.h    vr0,    vr0,   0
+    vssrlni.bu.h    vr1,    vr1,   0
+    vssrlni.bu.h    vr2,    vr2,   0
+    vssrlni.bu.h    vr3,    vr3,   0
+    vssrlni.bu.h    vr4,    vr4,   0
+    vssrlni.bu.h    vr5,    vr5,   0
+    vssrlni.bu.h    vr6,    vr6,   0
+    vssrlni.bu.h    vr7,    vr7,   0
+    vpermi.w        vr2,    vr0,   0x44
+    vpermi.w        vr3,    vr1,   0x44
+    vpermi.w        vr6,    vr4,   0x44
+    vpermi.w        vr7,    vr5,   0x44
+    vsle.bu         vr2,    vr8,   vr2
+    vsle.bu         vr3,    vr8,   vr3
+    vsle.bu         vr6,    vr8,   vr6
+    vsle.bu         vr7,    vr8,   vr7
+    vssrlni.bu.h    vr2,    vr2,   4
+    vssrlni.bu.h    vr3,    vr3,   4
+    vssrlni.bu.h    vr6,    vr6,   4
+    vssrlni.bu.h    vr7,    vr7,   4
+    vpermi.w        vr6,    vr2,   0x44
+    vpermi.w        vr7,    vr3,   0x44
+    vpermi.w        vr11,   vr7,   0x0E
+    vpermi.w        vr7,    vr6,   0x44
+    vpermi.w        vr7,    vr7,   0xD8
+    vpermi.w        vr11,   vr6,   0x4E
+    vpermi.w        vr11,   vr11,  0xD8
+    vclz.w          vr7,    vr7
+    vclz.w          vr11,   vr11
+    vssrlni.hu.w    vr7,    vr7,   2
+    vssrlni.hu.w    vr11,   vr11,  2
+    vpermi.w        vr12,   vr11,  0x0E
+    vpermi.w        vr11,   vr7,   0x44
+    vpermi.w        vr12,   vr7,   0x4E
+    vsub.h          vr11,    vr9,   vr11
+    vsub.h          vr12,    vr9,   vr12
+    vsll.h          vr13,    vr10,  vr11
+    vsll.h          vr14,    vr10,  vr12
+    vssrlni.bu.h    vr13,    vr13,  1
+    vssrlni.bu.h    vr14,    vr14,  1
+
+    vclz.d          vr15,    vr14
+    vpickve2gr.w    t1,      vr15,   0
+    sub.w           a0,      t0,    t1
+endfunc
+
 /*
  * int coeff_last16_c(dctcoef *l)
  */
@@ -344,6 +543,24 @@ function coeff_last16_lasx
     sub.w           a0,     t0,    t1
 endfunc
 
+function coeff_last16_lsx
+    addi.w          t0,     zero,  15
+    vld             vr0,    a0,    0
+    vld             vr1,    a0,    16
+    vldi            vr2,    1
+
+    vssrlni.bu.h    vr0,    vr0,   0
+    vssrlni.bu.h    vr1,    vr1,   0
+    vpermi.w        vr1,    vr0,   0x44
+    vsle.bu         vr3,    vr2,   vr1
+    vssrlni.bu.h    vr3,    vr3,   4
+    vclz.d          vr4,    vr3
+    vpickve2gr.w    t1,     vr4,   0
+
+    srai.w          t1,     t1,    2
+    sub.w           a0,     t0,    t1
+endfunc
+
 /*
  * int coeff_last15_c(dctcoef *l)
  */
@@ -368,6 +585,58 @@ function coeff_last15_lasx
     sub.w           a0,     t0,    t1
 endfunc
 
+function coeff_last15_lsx
+    addi.w          t0,     zero,  15
+    vld             vr0,    a0,    0
+    vld             vr1,    a0,    16
+    vldi            vr2,    1
+    vinsgr2vr.h     vr1,    zero,  7
+
+    vssrlni.bu.h    vr0,    vr0,   0
+    vssrlni.bu.h    vr1,    vr1,   0
+    vpermi.w        vr1,    vr0,   0x44
+    vsle.bu         vr3,    vr2,   vr1
+    vssrlni.bu.h    vr3,    vr3,   4
+    vclz.d          vr4,    vr3
+    vpickve2gr.w    t1,     vr4,   0
+
+    srai.w          t1,     t1,    2
+    sub.w           a0,     t0,    t1
+endfunc
+
+/*
+ * int coeff_last8_c(dctcoef *l)
+ */
+function coeff_last8_lsx
+    addi.w          t0,     zero,  7
+    vld             vr0,    a0,    0
+    vclz.d          vr1,    vr0
+    vpickve2gr.w    t1,     vr1,   0
+    vpickve2gr.w    t2,     vr1,   2
+    li.d            t3,     64
+    bne             t2,     t3,    .LAST8_LOW_LSX
+    addi.d          t4,     t1,    0
+    addi.d          t0,     t0,    -4
+    b               .LAST8_END_LSX
+.LAST8_LOW_LSX:
+    addi.d          t4,     t2,    0
+.LAST8_END_LSX:
+    srai.w          t4,     t4,    4
+    sub.w           a0,     t0,    t4
+endfunc
+
+/*
+ * int coeff_last4_c(dctcoef *l)
+ */
+function coeff_last4_lsx
+    addi.w          t0,     zero,  3
+    vld             vr0,    a0,    0
+    vclz.d          vr1,    vr0
+    vpickve2gr.w    t1,     vr1,   0
+    srai.w          t1,     t1,    4
+    sub.w           a0,     t0,    t1
+endfunc
+
 // (dct[i] * dequant_mf[i]) << (i_qbits)
 .macro DCT_MF a0, a1, in0, out0, out1
     vld             vr1,    \a0,   0
@@ -430,6 +699,78 @@ function dequant_4x4_lasx
     xvst            xr8,    a0,    0
 endfunc
 
+.macro DCT_MF_LSX   tmp0, tmp1, in0, out0, out1, out2, out3
+    vld             vr0,    \tmp0,    0
+    vld             vr1,    \tmp1,    0
+    vld             vr2,    \tmp1,    16
+    vexth.w.h       vr4,    vr0
+    vsllwil.w.h     vr3,    vr0,    0
+    vmul.w          vr3,    vr3,    vr1
+    vmul.w          vr4,    vr4,    vr2
+    vsll.w          \out0,  vr3,    \in0
+    vsll.w          \out1,  vr4,    \in0
+
+    vld             vr0,    \tmp0,    16
+    vld             vr1,    \tmp1,    32
+    vld             vr2,    \tmp1,    48
+    vsllwil.w.h     vr3,    vr0,    0
+    vpermi.w        vr4,    vr0,    0x0E
+    vsllwil.w.h     vr4,    vr4,    0
+    vmul.w          vr3,    vr3,    vr1
+    vmul.w          vr4,    vr4,    vr2
+    vsll.w          \out2,  vr3,    \in0
+    vsll.w          \out3,  vr4,    \in0
+.endm
+
+.macro DCT_MF_F_LSX tmp0, tmp1, in0, out0, out1, out2, out3
+    vld             vr0,    \tmp0,    0
+    vld             vr1,    \tmp1,    0
+    vld             vr2,    \tmp1,    16
+    vexth.w.h       vr4,    vr0
+    vsllwil.w.h     vr3,    vr0,    0
+    vmul.w          vr3,    vr3,    vr1
+    vmul.w          vr4,    vr4,    vr2
+    vsrar.w         \out0,  vr3,    \in0
+    vsrar.w         \out1,  vr4,    \in0
+
+    vld             vr0,    \tmp0,    16
+    vld             vr1,    \tmp1,    32
+    vld             vr2,    \tmp1,    48
+    vexth.w.h       vr4,    vr0
+    vsllwil.w.h     vr3,    vr0,    0
+    vmul.w          vr3,    vr3,    vr1
+    vmul.w          vr4,    vr4,    vr2
+    vsrar.w         \out2,  vr3,    \in0
+    vsrar.w         \out3,  vr4,    \in0
+.endm
+
+function dequant_4x4_lsx
+    addi.w          t1,     zero,   6
+    addi.w          t2,     zero,   4
+    div.w           t0,     a2,     t1
+    sub.w           t0,     t0,     t2
+    mod.w           t1,     a2,     t1
+    slli.w          t1,     t1,     6
+    add.d           a1,     a1,     t1
+    blt             t0,     zero,   .DQ4x4_DEQUANT_SHR_LSX
+
+    vreplgr2vr.w    vr6,    t0
+    DCT_MF_LSX      a0,     a1,     vr6,    vr7,    vr8,    vr9,    vr10
+    b               .DQ4x4_END_LSX
+
+.DQ4x4_DEQUANT_SHR_LSX:
+    sub.w           t4,     zero,   t0
+    vreplgr2vr.w    vr6,    t4
+    DCT_MF_F_LSX    a0,     a1,     vr6,    vr7,    vr8,    vr9,    vr10
+.DQ4x4_END_LSX:
+    vpickev.h       vr11,    vr9,    vr7
+    vpickev.h       vr12,    vr10,   vr8
+    vpermi.w        vr13,    vr12,   0x0E
+    vpermi.w        vr12,    vr11,   0x44
+    vpermi.w        vr13,    vr11,   0x4E
+    vst             vr12,    a0,     0
+    vst             vr13,    a0,    16
+endfunc
 /*
  * void dequant_8x8( dctcoef dct[64], int dequant_mf[6][64], int i_qp )
  */
@@ -479,7 +820,64 @@ function dequant_8x8_lasx
 .endr
 
 .DQ8x8_END:
+endfunc
 
+function dequant_8x8_lsx
+    addi.w          t1,     zero,   6
+    div.w           t0,     a2,     t1
+    sub.w           t0,     t0,     t1
+    mod.w           t1,     a2,     t1
+    slli.w          t1,     t1,     8
+    add.d           a1,     a1,     t1
+
+    blt             t0,     zero,   .DQ8x8_DEQUANT_SHR_LSX
+    vreplgr2vr.w    vr6,    t0
+    DCT_MF_LSX      a0,     a1,     vr6,    vr7,    vr8,    vr9,    vr10
+    vpickev.h       vr11,   vr9,    vr7
+    vpickev.h       vr12,   vr10,   vr8
+    vpermi.w        vr13,   vr12,   0x0E
+    vpermi.w        vr12,   vr11,   0x44
+    vpermi.w        vr13,   vr11,   0x4E
+    vst             vr12,   a0,     0
+    vst             vr13,   a0,     16
+.rept 3
+    addi.d          a0,     a0,     32
+    addi.d          a1,     a1,     64
+    DCT_MF_LSX      a0,     a1,     vr6,    vr7,    vr8,    vr9,    vr10
+    vpickev.h       vr11,   vr9,    vr7
+    vpickev.h       vr12,   vr10,   vr8
+    vpermi.w        vr13,   vr12,   0x0E
+    vpermi.w        vr12,   vr11,   0x44
+    vpermi.w        vr13,   vr11,   0x4E
+    vst             vr12,   a0,     0
+    vst             vr13,   a0,     16
+.endr
+    b               .DQ8x8_END_LSX
+
+.DQ8x8_DEQUANT_SHR_LSX:
+    sub.w           t4,     zero,   t0
+    vreplgr2vr.w    vr6,    t4
+    DCT_MF_F_LSX    a0,     a1,     vr6,    vr7,    vr8,    vr9,    vr10
+    vpickev.h       vr11,   vr9,    vr7
+    vpickev.h       vr12,   vr10,   vr8
+    vpermi.w        vr13,   vr12,   0x0E
+    vpermi.w        vr12,   vr11,   0x44
+    vpermi.w        vr13,   vr11,   0x4E
+    vst             vr12,   a0,     0
+    vst             vr13,   a0,     16
+.rept 3
+    addi.d          a0,     a0,     32
+    addi.d          a1,     a1,     64
+    DCT_MF_F_LSX    a0,     a1,     vr6,    vr7,    vr8,    vr9,    vr10
+    vpickev.h       vr11,   vr9,    vr7
+    vpickev.h       vr12,   vr10,   vr8
+    vpermi.w        vr13,   vr12,   0x0E
+    vpermi.w        vr12,   vr11,   0x44
+    vpermi.w        vr13,   vr11,   0x4E
+    vst             vr12,   a0,     0
+    vst             vr13,   a0,     16
+.endr
+.DQ8x8_END_LSX:
 endfunc
 
 /*
@@ -534,6 +932,60 @@ function dequant_4x4_dc_lasx
     xvst            xr8,    a0,    0
 endfunc
 
+function dequant_4x4_dc_lsx
+    addi.w          t0,     zero,   6
+    div.w           t1,     a2,     t0
+    sub.w           t1,     t1,     t0
+
+    blt             t1,     zero,   .DQ4x4DC_LT_ZERO_LSX
+    mod.w           t2,     a2,     t0
+    slli.w          t2,     t2,     6
+    ldx.w           t0,     a1,     t2
+    sll.w           t0,     t0,     t1
+    vld             vr1,    a0,     0
+    vld             vr2,    a0,     16
+    vreplgr2vr.w    vr3,    t0
+    vexth.w.h       vr6,    vr1
+    vsllwil.w.h     vr5,    vr1,   0
+    vmul.w          vr5,    vr5,   vr3
+    vmul.w          vr6,    vr6,   vr3
+
+    vexth.w.h       vr8,    vr2
+    vsllwil.w.h     vr7,    vr2,   0
+    vmul.w          vr7,    vr7,   vr3
+    vmul.w          vr8,    vr8,   vr3
+    b               .DQ4x4DC_END_LSX
+.DQ4x4DC_LT_ZERO_LSX:
+    mod.w           t2,     a2,     t0
+    slli.w          t2,     t2,     6
+    ldx.w           t0,     a1,     t2
+    sub.w           t3,     zero,   t1
+    vld             vr1,    a0,     0
+    vld             vr2,    a0,     16
+    vreplgr2vr.w    vr3,    t0
+    vreplgr2vr.w    vr4,    t3
+    vexth.w.h       vr6,    vr1
+    vsllwil.w.h     vr5,    vr1,    0
+    vexth.w.h       vr8,    vr2
+    vsllwil.w.h     vr7,    vr2,    0
+    vmul.w          vr5,    vr5,    vr3
+    vmul.w          vr6,    vr6,    vr3
+    vmul.w          vr7,    vr7,    vr3
+    vmul.w          vr8,    vr8,    vr3
+    vsrar.w         vr5,    vr5,    vr4
+    vsrar.w         vr6,    vr6,    vr4
+    vsrar.w         vr7,    vr7,    vr4
+    vsrar.w         vr8,    vr8,    vr4
+.DQ4x4DC_END_LSX:
+    vpickev.h       vr9,    vr7,    vr5
+    vpickev.h       vr10,   vr8,    vr6
+    vpermi.w        vr11,   vr10,   0x0E
+    vpermi.w        vr10,   vr9,    0x44
+    vpermi.w        vr11,   vr9,    0x4E
+    vst             vr10,   a0,     0
+    vst             vr11,   a0,     16
+endfunc
+
 /*
  * int decimate_score15( dctcoef *dct )
  */
@@ -585,6 +1037,57 @@ function decimate_score15_lasx
 .END_SCORE_15:
 endfunc
 
+function decimate_score15_lsx
+    addi.w          t0,     zero,   15
+    la.local        t3,     decimate_table4
+    addi.d          t4,     a0,     2
+
+    vld             vr0,    t4,     0
+    vld             vr1,    t4,     16
+    vldi            vr3,    1
+    vinsgr2vr.h     vr1,    zero,   7
+    vssrlni.bu.h    vr0,    vr0,    0
+    vssrlni.bu.h    vr1,    vr1,    0
+    vpermi.w        vr2,    vr1,    0x0E
+    vpermi.w        vr1,    vr0,    0x44
+    vpermi.w        vr2,    vr0,    0x4E
+    vsle.bu         vr4,    vr3,    vr1
+    vsle.bu         vr5,    vr3,    vr2
+    vssrlni.bu.h    vr4,    vr4,    4
+    vssrlni.bu.h    vr5,    vr5,    4
+    vclz.d          vr4,    vr4
+    vclz.d          vr5,    vr5
+    vpickve2gr.w    t1,     vr4,    0
+
+    srai.w          t1,     t1,     2
+    sub.w           t2,     t0,     t1
+    addi.w          t0,     zero,   2
+    move            a0,     zero
+    slli.d          t2,     t2,     1
+.LOOP_SCORE_15_LSX:
+    blt             t2,     zero,  .END_SCORE_15_LSX
+    ldx.h           t5,     t4,    t2
+    addi.d          t6,     t5,    1
+    bltu            t0,     t6,    .RET_SCORE_15_1_LSX
+    addi.d          t2,     t2,    -2
+    move            t5,     zero
+.WHILE_SCORE_15_LSX:
+    blt             t2,     zero,  .END_WHILE_15_LSX
+    ldx.h           t1,     t4,    t2
+    bnez            t1,     .END_WHILE_15_LSX
+    addi.d          t2,     t2,    -2
+    addi.d          t5,     t5,    1
+    b               .WHILE_SCORE_15_LSX
+.END_WHILE_15_LSX:
+    ldx.b           t1,     t3,    t5
+    add.d           a0,     a0,    t1
+    b               .LOOP_SCORE_15_LSX
+.RET_SCORE_15_1_LSX:
+    addi.d          a0,     zero,  9
+    jirl            $r0,    $r1,   0x0
+.END_SCORE_15_LSX:
+endfunc
+
 /*
  * int decimate_score16( dctcoef *dct )
  */
@@ -634,6 +1137,57 @@ function decimate_score16_lasx
 .END_SCORE_16:
 endfunc
 
+function decimate_score16_lsx
+    addi.w          t0,     zero,   15
+    la.local        t3,     decimate_table4
+    addi.w          t0,     zero,   15
+    vld             vr0,    a0,     0
+    vld             vr1,    a0,     16
+    vldi            vr2,    1
+
+    vssrlni.bu.h    vr0,    vr0,    0
+    vssrlni.bu.h    vr1,    vr1,    0
+    vpermi.w        vr3,    vr1,    0x0E
+    vpermi.w        vr1,    vr0,    0x44
+    vpermi.w        vr3,    vr0,    0x4E
+    vsle.bu         vr4,    vr2,    vr1
+    vsle.bu         vr5,    vr2,    vr3
+    vssrlni.bu.h    vr4,    vr4,    4
+    vssrlni.bu.h    vr5,    vr5,    4
+    vclz.d          vr4,    vr4
+    vclz.d          vr5,    vr5
+    vpickve2gr.w    t1,     vr4,    0
+
+    srai.w          t1,     t1,     2
+    sub.w           t2,     t0,     t1
+    move            t4,     a0
+    addi.d          t0,     zero,   2
+    move            a0,     zero
+    slli.d          t2,     t2,     1
+.LOOP_SCORE_16_LSX:
+    blt             t2,     zero,  .END_SCORE_16_LSX
+    ldx.h           t5,     t4,    t2
+    addi.d          t6,     t5,    1
+    bltu            t0,     t6,    .RET_SCORE_16_1_LSX
+    addi.d          t2,     t2,    -2
+    move            t5,     zero
+.WHILE_SCORE_16_LSX:
+    blt             t2,     zero,  .END_WHILE_16_LSX
+    ldx.h           t1,     t4,    t2
+    bnez            t1,     .END_WHILE_16_LSX
+    addi.d          t2,     t2,    -2
+    addi.d          t5,     t5,    1
+    b               .WHILE_SCORE_16_LSX
+.END_WHILE_16_LSX:
+    ldx.b           t1,     t3,    t5
+    add.d           a0,     a0,    t1
+    b               .LOOP_SCORE_16_LSX
+.RET_SCORE_16_1_LSX:
+    addi.d          a0,     zero,  9
+    jirl            $r0,    $r1,   0x0
+.END_SCORE_16_LSX:
+endfunc
+
 /*
  * int decimate_score64( dctcoef *dct )
  */
@@ -696,3 +1250,92 @@ function decimate_score64_lasx
     jirl            $r0,    $r1,   0x0
 .END_SCORE_64:
 endfunc
+
+function decimate_score64_lsx
+    addi.w          t0,     zero,  63
+    la.local        t3,     decimate_table8
+    vxor.v          vr20,   vr0,   vr0
+    vld             vr0,    a0,    0
+    vld             vr1,    a0,    16
+    vld             vr2,    a0,    32
+    vld             vr3,    a0,    48
+    vld             vr4,    a0,    64
+    vld             vr5,    a0,    80
+    vld             vr6,    a0,    96
+    vld             vr7,    a0,    112
+    vldi            vr8,    1
+    vldi            vr9,    0x408
+    vldi            vr10,   0x401
+
+    vssrlni.bu.h    vr0,    vr0,   0
+    vssrlni.bu.h    vr1,    vr1,   0
+    vssrlni.bu.h    vr2,    vr2,   0
+    vssrlni.bu.h    vr3,    vr3,   0
+    vssrlni.bu.h    vr4,    vr4,   0
+    vssrlni.bu.h    vr5,    vr5,   0
+    vssrlni.bu.h    vr6,    vr6,   0
+    vssrlni.bu.h    vr7,    vr7,   0
+    vpermi.w        vr2,    vr0,   0x44
+    vpermi.w        vr3,    vr1,   0x44
+    vpermi.w        vr6,    vr4,   0x44
+    vpermi.w        vr7,    vr5,   0x44
+    vsle.bu         vr2,    vr8,   vr2
+    vsle.bu         vr3,    vr8,   vr3
+    vsle.bu         vr6,    vr8,   vr6
+    vsle.bu         vr7,    vr8,   vr7
+    vssrlni.bu.h    vr2,    vr2,   4
+    vssrlni.bu.h    vr3,    vr3,   4
+    vssrlni.bu.h    vr6,    vr6,   4
+    vssrlni.bu.h    vr7,    vr7,   4
+    vpermi.w        vr6,    vr2,   0x44
+    vpermi.w        vr7,    vr3,   0x44
+    vpermi.w        vr11,   vr7,   0x0E
+    vpermi.w        vr7,    vr6,   0x44
+    vpermi.w        vr7,    vr7,   0xD8
+    vpermi.w        vr11,   vr6,   0x4E
+    vpermi.w        vr11,   vr11,  0xD8
+    vclz.w          vr7,    vr7
+    vclz.w          vr11,   vr11
+    vssrlni.hu.w    vr7,    vr7,   2
+    vssrlni.hu.w    vr11,   vr11,  2
+    vpermi.w        vr12,   vr11,  0x0E
+    vpermi.w        vr11,   vr7,   0x44
+    vpermi.w        vr12,   vr7,   0x4E
+    vsub.h          vr11,   vr9,   vr11
+    vsub.h          vr12,   vr9,   vr12
+    vsll.h          vr13,   vr10,  vr11
+    vsll.h          vr14,   vr10,  vr12
+    vssrlni.bu.h    vr13,   vr13,  1
+    vssrlni.bu.h    vr14,   vr14,  1
+
+    vclz.d          vr15,   vr14
+    vpickve2gr.w    t1,     vr15,   0
+    sub.w           t2,     t0,    t1
+    move            t4,     a0
+    addi.d          t0,     zero,  2
+    slli.d          t2,     t2,    1
+    move            a0,     zero
+.LOOP_SCORE_64_LSX:
+    blt             t2,     zero,  .END_SCORE_64_LSX
+    ldx.h           t5,     t4,    t2
+    addi.d          t6,     t5,    1
+    bltu            t0,     t6,    .RET_SCORE_64_1_LSX
+    addi.d          t2,     t2,    -2
+    move            t5,     zero
+.WHILE_SCORE_64_LSX:
+    blt             t2,     zero,  .END_WHILE_64_LSX
+    ldx.h           t1,     t4,    t2
+    bnez            t1,     .END_WHILE_64_LSX
+    addi.d          t2,     t2,    -2
+    addi.d          t5,     t5,    1
+    b               .WHILE_SCORE_64_LSX
+.END_WHILE_64_LSX:
+    ldx.b           t1,     t3,    t5
+    add.d           a0,     a0,    t1
+    b               .LOOP_SCORE_64_LSX
+.RET_SCORE_64_1_LSX:
+    addi.d          a0,     zero,  9
+    jirl            $r0,    $r1,   0x0
+.END_SCORE_64_LSX:
+endfunc
+
diff --git a/common/loongarch/quant.h b/common/loongarch/quant.h
index a924726c..3b6b9c6e 100644
--- a/common/loongarch/quant.h
+++ b/common/loongarch/quant.h
@@ -27,6 +27,43 @@
 #ifndef X264_LOONGARCH_QUANT_H
 #define X264_LOONGARCH_QUANT_H
 
+#define x264_coeff_last64_lsx x264_template(coeff_last64_lsx)
+int32_t x264_coeff_last64_lsx( int16_t *p_src );
+#define x264_coeff_last16_lsx x264_template(coeff_last16_lsx)
+int32_t x264_coeff_last16_lsx( int16_t *p_src );
+#define x264_coeff_last15_lsx x264_template(coeff_last15_lsx)
+int32_t x264_coeff_last15_lsx( int16_t *p_src );
+#define x264_coeff_last8_lsx x264_template(coeff_last8_lsx)
+int32_t x264_coeff_last8_lsx( int16_t *p_src );
+#define x264_coeff_last4_lsx x264_template(coeff_last4_lsx)
+int32_t x264_coeff_last4_lsx( int16_t *p_src );
+
+#define x264_quant_4x4_lsx x264_template(quant_4x4_lsx)
+int32_t x264_quant_4x4_lsx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
+#define x264_quant_4x4x4_lsx x264_template(quant_4x4x4_lsx)
+int32_t x264_quant_4x4x4_lsx( int16_t p_dct[4][16],
+                               uint16_t pu_mf[16], uint16_t pu_bias[16] );
+#define x264_quant_8x8_lsx x264_template(quant_8x8_lsx)
+int32_t x264_quant_8x8_lsx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
+#define x264_quant_4x4_dc_lsx x264_template(quant_4x4_dc_lsx)
+int32_t x264_quant_4x4_dc_lsx( dctcoef dct[16], int32_t mf, int32_t bias );
+#define x264_quant_2x2_dc_lsx x264_template(quant_2x2_dc_lsx)
+int32_t x264_quant_2x2_dc_lsx( dctcoef dct[4], int32_t mf, int32_t bias );
+
+#define x264_dequant_4x4_lsx x264_template(dequant_4x4_lsx)
+void x264_dequant_4x4_lsx( dctcoef dct[16], int dequant_mf[6][16], int i_qp );
+#define x264_dequant_8x8_lsx x264_template(dequant_8x8_lsx)
+void x264_dequant_8x8_lsx( dctcoef dct[64], int dequant_mf[6][64], int i_qp );
+#define x264_dequant_4x4_dc_lsx x264_template(dequant_4x4_dc_lsx)
+void x264_dequant_4x4_dc_lsx( dctcoef dct[16], int dequant_mf[6][16], int i_qp );
+
+#define x264_decimate_score15_lsx x264_template(decimate_score15_lsx)
+int x264_decimate_score15_lsx( dctcoef *dct );
+#define x264_decimate_score16_lsx x264_template(decimate_score16_lsx)
+int x264_decimate_score16_lsx( dctcoef *dct );
+#define x264_decimate_score64_lsx x264_template(decimate_score64_lsx)
+int x264_decimate_score64_lsx( dctcoef *dct );
+
 #define x264_coeff_last64_lasx x264_template(coeff_last64_lasx)
 int32_t x264_coeff_last64_lasx( int16_t *p_src );
 #define x264_coeff_last16_lasx x264_template(coeff_last16_lasx)
diff --git a/common/loongarch/sad-a.S b/common/loongarch/sad-a.S
index 8207f6d6..5d988aba 100644
--- a/common/loongarch/sad-a.S
+++ b/common/loongarch/sad-a.S
@@ -25,7 +25,7 @@
  * For more information, contact us at licensing@x264.com.
  *****************************************************************************/
 
-#include "asm.S"
+#include "loongson_asm.S"
 
 #if !HIGH_BIT_DEPTH
 
@@ -2138,10 +2138,10 @@ function pixel_sad_4x8_lasx
     add.d           a0,    t2,   t3
 endfunc
 
-/* int32_t x264_pixel_sad_4x4_lasx(uint8_t *p_src, intptr_t i_src_stride,
- *                                 uint8_t *p_ref, intptr_t i_ref_stride)
+/* int32_t x264_pixel_sad_4x4_lsx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                uint8_t *p_ref, intptr_t i_ref_stride)
  */
-function pixel_sad_4x4_lasx
+function pixel_sad_4x4_lsx
     slli.d          t1,    a1,   1
     slli.d          t2,    a3,   1
     add.d           t3,    a1,   t1
@@ -2166,6 +2166,1554 @@ function pixel_sad_4x4_lasx
     vpickve2gr.wu   a0,    vr6,  0
 endfunc
 
-.end     sad-a.S
+/* int32_t x264_pixel_sad_4x8_lsx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_4x8_lsx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    // Load data from p_src and p_ref
+    FLDS_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDS_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.w         vr3,   vr5,  vr3
+    vilvl.w         vr4,   vr6,  vr4
+    vilvl.w         vr7,   vr9,  vr7
+    vilvl.w         vr8,   vr10, vr8
+    vilvl.d         vr3,   vr7,  vr3
+    vilvl.d         vr4,   vr8,  vr4
+    vabsd.bu        vr11,  vr3,  vr4
+    vhaddw.hu.bu    vr11,  vr11, vr11
+
+    alsl.d          a0,    a1,   a0,  2
+    alsl.d          a2,    a3,   a2,  2
+    FLDS_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDS_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.w         vr3,   vr5,  vr3
+    vilvl.w         vr4,   vr6,  vr4
+    vilvl.w         vr7,   vr9,  vr7
+    vilvl.w         vr8,   vr10, vr8
+    vilvl.d         vr3,   vr7,  vr3
+    vilvl.d         vr4,   vr8,  vr4
+    vabsd.bu        vr5,   vr3,  vr4
+    vhaddw.hu.bu    vr5,   vr5,  vr5
+
+    vadd.h          vr6,   vr11, vr5
+    vhaddw.wu.hu    vr6,   vr6,  vr6
+    vhaddw.du.wu    vr6,   vr6,  vr6
+    vhaddw.qu.du    vr6,   vr6,  vr6
+    vpickve2gr.wu   a0,    vr6,  0
+endfunc
+
+/* int32_t x264_pixel_sad_4x16_lsx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                 uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_4x16_lsx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    // Load data from p_src and p_ref
+    FLDS_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDS_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.w         vr3,   vr5,  vr3
+    vilvl.w         vr4,   vr6,  vr4
+    vilvl.w         vr7,   vr9,  vr7
+    vilvl.w         vr8,   vr10, vr8
+    vilvl.d         vr3,   vr7,  vr3
+    vilvl.d         vr4,   vr8,  vr4
+    vabsd.bu        vr11,  vr3,  vr4
+    vhaddw.hu.bu    vr11,  vr11, vr11
+
+.rept 3
+    alsl.d          a0,    a1,   a0,  2
+    alsl.d          a2,    a3,   a2,  2
+    FLDS_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDS_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.w         vr3,   vr5,  vr3
+    vilvl.w         vr4,   vr6,  vr4
+    vilvl.w         vr7,   vr9,  vr7
+    vilvl.w         vr8,   vr10, vr8
+    vilvl.d         vr3,   vr7,  vr3
+    vilvl.d         vr4,   vr8,  vr4
+    vabsd.bu        vr12,  vr3,  vr4
+    vhaddw.hu.bu    vr12,  vr12, vr12
+    vadd.h          vr11,  vr11, vr12
+.endr
+
+    vhaddw.wu.hu    vr11,  vr11, vr11
+    vhaddw.du.wu    vr11,  vr11, vr11
+    vhaddw.qu.du    vr11,  vr11, vr11
+    vpickve2gr.wu   a0,    vr11, 0
+endfunc
+
+/* int32_t x264_pixel_sad_8x4_lsx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_8x4_lsx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr8,   vr10, vr8
+    vabsd.bu        vr11,  vr3,  vr4
+    vabsd.bu        vr12,  vr7,  vr8
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vhaddw.hu.bu    vr12,  vr12, vr12
+    vadd.h          vr6,   vr11, vr12
+    vhaddw.wu.hu    vr6,   vr6,  vr6
+    vhaddw.du.wu    vr6,   vr6,  vr6
+    vhaddw.qu.du    vr6,   vr6,  vr6
+    vpickve2gr.wu   a0,    vr6,  0
+endfunc
+
+/* int32_t x264_pixel_sad_8x8_lsx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_8x8_lsx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr8,   vr10, vr8
+    vabsd.bu        vr11,  vr3,  vr4
+    vabsd.bu        vr12,  vr7,  vr8
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vhaddw.hu.bu    vr12,  vr12, vr12
+    vadd.h          vr13,  vr11, vr12
+
+    alsl.d          a0,    a1,   a0,  2
+    alsl.d          a2,    a3,   a2,  2
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr8,   vr10, vr8
+    vabsd.bu        vr11,  vr3,  vr4
+    vabsd.bu        vr12,  vr7,  vr8
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vhaddw.hu.bu    vr12,  vr12, vr12
+    vadd.h          vr6,   vr11, vr12
+    vadd.h          vr6,   vr6,  vr13
+    vhaddw.wu.hu    vr6,   vr6,  vr6
+    vhaddw.du.wu    vr6,   vr6,  vr6
+    vhaddw.qu.du    vr6,   vr6,  vr6
+    vpickve2gr.wu   a0,    vr6,  0
+endfunc
+
+/* int32_t x264_pixel_sad_8x16_lsx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_8x16_lsx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr8,   vr10, vr8
+    vabsd.bu        vr11,  vr3,  vr4
+    vabsd.bu        vr12,  vr7,  vr8
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vhaddw.hu.bu    vr12,  vr12, vr12
+    vadd.h          vr13,  vr11, vr12
+
+.rept 3
+    alsl.d          a0,    a1,   a0,  2
+    alsl.d          a2,    a3,   a2,  2
+    FLDD_LOADX_4    a0,    a1,   t1,  t3,  f3, f5, f7, f9
+    FLDD_LOADX_4    a2,    a3,   t2,  t4,  f4, f6, f8, f10
+    vilvl.d         vr3,   vr5,  vr3
+    vilvl.d         vr7,   vr9,  vr7
+    vilvl.d         vr4,   vr6,  vr4
+    vilvl.d         vr8,   vr10, vr8
+    vabsd.bu        vr11,  vr3,  vr4
+    vabsd.bu        vr12,  vr7,  vr8
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vhaddw.hu.bu    vr12,  vr12, vr12
+    vadd.h          vr14,  vr11, vr12
+    vadd.h          vr13,  vr13, vr14
+.endr
+    vhaddw.wu.hu    vr13,  vr13,  vr13
+    vhaddw.du.wu    vr13,  vr13,  vr13
+    vhaddw.qu.du    vr13,  vr13,  vr13
+    vpickve2gr.wu   a0,    vr13,  0
+endfunc
+
+/* int32_t x264_pixel_sad_16x8_lsx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                 uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_16x8_lsx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    LSX_LOADX_4     a0,    a1,   t1, t3, vr0, vr1, vr2, vr3
+    LSX_LOADX_4     a2,    a3,   t2, t4, vr4, vr5, vr6, vr7
+    vabsd.bu        vr8,   vr0,  vr4
+    vabsd.bu        vr9,   vr1,  vr5
+    vabsd.bu        vr10,  vr2,  vr6
+    vabsd.bu        vr11,  vr3,  vr7
+    vhaddw.hu.bu    vr8,   vr8,  vr8
+    vhaddw.hu.bu    vr9,   vr9,  vr9
+    vhaddw.hu.bu    vr10,  vr10, vr10
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vadd.h          vr8,   vr8,  vr9
+    vadd.h          vr9,   vr10, vr11
+    vadd.h          vr14,  vr8,  vr9
+
+    alsl.d          a0,    a1,   a0,   2
+    alsl.d          a2,    a3,   a2,   2
+    LSX_LOADX_4     a0,    a1,   t1, t3, vr0, vr1, vr2, vr3
+    LSX_LOADX_4     a2,    a3,   t2, t4, vr4, vr5, vr6, vr7
+    vabsd.bu        vr8,   vr0,  vr4
+    vabsd.bu        vr9,   vr1,  vr5
+    vabsd.bu        vr10,  vr2,  vr6
+    vabsd.bu        vr11,  vr3,  vr7
+    vhaddw.hu.bu    vr8,   vr8,  vr8
+    vhaddw.hu.bu    vr9,   vr9,  vr9
+    vhaddw.hu.bu    vr10,  vr10, vr10
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vadd.h          vr8,   vr8,  vr9
+    vadd.h          vr9,   vr10, vr11
+    vadd.h          vr12,  vr8,  vr9
+
+    vadd.h          vr13,  vr12, vr14
+    vhaddw.wu.hu    vr13,  vr13, vr13
+    vhaddw.du.wu    vr13,  vr13, vr13
+    vhaddw.qu.du    vr13,  vr13, vr13
+    vpickve2gr.wu   a0,    vr13, 0
+endfunc
+
+/* int32_t x264_pixel_sad_16x16_lsx(uint8_t *p_src, intptr_t i_src_stride,
+ *                                  uint8_t *p_ref, intptr_t i_ref_stride)
+ */
+function pixel_sad_16x16_lsx
+    slli.d          t1,    a1,   1
+    slli.d          t2,    a3,   1
+    add.d           t3,    a1,   t1
+    add.d           t4,    a3,   t2
+
+    LSX_LOADX_4     a0,    a1,   t1, t3, vr0, vr1, vr2, vr3
+    LSX_LOADX_4     a2,    a3,   t2, t4, vr4, vr5, vr6, vr7
+    vabsd.bu        vr8,   vr0,  vr4
+    vabsd.bu        vr9,   vr1,  vr5
+    vabsd.bu        vr10,  vr2,  vr6
+    vabsd.bu        vr11,  vr3,  vr7
+    vhaddw.hu.bu    vr8,   vr8,  vr8
+    vhaddw.hu.bu    vr9,   vr9,  vr9
+    vhaddw.hu.bu    vr10,  vr10, vr10
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vadd.h          vr8,   vr8,  vr9
+    vadd.h          vr9,   vr10, vr11
+    vadd.h          vr13,  vr8,  vr9
+
+.rept 3
+    alsl.d          a0,    a1,   a0,   2
+    alsl.d          a2,    a3,   a2,   2
+    LSX_LOADX_4     a0,    a1,   t1, t3, vr0, vr1, vr2, vr3
+    LSX_LOADX_4     a2,    a3,   t2, t4, vr4, vr5, vr6, vr7
+    vabsd.bu        vr8,   vr0,  vr4
+    vabsd.bu        vr9,   vr1,  vr5
+    vabsd.bu        vr10,  vr2,  vr6
+    vabsd.bu        vr11,  vr3,  vr7
+    vhaddw.hu.bu    vr8,   vr8,  vr8
+    vhaddw.hu.bu    vr9,   vr9,  vr9
+    vhaddw.hu.bu    vr10,  vr10, vr10
+    vhaddw.hu.bu    vr11,  vr11, vr11
+    vadd.h          vr8,   vr8,  vr9
+    vadd.h          vr9,   vr10, vr11
+    vadd.h          vr12,  vr8,  vr9
+    vadd.h          vr13,  vr12, vr13
+.endr
+
+    vhaddw.wu.hu    vr13,  vr13, vr13
+    vhaddw.du.wu    vr13,  vr13, vr13
+    vhaddw.qu.du    vr13,  vr13, vr13
+    vpickve2gr.wu   a0,    vr13, 0
+endfunc
+
+/*
+ * void x264_pixel_sad_x3_4x8_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                intptr_t i_ref_stride,
+ *                                int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_4x8_lsx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.s          f3,     a0,    0
+    fld.s          f7,     a0,    16
+    fld.s          f11,    a0,    32
+    fld.s          f15,    a0,    48
+    FLDS_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDS_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDS_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.w        vr3,    vr7,   vr3
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.w        vr12,   vr16,  vr12
+    vilvl.w        vr13,   vr17,  vr13
+    vilvl.w        vr14,   vr18,  vr14
+    vilvl.d        vr3,    vr11,  vr3
+    vilvl.d        vr4,    vr12,  vr4
+    vilvl.d        vr5,    vr13,  vr5
+    vilvl.d        vr6,    vr14,  vr6
+    vabsd.bu       vr0,    vr3,   vr4
+    vabsd.bu       vr1,    vr3,   vr5
+    vabsd.bu       vr2,    vr3,   vr6
+
+    alsl.d         a1,     a4,    a1,   2
+    alsl.d         a2,     a4,    a2,   2
+    alsl.d         a3,     a4,    a3,   2
+    fld.s          f3,     a0,    64
+    fld.s          f7,     a0,    80
+    fld.s          f11,    a0,    96
+    fld.s          f15,    a0,    112
+    FLDS_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDS_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDS_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.w        vr3,    vr7,   vr3
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr11,   vr15,  vr11
+    vilvl.w        vr12,   vr16,  vr12
+    vilvl.w        vr13,   vr17,  vr13
+    vilvl.w        vr14,   vr18,  vr14
+    vilvl.d        vr3,    vr11,  vr3
+    vilvl.d        vr4,    vr12,  vr4
+    vilvl.d        vr5,    vr13,  vr5
+    vilvl.d        vr6,    vr14,  vr6
+    vabsd.bu       vr7,    vr3,   vr4
+    vabsd.bu       vr8,    vr3,   vr5
+    vabsd.bu       vr9,    vr3,   vr6
+
+    vhaddw.hu.bu   vr0,    vr0,   vr0
+    vhaddw.hu.bu   vr1,    vr1,   vr1
+    vhaddw.hu.bu   vr2,    vr2,   vr2
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vadd.h         vr7,    vr7,   vr0
+    vadd.h         vr8,    vr8,   vr1
+    vadd.h         vr9,    vr9,   vr2
+    vhaddw.wu.hu   vr7,    vr7,   vr7
+    vhaddw.wu.hu   vr8,    vr8,   vr8
+    vhaddw.wu.hu   vr9,    vr9,   vr9
+    vhaddw.du.wu   vr7,    vr7,   vr7
+    vhaddw.du.wu   vr8,    vr8,   vr8
+    vhaddw.du.wu   vr9,    vr9,   vr9
+    vhaddw.qu.du   vr7,    vr7,   vr7
+    vhaddw.qu.du   vr8,    vr8,   vr8
+    vhaddw.qu.du   vr9,    vr9,   vr9
+
+    // Store data to p_sad_array
+    vstelm.w       vr7,    a5,    0,  0
+    vstelm.w       vr8,    a5,    4,  0
+    vstelm.w       vr9,    a5,    8,  0
+endfunc
+
+/*
+ * void x264_pixel_sad_x3_8x4_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                intptr_t i_ref_stride,
+ *                                int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_8x4_lsx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    0
+    fld.d          f7,     a0,    16
+    fld.d          f11,    a0,    32
+    fld.d          f15,    a0,    48
+    FLDD_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr11,   vr15,  vr11
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr14,   vr18,  vr14
+    vabsd.bu       vr0,    vr3,   vr4
+    vabsd.bu       vr1,    vr3,   vr5
+    vabsd.bu       vr2,    vr3,   vr6
+    vabsd.bu       vr3,    vr11,  vr12
+    vabsd.bu       vr4,    vr11,  vr13
+    vabsd.bu       vr5,    vr11,  vr14
+    vhaddw.hu.bu   vr0,    vr0,   vr0
+    vhaddw.hu.bu   vr1,    vr1,   vr1
+    vhaddw.hu.bu   vr2,    vr2,   vr2
+    vhaddw.hu.bu   vr3,    vr3,   vr3
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vadd.h         vr7,    vr0,   vr3
+    vadd.h         vr8,    vr1,   vr4
+    vadd.h         vr9,    vr2,   vr5
+    vhaddw.wu.hu   vr7,    vr7,   vr7
+    vhaddw.wu.hu   vr8,    vr8,   vr8
+    vhaddw.wu.hu   vr9,    vr9,   vr9
+    vhaddw.du.wu   vr7,    vr7,   vr7
+    vhaddw.du.wu   vr8,    vr8,   vr8
+    vhaddw.du.wu   vr9,    vr9,   vr9
+    vhaddw.qu.du   vr7,    vr7,   vr7
+    vhaddw.qu.du   vr8,    vr8,   vr8
+    vhaddw.qu.du   vr9,    vr9,   vr9
+
+    // Store data to p_sad_array
+    vstelm.w       vr7,    a5,    0,  0
+    vstelm.w       vr8,    a5,    4,  0
+    vstelm.w       vr9,    a5,    8,  0
+endfunc
 
+/*
+ * void x264_pixel_sad_x3_8x8_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                intptr_t i_ref_stride,
+ *                                int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_8x8_lsx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    0
+    fld.d          f7,     a0,    16
+    fld.d          f11,    a0,    32
+    fld.d          f15,    a0,    48
+    FLDD_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr11,   vr15,  vr11
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr14,   vr18,  vr14
+    vabsd.bu       vr7,    vr3,   vr4
+    vabsd.bu       vr8,    vr3,   vr5
+    vabsd.bu       vr9,    vr3,   vr6
+    vabsd.bu       vr10,   vr11,  vr12
+    vabsd.bu       vr15,   vr11,  vr13
+    vabsd.bu       vr16,   vr11,  vr14
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vhaddw.hu.bu   vr16,   vr16,  vr16
+    vadd.h         vr0,    vr7,   vr10
+    vadd.h         vr1,    vr8,   vr15
+    vadd.h         vr2,    vr9,   vr16
+
+    alsl.d         a1,     a4,    a1,   2
+    alsl.d         a2,     a4,    a2,   2
+    alsl.d         a3,     a4,    a3,   2
+    fld.d          f3,     a0,    64
+    fld.d          f7,     a0,    80
+    fld.d          f11,    a0,    96
+    fld.d          f15,    a0,    112
+    FLDD_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr11,   vr15,  vr11
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr14,   vr18,  vr14
+    vabsd.bu       vr7,    vr3,   vr4
+    vabsd.bu       vr8,    vr3,   vr5
+    vabsd.bu       vr9,    vr3,   vr6
+    vabsd.bu       vr10,   vr11,  vr12
+    vabsd.bu       vr15,   vr11,  vr13
+    vabsd.bu       vr16,   vr11,  vr14
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vhaddw.hu.bu   vr16,   vr16,  vr16
+    vadd.h         vr7,    vr7,   vr10
+    vadd.h         vr8,    vr8,   vr15
+    vadd.h         vr9,    vr9,   vr16
+
+    vadd.h         vr7,    vr7,   vr0
+    vadd.h         vr8,    vr8,   vr1
+    vadd.h         vr9,    vr9,   vr2
+    vhaddw.wu.hu   vr7,    vr7,   vr7
+    vhaddw.wu.hu   vr8,    vr8,   vr8
+    vhaddw.wu.hu   vr9,    vr9,   vr9
+    vhaddw.du.wu   vr7,    vr7,   vr7
+    vhaddw.du.wu   vr8,    vr8,   vr8
+    vhaddw.du.wu   vr9,    vr9,   vr9
+    vhaddw.qu.du   vr7,    vr7,   vr7
+    vhaddw.qu.du   vr8,    vr8,   vr8
+    vhaddw.qu.du   vr9,    vr9,   vr9
+
+    // Store data to p_sad_array
+    vstelm.w       vr7,    a5,    0,  0
+    vstelm.w       vr8,    a5,    4,  0
+    vstelm.w       vr9,    a5,    8,  0
+endfunc
+
+/*
+ * void x264_pixel_sad_x3_8x16_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_8x16_lsx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f3,     a0,    0
+    fld.d          f7,     a0,    16
+    fld.d          f11,    a0,    32
+    fld.d          f15,    a0,    48
+    FLDD_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr11,   vr15,  vr11
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr14,   vr18,  vr14
+    vabsd.bu       vr7,    vr3,   vr4
+    vabsd.bu       vr8,    vr3,   vr5
+    vabsd.bu       vr9,    vr3,   vr6
+    vabsd.bu       vr10,   vr11,  vr12
+    vabsd.bu       vr15,   vr11,  vr13
+    vabsd.bu       vr16,   vr11,  vr14
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vhaddw.hu.bu   vr16,   vr16,  vr16
+    vadd.h         vr0,    vr7,   vr10
+    vadd.h         vr1,    vr8,   vr15
+    vadd.h         vr2,    vr9,   vr16
+
+.rept 3
+    alsl.d         a1,     a4,    a1,   2
+    alsl.d         a2,     a4,    a2,   2
+    alsl.d         a3,     a4,    a3,   2
+    addi.d         a0,     a0,    64
+    fld.d          f3,     a0,    0
+    fld.d          f7,     a0,    16
+    fld.d          f11,    a0,    32
+    fld.d          f15,    a0,    48
+    FLDD_LOADX_4   a1,     a4,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a4,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a4,    t1,  t2,  f6, f10, f14, f18
+    vilvl.d        vr3,    vr7,   vr3
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr11,   vr15,  vr11
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr14,   vr18,  vr14
+    vabsd.bu       vr7,    vr3,   vr4
+    vabsd.bu       vr8,    vr3,   vr5
+    vabsd.bu       vr9,    vr3,   vr6
+    vabsd.bu       vr10,   vr11,  vr12
+    vabsd.bu       vr15,   vr11,  vr13
+    vabsd.bu       vr16,   vr11,  vr14
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vhaddw.hu.bu   vr16,   vr16,  vr16
+    vadd.h         vr7,    vr7,   vr10
+    vadd.h         vr8,    vr8,   vr15
+    vadd.h         vr9,    vr9,   vr16
+    vadd.h         vr0,    vr7,   vr0
+    vadd.h         vr1,    vr8,   vr1
+    vadd.h         vr2,    vr9,   vr2
+.endr
+
+    vhaddw.wu.hu   vr0,    vr0,   vr0
+    vhaddw.wu.hu   vr1,    vr1,   vr1
+    vhaddw.wu.hu   vr2,    vr2,   vr2
+    vhaddw.du.wu   vr0,    vr0,   vr0
+    vhaddw.du.wu   vr1,    vr1,   vr1
+    vhaddw.du.wu   vr2,    vr2,   vr2
+    vhaddw.qu.du   vr0,    vr0,   vr0
+    vhaddw.qu.du   vr1,    vr1,   vr1
+    vhaddw.qu.du   vr2,    vr2,   vr2
+
+    // Store data to p_sad_array
+    vstelm.w       vr0,    a5,    0,  0
+    vstelm.w       vr1,    a5,    4,  0
+    vstelm.w       vr2,    a5,    8,  0
+endfunc
+
+/*
+ * void x264_pixel_sad_x3_16x8_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_16x8_lsx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+
+    vld            vr0,    a0,    0
+    vld            vr1,    a0,    16
+    vld            vr2,    a0,    32
+    vld            vr3,    a0,    48
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr1,   vr7
+    vabsd.bu       vr8,    vr1,   vr8
+    vabsd.bu       vr9,    vr1,   vr9
+    vabsd.bu       vr10,   vr2,   vr10
+    vabsd.bu       vr11,   vr2,   vr11
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr3,   vr13
+    vabsd.bu       vr14,   vr3,   vr14
+    vabsd.bu       vr15,   vr3,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr0,    vr7,   vr4
+    vadd.h         vr1,    vr13,  vr10
+    vadd.h         vr16,   vr1,   vr0
+    vadd.h         vr0,    vr8,   vr5
+    vadd.h         vr1,    vr14,  vr11
+    vadd.h         vr17,   vr1,   vr0
+    vadd.h         vr0,    vr9,   vr6
+    vadd.h         vr1,    vr15,  vr12
+    vadd.h         vr18,   vr1,   vr0
+
+    // vr16, vr17, vr18
+    alsl.d         a1,     a4,    a1,   2
+    alsl.d         a2,     a4,    a2,   2
+    alsl.d         a3,     a4,    a3,   2
+    vld            vr0,    a0,    64
+    vld            vr1,    a0,    80
+    vld            vr2,    a0,    96
+    vld            vr3,    a0,    112
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr1,   vr7
+    vabsd.bu       vr8,    vr1,   vr8
+    vabsd.bu       vr9,    vr1,   vr9
+    vabsd.bu       vr10,   vr2,   vr10
+    vabsd.bu       vr11,   vr2,   vr11
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr3,   vr13
+    vabsd.bu       vr14,   vr3,   vr14
+    vabsd.bu       vr15,   vr3,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr0,    vr7,   vr4
+    vadd.h         vr1,    vr13,  vr10
+    vadd.h         vr2,    vr1,   vr0
+    vadd.h         vr0,    vr8,   vr5
+    vadd.h         vr1,    vr14,  vr11
+    vadd.h         vr3,    vr1,   vr0
+    vadd.h         vr0,    vr9,   vr6
+    vadd.h         vr1,    vr15,  vr12
+    vadd.h         vr4,    vr1,   vr0
+
+    vadd.h         vr0,    vr16,  vr2
+    vadd.h         vr1,    vr17,  vr3
+    vadd.h         vr2,    vr18,  vr4
+    vhaddw.wu.hu   vr0,    vr0,   vr0
+    vhaddw.wu.hu   vr1,    vr1,   vr1
+    vhaddw.wu.hu   vr2,    vr2,   vr2
+    vhaddw.du.wu   vr0,    vr0,   vr0
+    vhaddw.du.wu   vr1,    vr1,   vr1
+    vhaddw.du.wu   vr2,    vr2,   vr2
+    vhaddw.qu.du   vr0,    vr0,   vr0
+    vhaddw.qu.du   vr1,    vr1,   vr1
+    vhaddw.qu.du   vr2,    vr2,   vr2
+
+    // Store data to p_sad_array
+    vstelm.w       vr0,    a5,    0,  0
+    vstelm.w       vr1,    a5,    4,  0
+    vstelm.w       vr2,    a5,    8,  0
+endfunc
+
+/*
+ * void x264_pixel_sad_x3_16x16_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                  uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                  intptr_t i_ref_stride,
+ *                                  int32_t p_sad_array[3])
+ */
+function pixel_sad_x3_16x16_lsx
+    slli.d         t1,     a4,    1
+    add.d          t2,     a4,    t1
+
+    vld            vr0,    a0,    0
+    vld            vr1,    a0,    16
+    vld            vr2,    a0,    32
+    vld            vr3,    a0,    48
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr1,   vr7
+    vabsd.bu       vr8,    vr1,   vr8
+    vabsd.bu       vr9,    vr1,   vr9
+    vabsd.bu       vr10,   vr2,   vr10
+    vabsd.bu       vr11,   vr2,   vr11
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr3,   vr13
+    vabsd.bu       vr14,   vr3,   vr14
+    vabsd.bu       vr15,   vr3,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr0,    vr7,   vr4
+    vadd.h         vr1,    vr13,  vr10
+    vadd.h         vr16,   vr1,   vr0
+    vadd.h         vr0,    vr8,   vr5
+    vadd.h         vr1,    vr14,  vr11
+    vadd.h         vr17,   vr1,   vr0
+    vadd.h         vr0,    vr9,   vr6
+    vadd.h         vr1,    vr15,  vr12
+    vadd.h         vr18,   vr1,   vr0
+
+.rept 3
+    alsl.d         a1,     a4,    a1,   2
+    alsl.d         a2,     a4,    a2,   2
+    alsl.d         a3,     a4,    a3,   2
+    addi.d         a0,     a0,    64
+    vld            vr0,    a0,    0
+    vld            vr1,    a0,    16
+    vld            vr2,    a0,    32
+    vld            vr3,    a0,    48
+    LSX_LOADX_4    a1,     a4,    t1,  t2,  vr4, vr7, vr10, vr13
+    LSX_LOADX_4    a2,     a4,    t1,  t2,  vr5, vr8, vr11, vr14
+    LSX_LOADX_4    a3,     a4,    t1,  t2,  vr6, vr9, vr12, vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr1,   vr7
+    vabsd.bu       vr8,    vr1,   vr8
+    vabsd.bu       vr9,    vr1,   vr9
+    vabsd.bu       vr10,   vr2,   vr10
+    vabsd.bu       vr11,   vr2,   vr11
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr3,   vr13
+    vabsd.bu       vr14,   vr3,   vr14
+    vabsd.bu       vr15,   vr3,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr0,    vr7,   vr4
+    vadd.h         vr1,    vr13,  vr10
+    vadd.h         vr2,    vr1,   vr0
+    vadd.h         vr0,    vr8,   vr5
+    vadd.h         vr1,    vr14,  vr11
+    vadd.h         vr3,    vr1,   vr0
+    vadd.h         vr0,    vr9,   vr6
+    vadd.h         vr1,    vr15,  vr12
+    vadd.h         vr4,    vr1,   vr0
+
+    vadd.h         vr16,   vr16,  vr2
+    vadd.h         vr17,   vr17,  vr3
+    vadd.h         vr18,   vr18,  vr4
+.endr
+
+    vhaddw.wu.hu   vr16,   vr16,  vr16
+    vhaddw.wu.hu   vr17,   vr17,  vr17
+    vhaddw.wu.hu   vr18,   vr18,  vr18
+    vhaddw.du.wu   vr16,   vr16,  vr16
+    vhaddw.du.wu   vr17,   vr17,  vr17
+    vhaddw.du.wu   vr18,   vr18,  vr18
+    vhaddw.qu.du   vr16,   vr16,  vr16
+    vhaddw.qu.du   vr17,   vr17,  vr17
+    vhaddw.qu.du   vr18,   vr18,  vr18
+
+    // Store data to p_sad_array
+    vstelm.w       vr16,    a5,    0,  0
+    vstelm.w       vr17,    a5,    4,  0
+    vstelm.w       vr18,    a5,    8,  0
+endfunc
+
+/*
+ * void x264_pixel_sad_x4_4x8_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_4x8_lsx
+    slli.d         t1,     a5,    1
+    add.d          t2,     a5,    t1
+
+    fld.s          f0,     a0,    0
+    fld.s          f1,     a0,    16
+    fld.s          f2,     a0,    32
+    fld.s          f3,     a0,    48
+    FLDS_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f12, f16
+    FLDS_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f13, f17
+    FLDS_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f14, f18
+    FLDS_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f15, f19
+
+    vilvl.w        vr0,    vr1,   vr0
+    vilvl.w        vr2,    vr3,   vr2
+    vilvl.d        vr0,    vr2,   vr0
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr12,   vr16,  vr12
+    vilvl.d        vr1,    vr12,  vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr13,   vr17,  vr13
+    vilvl.d        vr2,    vr13,  vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr14,   vr18,  vr14
+    vilvl.d        vr3,    vr14,  vr6
+    vilvl.w        vr7,    vr11,  vr7
+    vilvl.w        vr15,   vr19,  vr15
+    vilvl.d        vr4,    vr15,  vr7
+    vabsd.bu       vr1,    vr0,   vr1
+    vabsd.bu       vr2,    vr0,   vr2
+    vabsd.bu       vr3,    vr0,   vr3
+    vabsd.bu       vr4,    vr0,   vr4
+    vhaddw.hu.bu   vr20,   vr1,   vr1
+    vhaddw.hu.bu   vr21,   vr2,   vr2
+    vhaddw.hu.bu   vr22,   vr3,   vr3
+    vhaddw.hu.bu   vr23,   vr4,   vr4
+
+    alsl.d         a1,     a5,    a1,   2
+    alsl.d         a2,     a5,    a2,   2
+    alsl.d         a3,     a5,    a3,   2
+    alsl.d         a4,     a5,    a4,   2
+    fld.s          f0,     a0,    64
+    fld.s          f1,     a0,    80
+    fld.s          f2,     a0,    96
+    fld.s          f3,     a0,    112
+    FLDS_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f12, f16
+    FLDS_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f13, f17
+    FLDS_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f14, f18
+    FLDS_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f15, f19
+
+    vilvl.w        vr0,    vr1,   vr0
+    vilvl.w        vr2,    vr3,   vr2
+    vilvl.d        vr0,    vr2,   vr0
+    vilvl.w        vr4,    vr8,   vr4
+    vilvl.w        vr12,   vr16,  vr12
+    vilvl.d        vr1,    vr12,  vr4
+    vilvl.w        vr5,    vr9,   vr5
+    vilvl.w        vr13,   vr17,  vr13
+    vilvl.d        vr2,    vr13,  vr5
+    vilvl.w        vr6,    vr10,  vr6
+    vilvl.w        vr14,   vr18,  vr14
+    vilvl.d        vr3,    vr14,  vr6
+    vilvl.w        vr7,    vr11,  vr7
+    vilvl.w        vr15,   vr19,  vr15
+    vilvl.d        vr4,    vr15,  vr7
+    vabsd.bu       vr1,    vr0,   vr1
+    vabsd.bu       vr2,    vr0,   vr2
+    vabsd.bu       vr3,    vr0,   vr3
+    vabsd.bu       vr4,    vr0,   vr4
+    vhaddw.hu.bu   vr1,    vr1,   vr1
+    vhaddw.hu.bu   vr2,    vr2,   vr2
+    vhaddw.hu.bu   vr3,    vr3,   vr3
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vadd.h         vr16,   vr20,  vr1
+    vadd.h         vr17,   vr21,  vr2
+    vadd.h         vr18,   vr22,  vr3
+    vadd.h         vr19,   vr23,  vr4
+
+    vhaddw.wu.hu   vr16,   vr16,  vr16
+    vhaddw.wu.hu   vr17,   vr17,  vr17
+    vhaddw.wu.hu   vr18,   vr18,  vr18
+    vhaddw.wu.hu   vr19,   vr19,  vr19
+    vhaddw.du.wu   vr16,   vr16,  vr16
+    vhaddw.du.wu   vr17,   vr17,  vr17
+    vhaddw.du.wu   vr18,   vr18,  vr18
+    vhaddw.du.wu   vr19,   vr19,  vr19
+    vhaddw.qu.du   vr16,   vr16,  vr16
+    vhaddw.qu.du   vr17,   vr17,  vr17
+    vhaddw.qu.du   vr18,   vr18,  vr18
+    vhaddw.qu.du   vr19,   vr19,  vr19
+
+    // Store data to p_sad_array
+    vstelm.w       vr16,   a6,    0,      0
+    vstelm.w       vr17,   a6,    4,      0
+    vstelm.w       vr18,   a6,    8,      0
+    vstelm.w       vr19,   a6,    12,     0
+endfunc
+
+/*
+ * void x264_pixel_sad_x4_8x4_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_8x4_lsx
+    slli.d         t1,     a5,    1
+    add.d          t2,     a5,    t1
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f0,     a0,    0
+    fld.d          f1,     a0,    16
+    fld.d          f2,     a0,    32
+    fld.d          f3,     a0,    48
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f14, f18
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f15, f19
+    vilvl.d        vr0,    vr1,   vr0
+    vilvl.d        vr2,    vr3,   vr2
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr15,   vr19,  vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr16,   vr4,   vr12
+    vadd.h         vr17,   vr5,   vr13
+    vadd.h         vr18,   vr6,   vr14
+    vadd.h         vr19,   vr7,   vr15
+    vhaddw.wu.hu   vr16,   vr16,  vr16
+    vhaddw.wu.hu   vr17,   vr17,  vr17
+    vhaddw.wu.hu   vr18,   vr18,  vr18
+    vhaddw.wu.hu   vr19,   vr19,  vr19
+    vhaddw.du.wu   vr16,   vr16,  vr16
+    vhaddw.du.wu   vr17,   vr17,  vr17
+    vhaddw.du.wu   vr18,   vr18,  vr18
+    vhaddw.du.wu   vr19,   vr19,  vr19
+    vhaddw.qu.du   vr16,   vr16,  vr16
+    vhaddw.qu.du   vr17,   vr17,  vr17
+    vhaddw.qu.du   vr18,   vr18,  vr18
+    vhaddw.qu.du   vr19,   vr19,  vr19
+
+    // Store data to p_sad_array
+    vstelm.w       vr16,   a6,    0,      0
+    vstelm.w       vr17,   a6,    4,      0
+    vstelm.w       vr18,   a6,    8,      0
+    vstelm.w       vr19,   a6,    12,     0
+endfunc
+
+/*
+ * void x264_pixel_sad_x4_8x8_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_8x8_lsx
+    slli.d         t1,     a5,    1
+    add.d          t2,     a5,    t1
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f0,     a0,    0
+    fld.d          f1,     a0,    16
+    fld.d          f2,     a0,    32
+    fld.d          f3,     a0,    48
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f14, f18
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f15, f19
+    vilvl.d        vr0,    vr1,   vr0
+    vilvl.d        vr2,    vr3,   vr2
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr15,   vr19,  vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr20,   vr4,   vr12
+    vadd.h         vr21,   vr5,   vr13
+    vadd.h         vr22,   vr6,   vr14
+    vadd.h         vr23,   vr7,   vr15
+
+    alsl.d         a1,     a5,    a1,   2
+    alsl.d         a2,     a5,    a2,   2
+    alsl.d         a3,     a5,    a3,   2
+    alsl.d         a4,     a5,    a4,   2
+    fld.d          f0,     a0,    64
+    fld.d          f1,     a0,    80
+    fld.d          f2,     a0,    96
+    fld.d          f3,     a0,    112
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f14, f18
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f15, f19
+    vilvl.d        vr0,    vr1,   vr0
+    vilvl.d        vr2,    vr3,   vr2
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr15,   vr19,  vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr16,   vr4,   vr12
+    vadd.h         vr17,   vr5,   vr13
+    vadd.h         vr18,   vr6,   vr14
+    vadd.h         vr19,   vr7,   vr15
+
+    vadd.h         vr16,   vr16,  vr20
+    vadd.h         vr17,   vr17,  vr21
+    vadd.h         vr18,   vr18,  vr22
+    vadd.h         vr19,   vr19,  vr23
+    vhaddw.wu.hu   vr16,   vr16,  vr16
+    vhaddw.wu.hu   vr17,   vr17,  vr17
+    vhaddw.wu.hu   vr18,   vr18,  vr18
+    vhaddw.wu.hu   vr19,   vr19,  vr19
+    vhaddw.du.wu   vr16,   vr16,  vr16
+    vhaddw.du.wu   vr17,   vr17,  vr17
+    vhaddw.du.wu   vr18,   vr18,  vr18
+    vhaddw.du.wu   vr19,   vr19,  vr19
+    vhaddw.qu.du   vr16,   vr16,  vr16
+    vhaddw.qu.du   vr17,   vr17,  vr17
+    vhaddw.qu.du   vr18,   vr18,  vr18
+    vhaddw.qu.du   vr19,   vr19,  vr19
+    // Store data to p_sad_array
+    vstelm.w       vr16,   a6,    0,      0
+    vstelm.w       vr17,   a6,    4,      0
+    vstelm.w       vr18,   a6,    8,      0
+    vstelm.w       vr19,   a6,    12,     0
+endfunc
+
+/*
+ * void x264_pixel_sad_x4_8x16_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_8x16_lsx
+    slli.d         t1,     a5,    1
+    add.d          t2,     a5,    t1
+
+    // Load data from p_src, p_ref0, p_ref1 and p_ref2
+    fld.d          f0,     a0,    0
+    fld.d          f1,     a0,    16
+    fld.d          f2,     a0,    32
+    fld.d          f3,     a0,    48
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f14, f18
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f15, f19
+    vilvl.d        vr0,    vr1,   vr0
+    vilvl.d        vr2,    vr3,   vr2
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr15,   vr19,  vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr20,   vr4,   vr12
+    vadd.h         vr21,   vr5,   vr13
+    vadd.h         vr22,   vr6,   vr14
+    vadd.h         vr23,   vr7,   vr15
+
+.rept 3
+    alsl.d         a1,     a5,    a1,   2
+    alsl.d         a2,     a5,    a2,   2
+    alsl.d         a3,     a5,    a3,   2
+    alsl.d         a4,     a5,    a4,   2
+    addi.d         a0,     a0,    64
+    fld.d          f0,     a0,    0
+    fld.d          f1,     a0,    16
+    fld.d          f2,     a0,    32
+    fld.d          f3,     a0,    48
+    FLDD_LOADX_4   a1,     a5,    t1,  t2,  f4, f8,  f12, f16
+    FLDD_LOADX_4   a2,     a5,    t1,  t2,  f5, f9,  f13, f17
+    FLDD_LOADX_4   a3,     a5,    t1,  t2,  f6, f10, f14, f18
+    FLDD_LOADX_4   a4,     a5,    t1,  t2,  f7, f11, f15, f19
+    vilvl.d        vr0,    vr1,   vr0
+    vilvl.d        vr2,    vr3,   vr2
+    vilvl.d        vr4,    vr8,   vr4
+    vilvl.d        vr12,   vr16,  vr12
+    vilvl.d        vr5,    vr9,   vr5
+    vilvl.d        vr13,   vr17,  vr13
+    vilvl.d        vr6,    vr10,  vr6
+    vilvl.d        vr14,   vr18,  vr14
+    vilvl.d        vr7,    vr11,  vr7
+    vilvl.d        vr15,   vr19,  vr15
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vadd.h         vr16,   vr4,   vr12
+    vadd.h         vr17,   vr5,   vr13
+    vadd.h         vr18,   vr6,   vr14
+    vadd.h         vr19,   vr7,   vr15
+    vadd.h         vr20,   vr16,  vr20
+    vadd.h         vr21,   vr17,  vr21
+    vadd.h         vr22,   vr18,  vr22
+    vadd.h         vr23,   vr19,  vr23
+.endr
+    vhaddw.wu.hu   vr20,   vr20,  vr20
+    vhaddw.wu.hu   vr21,   vr21,  vr21
+    vhaddw.wu.hu   vr22,   vr22,  vr22
+    vhaddw.wu.hu   vr23,   vr23,  vr23
+    vhaddw.du.wu   vr20,   vr20,  vr20
+    vhaddw.du.wu   vr21,   vr21,  vr21
+    vhaddw.du.wu   vr22,   vr22,  vr22
+    vhaddw.du.wu   vr23,   vr23,  vr23
+    vhaddw.qu.du   vr20,   vr20,  vr20
+    vhaddw.qu.du   vr21,   vr21,  vr21
+    vhaddw.qu.du   vr22,   vr22,  vr22
+    vhaddw.qu.du   vr23,   vr23,  vr23
+    // Store data to p_sad_array
+    vstelm.w       vr20,   a6,    0,      0
+    vstelm.w       vr21,   a6,    4,      0
+    vstelm.w       vr22,   a6,    8,      0
+    vstelm.w       vr23,   a6,    12,     0
+endfunc
+
+/*
+ * void x264_pixel_sad_x4_16x8_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                 uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                 int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_16x8_lsx
+    slli.d         t1,     a5,    1
+    add.d          t2,     a5,    t1
+
+    vld            vr0,    a0,    0
+    vld            vr1,    a0,    16
+    vld            vr2,    a0,    32
+    vld            vr3,    a0,    48
+    LSX_LOADX_4    a1,     a5,    t1,  t2,  vr4, vr8, vr12, vr16
+    LSX_LOADX_4    a2,     a5,    t1,  t2,  vr5, vr9, vr13, vr17
+    LSX_LOADX_4    a3,     a5,    t1,  t2,  vr6, vr10, vr14, vr18
+    LSX_LOADX_4    a4,     a5,    t1,  t2,  vr7, vr11, vr15, vr19
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr8,    vr1,   vr8
+    vabsd.bu       vr9,    vr1,   vr9
+    vabsd.bu       vr10,   vr1,   vr10
+    vabsd.bu       vr11,   vr1,   vr11
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vabsd.bu       vr16,   vr3,   vr16
+    vabsd.bu       vr17,   vr3,   vr17
+    vabsd.bu       vr18,   vr3,   vr18
+    vabsd.bu       vr19,   vr3,   vr19
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vhaddw.hu.bu   vr16,   vr16,  vr16
+    vhaddw.hu.bu   vr17,   vr17,  vr17
+    vhaddw.hu.bu   vr18,   vr18,  vr18
+    vhaddw.hu.bu   vr19,   vr19,  vr19
+    vadd.h         vr0,    vr4,   vr8
+    vadd.h         vr1,    vr12,  vr16
+    vadd.h         vr20,   vr0,   vr1
+    vadd.h         vr0,    vr5,   vr9
+    vadd.h         vr1,    vr13,  vr17
+    vadd.h         vr21,   vr0,   vr1
+    vadd.h         vr0,    vr6,   vr10
+    vadd.h         vr1,    vr14,  vr18
+    vadd.h         vr22,   vr0,   vr1
+    vadd.h         vr0,    vr7,   vr11
+    vadd.h         vr1,    vr15,  vr19
+    vadd.h         vr23,   vr0,   vr1
+
+    alsl.d         a1,     a5,    a1,   2
+    alsl.d         a2,     a5,    a2,   2
+    alsl.d         a3,     a5,    a3,   2
+    alsl.d         a4,     a5,    a4,   2
+    vld            vr0,    a0,    64
+    vld            vr1,    a0,    80
+    vld            vr2,    a0,    96
+    vld            vr3,    a0,    112
+    LSX_LOADX_4    a1,     a5,    t1,  t2,  vr4, vr8, vr12, vr16
+    LSX_LOADX_4    a2,     a5,    t1,  t2,  vr5, vr9, vr13, vr17
+    LSX_LOADX_4    a3,     a5,    t1,  t2,  vr6, vr10, vr14, vr18
+    LSX_LOADX_4    a4,     a5,    t1,  t2,  vr7, vr11, vr15, vr19
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr8,    vr1,   vr8
+    vabsd.bu       vr9,    vr1,   vr9
+    vabsd.bu       vr10,   vr1,   vr10
+    vabsd.bu       vr11,   vr1,   vr11
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vabsd.bu       vr16,   vr3,   vr16
+    vabsd.bu       vr17,   vr3,   vr17
+    vabsd.bu       vr18,   vr3,   vr18
+    vabsd.bu       vr19,   vr3,   vr19
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vhaddw.hu.bu   vr16,   vr16,  vr16
+    vhaddw.hu.bu   vr17,   vr17,  vr17
+    vhaddw.hu.bu   vr18,   vr18,  vr18
+    vhaddw.hu.bu   vr19,   vr19,  vr19
+    vadd.h         vr0,    vr4,   vr8
+    vadd.h         vr1,    vr12,  vr16
+    vadd.h         vr16,   vr0,   vr1
+    vadd.h         vr0,    vr5,   vr9
+    vadd.h         vr1,    vr13,  vr17
+    vadd.h         vr17,   vr0,   vr1
+    vadd.h         vr0,    vr6,   vr10
+    vadd.h         vr1,    vr14,  vr18
+    vadd.h         vr18,   vr0,   vr1
+    vadd.h         vr0,    vr7,   vr11
+    vadd.h         vr1,    vr15,  vr19
+    vadd.h         vr19,   vr0,   vr1
+
+    vadd.h         vr20,   vr16,  vr20
+    vadd.h         vr21,   vr17,  vr21
+    vadd.h         vr22,   vr18,  vr22
+    vadd.h         vr23,   vr19,  vr23
+    vhaddw.wu.hu   vr20,   vr20,  vr20
+    vhaddw.wu.hu   vr21,   vr21,  vr21
+    vhaddw.wu.hu   vr22,   vr22,  vr22
+    vhaddw.wu.hu   vr23,   vr23,  vr23
+    vhaddw.du.wu   vr20,   vr20,  vr20
+    vhaddw.du.wu   vr21,   vr21,  vr21
+    vhaddw.du.wu   vr22,   vr22,  vr22
+    vhaddw.du.wu   vr23,   vr23,  vr23
+    vhaddw.qu.du   vr20,   vr20,  vr20
+    vhaddw.qu.du   vr21,   vr21,  vr21
+    vhaddw.qu.du   vr22,   vr22,  vr22
+    vhaddw.qu.du   vr23,   vr23,  vr23
+    // Store data to p_sad_array
+    vstelm.w       vr20,   a6,    0,      0
+    vstelm.w       vr21,   a6,    4,      0
+    vstelm.w       vr22,   a6,    8,      0
+    vstelm.w       vr23,   a6,    12,     0
+endfunc
+
+/*
+ * void x264_pixel_sad_x4_16x16_lsx(uint8_t *p_src, uint8_t *p_ref0,
+ *                                  uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                  int32_t p_sad_array[4])
+ */
+function pixel_sad_x4_16x16_lsx
+    slli.d         t1,     a5,    1
+    add.d          t2,     a5,    t1
+
+    vld            vr0,    a0,    0
+    vld            vr1,    a0,    16
+    vld            vr2,    a0,    32
+    vld            vr3,    a0,    48
+    LSX_LOADX_4    a1,     a5,    t1,  t2,  vr4, vr8,  vr12, vr16
+    LSX_LOADX_4    a2,     a5,    t1,  t2,  vr5, vr9,  vr13, vr17
+    LSX_LOADX_4    a3,     a5,    t1,  t2,  vr6, vr10, vr14, vr18
+    LSX_LOADX_4    a4,     a5,    t1,  t2,  vr7, vr11, vr15, vr19
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr8,    vr1,   vr8
+    vabsd.bu       vr9,    vr1,   vr9
+    vabsd.bu       vr10,   vr1,   vr10
+    vabsd.bu       vr11,   vr1,   vr11
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vabsd.bu       vr16,   vr3,   vr16
+    vabsd.bu       vr17,   vr3,   vr17
+    vabsd.bu       vr18,   vr3,   vr18
+    vabsd.bu       vr19,   vr3,   vr19
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vhaddw.hu.bu   vr16,   vr16,  vr16
+    vhaddw.hu.bu   vr17,   vr17,  vr17
+    vhaddw.hu.bu   vr18,   vr18,  vr18
+    vhaddw.hu.bu   vr19,   vr19,  vr19
+    vadd.h         vr0,    vr4,   vr8
+    vadd.h         vr1,    vr12,  vr16
+    vadd.h         vr20,   vr0,   vr1
+    vadd.h         vr0,    vr5,   vr9
+    vadd.h         vr1,    vr13,  vr17
+    vadd.h         vr21,   vr0,   vr1
+    vadd.h         vr0,    vr6,   vr10
+    vadd.h         vr1,    vr14,  vr18
+    vadd.h         vr22,   vr0,   vr1
+    vadd.h         vr0,    vr7,   vr11
+    vadd.h         vr1,    vr15,  vr19
+    vadd.h         vr23,   vr0,   vr1
+
+.rept 3
+    alsl.d         a1,     a5,    a1,   2
+    alsl.d         a2,     a5,    a2,   2
+    alsl.d         a3,     a5,    a3,   2
+    alsl.d         a4,     a5,    a4,   2
+    addi.d         a0,     a0,    64
+    vld            vr0,    a0,    0
+    vld            vr1,    a0,    16
+    vld            vr2,    a0,    32
+    vld            vr3,    a0,    48
+    LSX_LOADX_4    a1,     a5,    t1,  t2,  vr4, vr8,  vr12, vr16
+    LSX_LOADX_4    a2,     a5,    t1,  t2,  vr5, vr9,  vr13, vr17
+    LSX_LOADX_4    a3,     a5,    t1,  t2,  vr6, vr10, vr14, vr18
+    LSX_LOADX_4    a4,     a5,    t1,  t2,  vr7, vr11, vr15, vr19
+    vabsd.bu       vr4,    vr0,   vr4
+    vabsd.bu       vr5,    vr0,   vr5
+    vabsd.bu       vr6,    vr0,   vr6
+    vabsd.bu       vr7,    vr0,   vr7
+    vabsd.bu       vr8,    vr1,   vr8
+    vabsd.bu       vr9,    vr1,   vr9
+    vabsd.bu       vr10,   vr1,   vr10
+    vabsd.bu       vr11,   vr1,   vr11
+    vabsd.bu       vr12,   vr2,   vr12
+    vabsd.bu       vr13,   vr2,   vr13
+    vabsd.bu       vr14,   vr2,   vr14
+    vabsd.bu       vr15,   vr2,   vr15
+    vabsd.bu       vr16,   vr3,   vr16
+    vabsd.bu       vr17,   vr3,   vr17
+    vabsd.bu       vr18,   vr3,   vr18
+    vabsd.bu       vr19,   vr3,   vr19
+    vhaddw.hu.bu   vr4,    vr4,   vr4
+    vhaddw.hu.bu   vr5,    vr5,   vr5
+    vhaddw.hu.bu   vr6,    vr6,   vr6
+    vhaddw.hu.bu   vr7,    vr7,   vr7
+    vhaddw.hu.bu   vr8,    vr8,   vr8
+    vhaddw.hu.bu   vr9,    vr9,   vr9
+    vhaddw.hu.bu   vr10,   vr10,  vr10
+    vhaddw.hu.bu   vr11,   vr11,  vr11
+    vhaddw.hu.bu   vr12,   vr12,  vr12
+    vhaddw.hu.bu   vr13,   vr13,  vr13
+    vhaddw.hu.bu   vr14,   vr14,  vr14
+    vhaddw.hu.bu   vr15,   vr15,  vr15
+    vhaddw.hu.bu   vr16,   vr16,  vr16
+    vhaddw.hu.bu   vr17,   vr17,  vr17
+    vhaddw.hu.bu   vr18,   vr18,  vr18
+    vhaddw.hu.bu   vr19,   vr19,  vr19
+    vadd.h         vr0,    vr4,   vr8
+    vadd.h         vr1,    vr12,  vr16
+    vadd.h         vr16,   vr0,   vr1
+    vadd.h         vr0,    vr5,   vr9
+    vadd.h         vr1,    vr13,  vr17
+    vadd.h         vr17,   vr0,   vr1
+    vadd.h         vr0,    vr6,   vr10
+    vadd.h         vr1,    vr14,  vr18
+    vadd.h         vr18,   vr0,   vr1
+    vadd.h         vr0,    vr7,   vr11
+    vadd.h         vr1,    vr15,  vr19
+    vadd.h         vr19,   vr0,   vr1
+    vadd.h         vr20,   vr16,  vr20
+    vadd.h         vr21,   vr17,  vr21
+    vadd.h         vr22,   vr18,  vr22
+    vadd.h         vr23,   vr19,  vr23
+.endr
+    vhaddw.wu.hu   vr20,   vr20,  vr20
+    vhaddw.wu.hu   vr21,   vr21,  vr21
+    vhaddw.wu.hu   vr22,   vr22,  vr22
+    vhaddw.wu.hu   vr23,   vr23,  vr23
+    vhaddw.du.wu   vr20,   vr20,  vr20
+    vhaddw.du.wu   vr21,   vr21,  vr21
+    vhaddw.du.wu   vr22,   vr22,  vr22
+    vhaddw.du.wu   vr23,   vr23,  vr23
+    vhaddw.qu.du   vr20,   vr20,  vr20
+    vhaddw.qu.du   vr21,   vr21,  vr21
+    vhaddw.qu.du   vr22,   vr22,  vr22
+    vhaddw.qu.du   vr23,   vr23,  vr23
+    // Store data to p_sad_array
+    vstelm.w       vr20,   a6,    0,      0
+    vstelm.w       vr21,   a6,    4,      0
+    vstelm.w       vr22,   a6,    8,      0
+    vstelm.w       vr23,   a6,    12,     0
+endfunc
 #endif /* !HIGH_BIT_DEPTH */
diff --git a/common/mc.c b/common/mc.c
index e81334a4..9b3ca15a 100644
--- a/common/mc.c
+++ b/common/mc.c
@@ -41,7 +41,7 @@
 #if ARCH_MIPS
 #include "mips/mc.h"
 #endif
-#if ARCH_LOONGARCH
+#if HAVE_LSX || HAVE_LASX
 #   include "loongarch/mc.h"
 #endif
 
@@ -689,7 +689,7 @@ void x264_mc_init( int cpu, x264_mc_functions_t *pf, int cpu_independent )
     if( cpu&X264_CPU_MSA )
         x264_mc_init_mips( cpu, pf );
 #endif
-#if HAVE_LASX
+#if HAVE_LSX || HAVE_LASX
     x264_mc_init_loongarch( cpu, pf );
 #endif
 
diff --git a/common/pixel.c b/common/pixel.c
index a8af2f95..894a0a37 100644
--- a/common/pixel.c
+++ b/common/pixel.c
@@ -45,7 +45,7 @@
 #if ARCH_MIPS
 #   include "mips/pixel.h"
 #endif
-#if ARCH_LOONGARCH
+#if HAVE_LSX || HAVE_LASX
 #   include "loongarch/pixel.h"
 #endif
 
@@ -1511,34 +1511,79 @@ void x264_pixel_init( int cpu, x264_pixel_function_t *pixf )
     }
 #endif // HAVE_MSA
 
-#if HAVE_LASX
+#if HAVE_LSX || HAVE_LASX
+    if( cpu&X264_CPU_LSX )
+    {
+        INIT8( sad, _lsx );
+        INIT8_NAME( sad_aligned, sad, _lsx );
+        INIT8( ssd, _lsx );
+        INIT7( sad_x3, _lsx );
+        INIT7( sad_x4, _lsx );
+        INIT8( satd, _lsx );
+        INIT4( hadamard_ac, _lsx );
+
+        pixf->intra_sad_x3_4x4  = x264_intra_sad_x3_4x4_lsx;
+        pixf->intra_satd_x3_4x4 = x264_intra_satd_x3_4x4_lsx;
+
+        pixf->intra_sad_x3_8x8    = x264_intra_sad_x3_8x8_lsx;
+        pixf->intra_sad_x3_8x8c   = x264_intra_sad_x3_8x8c_lsx;
+        pixf->intra_sad_x3_16x16  = x264_intra_sad_x3_16x16_lsx;
+        pixf->intra_satd_x3_16x16 = x264_intra_satd_x3_16x16_lsx;
+        pixf->intra_satd_x3_8x8c  = x264_intra_satd_x3_8x8c_lsx;
+        pixf->intra_sa8d_x3_8x8   = x264_intra_sa8d_x3_8x8_lsx;
+
+        pixf->var2[PIXEL_8x16] = x264_pixel_var2_8x16_lsx;
+        pixf->var2[PIXEL_8x8]  = x264_pixel_var2_8x8_lsx;
+        pixf->var[PIXEL_16x16] = x264_pixel_var_16x16_lsx;
+        pixf->var[PIXEL_8x16]  = x264_pixel_var_8x16_lsx;
+        pixf->var[PIXEL_8x8]   = x264_pixel_var_8x8_lsx;
+        pixf->sa8d[PIXEL_16x16] = x264_pixel_sa8d_16x16_lsx;
+        pixf->sa8d[PIXEL_8x8]   = x264_pixel_sa8d_8x8_lsx;
+    }
     if( cpu&X264_CPU_LASX )
     {
-        INIT8( sad, _lasx );
-        INIT8_NAME( sad_aligned, sad, _lasx );
         INIT8( ssd, _lasx );
-        INIT8( satd, _lasx );
         INIT4( hadamard_ac, _lasx );
 
+        pixf->satd[PIXEL_16x16] = x264_pixel_satd_16x16_lasx;
+        pixf->satd[PIXEL_16x8] = x264_pixel_satd_16x8_lasx;
+        pixf->satd[PIXEL_8x16] = x264_pixel_satd_8x16_lasx;
+        pixf->satd[PIXEL_8x8] = x264_pixel_satd_8x8_lasx;
+        pixf->satd[PIXEL_8x4] = x264_pixel_satd_8x4_lasx;
+        pixf->satd[PIXEL_4x16] = x264_pixel_satd_4x16_lasx;
+        pixf->satd[PIXEL_4x8] = x264_pixel_satd_4x8_lasx;
+
+        pixf->sad[PIXEL_16x16] = x264_pixel_sad_16x16_lasx;
+        pixf->sad[PIXEL_16x8] = x264_pixel_sad_16x8_lasx;
+        pixf->sad[PIXEL_8x16] = x264_pixel_sad_8x16_lasx;
+        pixf->sad[PIXEL_8x8] = x264_pixel_sad_8x8_lasx;
+        pixf->sad[PIXEL_8x4] = x264_pixel_sad_8x4_lasx;
+        pixf->sad[PIXEL_4x16] = x264_pixel_sad_4x16_lasx;
+        pixf->sad[PIXEL_4x8] = x264_pixel_sad_4x8_lasx;
+
+        pixf->sad_aligned[PIXEL_16x16] = x264_pixel_sad_16x16_lasx;
+        pixf->sad_aligned[PIXEL_16x8] = x264_pixel_sad_16x8_lasx;
+        pixf->sad_aligned[PIXEL_8x16] = x264_pixel_sad_8x16_lasx;
+        pixf->sad_aligned[PIXEL_8x8] = x264_pixel_sad_8x8_lasx;
+        pixf->sad_aligned[PIXEL_8x4] = x264_pixel_sad_8x4_lasx;
+        pixf->sad_aligned[PIXEL_4x16] = x264_pixel_sad_4x16_lasx;
+        pixf->sad_aligned[PIXEL_4x8] = x264_pixel_sad_4x8_lasx;
+
         pixf->sad_x4[PIXEL_16x16] = x264_pixel_sad_x4_16x16_lasx;
         pixf->sad_x4[PIXEL_16x8] = x264_pixel_sad_x4_16x8_lasx;
         pixf->sad_x4[PIXEL_8x16] = x264_pixel_sad_x4_8x16_lasx;
         pixf->sad_x4[PIXEL_8x8] = x264_pixel_sad_x4_8x8_lasx;
         pixf->sad_x4[PIXEL_8x4] = x264_pixel_sad_x4_8x4_lasx;
         pixf->sad_x4[PIXEL_4x8] = x264_pixel_sad_x4_4x8_lasx;
-        pixf->sad_x4[PIXEL_4x4] = x264_pixel_sad_x4_4x4_lsx;
         pixf->sad_x3[PIXEL_16x16] = x264_pixel_sad_x3_16x16_lasx;
         pixf->sad_x3[PIXEL_16x8] = x264_pixel_sad_x3_16x8_lasx;
         pixf->sad_x3[PIXEL_8x16] = x264_pixel_sad_x3_8x16_lasx;
         pixf->sad_x3[PIXEL_8x8] = x264_pixel_sad_x3_8x8_lasx;
         pixf->sad_x3[PIXEL_8x4] = x264_pixel_sad_x3_8x4_lasx;
         pixf->sad_x3[PIXEL_4x8] = x264_pixel_sad_x3_4x8_lasx;
-        pixf->sad_x3[PIXEL_4x4] = x264_pixel_sad_x3_4x4_lsx;
-        pixf->intra_sad_x3_4x4   = x264_intra_sad_x3_4x4_lasx;
         pixf->intra_sad_x3_8x8   = x264_intra_sad_x3_8x8_lasx;
         pixf->intra_sad_x3_8x8c  = x264_intra_sad_x3_8x8c_lasx;
         pixf->intra_sad_x3_16x16 = x264_intra_sad_x3_16x16_lasx;
-        pixf->intra_satd_x3_4x4   = x264_intra_satd_x3_4x4_lasx;
         pixf->intra_satd_x3_16x16 = x264_intra_satd_x3_16x16_lasx;
         pixf->intra_satd_x3_8x8c  = x264_intra_satd_x3_8x8c_lasx;
         pixf->intra_sa8d_x3_8x8   = x264_intra_sa8d_x3_8x8_lasx;
@@ -1551,7 +1596,7 @@ void x264_pixel_init( int cpu, x264_pixel_function_t *pixf )
         pixf->sa8d[PIXEL_16x16] = x264_pixel_sa8d_16x16_lasx;
         pixf->sa8d[PIXEL_8x8]   = x264_pixel_sa8d_8x8_lasx;
     }
-#endif // HAVE_LASX
+#endif // HAVE_LSX || HAVE_LASX
 
 #endif // HIGH_BIT_DEPTH
 #if HAVE_ALTIVEC
diff --git a/common/predict.c b/common/predict.c
index 8238f939..4e892354 100644
--- a/common/predict.c
+++ b/common/predict.c
@@ -46,7 +46,7 @@
 #if ARCH_MIPS
 #   include "mips/predict.h"
 #endif
-#if ARCH_LOONGARCH
+#if HAVE_LSX || HAVE_LASX
 #   include "loongarch/predict.h"
 #endif
 /****************************************************************************
@@ -927,8 +927,8 @@ void x264_predict_16x16_init( int cpu, x264_predict_t pf[7] )
 #endif
 #endif
 
-#if ARCH_LOONGARCH64
-    x264_predict_16x16_init_lasx( cpu, pf );
+#if HAVE_LSX || HAVE_LASX
+    x264_predict_16x16_init_loongarch( cpu, pf );
 #endif
 }
 
@@ -968,8 +968,8 @@ void x264_predict_8x8c_init( int cpu, x264_predict_t pf[7] )
 #endif
 #endif
 
-#if HAVE_LASX
-    x264_predict_8x8c_init_lasx( cpu, pf );
+#if HAVE_LSX || HAVE_LASX
+    x264_predict_8x8c_init_loongarch( cpu, pf );
 #endif
 }
 
@@ -1033,8 +1033,8 @@ void x264_predict_8x8_init( int cpu, x264_predict8x8_t pf[12], x264_predict_8x8_
 #endif
 #endif
 
-#if HAVE_LASX
-    x264_predict_8x8_init_lasx( cpu, pf, predict_filter );
+#if HAVE_LSX || HAVE_LASX
+    x264_predict_8x8_init_loongarch( cpu, pf, predict_filter );
 #endif
 }
 
@@ -1065,8 +1065,8 @@ void x264_predict_4x4_init( int cpu, x264_predict_t pf[12] )
     x264_predict_4x4_init_aarch64( cpu, pf );
 #endif
 
-#if HAVE_LASX
-    x264_predict_4x4_init_lasx( cpu, pf );
+#if HAVE_LSX || HAVE_LASX
+    x264_predict_4x4_init_loongarch( cpu, pf );
 #endif
 }
 
diff --git a/common/quant.c b/common/quant.c
index 971f47cd..8d45efe8 100644
--- a/common/quant.c
+++ b/common/quant.c
@@ -43,7 +43,7 @@
 #if ARCH_MIPS
 #   include "mips/quant.h"
 #endif
-#if ARCH_LOONGARCH
+#if HAVE_LSX || HAVE_LASX
 #   include "loongarch/quant.h"
 #endif
 
@@ -808,10 +808,25 @@ void x264_quant_init( x264_t *h, int cpu, x264_quant_function_t *pf )
     }
 #endif
 
-#if HAVE_LASX
-    if( cpu&X264_CPU_LASX )
+#if HAVE_LSX || HAVE_LASX
+    if( cpu&X264_CPU_LSX )
     {
+        pf->quant_4x4      = x264_quant_4x4_lsx;
+        pf->quant_4x4x4    = x264_quant_4x4x4_lsx;
+        pf->quant_8x8      = x264_quant_8x8_lsx;
+        pf->quant_4x4_dc   = x264_quant_4x4_dc_lsx;
         pf->quant_2x2_dc   = x264_quant_2x2_dc_lsx;
+        pf->dequant_4x4    = x264_dequant_4x4_lsx;
+        pf->dequant_8x8    = x264_dequant_8x8_lsx;
+        pf->dequant_4x4_dc = x264_dequant_4x4_dc_lsx;
+        pf->coeff_last4    = x264_coeff_last4_lsx;
+        pf->coeff_last8    = x264_coeff_last8_lsx;
+        pf->coeff_last[ DCT_LUMA_AC] = x264_coeff_last15_lsx;
+        pf->coeff_last[DCT_LUMA_4x4] = x264_coeff_last16_lsx;
+        pf->coeff_last[DCT_LUMA_8x8] = x264_coeff_last64_lsx;
+    }
+    if( cpu&X264_CPU_LASX )
+    {
         pf->quant_4x4      = x264_quant_4x4_lasx;
         pf->quant_4x4_dc   = x264_quant_4x4_dc_lasx;
         pf->quant_4x4x4    = x264_quant_4x4x4_lasx;
@@ -827,6 +842,7 @@ void x264_quant_init( x264_t *h, int cpu, x264_quant_function_t *pf )
         pf->decimate_score64 = x264_decimate_score64_lasx;
     }
 #endif
+
 #endif // HIGH_BIT_DEPTH
     pf->coeff_last[DCT_LUMA_DC]     = pf->coeff_last[DCT_CHROMAU_DC]  = pf->coeff_last[DCT_CHROMAV_DC] =
     pf->coeff_last[DCT_CHROMAU_4x4] = pf->coeff_last[DCT_CHROMAV_4x4] = pf->coeff_last[DCT_LUMA_4x4];
diff --git a/configure b/configure
index 44b6aeb0..43b189eb 100755
--- a/configure
+++ b/configure
@@ -32,6 +32,8 @@ Configuration options:
   --disable-interlaced     disable interlaced encoding support
   --bit-depth=BIT_DEPTH    set output bit depth (8, 10, all) [all]
   --chroma-format=FORMAT   output chroma format (420, 422, 444, all) [all]
+  --disable-lsx            disable LSX optimizations
+  --disable-lasx           disable LASX optimizations
 
 Advanced options:
   --disable-asm            disable platform-specific assembly optimizations
@@ -378,6 +380,8 @@ compiler="GNU"
 compiler_style="GNU"
 opencl="yes"
 vsx="auto"
+lsx="auto"
+lasx="auto"
 
 CFLAGS="$CFLAGS -Wall -I. -I\$(SRCPATH)"
 LDFLAGS="$LDFLAGS"
@@ -396,7 +400,7 @@ NL="
 # list of all preprocessor HAVE values we can define
 CONFIG_HAVE="MALLOC_H ALTIVEC ALTIVEC_H MMX ARMV6 ARMV6T2 NEON BEOSTHREAD POSIXTHREAD WIN32THREAD THREAD LOG2F SWSCALE \
              LAVF FFMS GPAC AVS GPL VECTOREXT INTERLACED CPU_COUNT OPENCL THP LSMASH X86_INLINE_ASM AS_FUNC INTEL_DISPATCHER \
-             MSA LSX LASX MMAP WINRT VSX ARM_INLINE_ASM STRTOK_R BITDEPTH8 BITDEPTH10"
+             MSA LSX LASX LOONGARCH_ASM MMAP WINRT VSX ARM_INLINE_ASM STRTOK_R BITDEPTH8 BITDEPTH10"
 
 # parse options
 
@@ -454,6 +458,13 @@ for opt do
         --disable-gpl)
             gpl="no"
             ;;
+        --disable-lasx)
+            lasx="no"
+            ;;
+        --disable-lsx)
+            lsx="no"
+            lasx="no"
+            ;;
         --extra-asflags=*)
             ASFLAGS="$ASFLAGS $optarg"
             ;;
@@ -974,13 +985,18 @@ if [ $asm = auto -a \( $ARCH = ARM -o $ARCH = AARCH64 \) ] ; then
 fi
 
 if [ $asm = auto -a $ARCH = LOONGARCH ] ; then
-    if cc_check '' '-mlsx' '__asm__("vadd.b $vr0, $vr1, $vr2");' ; then
-        LSX_CFLAGS="-mlsx"
-        define HAVE_LSX
+    cc_check '' '' '__asm__("add.w $r0, $r1, $r2");' && define HAVE_LOONGARCH_ASM
+    if [ $lsx = auto ]; then
+        if cc_check '' '-mlsx' '__asm__("vadd.b $vr0, $vr1, $vr2");' ; then
+            LSX_CFLAGS="-mlsx"
+            define HAVE_LSX
+        fi
     fi
-    if cc_check '' '-mlasx' '__asm__("xvadd.b $xr0, $xr1, $xr2");' ; then
-        LASX_CFLAGS="-mlasx"
-        define HAVE_LASX
+    if [ $lasx = auto ]; then
+        if cc_check '' '-mlasx' '__asm__("xvadd.b $xr0, $xr1, $xr2");' ; then
+            LASX_CFLAGS="-mlasx"
+            define HAVE_LASX
+        fi
     fi
 fi
 
diff --git a/tools/checkasm.c b/tools/checkasm.c
index cdcaa5b5..4d2a6f53 100644
--- a/tools/checkasm.c
+++ b/tools/checkasm.c
@@ -209,6 +209,7 @@ static void print_bench(void)
                     b->cpu&X264_CPU_MSA ? "msa" :
 #elif ARCH_LOONGARCH
                     b->cpu&X264_CPU_LASX ? "lasx" :
+                    b->cpu&X264_CPU_LSX ? "lsx" :
 #endif
                     "c",
 #if HAVE_MMX
@@ -2607,6 +2608,8 @@ DECL_CABAC(c)
 DECL_CABAC(asm)
 #elif defined(ARCH_AARCH64)
 DECL_CABAC(asm)
+#elif HAVE_LOONGARCH_ASM && !HIGH_BIT_DEPTH
+DECL_CABAC(asm)
 #else
 #define run_cabac_decision_asm run_cabac_decision_c
 #define run_cabac_bypass_asm run_cabac_bypass_c
@@ -2915,6 +2918,8 @@ static int check_all_flags( void )
     if( cpu_detect & X264_CPU_MSA )
         ret |= add_flags( &cpu0, &cpu1, X264_CPU_MSA, "MSA" );
 #elif ARCH_LOONGARCH
+    if( cpu_detect & X264_CPU_LSX )
+        ret |= add_flags( &cpu0, &cpu1, X264_CPU_LSX, "LSX" );
     if( cpu_detect & X264_CPU_LASX )
         ret |= add_flags( &cpu0, &cpu1, X264_CPU_LASX, "LASX" );
 #endif
